{"config":{"lang":["en","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to KC's Data &amp; Life Notes","text":"<p>Welcome! I'm KC (Kuan-Chou), a software engineer with a passion for data, AI, and continuous learning. This website is a space where I share my projects, learning journey, and experiences. Feel free to explore and get to know me better!</p> <ul> <li> <p> About Me</p> <p>Learn more about my background, including education, work experience, public speaking, and other activities.</p> <p> About Me</p> </li> <li> <p> Side Projects</p> <p>Explore my personal projects in data, AI, and programming.</p> <p> Side Projects</p> </li> <li> <p> Learning Plans</p> <p>A look into what I'm currently learning and planning to study next.</p> <p> Learning Plans</p> </li> <li> <p> Blog</p> <p>Read my thoughts and insights on data, AI, books, and more.</p> <p> Blog</p> </li> </ul>"},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"about-me/","title":"About Me","text":"<p>I'm a data-driven problem solver with experience in data engineering, machine learning, and public speaking. I enjoy turning complex ideas into actionable solutions and am always looking to grow\u2014both personally and professionally. Learn more about my background, skills, and what drives me.</p>"},{"location":"about-me/#work-experience","title":"Work Experience","text":"Period Role Company Type Link May 2024 - Sep 2024 Data Engineer UST Global Full-time, Remote Jun 2021 - Feb 2024 Data Engineer TVBS Media Inc. Full-time, Hybrid (Taipei, Taiwan) Jun 2020 - Sep 2020 ML Researcher NinoX Contract, Remote Jul 2018 - Aug 2018 Industry Analyst Intern ITRI Intern, On-site (Hsinchu, Taiwan)"},{"location":"about-me/#education","title":"Education","text":"Period Role Institution Location Link Sep 2019 - Feb 2020 Big Data Engineering Trainee III Taipei, Taiwan Jul 2017 - May 2023 M.B.A. NCCU Taipei, Taiwan Sep 2013 - Jun 2017 B.S. Psychology CCU Chiayi, Taiwan <p>Home </p>"},{"location":"about-me/education/ccu/","title":"B.S. Psychology, CCU","text":"<p>About Me </p>"},{"location":"about-me/education/iii/","title":"Big Data Engineering Trainee, III","text":"<p>About Me </p>"},{"location":"about-me/education/nccu/","title":"M.B.A., NCCU","text":"<p>National Chengchi University</p> <p>About Me </p>"},{"location":"about-me/public-speakings/","title":"Public Speakings","text":"<p>Sharing knowledge and ideas with others is something I value deeply. Here, you can find my past speaking engagements, talks, and presentations, along with topics I'm passionate about. I'm always open to new opportunities to connect and speak.</p>"},{"location":"about-me/public-speakings/#taipei-dbt-meetup-27","title":"Taipei dbt Meetup #27","text":""},{"location":"about-me/public-speakings/#devopsdays-taipei-2024","title":"DevOpsDays Taipei 2024","text":"<p>About Me </p>"},{"location":"about-me/work-experience/itri/itri/","title":"Industry Analyst Intern, ITRI","text":"<p>Abstract</p> <p>Researched and analyzed the 7 key roles (DSP, SSP, DMP etc.) within the \"AdTech industry\", using \"eLand Information\", a leading AI &amp; Data Analytics company in Taiwan, as a case study to predict future industry trends and propose strategies.</p> <p>Abstract</p> <p>Analyzed technical trends within the cloud services industry, contributing to the authorship of 2 research articles on the subject.</p> <p>Abstract</p> <p>Systematized and summarized 100+ benchmark cases in the 4 major domains of IoT \u2013 water resources, air quality, earthquakes, and disaster prevention.</p> <p>About Me </p>"},{"location":"about-me/work-experience/ninox/ninox/","title":"ML Researcher, NinoX","text":""},{"location":"about-me/work-experience/ninox/ninox/#the-obi-ctr-model","title":"The obi-CTR model","text":"<p>Abstract</p> <p>Developed the Online Bayesian Inference (OBI) algorithm for the Collaborative Topic Regression (CTR) model\u2014a hybrid system combining Collaborative Filtering and Topic Modeling\u2014using Python with robust OOP design, while leveraging NumPy and SciPy for scalable, real-time recommendations.</p>"},{"location":"about-me/work-experience/ninox/ninox/#23-boost-in-computational-efficiency","title":"23% boost in computational efficiency","text":"<p>Abstract</p> <p>Achieved a 23% boost in computational efficiency by speeding up the MCMC sampling core with Cython.</p>"},{"location":"about-me/work-experience/ninox/ninox/#cicd-pipelines-with-travisci","title":"CI/CD pipelines with TravisCI","text":"<p>Abstract</p> <p>Established CI/CD pipelines with TravisCI for automated unit tests and a docs-as-code workflow using pytest and Sphinx, streamlining both development and documentation processes and ensuring consistent release quality</p> <p></p> <p>About Me </p>"},{"location":"about-me/work-experience/tvbs/tvbs/","title":"Data Engineer, TVBS Media Inc.","text":"Organization Diagram"},{"location":"about-me/work-experience/tvbs/tvbs/#cost-effective-scalable-etlelt-modern-data-stack","title":"Cost-effective, scalable ETL/ELT Modern Data Stack","text":"<p>Abstract</p> <p>Architected a cost-effective, scalable ETL / ELT Modern Data Stack (dbt, BigQuery, Airflow, Airbyte, Looker Studio, etc.) and introduced a streamlined DataOps workflow, processing 20M+ events daily at TB+ scale (300+ data models, 600+ daily quality checks), cutting cloud costs by 63%.</p> \u21901 / 7\u2192\u26f6"},{"location":"about-me/work-experience/tvbs/tvbs/#the-organization-wide-adoption-of-data-mesh","title":"The organization-wide adoption of Data Mesh","text":"<p>Abstract</p> <p>Directed the organization-wide adoption of Data Mesh principles to strengthen data governance and improve data availability, empowering 7 domain teams through self-service reporting across 30+ data products, and achieving a previously unattainable holistic brand analysis through the expansion of data sources from 4 to 9+.</p> \u21901 / 4\u2192\u26f6"},{"location":"about-me/work-experience/tvbs/tvbs/#iac-implementation-with-terraform","title":"IaC implementation with Terraform","text":"<p>Abstract</p> <p>Led IaC implementation with Terraform for over 500 cross-cloud data assets (AWS, GCP, dbt Cloud, etc.) and conducted internal DevOps workshops, slashing provisioning lead time from days to hours by integrating CI/CD pipelines with GitHub Actions and improving team IaC adoption by 80% within 6 months.</p> \u21901 / 4\u2192\u26f6"},{"location":"about-me/work-experience/tvbs/tvbs/#the-migration-to-ga4-and-bigquery","title":"The migration to GA4 and BigQuery","text":"<p>Abstract</p> <p>Led the migration to GA4 and BigQuery to build a data lakehouse platform while maintaining a legacy event tracking pipeline (AWS Kinesis, MongoDB, PostgreSQL), saving $2M by retiring NoSQL database and ensuring real-time analytics for both anonymous and logged-in users.</p>"},{"location":"about-me/work-experience/tvbs/tvbs/#an-organization-wide-experimentation-mindset","title":"An organization-wide experimentation mindset","text":"<p>Abstract</p> <p>Championed an organization-wide experimentation mindset, engaged 60+ colleagues, and orchestrated 20+ A/B tests via Google Optimize and Firebase within 6 months, boosting mobile ad revenue by 27% and web pageviews by 6%.</p> <p>About Me </p>"},{"location":"about-me/work-experience/tvbs/slide-data-mesh/1/","title":"Challenges","text":"<ul> <li>1</li> <li>2</li> <li>3</li> </ul>"},{"location":"about-me/work-experience/tvbs/slide-data-mesh/2/","title":"Goals","text":"<ul> <li>A</li> <li>B</li> <li>C</li> </ul>"},{"location":"about-me/work-experience/tvbs/slide-data-mesh/3/","title":"Actions","text":""},{"location":"about-me/work-experience/tvbs/slide-data-mesh/4/","title":"Results","text":""},{"location":"about-me/work-experience/tvbs/slide-events-data-pipeline/deal-with-data-latency/","title":"data latency","text":""},{"location":"about-me/work-experience/tvbs/slide-events-data-pipeline/deal-with-data-latency/#situation","title":"Situation","text":"<ul> <li>Events occured on Jan. 1, but were sent to server on Jan. 3</li> <li>Latency at most 5 days</li> <li>Metrics are calculated in error due to data latency</li> <li></li> </ul>"},{"location":"about-me/work-experience/tvbs/slide-events-data-pipeline/deal-with-data-latency/#target","title":"Target","text":""},{"location":"about-me/work-experience/tvbs/slide-events-data-pipeline/deal-with-data-latency/#actions","title":"Actions","text":""},{"location":"about-me/work-experience/tvbs/slide-events-data-pipeline/deal-with-data-latency/#result","title":"Result","text":""},{"location":"about-me/work-experience/tvbs/slide-iac/1/","title":"Challenges","text":"<ul> <li>1</li> <li>2</li> <li>3</li> </ul>"},{"location":"about-me/work-experience/tvbs/slide-iac/2/","title":"Goals","text":"<ul> <li>A</li> <li>B</li> <li>C</li> </ul>"},{"location":"about-me/work-experience/tvbs/slide-iac/3/","title":"Actions","text":""},{"location":"about-me/work-experience/tvbs/slide-iac/4/","title":"Results","text":""},{"location":"about-me/work-experience/tvbs/slide-modern-data-stack/1-gaps-between-teams/","title":"Challenges - Data and Upstream Teams Misaligned","text":""},{"location":"about-me/work-experience/tvbs/slide-modern-data-stack/1-gaps-within-team/","title":"Challenges - Analysts Rely on Engineers","text":""},{"location":"about-me/work-experience/tvbs/slide-modern-data-stack/1-legacy-data-stack/","title":"Challenges - Legacy Data Stack","text":""},{"location":"about-me/work-experience/tvbs/slide-modern-data-stack/1-summary/","title":"Challenges - Summary","text":""},{"location":"about-me/work-experience/tvbs/slide-modern-data-stack/1-summary/#technical","title":"Technical","text":"<ul> <li>A single VM taking on too many responsibilities</li> <li>Tightly coupled data pipelines</li> <li>Lack of software engineering best practices</li> <li>Scattered business logic across various codebases</li> </ul>"},{"location":"about-me/work-experience/tvbs/slide-modern-data-stack/1-summary/#organizational","title":"Organizational","text":"<ul> <li>Analysts rely on engineers \u2014 big technical gap</li> <li>Data and upstream teams misaligned \u2014 downstream needs ignored</li> <li>Shortage of data talent (2 members)</li> </ul>"},{"location":"about-me/work-experience/tvbs/slide-modern-data-stack/2/","title":"Goals","text":"<ul> <li>Empower more colleagues to interact more easily with a wider variety of data in more diverse ways.</li> <li>Drive down costs by streamlining workflows and reducing data friction.</li> </ul>"},{"location":"about-me/work-experience/tvbs/slide-modern-data-stack/3/","title":"Actions","text":"<ol> <li>Researched AWS-based Data Lake Architectures: Explored best practices using AWS Glue, Athena, S3, DMS, and open table formats like Iceberg, Hudi, and Delta Lake.</li> <li>Built a Minimum Viable Architecture on GCP: Delivered end-to-end analytics using Airbyte, dbt, BigQuery, and Looker Studio within a single sprint in order to deliver fast.</li> <li>Estimated and Managed Project Budget: Conducted budgeting calculations to ensure cost-effective data architecture planning and scaling.</li> <li>Rolled Out Solution to All: Successfully deployed the solution organization-wide to support broader data accessibility and insights.</li> </ol>"},{"location":"about-me/work-experience/tvbs/slide-modern-data-stack/4/","title":"Results","text":"<ol> <li>How to cut down 63%<ul> <li>Tableau -&gt; Looker Studio</li> <li>Cloud SQL -&gt; BigQuery</li> <li>VM -&gt; X</li> </ul> </li> </ol>"},{"location":"about-me/work-experience/ust/ust/","title":"Data Engineer, UST Global","text":""},{"location":"about-me/work-experience/ust/ust/#server-monitoring-solutions","title":"Server Monitoring Solutions","text":"<p>Abstract</p> <p>Built server monitoring solutions for Microsoft using Kusto (KQL) on Azure Data Explorer (ADX), Azure Monitor Log Analytics, and Azure Sentinel, delivering actionable insights via Power BI for operational excellence.</p> <p>About Me </p>"},{"location":"blog/","title":"Blog","text":""},{"location":"learning-plans/","title":"Learning Plan","text":"<p>Staying curious is at the core of how I grow. This is my personal learning roadmap\u2014what I'm currently exploring, skills I'm sharpening, and resources I've found useful. I believe in learning out loud, and I'm happy to share my journey.</p> <p>2025</p>"},{"location":"learning-plans/2025/","title":"2025","text":"MLOpsKafkaAI Workflow Automation <p>Action Plans</p> <ul> <li> Real-world End to End Machine Learning Ops on Google Cloud</li> <li> MLflow in Action - Master the art of MLOps using MLflow tool</li> </ul> <p>Action Plans</p> <ul> <li> Apache Kafka Series - Learn Apache Kafka for Beginners v3</li> </ul> <p>Action Plans</p> <ul> <li> n8n</li> <li> make</li> </ul>"},{"location":"side-projects/","title":"Side Projects","text":"<p>I love exploring new ideas and building things in my spare time. Here, you'll find a collection of side projects that showcase my interests, technical skills, and curiosity beyond work. Each project reflects my hands-on approach to learning and creating.</p>"},{"location":"side-projects/cross-cloud-unified-sql-data-pipelines/","title":"Cross Cloud Unified Sql Data Pipelines","text":""},{"location":"side-projects/sql-based-rag-application/","title":"SQL-based RAG Application","text":""},{"location":"side-projects/sql-based-rag-application/#create-a-bucket-in-gcs-and-upload-a-pdf-file","title":"Create a Bucket in GCS and upload a pdf file","text":""},{"location":"side-projects/sql-based-rag-application/#create-a-object-table-in-bigquery","title":"Create a Object Table in BigQuery","text":"<pre><code>create or replace external table `us_test2.pdf`\nwith connection `us.bg-object-tables`\noptions(\n  object_metadata = 'SIMPLE',\n  uris = ['gs://kcl-us-test/scf23.pdf']\n);\n</code></pre>"},{"location":"side-projects/sql-based-rag-application/#create-a-layout-parser-type-of-processor-in-document-ai-and-a-remote-model-corresponding-to-the-processor","title":"Create a Layout Parser type of Processor in Document AI and a Remote Model corresponding to the processor","text":"<pre><code>create or replace model `us_test2.doc_parser`\nremote with connection `us.document_ai`\noptions(\n  remote_service_type='CLOUD_AI_DOCUMENT_V1',\n  document_processor='ec023753643cb1be'\n);\n</code></pre>"},{"location":"side-projects/sql-based-rag-application/#create-a-embeddings-remote-model-in-bigquery","title":"Create a embeddings Remote Model in BigQuery","text":"<pre><code>create or replace model `us_test2.embedding_model`\nremote with connection `us.vertex_ai`\noptions (\n  endpoint='text-embedding-004'\n)\n</code></pre>"},{"location":"side-projects/sql-based-rag-application/#create-a-generative-text-remote-model-in-bigquery","title":"Create a generative text Remote Model in BigQuery","text":"<pre><code>create or replace model `us_test2.text_model`\nremote with connection `us.vertex_ai`\noptions(\n  endpoint = 'gemini-1.5-flash-002'\n)\n</code></pre>"},{"location":"side-projects/data-mesh/","title":"Data Mesh","text":"<p>Homepage</p>"},{"location":"side-projects/data-mesh/docs/org/","title":"Organization","text":""},{"location":"side-projects/dcard-hw/","title":"2020 Dcard Data Engineering Intern","text":"<p>Dcard is a popular social media platform in Taiwan, especially among college students and young adults. It was launched in 2011 as a university-only online forum, similar in spirit to how Facebook started within universities.</p> <p>This project is a pre-interview assignment for the 2020 Dcard Data Engineer Internship Program.</p> <p>On Dcard's app and website, there is an important section called \"Trending Posts,\" where users can find the hottest discussion topics on the platform. As data enthusiasts, we are also curious about which posts have the potential to become trending. If we consider this factor in our recommendations, we might help users discover great posts faster. Therefore, in this assignment, we aim to predict whether a post has the potential to appear in the \"Trending Posts\" section based on some data.</p> <p>To simplify the problem, we define a trending post as one that receives at least 1000 likes within 36 hours of being posted. During testing, we will calculate whether a post's like count exceeds 1000 within 36 hours to determine the ground truth or prediction benchmark.</p> <p>Abstract</p> <pre><code>$ tree\n.\n\u251c\u2500\u2500 requirements.txt: A list of required Python packages and their versions.\n\u251c\u2500\u2500 preprocessing.py: A shared utility script for database connections, preprocessing, and other common functions.\n\u251c\u2500\u2500 training.py: A utility script for training the model.\n\u251c\u2500\u2500 predict.py: A utility script for making predictions.\n\u251c\u2500\u2500 outputs\n\u2502   \u251c\u2500\u2500 best_model.h5: The best model obtained after training.\n\u2502   \u251c\u2500\u2500 cv_results.csv: Cross-validation results.\n\u2502   \u2514\u2500\u2500 output.csv: Prediction results for the public testing dataset.\n\u2514\u2500\u2500 eda_evaluation.ipynb: A Jupyter notebook used for generating visualizations.\n</code></pre> <p>The training dataset includes articles spanning from April 1, 2019, to the end of October 2019, covering approximately seven months. The dataset contains around 793,000 articles, of which about 2.32% (approximately 18,000 articles) are classified as popular. Through exploratory data analysis, we observed high correlations among variables. Additionally, the timing of article publication significantly influences the proportion of popular articles and the total number of likes within the first 36 hours of posting.</p> <p>We decided to use a \"binary classification model without considering sequential information\" as our primary approach, focusing on handling imbalanced datasets, tree-based ensemble models, and subsequent discussions. The training process was divided into three main stages:</p> <ol> <li>Resampling</li> <li>Feature Transformation</li> <li>Classification</li> </ol> <p>After experimentation, we opted to omit the \"Feature Transformation\" stage. In total, 108 combinations were tested using <code>GridSearchCV</code> with <code>cv=3</code> to find the optimal configuration.</p> <p>Using the f1-score as the evaluation metric, the best-performing model was an <code>AdaBoostClassifier</code> without any resampling. This model consisted of 100 decision trees, each limited to a depth of 2. The average f1-score from cross-validation was 0.56, while the f1-score on the public test set was 0.53. Key findings from the experiments include:</p> <ul> <li>Different resampling strategies significantly impact the f1-score.</li> <li>Resampling strategies can effectively identify genuinely popular articles. However, this comes at the cost of reduced trust in the model's predictions of popular articles.</li> <li>Under both \"SMOTE resampling\" and \"no resampling\" scenarios, the choice of classifier did not lead to substantial changes in the f1-score.</li> <li>The choice of classifier had a relatively minor impact on the f1-score.</li> </ul> <p>Finally, we discussed several potential future directions, including exploring other resampling techniques, alternative evaluation metrics, and incorporating sequential information.</p>"},{"location":"side-projects/dcard-hw/#training-dataset","title":"Training Dataset","text":"<p>The training dataset covers posts from April 1, 2019, to the end of October 2019, approximately 7 months. It contains around 794,000 posts, of which about 2.32% (approximately 18,000 posts) are trending.</p> <pre><code>posts_train                Contains 793,751 records and 3 columns\npost_shared_train          Contains 304,260 records and 3 columns\npost_comment_created_train Contains 2,372,228 records and 3 columns\npost_liked_train           Contains 3,395,903 records and 3 columns\npost_collected_train       Contains 1,235,126 records and 3 columns\n</code></pre> <p>Table: <code>posts_train</code></p> column_name data_type description <code>post_key</code> string Unique identifier of the post <code>created_at_hour</code> datetime The hour when the post was created <code>like_count_36_hour</code> integer Number of likes the post received within 36 hours (only in train table) <p>Table: <code>post_shared_train</code></p> column_name data_type description <code>post_key</code> string Unique identifier of the post <code>created_at_hour</code> datetime The hour of the sharing activity <code>count</code> integer Number of shares the post received in that hour <p>Table: <code>post_comment_created_train</code></p> column_name data_type description <code>post_key</code> string Unique identifier of the post <code>created_at_hour</code> datetime The hour of the comment activity <code>count</code> integer Number of comments the post received in that hour <p>Table: <code>post_liked_train</code></p> column_name data_type description <code>post_key</code> string Unique identifier of the post <code>created_at_hour</code> datetime The hour of the like activity <code>count</code> integer Number of likes the post received in that hour <p>Table: <code>post_collected_train</code></p> column_name data_type description <code>post_key</code> string Unique identifier of the post <code>created_at_hour</code> datetime The hour of the collection activity <code>count</code> integer Number of times the post was bookmarked in that hour"},{"location":"side-projects/dcard-hw/#testing-dataset","title":"Testing Dataset","text":"<pre><code>posts_test                 Contains 225,986 records and 3 columns\npost_shared_test           Contains 83,376 records and 3 columns\npost_comment_created_test  Contains 607,251 records and 3 columns\npost_liked_test            Contains 908,910 records and 3 columns\npost_collected_test        Contains 275,073 records and 3 columns\n</code></pre>"},{"location":"side-projects/dcard-hw/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>For offline evaluation, only the first 10 hours of data for each post will be used as input for prediction. The primary evaluation metric is the F1-score.</p>"},{"location":"side-projects/dcard-hw/#submission-requirements","title":"Submission Requirements","text":"<p>Upon completing the assignment, you must submit at least the following four files. Failure to include any of these will be considered incomplete.</p> <ol> <li><code>Report.pdf</code><ul> <li>Instructions on how to use your code</li> <li>Methods and rationale</li> <li>Evaluation results on the provided testing data</li> <li>Experimental observations</li> </ul> </li> <li><code>train.py</code></li> <li><code>predict.py</code></li> <li><code>requirements.txt</code> or Pipfile</li> <li>(Optional) If your prediction requires a model file, please include it (we will not train it for you) and explain how to use it in Report.pdf.</li> </ol> <p>We have some requirements for the program structure to facilitate testing:</p> <ul> <li> <p>Training</p> <ul> <li>The outermost layer should be wrapped in train.py.</li> <li>The program should be executable as <code>python train.py {database_host} {model_filepath}</code>.</li> <li>Example: <code>python train.py localhost:8080 ./model.h5</code></li> </ul> </li> <li> <p>Prediction</p> <ul> <li>The program should be executable as <code>python predict.py {database_host} {model_filepath} {output_filepath}</code>.</li> <li>Specify where your model_filepath is located.</li> <li>Example: <code>python predict.py localhost:8080 ./model.h5 ./sample_output.csv</code></li> <li>Your program must achieve the following during prediction:<ul> <li>Read data from the database. The data format will match the tables described in the next section. For evaluation, we will use our own test data.</li> <li>Use another database's xxx_test tables as the test set during actual testing. Your predict.py should use these tables as input.</li> <li>Output a CSV file with two columns as shown below, including a header (refer to the provided sample_output.csv):<ul> <li>post_key: string type</li> <li>is_trending: bool type</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"side-projects/dcard-hw/#usage-instructions","title":"Usage Instructions","text":"<p>Environment:</p> <ul> <li>Operating System: Ubuntu 18.04 LTS Desktop</li> <li>Python version: Python 3.6.8</li> <li>Required Python packages and their versions are listed in <code>requirements.txt</code>.</li> </ul>"},{"location":"side-projects/dcard-hw/#trainingpy","title":"<code>training.py</code>","text":"<p>The usage of <code>training.py</code> is as follows:</p> <pre><code>usage: training.py [-h] -u USERNAME -p PASSWORD --host HOSTNAME\n                   [--port PORTNUMBER] [--protocol PROTOCOL]\n                   DATABASE OUTPUT_PATH\n</code></pre> <p>At a minimum, you must provide five parameters: \"username,\" \"password,\" \"host IP address,\" \"database name,\" and \"output path.\" To train on the training set, use the following command:</p> <pre><code>python training.py -u \"USERNAME\"\\\n                   -p \"PASSWORD\"\\\n                   --host \"HOSTNAME\"\\\n                   \"DATABASE\"\\\n                   \"OUTPUT_PATH\"\n</code></pre> <p>By default, the program connects to a PostgreSQL database on port 5432. If needed, you can use the <code>--protocol</code> and <code>--port</code> options to connect to other databases, such as MySQL:</p> Note <pre><code>python training.py -u \"USERNAME\"\\\n                -p \"PASSWORD\"\\\n                --host \"HOSTNAME\"\\\n                --port \"3306\"\\\n                --protocol \"mysql\"\\\n                \"DATABASE\"\\\n                \"OUTPUT_PATH\"\n</code></pre> <p>Danger</p> <p>After training, the program generates two files: \"best model\" and \"cross-validation results.\" The default filenames are <code>best_model.h5</code> and <code>cv_results.csv</code> (these cannot be changed). Therefore, when specifying <code>OUTPUT_PATH</code>, only the folder name is required.</p> <p>For more details, use the <code>-h</code> or <code>--help</code> options:</p> Note <pre><code>$ python training.py -h\nusage: training.py [-h] -u USERNAME -p PASSWORD --host HOSTNAME\n                [--port PORTNUMBER] [--protocol PROTOCOL]\n                DATABASE OUTPUT_PATH\n\npositional arguments:\nDATABASE             (Required) Database to use when connecting to server.\nOUTPUT_PATH          (Required) Best prediction model and cross validation\n                    results outputs file path.\n\noptional arguments:\n-h, --help           show this help message and exit\n-u USERNAME          (Required) User for login if not current user.\n-p PASSWORD          (Required) Password to use when connecting to server.\n--host HOSTNAME      (Required) Host address to connect.\n--port PORTNUMBER    Port number to use for connection (default: 5432)\n--protocol PROTOCOL  Protocol to connect. (default: postgres)\n</code></pre>"},{"location":"side-projects/dcard-hw/#predictpy","title":"<code>predict.py</code>","text":"<p>The usage of <code>predict.py</code> is as follows:</p> <pre><code>usage: predict.py [-h] -u USERNAME -p PASSWORD --host HOSTNAME\n                  [--port PORTNUMBER] [--protocol PROTOCOL] [-n]\n                  DATABASE MODEL_NAME OUTPUT_PATH\n</code></pre> <p>Similar to <code>training.py</code>, you must provide five parameters, with an additional parameter for the \"model path\" used to predict trending posts. To predict on the public test set, use the following command:</p> <pre><code>python predict.py -u \"USERNAME\"\\\n                  -p \"PASSWORD\"\\\n                  --host \"HOSTNAME\"\\\n                  \"DATABASE\"\\\n                  \"MODELNAME\"\\\n                  \"OUTPUT_PATH\"\n</code></pre> <p>After execution, the program outputs <code>output.csv</code> (filename cannot be changed) to the specified folder. Note that the <code>MODEL_NAME</code> option must include the model file name, not the folder path.</p> <p>As mentioned in the \"Assignment Supplementary Notes and Corrections\" email, the <code>posts_test</code> table in the private test set does not include the <code>like_count_36_hour</code> column. Therefore, you must use the <code>-n</code> option to indicate that this column is absent. To predict on the private test set, use the following command:</p> <pre><code>python predict.py -u \"USERNAME\"\\\n                  -p \"PASSWORD\"\\\n                  --host \"HOSTNAME\"\\\n                  -n\\\n                  \"DATABASE\"\\\n                  \"MODELNAME\"\\\n                  \"OUTPUT_PATH\"\n</code></pre> <p>If needed, you can also use the <code>--port</code> and <code>--protocol</code> options to connect to other databases.</p> <p>For more details, use the <code>-h</code> or <code>--help</code> options:</p> Note <pre><code>$ python predict.py -h\nusage: predict.py [-h] -u USERNAME -p PASSWORD --host HOSTNAME\n                [--port PORTNUMBER] [--protocol PROTOCOL] [-n]\n                DATABASE MODEL_NAME OUTPUT_PATH\n\npositional arguments:\nDATABASE             (Required) Database to use when connecting to server.\nMODEL_NAME           (Required) Prediction model name. If it is not in the\n                    current directory, please specify where it is.\nOUTPUT_PATH          (Required) File path of predicted results.\n\noptional arguments:\n-h, --help           show this help message and exit\n-u USERNAME          (Required) User for login if not current user.\n-p PASSWORD          (Required) Password to use when connecting to server.\n--host HOSTNAME      (Required) Host address to connect.\n--port PORTNUMBER    Port number to use for connection (default: 5432)\n--protocol PROTOCOL  Protocol to connect. (default: postgres)\n-n                   No like_count_36_hour column when the option is given.\n</code></pre>"},{"location":"side-projects/dcard-hw/eda_evaluation/","title":"\u532f\u5165\u76f8\u95dc\u5957\u4ef6","text":"Table of Contents <ul><li>1\u00a0\u00a0\u532f\u5165\u76f8\u95dc\u5957\u4ef6</li><li>2\u00a0\u00a0\u4e8b\u524d\u6e96\u5099</li><li>3\u00a0\u00a0EDA</li><li>4\u00a0\u00a0Evaluation<ul><li>4.1\u00a0\u00a0Resampler</li><li>4.2\u00a0\u00a0Resampler + Classifier</li><li>4.3\u00a0\u00a0Classifier</li><li>4.4\u00a0\u00a0Classifier + n_estimator</li><li>4.5\u00a0\u00a0<code>AdaBoostClassifier</code> + <code>max_depth</code></li><li>4.6\u00a0\u00a0<code>GradientBoostingClassifier</code>, <code>XGBClassifier</code> + <code>learning_rate</code></li><li>4.7\u00a0\u00a0Best Model<ul><li>4.7.1\u00a0\u00a0f1-score</li><li>4.7.2\u00a0\u00a0balanced accuracy</li></ul></li></ul></li></ul> In\u00a0[1]: Copied! <pre># Import built-in packages\nfrom math import isnan\nfrom functools import reduce\n\n# Import 3-rd party packages\nimport sqlalchemy\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom plotnine import *\n</pre> # Import built-in packages from math import isnan from functools import reduce  # Import 3-rd party packages import sqlalchemy import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from plotnine import * In\u00a0[2]: Copied! <pre>def print_info(info, width=61, fillchar='='):\n    \"\"\"\n    \u5370\u51fa\u683c\u5f0f\u5316\u7684\u8cc7\u8a0a\n    \"\"\"\n    temp_width = width - (width-len(info))//2\n    print(info.rjust(temp_width, fillchar).ljust(width, fillchar))\n</pre> def print_info(info, width=61, fillchar='='):     \"\"\"     \u5370\u51fa\u683c\u5f0f\u5316\u7684\u8cc7\u8a0a     \"\"\"     temp_width = width - (width-len(info))//2     print(info.rjust(temp_width, fillchar).ljust(width, fillchar)) In\u00a0[3]: Copied! <pre>def get_connector(user, host, database, password=None, port='5432', protocol='postgres'):\n    \"\"\"\n    \u53d6\u5f97\u9023\u7dda\u5f15\u64ce\uff0c\u9810\u8a2d\u70ba\u9023\u7dda\u81f3 PostgreSQL\uff0c\u57e0\u865f\u9810\u8a2d\u70ba 5432\u3002\n    \"\"\"\n    print_info(\"GETTING CONNECTOR START!\")\n    user_info = f'{user}:{password}' if password else user\n    url = f'{protocol}://{user_info}@{host}:{port}/{database}'\n    engine = sqlalchemy.create_engine(url, client_encoding='utf-8')\n    print_info(\"DONE!\")\n    return engine\n</pre> def get_connector(user, host, database, password=None, port='5432', protocol='postgres'):     \"\"\"     \u53d6\u5f97\u9023\u7dda\u5f15\u64ce\uff0c\u9810\u8a2d\u70ba\u9023\u7dda\u81f3 PostgreSQL\uff0c\u57e0\u865f\u9810\u8a2d\u70ba 5432\u3002     \"\"\"     print_info(\"GETTING CONNECTOR START!\")     user_info = f'{user}:{password}' if password else user     url = f'{protocol}://{user_info}@{host}:{port}/{database}'     engine = sqlalchemy.create_engine(url, client_encoding='utf-8')     print_info(\"DONE!\")     return engine In\u00a0[4]: Copied! <pre>def get_tables(engine, table_names):\n    \"\"\"\n    \u4f9d\u7167 `tables_names` \u7684\u9806\u5e8f\uff0c\u53d6\u5f97 tables\uff0c\u4e26\u4f9d\u5e8f\u5132\u5b58\u65bc `list` \u7576\u4e2d\uff0c\u56de\u50b3\u578b\u614b\u70ba `list`\uff0c\u6bcf\u500b element \u70ba `DataFrame`\u3002\n    \"\"\"\n    print_info(\"GETTING TABLES START!\")\n    rslt = []\n    for tn in table_names:\n        query = f'SELECT * FROM {tn}'\n        exec(f'{tn} = pd.read_sql(query, engine)')\n        # exec(f\"{tn} = pd.read_csv('{tn}.csv', encoding='utf8')\") # from current working directory\n        print(\n            f'{format(tn, \"26s\")} \u7e3d\u5171\u6709 {eval(f\"{tn}.shape[0]\"):9,} \u7b46\u8cc7\u6599\u548c {eval(f\"{tn}.shape[1]\")} \u500b\u6b04\u4f4d')\n        exec(f'rslt.append({tn})')\n    print_info(\"DONE!\")\n    return rslt\n</pre> def get_tables(engine, table_names):     \"\"\"     \u4f9d\u7167 `tables_names` \u7684\u9806\u5e8f\uff0c\u53d6\u5f97 tables\uff0c\u4e26\u4f9d\u5e8f\u5132\u5b58\u65bc `list` \u7576\u4e2d\uff0c\u56de\u50b3\u578b\u614b\u70ba `list`\uff0c\u6bcf\u500b element \u70ba `DataFrame`\u3002     \"\"\"     print_info(\"GETTING TABLES START!\")     rslt = []     for tn in table_names:         query = f'SELECT * FROM {tn}'         exec(f'{tn} = pd.read_sql(query, engine)')         # exec(f\"{tn} = pd.read_csv('{tn}.csv', encoding='utf8')\") # from current working directory         print(             f'{format(tn, \"26s\")} \u7e3d\u5171\u6709 {eval(f\"{tn}.shape[0]\"):9,} \u7b46\u8cc7\u6599\u548c {eval(f\"{tn}.shape[1]\")} \u500b\u6b04\u4f4d')         exec(f'rslt.append({tn})')     print_info(\"DONE!\")     return rslt In\u00a0[5]: Copied! <pre>def merge_tables(tables, table_names, how):\n    \"\"\"\n    \u5408\u4f75\u6240\u6709 tables\uff0c\u56de\u50b3\u578b\u614b\u70ba `DataFrame`\u3002\n    \"\"\"\n    print_info(\"MERGING TABLES START!\")\n    # \u5206\u5225\u8655\u7406 post_{shared, comment_created, liked, collected}_{train, test} \u56db\u500b tables\n    # groupby \u6bcf\u7bc7\u6587\u7ae0\uff0c\u5c07\u524d\u5341\u5c0f\u6642\u7684\u5206\u4eab\u6578\u3001\u8a55\u8ad6\u6578\u3001\u611b\u5fc3\u6578\u3001\u6536\u85cf\u6578\u52a0\u7e3d\u8d77\u4f86\n    for idx, (table, tn) in enumerate(zip(tables, table_names)):\n        if len(tn.split('_'))==2: continue                  # for handling posts_{train, test} table\n        col_name = f\"{tn.split('_')[1]}_count\"              # tn.split('_')[1] is either {shared, comment, liked, collected}\n        mapper = {'count': col_name}\n        exec(f\"tables[{idx}] = table.groupby(['post_key'], as_index=False).sum().rename(columns=mapper)\")\n    # \u5c07 tables \u5408\u4f75\u8d77\u4f86\u4e26\u56de\u50b3\u3002\n    total_df = reduce(lambda left, right: pd.merge(left, right, on=['post_key'], how=how), tables)\n    print_info(\"DONE!\")\n    return total_df\n</pre> def merge_tables(tables, table_names, how):     \"\"\"     \u5408\u4f75\u6240\u6709 tables\uff0c\u56de\u50b3\u578b\u614b\u70ba `DataFrame`\u3002     \"\"\"     print_info(\"MERGING TABLES START!\")     # \u5206\u5225\u8655\u7406 post_{shared, comment_created, liked, collected}_{train, test} \u56db\u500b tables     # groupby \u6bcf\u7bc7\u6587\u7ae0\uff0c\u5c07\u524d\u5341\u5c0f\u6642\u7684\u5206\u4eab\u6578\u3001\u8a55\u8ad6\u6578\u3001\u611b\u5fc3\u6578\u3001\u6536\u85cf\u6578\u52a0\u7e3d\u8d77\u4f86     for idx, (table, tn) in enumerate(zip(tables, table_names)):         if len(tn.split('_'))==2: continue                  # for handling posts_{train, test} table         col_name = f\"{tn.split('_')[1]}_count\"              # tn.split('_')[1] is either {shared, comment, liked, collected}         mapper = {'count': col_name}         exec(f\"tables[{idx}] = table.groupby(['post_key'], as_index=False).sum().rename(columns=mapper)\")     # \u5c07 tables \u5408\u4f75\u8d77\u4f86\u4e26\u56de\u50b3\u3002     total_df = reduce(lambda left, right: pd.merge(left, right, on=['post_key'], how=how), tables)     print_info(\"DONE!\")     return total_df In\u00a0[6]: Copied! <pre>def preprocess_total_df(total_df):\n    \"\"\"\n    \u9810\u8655\u7406\u525b\u5408\u4f75\u597d\u7684 total_df \u4ee5\u7b26\u5408\u5f8c\u7e8c\u5efa\u6a21\u9700\u6c42\uff0c\u56de\u50b3\u578b\u614b\u70ba `DataFrame`\u3002\n    \"\"\"\n    print_info(\"PREPROCESSING TOTAL_DF START!\")\n    total_df.set_index('post_key', inplace=True)                                    # post_key \u6b04\u4f4d\u8a2d\u70ba\u7d22\u5f15\n    total_df['created_at_hour'] = pd.to_datetime(total_df['created_at_hour'])       # \u5c07 created_at_hour \u6b04\u4f4d\u8f49\u63db\u6210 datetime \u578b\u614b\n    total_df['weekday'] = total_df['created_at_hour'].dt.dayofweek                  # \u64f7\u53d6\u51fa\u767c\u6587\u7684 weekday\n    total_df['hour'] = total_df['created_at_hour'].dt.hour                          # \u64f7\u53d6\u51fa\u767c\u6587\u7684 hour\n    total_df.fillna(0, inplace=True)                                                # NaN \u503c\u88dc 0\n    total_df['is_trending'] = 0+(total_df['like_count_36_hour']&gt;=1000)              # \u8f49\u63db\u6210 is_trending \u985e\u5225\u6b04\u4f4d\n    total_df = total_df.drop(['created_at_hour'], axis=1)                           # drop \u6389\u4e0d\u5fc5\u8981\u7684\u6b04\u4f4d\n    # \u5c07\u8a08\u6b21\u6b04\u4f4d\u8f49\u63db\u6210 int \u578b\u614b\n    col_names = ['shared_count', 'comment_count', 'liked_count', 'collected_count']\n    for cn in col_names:\n        total_df[cn] = total_df[cn].astype(dtype='int')\n    print_info(\"DONE!\")\n    return total_df\n</pre> def preprocess_total_df(total_df):     \"\"\"     \u9810\u8655\u7406\u525b\u5408\u4f75\u597d\u7684 total_df \u4ee5\u7b26\u5408\u5f8c\u7e8c\u5efa\u6a21\u9700\u6c42\uff0c\u56de\u50b3\u578b\u614b\u70ba `DataFrame`\u3002     \"\"\"     print_info(\"PREPROCESSING TOTAL_DF START!\")     total_df.set_index('post_key', inplace=True)                                    # post_key \u6b04\u4f4d\u8a2d\u70ba\u7d22\u5f15     total_df['created_at_hour'] = pd.to_datetime(total_df['created_at_hour'])       # \u5c07 created_at_hour \u6b04\u4f4d\u8f49\u63db\u6210 datetime \u578b\u614b     total_df['weekday'] = total_df['created_at_hour'].dt.dayofweek                  # \u64f7\u53d6\u51fa\u767c\u6587\u7684 weekday     total_df['hour'] = total_df['created_at_hour'].dt.hour                          # \u64f7\u53d6\u51fa\u767c\u6587\u7684 hour     total_df.fillna(0, inplace=True)                                                # NaN \u503c\u88dc 0     total_df['is_trending'] = 0+(total_df['like_count_36_hour']&gt;=1000)              # \u8f49\u63db\u6210 is_trending \u985e\u5225\u6b04\u4f4d     total_df = total_df.drop(['created_at_hour'], axis=1)                           # drop \u6389\u4e0d\u5fc5\u8981\u7684\u6b04\u4f4d     # \u5c07\u8a08\u6b21\u6b04\u4f4d\u8f49\u63db\u6210 int \u578b\u614b     col_names = ['shared_count', 'comment_count', 'liked_count', 'collected_count']     for cn in col_names:         total_df[cn] = total_df[cn].astype(dtype='int')     print_info(\"DONE!\")     return total_df In\u00a0[7]: Copied! <pre># Get engine\nengine = get_connector(\n    user=\"candidate\",\n    password=\"dcard-data-intern-2020\",\n    host=\"35.187.144.113\",\n    database=\"intern_task\"\n)\n# Get tables from db\ntable_names_train = ['posts_train', 'post_shared_train', \n                     'post_comment_created_train', 'post_liked_train', 'post_collected_train']\ntables_train = get_tables(engine, table_names_train)\n# Merge tables\ntotal_df_train = merge_tables(tables_train, table_names_train, how='left')\n# Preprocess total_df\ntotal_df_train = preprocess_total_df(total_df_train)\n\nengine.dispose()\n</pre> # Get engine engine = get_connector(     user=\"candidate\",     password=\"dcard-data-intern-2020\",     host=\"35.187.144.113\",     database=\"intern_task\" ) # Get tables from db table_names_train = ['posts_train', 'post_shared_train',                       'post_comment_created_train', 'post_liked_train', 'post_collected_train'] tables_train = get_tables(engine, table_names_train) # Merge tables total_df_train = merge_tables(tables_train, table_names_train, how='left') # Preprocess total_df total_df_train = preprocess_total_df(total_df_train)  engine.dispose() <pre>===================GETTING CONNECTOR START!==================\n============================DONE!============================\n====================GETTING TABLES START!====================\nposts_train                \u7e3d\u5171\u6709   793,751 \u7b46\u8cc7\u6599\u548c 3 \u500b\u6b04\u4f4d\npost_shared_train          \u7e3d\u5171\u6709   304,260 \u7b46\u8cc7\u6599\u548c 3 \u500b\u6b04\u4f4d\npost_comment_created_train \u7e3d\u5171\u6709 2,372,228 \u7b46\u8cc7\u6599\u548c 3 \u500b\u6b04\u4f4d\npost_liked_train           \u7e3d\u5171\u6709 3,395,903 \u7b46\u8cc7\u6599\u548c 3 \u500b\u6b04\u4f4d\npost_collected_train       \u7e3d\u5171\u6709 1,235,126 \u7b46\u8cc7\u6599\u548c 3 \u500b\u6b04\u4f4d\n============================DONE!============================\n====================MERGING TABLES START!====================\n============================DONE!============================\n================PREPROCESSING TOTAL_DF START!================\n============================DONE!============================\n</pre> In\u00a0[8]: Copied! <pre>cv_results = pd.read_csv('./outputs/cv_results.csv')\n</pre> cv_results = pd.read_csv('./outputs/cv_results.csv') In\u00a0[9]: Copied! <pre>temp = total_df_train.drop(columns=['weekday', 'hour', 'is_trending'])\nsns.heatmap(temp.corr(), cmap='YlGnBu')\n</pre> temp = total_df_train.drop(columns=['weekday', 'hour', 'is_trending']) sns.heatmap(temp.corr(), cmap='YlGnBu') Out[9]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x23755717ba8&gt;</pre> In\u00a0[10]: Copied! <pre>mapper = dict(zip([0, 1, 2, 3, 4, 5, 6], ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']))\n</pre> mapper = dict(zip([0, 1, 2, 3, 4, 5, 6], ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])) In\u00a0[11]: Copied! <pre># \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\u7684\u767c\u6587\u6578\nnum_articles_heatmap_df = total_df_train.groupby(['weekday', 'hour']).size().reset_index().rename(columns={0:'count'})\nnum_articles_heatmap_df = num_articles_heatmap_df.pivot(index='weekday', columns='hour', values='count')\nnum_articles_heatmap_df = num_articles_heatmap_df.rename(mapper=mapper, axis=0)\nnum_articles_heatmap_df = num_articles_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday', \n                                                           'Thursday', 'Friday', 'Saturday', 'Sunday'])\nplt.figure(figsize=(20, 5))\nplt.title(f'Number of Articles by Day of Week / Hour of Day')\nsns.heatmap(num_articles_heatmap_df, cmap='OrRd', cbar=False)\n</pre> # \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\u7684\u767c\u6587\u6578 num_articles_heatmap_df = total_df_train.groupby(['weekday', 'hour']).size().reset_index().rename(columns={0:'count'}) num_articles_heatmap_df = num_articles_heatmap_df.pivot(index='weekday', columns='hour', values='count') num_articles_heatmap_df = num_articles_heatmap_df.rename(mapper=mapper, axis=0) num_articles_heatmap_df = num_articles_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday',                                                             'Thursday', 'Friday', 'Saturday', 'Sunday']) plt.figure(figsize=(20, 5)) plt.title(f'Number of Articles by Day of Week / Hour of Day') sns.heatmap(num_articles_heatmap_df, cmap='OrRd', cbar=False) Out[11]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x237490fb518&gt;</pre> In\u00a0[12]: Copied! <pre># \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\u7684\u71b1\u9580\u6587\u7ae0\u6bd4\u4f8b\nnum_pops_heatmap_df = total_df_train.groupby(['weekday', 'hour'])['is_trending'].sum().reset_index()\nnum_pops_heatmap_df = num_pops_heatmap_df.pivot(index='weekday', columns='hour', values='is_trending')\nnum_pops_heatmap_df = num_pops_heatmap_df.rename(mapper=mapper, axis=0)\nnum_pops_heatmap_df = num_pops_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday', \n                                                   'Thursday', 'Friday', 'Saturday', 'Sunday'])\npct_pops_heatmap_df = num_pops_heatmap_df/num_articles_heatmap_df\nplt.figure(figsize=(20, 5))\nplt.title(f'Percentage of Popular Articles by Day of Week / Hour of Day ')\nsns.heatmap(pct_pops_heatmap_df, cmap='Blues', cbar=False)\n</pre> # \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\u7684\u71b1\u9580\u6587\u7ae0\u6bd4\u4f8b num_pops_heatmap_df = total_df_train.groupby(['weekday', 'hour'])['is_trending'].sum().reset_index() num_pops_heatmap_df = num_pops_heatmap_df.pivot(index='weekday', columns='hour', values='is_trending') num_pops_heatmap_df = num_pops_heatmap_df.rename(mapper=mapper, axis=0) num_pops_heatmap_df = num_pops_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday',                                                     'Thursday', 'Friday', 'Saturday', 'Sunday']) pct_pops_heatmap_df = num_pops_heatmap_df/num_articles_heatmap_df plt.figure(figsize=(20, 5)) plt.title(f'Percentage of Popular Articles by Day of Week / Hour of Day ') sns.heatmap(pct_pops_heatmap_df, cmap='Blues', cbar=False) Out[12]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2375e9e5cc0&gt;</pre> In\u00a0[13]: Copied! <pre># \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\uff0c\u524d 10 \u5c0f\u6642\u611b\u5fc3\u5e73\u5747\u6578\nnum_likes_10_heatmap_df = total_df_train.groupby(['weekday', 'hour'])['liked_count'].mean().reset_index()\nnum_likes_10_heatmap_df = num_likes_10_heatmap_df.pivot(index='weekday', columns='hour', values='liked_count')\nnum_likes_10_heatmap_df = num_likes_10_heatmap_df.rename(mapper=mapper, axis=0)\nnum_likes_10_heatmap_df = num_likes_10_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday', \n                                                           'Thursday', 'Friday', 'Saturday', 'Sunday'])\nplt.figure(figsize=(20, 5))\nplt.title(f'Average Likes within 10 hours by Day of Week / Hour of Day')\nsns.heatmap(num_likes_10_heatmap_df, cmap='Purples', cbar=False)\n</pre> # \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\uff0c\u524d 10 \u5c0f\u6642\u611b\u5fc3\u5e73\u5747\u6578 num_likes_10_heatmap_df = total_df_train.groupby(['weekday', 'hour'])['liked_count'].mean().reset_index() num_likes_10_heatmap_df = num_likes_10_heatmap_df.pivot(index='weekday', columns='hour', values='liked_count') num_likes_10_heatmap_df = num_likes_10_heatmap_df.rename(mapper=mapper, axis=0) num_likes_10_heatmap_df = num_likes_10_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday',                                                             'Thursday', 'Friday', 'Saturday', 'Sunday']) plt.figure(figsize=(20, 5)) plt.title(f'Average Likes within 10 hours by Day of Week / Hour of Day') sns.heatmap(num_likes_10_heatmap_df, cmap='Purples', cbar=False) Out[13]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x237625a2ef0&gt;</pre> In\u00a0[14]: Copied! <pre># \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\uff0c\u524d 36 \u5c0f\u6642\u611b\u5fc3\u5e73\u5747\u6578\nnum_likes_36_heatmap_df = total_df_train.groupby(['weekday', 'hour'])['like_count_36_hour'].mean().reset_index()\nnum_likes_36_heatmap_df = num_likes_36_heatmap_df.pivot(index='weekday', columns='hour', values='like_count_36_hour')\nnum_likes_36_heatmap_df = num_likes_36_heatmap_df.rename(mapper=mapper, axis=0)\nnum_likes_36_heatmap_df = num_likes_36_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday', \n                                                           'Thursday', 'Friday', 'Saturday', 'Sunday'])\nplt.figure(figsize=(20, 5))\nplt.title(f'Average Likes within 36 hours by Day of Week / Hour of Day ')\nsns.heatmap(num_likes_36_heatmap_df, cmap='YlGn', cbar=False)\n</pre> # \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\uff0c\u524d 36 \u5c0f\u6642\u611b\u5fc3\u5e73\u5747\u6578 num_likes_36_heatmap_df = total_df_train.groupby(['weekday', 'hour'])['like_count_36_hour'].mean().reset_index() num_likes_36_heatmap_df = num_likes_36_heatmap_df.pivot(index='weekday', columns='hour', values='like_count_36_hour') num_likes_36_heatmap_df = num_likes_36_heatmap_df.rename(mapper=mapper, axis=0) num_likes_36_heatmap_df = num_likes_36_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday',                                                             'Thursday', 'Friday', 'Saturday', 'Sunday']) plt.figure(figsize=(20, 5)) plt.title(f'Average Likes within 36 hours by Day of Week / Hour of Day ') sns.heatmap(num_likes_36_heatmap_df, cmap='YlGn', cbar=False) Out[14]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2376129c400&gt;</pre> In\u00a0[15]: Copied! <pre># \u53bb\u9664\u7528\u4e0d\u5230\u7684\u6b04\u4f4d\u8cc7\u8a0a\ncv_results = cv_results.drop(columns=[col for col in cv_results.columns if 'split' in col])\n# \u5c07\u6b04\u4f4d\u5167\u5bb9\u5316\u7c21\ndef transform(elem, mapper):\n    if type(elem)==float and isnan(elem):\n        return elem\n    for sub_str in mapper:\n        if sub_str in elem:\n            return mapper[sub_str]\n    return elem\n# resampler\nmapper = {\n    'SMOTE': 'SMOTE',\n    'NearMiss': 'NearMiss'\n}\ncv_results['param_resampler'] = cv_results['param_resampler'].apply(transform, args=(mapper,))\n# classifier\nmapper = {\n    'AdaBoostClassifier': 'AdaBoostClassifier',\n    'XGBClassifier': 'XGBClassifier',\n    'GradientBoostingClassifier': 'GradientBoostingClassifier'\n}\ncv_results['param_classifier'] = cv_results['param_classifier'].apply(transform, args=(mapper,))\n# classifier__base_estimator\nmapper = {\n    'max_depth=1': 'DecisionTreeClassifier(max_depth=1)',\n    'max_depth=2': 'DecisionTreeClassifier(max_depth=2)',\n    'max_depth=3': 'DecisionTreeClassifier(max_depth=3)'\n}\ncv_results['param_classifier__base_estimator'] = cv_results['param_classifier__base_estimator'].apply(transform, args=(mapper,))\n</pre> # \u53bb\u9664\u7528\u4e0d\u5230\u7684\u6b04\u4f4d\u8cc7\u8a0a cv_results = cv_results.drop(columns=[col for col in cv_results.columns if 'split' in col]) # \u5c07\u6b04\u4f4d\u5167\u5bb9\u5316\u7c21 def transform(elem, mapper):     if type(elem)==float and isnan(elem):         return elem     for sub_str in mapper:         if sub_str in elem:             return mapper[sub_str]     return elem # resampler mapper = {     'SMOTE': 'SMOTE',     'NearMiss': 'NearMiss' } cv_results['param_resampler'] = cv_results['param_resampler'].apply(transform, args=(mapper,)) # classifier mapper = {     'AdaBoostClassifier': 'AdaBoostClassifier',     'XGBClassifier': 'XGBClassifier',     'GradientBoostingClassifier': 'GradientBoostingClassifier' } cv_results['param_classifier'] = cv_results['param_classifier'].apply(transform, args=(mapper,)) # classifier__base_estimator mapper = {     'max_depth=1': 'DecisionTreeClassifier(max_depth=1)',     'max_depth=2': 'DecisionTreeClassifier(max_depth=2)',     'max_depth=3': 'DecisionTreeClassifier(max_depth=3)' } cv_results['param_classifier__base_estimator'] = cv_results['param_classifier__base_estimator'].apply(transform, args=(mapper,)) In\u00a0[16]: Copied! <pre>temp = cv_results.groupby(['param_resampler'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean()\ntemp.reset_index(inplace=True)\n\n(ggplot(temp, aes(x='param_resampler', y='mean_test_f1_score', fill='param_resampler'))\n + geom_bar(stat=\"identity\")\n + ylim(0,1)\n + scale_fill_brewer('qualitative', 2)\n + geom_text(aes(label='mean_test_f1_score'), format_string='{:.2f}')\n + ggtitle(f'Average F1 Score by Resampler')\n + labs(fill=f'Resampler')\n + xlab('Resampler')\n + ylab(f'Average F1 score'))\n</pre> temp = cv_results.groupby(['param_resampler'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean() temp.reset_index(inplace=True)  (ggplot(temp, aes(x='param_resampler', y='mean_test_f1_score', fill='param_resampler'))  + geom_bar(stat=\"identity\")  + ylim(0,1)  + scale_fill_brewer('qualitative', 2)  + geom_text(aes(label='mean_test_f1_score'), format_string='{:.2f}')  + ggtitle(f'Average F1 Score by Resampler')  + labs(fill=f'Resampler')  + xlab('Resampler')  + ylab(f'Average F1 score')) Out[16]: <pre>&lt;ggplot: (-9223371884558871573)&gt;</pre> In\u00a0[17]: Copied! <pre>(ggplot(temp, aes(x='param_resampler', y='mean_test_recall', fill='param_resampler'))\n + geom_bar(stat=\"identity\")\n + ylim(0,1)\n + scale_fill_brewer('qualitative', 2)\n + geom_text(aes(label='mean_test_recall'), format_string='{:.2f}')\n + ggtitle(f'Average Recall by Resampler')\n + labs(fill=f'Resampler')\n + xlab('Resampler')\n + ylab(f'Average Recall'))\n</pre> (ggplot(temp, aes(x='param_resampler', y='mean_test_recall', fill='param_resampler'))  + geom_bar(stat=\"identity\")  + ylim(0,1)  + scale_fill_brewer('qualitative', 2)  + geom_text(aes(label='mean_test_recall'), format_string='{:.2f}')  + ggtitle(f'Average Recall by Resampler')  + labs(fill=f'Resampler')  + xlab('Resampler')  + ylab(f'Average Recall')) Out[17]: <pre>&lt;ggplot: (-9223371884558718762)&gt;</pre> In\u00a0[18]: Copied! <pre>(ggplot(temp, aes(x='param_resampler', y='mean_test_precision', fill='param_resampler'))\n + geom_bar(stat=\"identity\")\n + ylim(0,1)\n + scale_fill_brewer('qualitative', 2)\n + geom_text(aes(label='mean_test_precision'), format_string='{:.2f}')\n + ggtitle(f'Average Precision by Resampler')\n + labs(fill=f'Resampler')\n + xlab('Resampler')\n + ylab(f'Average Precision'))\n</pre> (ggplot(temp, aes(x='param_resampler', y='mean_test_precision', fill='param_resampler'))  + geom_bar(stat=\"identity\")  + ylim(0,1)  + scale_fill_brewer('qualitative', 2)  + geom_text(aes(label='mean_test_precision'), format_string='{:.2f}')  + ggtitle(f'Average Precision by Resampler')  + labs(fill=f'Resampler')  + xlab('Resampler')  + ylab(f'Average Precision')) Out[18]: <pre>&lt;ggplot: (152294750826)&gt;</pre> In\u00a0[19]: Copied! <pre>(ggplot(temp, aes(x='param_resampler', y='mean_test_balanced_accuracy', fill='param_resampler'))\n + geom_bar(stat=\"identity\")\n + ylim(0,1)\n + scale_fill_brewer('qualitative', 2)\n + geom_text(aes(label='mean_test_balanced_accuracy'), format_string='{:.2f}')\n + ggtitle(f'Average Balanced Accuracy by Resampler')\n + labs(fill=f'Resampler')\n + xlab('Resampler')\n + ylab(f'Average Balanced Accuracy'))\n</pre> (ggplot(temp, aes(x='param_resampler', y='mean_test_balanced_accuracy', fill='param_resampler'))  + geom_bar(stat=\"identity\")  + ylim(0,1)  + scale_fill_brewer('qualitative', 2)  + geom_text(aes(label='mean_test_balanced_accuracy'), format_string='{:.2f}')  + ggtitle(f'Average Balanced Accuracy by Resampler')  + labs(fill=f'Resampler')  + xlab('Resampler')  + ylab(f'Average Balanced Accuracy')) Out[19]: <pre>&lt;ggplot: (-9223371884547166951)&gt;</pre> In\u00a0[20]: Copied! <pre>temp = cv_results.groupby(['param_resampler', 'param_classifier'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean()\ntemp.reset_index(inplace=True)\n\n(ggplot(temp, aes(x='param_resampler', y='mean_test_f1_score', fill='param_classifier'))\n + geom_bar(position='dodge', stat=\"identity\")\n + ylim(0,1)\n + geom_text(aes(label='mean_test_f1_score'), position=position_dodge(width=0.9), format_string='{:.2f}')\n + ggtitle(f'Average F1 Score by Resampler and Classifier')\n + labs(fill=f'Classifier')\n + xlab('Resampler')\n + ylab(f'Average F1 score'))\n</pre> temp = cv_results.groupby(['param_resampler', 'param_classifier'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean() temp.reset_index(inplace=True)  (ggplot(temp, aes(x='param_resampler', y='mean_test_f1_score', fill='param_classifier'))  + geom_bar(position='dodge', stat=\"identity\")  + ylim(0,1)  + geom_text(aes(label='mean_test_f1_score'), position=position_dodge(width=0.9), format_string='{:.2f}')  + ggtitle(f'Average F1 Score by Resampler and Classifier')  + labs(fill=f'Classifier')  + xlab('Resampler')  + ylab(f'Average F1 score')) Out[20]: <pre>&lt;ggplot: (-9223371884550063342)&gt;</pre> In\u00a0[21]: Copied! <pre>temp = cv_results.groupby(['param_classifier'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean()\ntemp.reset_index(inplace=True)\n\n(ggplot(temp, aes(x='param_classifier', y='mean_test_f1_score', fill='param_classifier'))\n + geom_bar(stat=\"identity\")\n + ylim(0,1)\n + geom_text(aes(label='mean_test_f1_score'), format_string='{:.2f}')\n + ggtitle('Average F1 Score by Classifier')\n + labs(fill='Classifier')\n + xlab('Classifier')\n + ylab(f'Average F1 score'))\n</pre> temp = cv_results.groupby(['param_classifier'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean() temp.reset_index(inplace=True)  (ggplot(temp, aes(x='param_classifier', y='mean_test_f1_score', fill='param_classifier'))  + geom_bar(stat=\"identity\")  + ylim(0,1)  + geom_text(aes(label='mean_test_f1_score'), format_string='{:.2f}')  + ggtitle('Average F1 Score by Classifier')  + labs(fill='Classifier')  + xlab('Classifier')  + ylab(f'Average F1 score')) Out[21]: <pre>&lt;ggplot: (-9223371884545924818)&gt;</pre> In\u00a0[22]: Copied! <pre>temp = cv_results.groupby(['param_classifier', 'param_classifier__n_estimators'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean()\ntemp.reset_index(inplace=True)\n\n(ggplot(temp, aes(x='param_classifier__n_estimators', y='mean_test_f1_score', color='param_classifier'))\n + geom_line()\n + geom_point()\n + ylim(0,1)\n + ggtitle('Average F1 Score by Classifier and Number of Estimators')\n + labs(color='Classifier')\n + xlab('Number of Estimators')\n + ylab('Average F1 score'))\n</pre> temp = cv_results.groupby(['param_classifier', 'param_classifier__n_estimators'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean() temp.reset_index(inplace=True)  (ggplot(temp, aes(x='param_classifier__n_estimators', y='mean_test_f1_score', color='param_classifier'))  + geom_line()  + geom_point()  + ylim(0,1)  + ggtitle('Average F1 Score by Classifier and Number of Estimators')  + labs(color='Classifier')  + xlab('Number of Estimators')  + ylab('Average F1 score')) Out[22]: <pre>&lt;ggplot: (-9223371884546480871)&gt;</pre> In\u00a0[23]: Copied! <pre>cv_results[cv_results['param_resampler']=='passthrough'].groupby(['param_classifier', 'param_classifier__base_estimator'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean()\n</pre> cv_results[cv_results['param_resampler']=='passthrough'].groupby(['param_classifier', 'param_classifier__base_estimator'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean() Out[23]: mean_test_precision mean_test_recall mean_test_specificity mean_test_f1_score mean_test_balanced_accuracy param_classifier param_classifier__base_estimator AdaBoostClassifier DecisionTreeClassifier(max_depth=1) 0.738579 0.436288 0.996339 0.548524 0.716314 DecisionTreeClassifier(max_depth=2) 0.759336 0.443006 0.996670 0.559510 0.719838 DecisionTreeClassifier(max_depth=3) 0.755862 0.441223 0.996619 0.557159 0.718921 In\u00a0[24]: Copied! <pre>cv_results[cv_results['param_resampler']=='passthrough'].groupby(['param_classifier', 'param_classifier__learning_rate'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean()\n</pre> cv_results[cv_results['param_resampler']=='passthrough'].groupby(['param_classifier', 'param_classifier__learning_rate'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean() Out[24]: mean_test_precision mean_test_recall mean_test_specificity mean_test_f1_score mean_test_balanced_accuracy param_classifier param_classifier__learning_rate GradientBoostingClassifier 0.025 0.790585 0.395179 0.997518 0.526909 0.696348 0.050 0.780465 0.423388 0.997177 0.548966 0.710282 0.100 0.778204 0.434859 0.997062 0.557939 0.715961 XGBClassifier 0.025 0.754734 0.404196 0.996884 0.526422 0.700540 0.050 0.776283 0.406060 0.997226 0.533204 0.701643 0.100 0.783911 0.419787 0.997256 0.546763 0.708522 In\u00a0[25]: Copied! <pre>print(cv_results[cv_results['rank_test_f1_score']==1]['params'].values[0])\n</pre> print(cv_results[cv_results['rank_test_f1_score']==1]['params'].values[0]) <pre>{'classifier': AdaBoostClassifier(algorithm='SAMME.R',\n                   base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n                                                         class_weight=None,\n                                                         criterion='gini',\n                                                         max_depth=2,\n                                                         max_features=None,\n                                                         max_leaf_nodes=None,\n                                                         min_impurity_decrease=0.0,\n                                                         min_impurity_split=None,\n                                                         min_samples_leaf=1,\n                                                         min_samples_split=2,\n                                                         min_weight_fraction_leaf=0.0,\n                                                         presort='deprecated',\n                                                         random_state=None,\n                                                         splitter='best'),\n                   learning_rate=1.0, n_estimators=100, random_state=None), 'classifier__base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n                       max_depth=2, max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, presort='deprecated',\n                       random_state=None, splitter='best'), 'classifier__n_estimators': 100, 'resampler': 'passthrough'}\n</pre> In\u00a0[26]: Copied! <pre>temp = cv_results[cv_results['rank_test_f1_score']==1]\ntemp[['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy']]\n</pre> temp = cv_results[cv_results['rank_test_f1_score']==1] temp[['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy']] Out[26]: mean_test_precision mean_test_recall mean_test_specificity mean_test_f1_score mean_test_balanced_accuracy 15 0.759668 0.44419 0.996667 0.560527 0.720429 In\u00a0[27]: Copied! <pre>print(cv_results[cv_results['rank_test_balanced_accuracy']==1]['params'].values[0])\n</pre> print(cv_results[cv_results['rank_test_balanced_accuracy']==1]['params'].values[0]) <pre>{'classifier': GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n                           learning_rate=0.1, loss='deviance', max_depth=3,\n                           max_features=None, max_leaf_nodes=None,\n                           min_impurity_decrease=0.0, min_impurity_split=None,\n                           min_samples_leaf=1, min_samples_split=2,\n                           min_weight_fraction_leaf=0.0, n_estimators=100,\n                           n_iter_no_change=None, presort='deprecated',\n                           random_state=None, subsample=1.0, tol=0.0001,\n                           validation_fraction=0.1, verbose=0,\n                           warm_start=False), 'classifier__learning_rate': 0.025, 'classifier__n_estimators': 120, 'resampler': SMOTE(k_neighbors=5, n_jobs=None, random_state=None, sampling_strategy='auto')}\n</pre> In\u00a0[28]: Copied! <pre>temp = cv_results[cv_results['rank_test_balanced_accuracy']==1]\ntemp[['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy']]\n</pre> temp = cv_results[cv_results['rank_test_balanced_accuracy']==1] temp[['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy']] Out[28]: mean_test_precision mean_test_recall mean_test_specificity mean_test_f1_score mean_test_balanced_accuracy 46 0.199325 0.958312 0.908746 0.330003 0.933529"},{"location":"side-projects/dcard-hw/eda_evaluation/","title":"\u532f\u5165\u76f8\u95dc\u5957\u4ef6\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/","title":"\u4e8b\u524d\u6e96\u5099\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#eda","title":"EDA\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#evaluation","title":"Evaluation\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#resampler","title":"Resampler\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#resampler-classifier","title":"Resampler + Classifier\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#classifier","title":"Classifier\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#classifier-n_estimator","title":"Classifier + n_estimator\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#adaboostclassifier-max_depth","title":"<code>AdaBoostClassifier</code> + <code>max_depth</code>\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#gradientboostingclassifier-xgbclassifier-learning_rate","title":"<code>GradientBoostingClassifier</code>, <code>XGBClassifier</code> + <code>learning_rate</code>\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#best-model","title":"Best Model\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#f1-score","title":"f1-score\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#balanced-accuracy","title":"balanced accuracy\u00b6","text":""},{"location":"side-projects/dcard-hw/docs/1-eda/","title":"Exploratory Data Analysis (EDA)","text":"<p>The dataset is divided into training and testing sets. To avoid data leakage, only the training set is analyzed during EDA, leaving the testing set aside.</p> <p>When we first receive a dataset, the initial step is to examine its details, including the number of records and columns in each table. Below is the dataset information as of the update on 2020/04/13:</p> <pre><code>posts_train                Contains 793,751 records and 3 columns\npost_shared_train          Contains 304,260 records and 3 columns\npost_comment_created_train Contains 2,372,228 records and 3 columns\npost_liked_train           Contains 3,395,903 records and 3 columns\npost_collected_train       Contains 1,235,126 records and 3 columns\n</code></pre> <p>The training set covers posts from April 1, 2019, to the end of October 2019, spanning approximately seven months with around 793,000 posts. The goal is to build a predictive model that uses 10-hour post metrics (e.g., shares, comments, likes, and saves) to predict whether a post will receive 1,000 likes within 36 hours, classifying it as a \"popular post.\"</p> <p>Approximately 2.32% of the training posts are popular, equating to about 18,000 posts. This imbalance in the dataset necessitates techniques like over/undersampling during preprocessing and alternative evaluation metrics during model assessment.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#problem-definition","title":"Problem Definition","text":"<p>The task can be approached in four ways, based on \"whether sequence information is considered\" and \"whether the problem is framed as regression or binary classification\":</p> Regression Binary Classification With Sequence Info RNNs (e.g., GRU), traditional time series models (e.g., ARMA, ARIMA) Same as left Without Sequence Info Poisson regression, SVM, tree-based models, etc. Logistic regression, SVM, tree-based models, etc. <p>For simplicity and time constraints, we focus on \"without sequence info\" and \"binary classification,\" aggregating 10-hour metrics and building a binary classification model to predict popular posts. The focus will be on handling imbalanced data, tree-based models, and subsequent discussions.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#relationships-between-variables","title":"Relationships Between Variables","text":"<p>We simplify the dataset to include total shares, comments, likes, and saves within 10 hours and use a heatmap to observe their relationships with the total likes within 36 hours:</p> <p></p> <p>Info</p> <p>Key observations from the heatmap:</p> <ul> <li>Total likes within 36 hours moderately correlate with total likes within 10 hours (.58), shares (.36), and saves (.36), but weakly with comments (.17).</li> <li>Total likes within 10 hours moderately correlate with shares (.63) and saves (.61).</li> <li>Shares and saves within 10 hours moderately correlate (.48).</li> </ul> <p>In simple terms, posts with more likes within 10 hours tend to have more shares and saves. However, the strongest predictor of total likes within 36 hours is the likes within 10 hours. Comments show little correlation with total likes.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#heatmaps-of-key-metrics","title":"Heatmaps of Key Metrics","text":"<p>Danger</p> <p>To protect Dcard's proprietary information, color bars (<code>cbar=False</code>) are omitted, showing only relative relationships.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#total-posts-by-time","title":"Total Posts by Time","text":"<p>We examine whether the number of posts varies across different time periods:</p> <p></p> <p>The x-axis represents 24 hours, and the y-axis represents days of the week.</p> <p>Info</p> <p>Observations:</p> <ul> <li>Posts are concentrated during midday, afternoon, and evening (12:00\u201318:00), with weekdays slightly higher than weekends.</li> <li>The second-highest posting period is weekday mornings (05:00\u201312:00).</li> <li>Posts are relatively fewer during evenings (18:00\u201301:00) on both weekdays and weekends.</li> </ul> <p>These trends are reasonable, as students primarily post during the day. The relatively high number of early morning posts might be due to companies posting content before students wake up.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#popular-post-proportion-by-time","title":"Popular Post Proportion by Time","text":"<p>Next, we analyze whether certain time periods have a higher proportion of popular posts:</p> <p></p> <p>Info</p> <p>Observations:</p> <ul> <li>Posts during late-night and early-morning hours on weekends have a higher likelihood of being popular, likely due to increased user activity during these times.</li> <li>The heatmap confirms that the proportion of popular posts varies by time.</li> </ul>"},{"location":"side-projects/dcard-hw/docs/1-eda/#average-likes-within-10-hours-by-time","title":"Average Likes Within 10 Hours by Time","text":"<p>We then examine the average likes within 10 hours for posts made at different times:</p> <p></p> <p>Info</p> <p>Observations:</p> <ul> <li>Posts made between 21:00\u201311:00 generally receive more likes within 10 hours.</li> <li>Posts made between 11:00\u201321:00, especially during late afternoon and dinner hours, receive fewer likes on average.</li> </ul> <p>This difference might be because students are less active during late afternoon and dinner hours but more active during the evening. Early morning posts are also visible to students the next day.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#average-likes-within-36-hours-by-time","title":"Average Likes Within 36 Hours by Time","text":"<p>Finally, we analyze the average likes within 36 hours for posts made at different times:</p> <p></p> <p>The trends are consistent with the 10-hour analysis and are not elaborated further.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#summary","title":"Summary","text":"<p>Info</p> <p>Key takeaways:</p> <ul> <li>Variables are generally highly correlated. Polynomial transformations (<code>PolynomialFeatures</code>) may not yield significant improvements during feature engineering.</li> <li>Posting time significantly impacts the proportion of popular posts and the number of likes, and this information should be incorporated into the model.</li> </ul>"},{"location":"side-projects/dcard-hw/docs/2-training/","title":"Feature Engineering","text":"<p>After conducting exploratory data analysis (EDA), we gained a deeper understanding of the training dataset. Before diving into feature engineering, we first organize the training dataset into the following format:</p> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 793751 entries, 0 to 793750\nData columns (total 8 columns):\npost_key           793751 non-null object\nshared_count       793751 non-null int64\ncomment_count      793751 non-null int64\nlike_count         793751 non-null int64\ncollected_count    793751 non-null int64\nweekday            793751 non-null int64\nhour               793751 non-null int64\nis_trending        793751 non-null int64\ndtypes: bool(1), int64(6), object(1)\nmemory usage: 43.1+ MB\n</code></pre> <p>In this section, we will discuss the techniques and models used throughout the data pipeline, including over/undersampling, polynomial transformations, one-hot encoding, and tree-based models.</p> <p>Info</p> <p>The training process can be divided into three main stages:</p> <ol> <li>Resampling</li> <li>Column Transformation</li> <li>Classification</li> </ol> <p>These stages can be represented as the following <code>Pipeline</code> object:</p> <pre><code>cachedir = mkdtemp()\npipe = Pipeline(steps=[('resampler', 'passthrough'),\n                       # ('columntransformer', 'passthrough'),\n                       ('classifier', 'passthrough')],\n                memory=cachedir)\n</code></pre> <p>For each stage, we experiment with two to three different approaches and several hyperparameter settings to identify the optimal combination.</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#handling-imbalanced-datasets-stage-1","title":"Handling Imbalanced Datasets (STAGE 1)","text":"<p>In a binary classification problem, an imbalanced dataset refers to a scenario where the target variable (\\(y\\)) is predominantly of one class (majority) with only a small proportion belonging to the other class (minority). </p> <p>Training a model on such a dataset without addressing the imbalance often results in a biased model that predicts most samples as the majority class, ignoring valuable information from the minority class.</p> <p>A potential solution is resampling, which can be categorized into oversampling and undersampling:</p> <ul> <li>Oversampling: Increases the proportion of minority samples in the dataset.</li> <li>Undersampling: Reduces the proportion of majority samples in the dataset.</li> </ul> <p>Both methods help the model pay more attention to minority samples during training. The simplest approach is random sampling, where majority samples are removed, or minority samples are duplicated.</p> <p>The <code>imblearn</code> library provides implementations for various resampling techniques, including <code>RandomOverSampler</code> and <code>RandomUnderSampler</code>. Additionally, we utilize <code>SMOTE</code> and <code>NearMiss</code>. Below is a brief overview:</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#smote","title":"SMOTE","text":"<p>SMOTE (Synthetic Minority Oversampling Technique) is an oversampling method that synthesizes new minority samples between existing ones, increasing the proportion of the minority class. The following diagram illustrates this:</p> <p> Image Source</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#nearmiss","title":"NearMiss","text":"<p>NearMiss is an undersampling method with three versions. We focus on NearMiss-1, which calculates the average distance of all majority samples to their \\(k\\) nearest minority neighbors and removes the majority samples closest to the minority samples until the class ratio is 1:1. The following diagram illustrates this:</p> <p> Image Source</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#polynomial-transformation-and-one-hot-encoding-stage-2","title":"Polynomial Transformation and One-hot Encoding (STAGE 2)","text":"<p>Next, we use <code>sklearn</code>'s <code>PolynomialFeatures</code> and <code>OneHotEncoder</code> to transform specific features:</p> <ul> <li>For <code>shared_count</code>, <code>comment_count</code>, <code>liked_count</code>, and <code>collected_count</code>, we apply second-degree polynomial transformations to capture non-linear relationships and interactions between features.</li> <li>For the <code>weekday</code> feature, we convert integer values (<code>0</code> - <code>6</code>, representing Monday to Sunday) into one-hot encoded vectors, e.g., <code>[1, 0, 0, 0, 0, 0, 0]</code> for Monday.</li> </ul>"},{"location":"side-projects/dcard-hw/docs/2-training/#tree-based-ensemble-models-stage-3","title":"Tree-based Ensemble Models (STAGE 3)","text":"<p>For classification, we primarily use tree-based ensemble models, including <code>AdaBoostClassifier</code>, <code>GradientBoostingClassifier</code>, and <code>XGBClassifier</code>. These models are chosen for several reasons:</p> <ul> <li>They are invariant to monotonic transformations of features, reducing the need for extensive feature engineering.</li> <li>They offer high interpretability, making it easier to understand feature importance.</li> <li>They perform well on large and complex datasets and are often top performers in Kaggle competitions (e.g., <code>XGBoost</code>, <code>LightGBM</code>, <code>CatBoost</code>).</li> </ul> <p>Ensemble learning can be categorized into Bagging (bootstrap aggregating) and Boosting.</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#bagging","title":"Bagging","text":"<p>The most well-known Bagging application is Random Forest, which builds multiple decision trees using bootstrap sampling and random feature selection. Each tree learns a subset of features, and their predictions are aggregated for the final result.</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#boosting","title":"Boosting","text":""},{"location":"side-projects/dcard-hw/docs/2-training/#adaptive-boosting","title":"Adaptive Boosting","text":"<p>Adaptive Boosting (AdaBoost) sequentially builds \\(T\\) weak learners \\(h_t(x)\\), with each model focusing on samples misclassified by the previous one. Each model is assigned a weight \\(\\alpha_t\\) based on its performance:</p> <ul> <li>Higher weights indicate better performance.</li> <li>Lower weights indicate worse performance.</li> </ul> <p>The final model \\(H(x)\\) aggregates the predictions of all \\(T\\) weak learners. For more details, refer to this note.</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#gradient-boosting","title":"Gradient Boosting","text":"<p>Gradient Boosting builds \\(T\\) models \\(h_t(x)\\) sequentially, where each model predicts the gradient (pseudo-residuals) of the previous model's errors. The final model \\(H(x)\\) is the sum of all previous models. For mathematical derivations, refer to this note.</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#extreme-gradient-boosting","title":"Extreme Gradient Boosting","text":"<p>XGBoost is an optimized implementation of Gradient Boosting with enhancements like weighted quantile sketch, parallel learning, and cache-aware access. For more details, refer to this paper.</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#hyperparameter-optimization","title":"Hyperparameter Optimization","text":"<p>After understanding the techniques used in each stage, we define the possible methods and hyperparameter combinations for each stage as follows:</p> <p>Resampler - <code>passthrough</code>: No resampling. - <code>NearMiss</code>: Default parameters. - <code>SMOTE</code>: Default parameters.</p> <p>Column Transformer - <code>passthrough</code>: No feature transformation. - <code>col_trans</code>: Apply polynomial transformations and one-hot encoding.</p> <p>Classifier - <code>AdaBoostClassifier</code>: Default parameters with tree depth limited to <code>[1, 2, 3]</code>. - <code>GradientBoostingClassifier</code>, <code>XGBClassifier</code>: Default parameters with learning rates <code>[0.025, 0.05, 0.1]</code>.</p> <p>For all classifiers, we set the number of decision trees to <code>[90, 100, 110, 120]</code> and tune additional hyperparameters.</p> <p>Initially, there are 216 combinations to test, which is too many given time constraints. Experiments show that models with feature transformations generally perform worse, likely due to high feature correlations identified during EDA. As a result, we omit the \"Feature Transformation\" stage, reducing the combinations to 108. We use <code>GridSearchCV</code> with <code>cv=3</code> to find the best combination.</p> <p>The parameter grid is defined as follows:</p> <pre><code># poly_cols = ['shared_count', 'comment_count', 'liked_count', 'collected_count']\n# col_trans = make_column_transformer((OneHotEncoder(dtype='int'), ['weekday']),\n#                                     (PolynomialFeatures(include_bias=False), poly_cols),\n#                                     remainder='passthrough')\nparam_grid_ada = {\n    'resampler': ['passthrough', SMOTE(), NearMiss()],\n    # 'columntransformer': ['passthrough', col_trans],\n    'classifier': [AdaBoostClassifier()],\n    'classifier__n_estimators': [90, 100, 110, 120],\n    'classifier__base_estimator': [DecisionTreeClassifier(max_depth=1), \n                                   DecisionTreeClassifier(max_depth=2),\n                                   DecisionTreeClassifier(max_depth=3)]\n}\nparam_grid_gb = {\n    'resampler': ['passthrough', SMOTE(), NearMiss()],\n    # 'columntransformer': ['passthrough', col_trans],\n    'classifier': [GradientBoostingClassifier(), XGBClassifier()],\n    'classifier__n_estimators': [90, 100, 110, 120],\n    'classifier__learning_rate': [0.025, 0.05, 0.1]\n}\nparam_grid = [param_grid_ada, param_grid_gb]\n</code></pre>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/","title":"Results and Discussion","text":""},{"location":"side-projects/dcard-hw/docs/3-evaluation/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>Before discussing the results, let us revisit some commonly used metrics for binary classification, explained using this assignment as an example:</p> Actual\uff3cPredicted Negative Positive Negative \\(\\color{red}{\\text{TN}}\\) \\(\\color{blue}{\\text{FP}}\\) Positive \\(\\color{green}{\\text{FN}}\\) \\(\\color{orange}{\\text{TP}}\\) <p>\\(\\text{Precision}\\): Measures the proportion of articles predicted as popular that are actually popular. Higher values indicate greater trust in the model's predictions for popular articles. Formula: $$ \\text{Precision} = \\frac{\\color{orange}{\\text{TP}}}{\\color{blue}{\\text{FP}} + \\color{orange}{\\text{TP}}} $$</p> <p>\\(\\text{Recall}\\): Measures the proportion of actual popular articles that are correctly predicted by the model. Also known as True Positive Rate (TPR) or Sensitivity. Higher values indicate the model's ability to capture actual popular articles. Formula: $$ \\text{Recall} = \\dfrac{\\color{orange}{\\text{TP}}}{\\color{green}{\\text{FN}} + \\color{orange}{\\text{TP}}} $$ </p> <p>\\(\\text{Specificity}\\): Measures the proportion of actual non-popular articles that are correctly predicted by the model. Also known as True Negative Rate (TNR). Higher values indicate the model's ability to capture actual non-popular articles. Formula: $$ \\text{Specificity} = \\dfrac{\\color{red}{\\text{TN}}}{\\color{red}{\\text{TN}}+\\color{blue}{\\text{FP}}} $$</p> <p>\\(\\text{F1-score}\\): A harmonic mean of \\(\\text{Precision}\\) and \\(\\text{Recall}\\), ranging from \\(0\\) to \\(1\\), with higher values being better. Formula: $$ \\text{F1-score} = \\dfrac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$</p> <p>\\(\\text{Balanced Acc.}\\): A combined metric of \\(\\text{TPR}\\) and \\(\\text{TNR}\\), ranging from \\(0\\) to \\(1\\), with higher values being better. Formula: $$ \\text{Balanced Acc.} = \\dfrac{\\text{TNR} + \\text{TPR}}{2} $$</p> <p>When using <code>GridSearchCV</code> to find the best parameter combination, we record these five metrics and select the best combination based on the f1-score. Example code: <pre><code>scoring = {\n    'precision': 'precision',\n    'recall': 'recall',\n    'specificity': make_scorer(specificity_score),\n    'balanced_accuracy': 'balanced_accuracy',\n    'f1_score': 'f1',\n}\ngrid_search = GridSearchCV(pipe, param_grid=param_grid, scoring=scoring, refit='f1_score', \n                           n_jobs=-1, cv=3, return_train_score=True)\n</code></pre></p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#experimental-results","title":"Experimental Results","text":"<p>Info</p> <p>The best model is <code>AdaBoostClassifier</code> without any resampling, consisting of 100 decision trees with a maximum depth of 2. The average f1-score during cross-validation is 0.56, while the f1-score on the public test set is 0.53. Detailed prediction information is as follows:</p> <p></p> Note <pre><code>===================GETTING CONNECTOR START!==================\n============================DONE!============================\n====================GETTING TABLES START!====================\nposts_test                 Total:   225,986 rows, 3 columns\npost_shared_test           Total:    83,376 rows, 3 columns\npost_comment_created_test  Total:   607,251 rows, 3 columns\npost_liked_test            Total:   908,910 rows, 3 columns\npost_collected_test        Total:   275,073 rows, 3 columns\n============================DONE!============================\n====================MERGING TABLES START!====================\n============================DONE!============================\n================PREPROCESSING TOTAL_DF START!================\n============================DONE!============================\n==================PREDICTING TESTSET START!==================\nf1-score     = 0.53\nbalanced acc = 0.70\n\n            precision    recall  f1-score   support\n\n        0       0.99      1.00      0.99    221479\n        1       0.75      0.40      0.53      4507\n\n    accuracy                           0.99    225986\nmacro avg       0.87      0.70      0.76    225986\nweighted avg       0.98      0.99      0.98    225986\n\n============================DONE!============================\n</code></pre> <p>Now, let us analyze the experimental results. (All figures below are based on cross-validation results, not the entire training set or public test set.)</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#resampler","title":"Resampler","text":"<p>First, let us examine how different resampling strategies affect the f1-score: </p> <p>Info</p> <p>Different resampling strategies indeed affect the f1-score:</p> <ul> <li>NearMiss (undersampling) has the lowest f1-score, likely due to excessive removal of non-popular articles, losing too much majority class information.</li> <li>SMOTE (oversampling) achieves a moderate f1-score.</li> <li>No resampling achieves the highest f1-score.</li> </ul> <p>Next, we investigate how these resampling strategies impact precision and recall:</p> <p></p> <p>Info</p> <ul> <li>NearMiss and SMOTE significantly increase the model's focus on the minority class, resulting in excellent recall scores of 0.91 and 0.95, respectively. However, this comes at the cost of precision, which drops to 0.07 and 0.20, respectively.</li> <li>In other words, resampling strategies can capture actual popular articles but reduce the trustworthiness of the predicted popular articles.</li> </ul> <p>We further explore whether resampling strategies interact with different classifiers to influence the f1-score:</p> <p></p> <p>Info</p> <ul> <li>Under \"SMOTE\" and \"No Resampling\" strategies, different classifiers do not significantly affect the f1-score.</li> <li>However, under the NearMiss strategy, <code>XGBClassifier</code> achieves the highest f1-score (0.18), while <code>AdaBoostClassifier</code> has the lowest (0.07).<ul> <li><code>AdaBoostClassifier</code> performs poorly because it relies on weak classifiers, which struggle with limited majority class information.</li> <li><code>XGBClassifier</code> outperforms <code>GradientBoostingClassifier</code> due to its optimized GBDT implementation.</li> </ul> </li> </ul>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#classifier","title":"Classifier","text":"<p>Next, let us examine how different classifiers affect the f1-score:</p> <p></p> <p>Info</p> <ul> <li>Different classifiers have minimal impact on the f1-score. On average, <code>XGBClassifier</code> achieves the highest score (0.35), primarily due to its performance under the NearMiss strategy.</li> </ul> <p>Finally, we analyze whether the number of internal classifiers in ensemble models affects the f1-score:</p> <p></p> <p>Clearly, the number of classifiers has little impact. Similarly, the tree depth for <code>AdaBoostClassifier</code> and the learning rate for the other two models also have minimal impact on the f1-score (figures omitted).</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#future-directions","title":"Future Directions","text":"<p>The experimental results are summarized above. Due to time constraints, additional attempts were not included. Potential future directions are outlined below:</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#explore-other-resampling-techniques","title":"Explore Other Resampling Techniques","text":"<p>Resampling techniques can increase the model's focus on the minority class. Although the experimental results were not ideal, we can continue fine-tuning hyperparameters or exploring other resampling techniques. Refer to the \"Over-sampling\" and \"Under-sampling\" sections of the <code>imblearn</code> User Guide for potential directions.</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#consider-other-evaluation-metrics","title":"Consider Other Evaluation Metrics","text":"<p>The assignment requires using f1-score as the evaluation metric. However, if we use balanced accuracy instead, the best model would be a <code>GradientBoostingClassifier</code> trained with SMOTE, consisting of 120 classifiers and a learning rate of 0.1, achieving a balanced accuracy of 0.93.</p> <p>The impact of different resampling strategies on balanced accuracy is shown below:</p> <p></p> <p>Info</p> <p>SMOTE achieves the highest balanced accuracy. If the goal is to preliminarily identify potentially popular articles for subsequent workflows, balanced accuracy might be a better evaluation metric.</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#explore-other-feature-transformations-and-classifiers","title":"Explore Other Feature Transformations and Classifiers","text":"<p>The experiment only considered tree-based ensemble models, which require minimal feature transformation. However, we could explore logistic regression, support vector machines, Poisson regression, etc., combined with effective feature transformations. For example, converting <code>weekday</code> and <code>hour</code> into circular coordinates (refer to this post) could improve model performance.</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#incorporate-sequential-information","title":"Incorporate Sequential Information","text":"<p>The experiment ignored the \"time trends\" of shares, comments, likes, and collections within 10 hours of posting. One potential direction is to use recurrent neural networks (RNN, LSTM, GRU, etc.) to capture these trends and nonlinear relationships between variables.</p> <p>A simple approach is to combine the four count variables into a 4-dimensional vector (e.g., <code>[4, 23, 17, 0]</code> for 4 shares, 23 comments, etc.), with a sequence length of 10. Each article's sequential information would then be a <code>(10, 4)</code> matrix, which can be fed into the model for training.</p> <p>For details on LSTM models, refer to my notes.</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#explore-other-hyperparameter-optimization-methods","title":"Explore Other Hyperparameter Optimization Methods","text":"<p>The experiment used <code>GridSearchCV</code> for hyperparameter optimization. However, <code>RandomizedSearchCV</code> might be a better choice for optimizing a large number of hyperparameter combinations. Refer to this 2012 JMLR paper for details.</p> <p>Additionally, consider Bayesian optimization implementations provided by <code>optuna</code> or <code>hyperopt</code>. Watch this video for details, and compare the two libraries in this article.</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#incorporate-text-data-and-user-behavior","title":"Incorporate Text Data and User Behavior","text":"<p>The assignment does not include text data or user behavior. Since the ultimate goal is to \"recommend articles more accurately to users,\" consider incorporating Latent Dirichlet Allocation (LDA) topic modeling to enrich article topic information. Refer to my presentation for details on LDA.</p> <p>Additionally, combining user behavior data could enable more refined personalized text recommendations. Refer to this video and this paper for details.</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#tags-dcard","title":"tags: <code>dcard</code>","text":""},{"location":"side-projects/dcard-hw/docs/4-resampling/","title":"Resampling","text":""},{"location":"side-projects/dcard-hw/docs/4-resampling/#over-sampling","title":"Over Sampling","text":""},{"location":"side-projects/dcard-hw/docs/4-resampling/#under-sampling","title":"Under Sampling","text":""},{"location":"side-projects/restful-apis-with-flask/","title":"RESTful APIs with Flask","text":"<p>This repository documents my journey of learning how to build RESTful APIs using Flask. It includes step-by-step implementations of various concepts, from basic API design principles to advanced features like authentication, database integration, deployment, and third-party integrations. </p> <p>The content is based on two Udemy courses: \"REST APIs with Flask and Python\" and \"Advanced REST APIs with Flask and Python\". Each section highlights key topics, tools, and techniques, making it a comprehensive resource for anyone looking to learn Flask for API development.</p> <p></p> <p>Certificate</p> <p></p> <p>Certificate</p>"},{"location":"side-projects/restful-apis-with-flask/#about-this-project","title":"About This Project","text":"<p>The goal of this project is to: - Learn and experiment with Flask for building RESTful APIs. - Understand best practices for API design and implementation. - Explore integrations with databases, authentication, and other web technologies.</p>"},{"location":"side-projects/restful-apis-with-flask/#features","title":"Features","text":"<ul> <li>RESTful API Design: Follows REST principles for clean and scalable APIs.</li> <li>Flask Framework: Built using Flask for simplicity and flexibility.</li> <li>Database Integration: Includes examples of working with databases like SQLite or SQLAlchemy.</li> <li>Authentication: Demonstrates how to secure APIs with authentication mechanisms.</li> <li>Error Handling: Implements robust error handling for better user experience.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/#how-to-use-this-repository","title":"How to Use This Repository","text":"<p>Feel free to browse the code, read the documentation, and run the examples. If you're new to Flask or REST APIs, this project can serve as a learning resource.</p>"},{"location":"side-projects/restful-apis-with-flask/#getting-started","title":"Getting Started","text":"<ol> <li> <p>Clone the repository:     <pre><code>gh repo clone your-username/rest-apis-with-flask\ncd rest-apis-with-flask\n</code></pre></p> </li> <li> <p>Install dependencies:     <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Run the application:     <pre><code>python app.py\n</code></pre></p> </li> </ol> <p>Thank you for visiting, and I hope you find this project helpful!</p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/","title":"\u5c07 Flask \u61c9\u7528\u7a0b\u5f0f\u90e8\u7f72\u5728 Ubuntu 16.04 Server","text":"<p>\u9996\u5148\uff0c\u6211\u5011\u5fc5\u9808\u8a3b\u518a DigitalOcean \u5e33\u865f\u4e26\u5728\u4e0a\u9762\u79df\u7528\u4e00\u500b\u865b\u64ec\u4e3b\u6a5f\uff0c\u7248\u672c\u70ba Ubuntu 16.04 Server\u3002\u5982\u4f55\u4ee5 SSH \u9023\u7dda\u4e0d\u662f\u8ab2\u7a0b\u91cd\u9ede\uff0c\u56e0\u6b64\u6211\u5011\u5728\u6b64\u5148\u7565\u904e\u3002\u9023\u7dda\u5f8c\u7dca\u63a5\u8457\u9032\u884c\u5e7e\u9805\u4e8b\u524d\u6e96\u5099\uff1a</p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#_1","title":"\u66f4\u65b0\u5009\u5eab\u6e05\u55ae","text":"<pre><code># apt-get update\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#os","title":"\u5728 OS \u4e0a\u5275\u5efa\u65b0\u4f7f\u7528\u8005","text":"<pre><code># adduser &lt;username&gt;\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#super-user","title":"\u7d66\u4e88 super user \u6b0a\u9650","text":"<p>\u9032\u5165 <code>/etc/sudoers</code> \u6a94\u6848\uff1a <pre><code># visudo\n</code></pre></p> <p>\u5728 \"User privilege specification\" \u4e0b\u65b9\u66ff\u65b0\u4f7f\u7528\u8005\u52a0\u5165 super user \u7684\u6b0a\u9650\uff1a <pre><code>&lt;username&gt; ALL=(ALL:ALL) ALL\n</code></pre></p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#1-postresql","title":"1. \u5b89\u88dd\u4e26\u8a2d\u5b9a PostreSQL \u8cc7\u6599\u5eab","text":""},{"location":"side-projects/restful-apis-with-flask/basics/section9/#postgresql","title":"\u5b89\u88dd PostgreSQL","text":"<pre><code># apt-get install postgresql postgresql-contrib\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#postgres","title":"\u5207\u63db\u6210 <code>postgres</code> \u4f7f\u7528\u8005","text":"<pre><code># sudo -i -u postgres\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#postgresql_1","title":"\u66ff\u65b0\u4f7f\u7528\u8005\uff0c\u5275\u5efa PostgreSQL \u7576\u4e2d\u7684\u5e33\u865f\u548c\u8cc7\u6599\u5eab","text":"<pre><code>$ createuser &lt;username&gt; -P\n$ createdb &lt;username&gt;\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#postgresql_2","title":"\u5f37\u5236\u4ee5\u5bc6\u78bc\u767b\u5165 PostgreSQL","text":"<p>\u9032\u5165 <code>pg_hba.conf</code> \u914d\u7f6e\u6a94\uff1a <pre><code>$ nano /etc/postgresql/10/main/pg_hba.conf\n</code></pre></p> <p>\u5c07 <pre><code>local all all peer\n</code></pre> \u6539\u70ba <pre><code>local all all md5\n</code></pre></p> <p>\u5b8c\u6210\u5f8c\u8df3\u51fa\u914d\u7f6e\u6a94\uff0c\u63a5\u8457\u6539\u70ba\u65b0\u4f7f\u7528\u8005\u4f86\u64cd\u4f5c\u3002</p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#2-nginx","title":"2. \u5b89\u88dd\u4e26\u8a2d\u5b9a Nginx \u4f3a\u670d\u5668","text":""},{"location":"side-projects/restful-apis-with-flask/basics/section9/#nginx","title":"\u5b89\u88dd Nginx","text":"<pre><code>$ sudo apt-get install nginx\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#nginx-ssh","title":"\u958b\u555f\u9632\u706b\u7246\u4e26\u5141\u8a31 <code>nginx</code> \u548c <code>ssh</code>","text":"<pre><code>$ sudo ufw enable\n$ sudo ufw allow 'Nginx HTTP'\n$ sudo ufw allow ssh\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#flask-nginx","title":"\u66ff\u6211\u5011\u7684 Flask \u61c9\u7528\u7a0b\u5f0f\u52a0\u5165 Nginx \u914d\u7f6e\u6a94","text":"<p>\u5275\u5efa\u4e26\u9032\u5165 <code>items-rest.conf</code> \u914d\u7f6e\u6a94\uff1a <pre><code>$ sudo nano /etc/nginx/sites-available/items-rest.conf\n</code></pre></p> <p>\u8f38\u5165\u4ee5\u4e0b\u5167\u5bb9\uff1a <pre><code>server {\n    listen 80;\n    real_ip_header X-Forwarded-For;\n    set_real_ip_from 127.0.0.1;\n    server_name localhost;\n\n    location / {\n        include uwsgi_params;\n        uwsgi_pass unix:/var/www/html/items-rest/socket.sock;\n        uwsgi_modifier1 30;\n    }\n\n    error_page 404 /404.html;\n    location = 404.html {\n        root /usr/share/nginx/html;\n    }\n\n    error_page 500 502  503 504 50x.html;\n    location = /50x.html {\n        root /usr/share/nginx/html;\n    }\n}\n</code></pre></p> <p>\u5b8c\u6210\u5f8c\u8df3\u51fa\u914d\u7f6e\u6a94\uff0c\u63a5\u8457\u5efa\u7acb soft link\uff0c\u555f\u7528\u914d\u7f6e\uff1a <pre><code>$ sudo ln -s /etc/nginx/sites-available/items-rest.conf /etc/nginx/sites-enabled/\n</code></pre></p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#3-flask","title":"3. \u8a2d\u5b9a Flask \u61c9\u7528\u7a0b\u5f0f\u6240\u9700\u57f7\u884c\u74b0\u5883","text":""},{"location":"side-projects/restful-apis-with-flask/basics/section9/#_2","title":"\u5275\u5efa\u5c08\u6848\u76ee\u9304\u4e26\u7d66\u4e88\u9069\u7576\u6b0a\u9650","text":"<pre><code>$ sudo mkdir /var/www/html/items-rest\n$ sudo chown &lt;username&gt;:&lt;username&gt; /var/www/html/items-rest\n</code></pre> <p>\u5b8c\u6210\u5f8c\u9032\u5165\u8a72\u76ee\u9304\uff1a <pre><code>$ cd /var/www/html/items-rest\n</code></pre></p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#_3","title":"\u8a2d\u5b9a\u5c08\u6848\u6240\u9700\u74b0\u5883","text":"<p>\u4e0b\u8f09\u5c08\u6848\u5167\u5bb9\u4e26\u5275\u5efa\u65e5\u8a8c\u6a94\u76ee\u9304\uff1a <pre><code>$ git clone https://github.com/schoolofcode-me/stores-rest-api.git .\n$ mkdir log\n</code></pre></p> <p>\u5efa\u7acb\u865b\u64ec\u74b0\u5883\u4e26\u5b89\u88dd\u6240\u9700\u5957\u4ef6\uff1a <pre><code>$ sudo apt-get install python-pip python3-dev libpq-dev\n$ pip install virtualenv\n$ virtualenv venv --python=python3.6\n$ source venv/bin/activate\n(venv)$ pip install -r requirements.txt\n</code></pre></p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#4-uwsgi","title":"4. \u8a2d\u5b9a uWSGI \u4f3a\u670d\u5668","text":""},{"location":"side-projects/restful-apis-with-flask/basics/section9/#uwsgi_items_restservice-ubuntu","title":"\u5275\u5efa <code>uwsgi_items_rest.service</code> Ubuntu \u670d\u52d9","text":"<p>\u5275\u5efa\u4e26\u9032\u5165 <code>uwsgi_items_rest.service</code> \u6a94\uff1a <pre><code>$ sudo nano /etc/systemd/system/uwsgi_items_rest.service\n</code></pre></p> <p>\u8f38\u5165\u4ee5\u4e0b\u5167\u5bb9\uff1a <pre><code>[Unit]\nDescription=uWSGI items rest\n\n[Service]\nEnvironment=DATABASE_URL=postgres://jose:1234@localhost:5432/jose\nExecStart=/var/www/html/items-rest/venv/bin/uwsgi --master --emperor /var/www/html/items-rest/uwsgi.ini --die-on-term --uid jose --gid jose --logto /var/www/html/items-rest/log/emperor.log\nRestart=always\nKillSignal=SIGQUIT\nType=notify\nNotifyAccess=all\n\n[Install]\nWantedBy=multi-user.target\n</code></pre></p> <p>\u5b8c\u6210\u5f8c\u8df3\u51fa\u6a94\u6848\u3002</p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#uwsgiini","title":"\u4fee\u6539 <code>uwsgi.ini</code> \u914d\u7f6e\u6a94","text":"<p>\u9032\u5165\u5c08\u6848\u5167\u7684 <code>uwsgi.ini</code> \u914d\u7f6e\u6a94\uff1a <pre><code>$ nano uwsgi.ini\n</code></pre></p> <p>\u8f38\u5165\u4ee5\u4e0b\u5167\u5bb9\uff1a <pre><code>[uwsgi]\nbase = /var/www/html/items-rest\napp = run\nmodule = %(app)\n\nhome = %(base)/venv\npythonpath = %(base)\n\nsocket = %(base)/socket.sock\n\nchmod-socket = 777\n\nprocesses = 8\n\nthreads = 8\n\nharakiri = 15\n\ncallable = app\n\nlogto = /var/www/html/items-rest/log/%n.log\n</code></pre></p> <p>\u5b8c\u6210\u5f8c\u8df3\u51fa\u914d\u7f6e\u6a94\u3002</p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#5-flask","title":"5. \u555f\u52d5 Flask \u61c9\u7528\u7a0b\u5f0f","text":"<p>\u522a\u9664 Nginx \u9810\u8a2d\u914d\u7f6e\u6a94\uff0c\u907f\u514d\u8b80\u53d6\u932f\u8aa4\u7684\u914d\u7f6e\u6a94\uff0c\u63a5\u8457 reload \u4e26 restart\uff1a <pre><code>$ sudo rm /etc/nginx/sites-enabled/default\n$ sudo systemctl reload nginx \n$ sudo systemctl restart nginx\n</code></pre></p> <p>\u555f\u52d5 <code>uwsgi_items_rest</code> \u670d\u52d9\uff1a <pre><code>$ sudo systemctl start uwsgi_items_rest\n</code></pre></p> <p>\u5b8c\u6210\uff01</p>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/","title":"Advanced","text":""},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-1-course-preparations","title":"Section 1: Course Preparations","text":"<p>Preparations for the course:</p> <ul> <li>Simplified authentication mechanism.</li> <li>Added type hinting.</li> <li>Unified code style.</li> <li>Changed all <code>Resource</code> methods to class methods (using <code>@classmethod</code>).</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-2-marshmallow-integration","title":"Section 2: Marshmallow Integration","text":"<p>Introducing <code>marshmallow</code>, <code>flask-marshmallow</code>, and <code>marshmallow-sqlalchemy</code>:</p> <ul> <li>Simplified request parsing, <code>Model</code> object creation, and JSON responses by defining <code>Schema</code> for each <code>Resource</code>.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-3-email-verification","title":"Section 3: Email Verification","text":"<ul> <li>Implemented user email verification process (using Mailgun).</li> <li>Used <code>.env</code> files to store sensitive data.</li> <li>Returned <code>.html</code> files in <code>Flask-RESTful</code> using <code>make_response()</code> and <code>render_template()</code>.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-4-optimized-email-verification","title":"Section 4: Optimized Email Verification","text":"<p>Optimized the email verification process:</p> <ul> <li>Added expiration for verification and resend functionality.</li> <li>Refactored project structure by treating <code>confirmation</code> as a resource.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-6-secure-configuration-and-file-uploads","title":"Section 6: Secure Configuration and File Uploads","text":"<ul> <li>Configured the application more securely (using <code>from_object()</code> and <code>from_envvar()</code>).</li> <li>Learned the relationships between <code>WSGI</code>, <code>uwsgi</code>, <code>uWSGI</code>, and <code>Werkzeug</code>.</li> <li>Introduced <code>Flask-Uploads</code> for handling file uploads, downloads, and deletions (using <code>UploadSet</code>, <code>FileStorage</code>).</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-7-database-version-control","title":"Section 7: Database Version Control","text":"<ul> <li>Introduced <code>Flask-Migrate</code> for database version control, including adding, deleting, and modifying details.</li> <li>Common commands include <code>flask db init</code>, <code>flask db upgrade</code>, <code>flask db downgrade</code>.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-8-oauth-integration","title":"Section 8: OAuth Integration","text":"<ul> <li>Learned OAuth third-party login flow (e.g., GitHub), including authentication, authorization, and obtaining <code>access_token</code>.</li> <li>Introduced <code>Flask-OAuthlib</code>.</li> <li>Used Flask's <code>g</code> to store <code>access_token</code>.</li> <li>Allowed third-party login users to set passwords.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-9-payment-integration","title":"Section 9: Payment Integration","text":"<ul> <li>Integrated <code>Stripe</code> for third-party payment processing.</li> <li>Added an \"Order\" resource and implemented many-to-many relationships using <code>Flask-SQLAlchemy</code>.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/","title":"Basics","text":""},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-3-introduction-to-flask","title":"Section 3: Introduction to Flask","text":"<ul> <li>Introduction to the Flask web framework, using decorators to set up application routes.</li> <li>Understanding common HTTP request methods: GET, POST, PUT, DELETE.</li> <li>Understanding common HTTP status codes: 200, 201, 202, 401, 404.</li> <li>Understanding RESTful API design principles focusing on \"resources\" and statelessness.</li> <li>Implementing a RESTful API server application.</li> <li>Testing APIs using the Postman application.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-4-flask-restful-and-jwt","title":"Section 4: Flask-RESTful and JWT","text":"<ul> <li>Implementing RESTful API server applications using <code>Flask-RESTful</code>.</li> <li>Implementing JSON Web Token (JWT) authentication using <code>Flask-JWT</code>.</li> <li>Parsing user input JSON data using <code>RequestParser</code>.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-5-database-integration-with-sqlite","title":"Section 5: Database Integration with SQLite","text":"<ul> <li>Introducing <code>sqlite3</code> to store user and item information in a database.</li> <li>Implementing user registration functionality.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-6-database-integration-with-sqlalchemy","title":"Section 6: Database Integration with SQLAlchemy","text":"<ul> <li>Introducing <code>Flask-SQLAlchemy</code> to interact with the database using ORM.</li> <li>Adding store information with a one-to-many relationship to items.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-7-deploying-to-heroku","title":"Section 7: Deploying to Heroku","text":"<p>Deploying the Flask application to Heroku and using Heroku's PostgreSQL. Steps:</p> <ol> <li>Modify the project locally (e.g., add <code>Procfile</code>, <code>runtime.txt</code>, <code>uwsgi.ini</code>), then <code>commit</code> and <code>push</code> to the specified GitHub repo.</li> <li>Register on Heroku, create an application, connect it to the GitHub repo, and add the <code>heroku/python</code> buildpack and <code>Heroku Postgres</code> add-on.</li> <li>Install the Heroku CLI locally (see here) and log in using <code>heroku login</code>.</li> <li>Add a Heroku remote using <code>heroku git:remote -a &lt;app-name&gt;</code>.</li> <li>Deploy the project by pushing the <code>basics/section8</code> subdirectory to Heroku using <code>git subtree push --prefix basics/section8 heroku master</code>.</li> </ol> <p>Testing: Access here to retrieve all stores and their items in the database, returned in JSON format.</p>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-8-deploying-to-digitalocean","title":"Section 8: Deploying to DigitalOcean","text":"<p>Deploying the Flask application to a DigitalOcean Droplet. Steps:</p> <ol> <li>Register on DigitalOcean, create a Droplet with Ubuntu 16.04, set up SSH, and connect using PuTTY.</li> <li>Create a new user on the operating system.</li> <li>Install and configure PostgreSQL, including creating a new user and database with appropriate permissions.</li> <li>Install and configure the Nginx server, including firewall settings, error pages, and uwsgi parameters.</li> <li>Set up a Python virtual environment, install required packages, and clone the project from GitHub.</li> <li>Configure an Ubuntu service to run the uwsgi server, including log directories, processes, and threads.</li> </ol> <p>Testing: Access here (created on 2020/05/30) to retrieve all stores and their items in the database, returned in JSON format.</p>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-9-domain-and-https","title":"Section 9: Domain and HTTPS","text":"<p>Book</p> <ul> <li>Registering a domain and configuring DNS servers.</li> <li>Obtaining an SSL certificate for HTTPS communication and configuring Nginx.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-11-advanced-jwt-features","title":"Section 11: Advanced JWT Features","text":"<p>Introducing <code>Flask-JWT-Extended</code>:</p> <ul> <li>Implementing token-refreshing to improve user experience by avoiding frequent logins while requiring re-login for critical actions for security (using <code>@jwt_refresh_token_required</code>, <code>create_refresh_token()</code>, <code>create_access_token()</code>).</li> <li>Responding with appropriate data based on user roles (visitor, user, admin) using <code>@jwt.user_claims_loader</code>, <code>@jwt_optional</code>, <code>get_jwt_claims()</code>.</li> <li>Returning specific error messages for token-related issues using <code>@jwt.expired_token_loader</code>, <code>@jwt.invalid_token_loader</code>, <code>@jwt.needs_fresh_token_loader</code>.</li> <li>Implementing a logout mechanism using a blacklist (with <code>@jwt.token_in_blacklist_loader</code>, <code>get_raw_jwt()</code>).</li> </ul> <p>Book</p>"},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"}]}