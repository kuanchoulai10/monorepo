{"config":{"lang":["en","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to KC's Data &amp; Life Notes","text":"<p>Welcome! I'm KC (Kuan-Chou), a software engineer with a passion for data, AI, and continuous learning. This website is a space where I share my projects, learning journey, and experiences. Feel free to explore and get to know me better!</p> <ul> <li> <p> About Me</p> <p>Learn more about my background, including education, work experience, public speaking, and other activities.</p> <p> About Me</p> </li> <li> <p> Side Projects</p> <p>Explore my personal projects in data, AI, and programming.</p> <p> Side Projects</p> </li> <li> <p> Learning Plans</p> <p>A look into what I'm currently learning and planning to study next.</p> <p> Learning Plans</p> </li> <li> <p> Blog</p> <p>Read my thoughts and insights on data, AI, books, and more.</p> <p> Blog</p> </li> </ul>"},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"about-me/","title":"About Me","text":"<p>I'm a data-driven problem solver with experience in data engineering, machine learning, and public speaking. I enjoy turning complex ideas into actionable solutions and am always looking to grow\u2014both personally and professionally. Learn more about my background, skills, and what drives me.</p>"},{"location":"about-me/#work-experience","title":"Work Experience","text":"Period Role Company Type Link May 2024 - Sep 2024 Data Engineer UST Global Full-time, Remote Jun 2021 - Feb 2024 Data Engineer TVBS Media Inc. Full-time, Hybrid (Taipei, Taiwan) Jun 2020 - Sep 2020 ML Researcher NinoX Contract, Remote Jul 2018 - Aug 2018 Industry Analyst Intern ITRI Intern, On-site (Hsinchu, Taiwan)"},{"location":"about-me/#education","title":"Education","text":"Period Role Institution Location Link Sep 2019 - Feb 2020 Big Data Engineering Trainee III Taipei, Taiwan Jul 2017 - May 2023 M.B.A. NCCU Taipei, Taiwan Sep 2013 - Jun 2017 B.S. Psychology CCU Chiayi, Taiwan <p>Home </p>"},{"location":"about-me/education/ccu/","title":"B.S. Psychology, CCU","text":"<p>About Me </p>"},{"location":"about-me/education/iii/","title":"Big Data Engineering Trainee, III","text":"<p>About Me </p>"},{"location":"about-me/education/nccu/","title":"M.B.A., NCCU","text":"<p>National Chengchi University</p> <p>About Me </p>"},{"location":"about-me/public-speakings/","title":"Public Speakings","text":"<p>Sharing knowledge and ideas with others is something I value deeply. Here, you can find my past speaking engagements, talks, and presentations, along with topics I'm passionate about. I'm always open to new opportunities to connect and speak.</p>"},{"location":"about-me/public-speakings/#taipei-dbt-meetup-27","title":"Taipei dbt Meetup #27","text":""},{"location":"about-me/public-speakings/#devopsdays-taipei-2024","title":"DevOpsDays Taipei 2024","text":"<p>About Me </p>"},{"location":"about-me/work-experience/itri/itri/","title":"Industry Analyst Intern, ITRI","text":"<p>Abstract</p> <p>Researched and analyzed the 7 key roles (DSP, SSP, DMP etc.) within the \"AdTech industry\", using \"eLand Information\", a leading AI &amp; Data Analytics company in Taiwan, as a case study to predict future industry trends and propose strategies.</p> <p>Abstract</p> <p>Analyzed technical trends within the cloud services industry, contributing to the authorship of 2 research articles on the subject.</p> <p>Abstract</p> <p>Systematized and summarized 100+ benchmark cases in the 4 major domains of IoT \u2013 water resources, air quality, earthquakes, and disaster prevention.</p> <p>About Me </p>"},{"location":"about-me/work-experience/ninox/ninox/","title":"ML Researcher, NinoX","text":""},{"location":"about-me/work-experience/ninox/ninox/#the-obi-ctr-model","title":"The obi-CTR model","text":"<p>Abstract</p> <p>Developed the Online Bayesian Inference (OBI) algorithm for the Collaborative Topic Regression (CTR) model\u2014a hybrid system combining Collaborative Filtering and Topic Modeling\u2014using Python with robust OOP design, while leveraging NumPy and SciPy for scalable, real-time recommendations.</p>"},{"location":"about-me/work-experience/ninox/ninox/#23-boost-in-computational-efficiency","title":"23% boost in computational efficiency","text":"<p>Abstract</p> <p>Achieved a 23% boost in computational efficiency by speeding up the MCMC sampling core with Cython.</p>"},{"location":"about-me/work-experience/ninox/ninox/#cicd-pipelines-with-travisci","title":"CI/CD pipelines with TravisCI","text":"<p>Abstract</p> <p>Established CI/CD pipelines with TravisCI for automated unit tests and a docs-as-code workflow using pytest and Sphinx, streamlining both development and documentation processes and ensuring consistent release quality</p> <p></p> <p>About Me </p>"},{"location":"about-me/work-experience/tvbs/tvbs/","title":"Data Engineer, TVBS Media Inc.","text":"Organization Diagram"},{"location":"about-me/work-experience/tvbs/tvbs/#cost-effective-scalable-etlelt-modern-data-stack","title":"Cost-effective, scalable ETL/ELT Modern Data Stack","text":"<p>Abstract</p> <p>Architected a cost-effective, scalable ETL / ELT Modern Data Stack (dbt, BigQuery, Airflow, Airbyte, Looker Studio, etc.) and introduced a streamlined DataOps workflow, processing 20M+ events daily at TB+ scale (300+ data models, 600+ daily quality checks), cutting cloud costs by 63%.</p> \u21901 / 7\u2192\u26f6"},{"location":"about-me/work-experience/tvbs/tvbs/#the-organization-wide-adoption-of-data-mesh","title":"The organization-wide adoption of Data Mesh","text":"<p>Abstract</p> <p>Directed the organization-wide adoption of Data Mesh principles to strengthen data governance and improve data availability, empowering 7 domain teams through self-service reporting across 30+ data products, and achieving a previously unattainable holistic brand analysis through the expansion of data sources from 4 to 9+.</p> \u21901 / 4\u2192\u26f6"},{"location":"about-me/work-experience/tvbs/tvbs/#iac-implementation-with-terraform","title":"IaC implementation with Terraform","text":"<p>Abstract</p> <p>Led IaC implementation with Terraform for over 500 cross-cloud data assets (AWS, GCP, dbt Cloud, etc.) and conducted internal DevOps workshops, slashing provisioning lead time from days to hours by integrating CI/CD pipelines with GitHub Actions and improving team IaC adoption by 80% within 6 months.</p> \u21901 / 4\u2192\u26f6"},{"location":"about-me/work-experience/tvbs/tvbs/#the-migration-to-ga4-and-bigquery","title":"The migration to GA4 and BigQuery","text":"<p>Abstract</p> <p>Led the migration to GA4 and BigQuery to build a data lakehouse platform while maintaining a legacy event tracking pipeline (AWS Kinesis, MongoDB, PostgreSQL), saving $2M by retiring NoSQL database and ensuring real-time analytics for both anonymous and logged-in users.</p>"},{"location":"about-me/work-experience/tvbs/tvbs/#an-organization-wide-experimentation-mindset","title":"An organization-wide experimentation mindset","text":"<p>Abstract</p> <p>Championed an organization-wide experimentation mindset, engaged 60+ colleagues, and orchestrated 20+ A/B tests via Google Optimize and Firebase within 6 months, boosting mobile ad revenue by 27% and web pageviews by 6%.</p> <p>About Me </p>"},{"location":"about-me/work-experience/tvbs/slide-data-mesh/1/","title":"Challenges","text":"<ul> <li>1</li> <li>2</li> <li>3</li> </ul>"},{"location":"about-me/work-experience/tvbs/slide-data-mesh/2/","title":"Goals","text":"<ul> <li>A</li> <li>B</li> <li>C</li> </ul>"},{"location":"about-me/work-experience/tvbs/slide-data-mesh/3/","title":"Actions","text":""},{"location":"about-me/work-experience/tvbs/slide-data-mesh/4/","title":"Results","text":""},{"location":"about-me/work-experience/tvbs/slide-events-data-pipeline/deal-with-data-latency/","title":"data latency","text":""},{"location":"about-me/work-experience/tvbs/slide-events-data-pipeline/deal-with-data-latency/#situation","title":"Situation","text":"<ul> <li>Events occured on Jan. 1, but were sent to server on Jan. 3</li> <li>Latency at most 5 days</li> <li>Metrics are calculated in error due to data latency</li> <li></li> </ul>"},{"location":"about-me/work-experience/tvbs/slide-events-data-pipeline/deal-with-data-latency/#target","title":"Target","text":""},{"location":"about-me/work-experience/tvbs/slide-events-data-pipeline/deal-with-data-latency/#actions","title":"Actions","text":""},{"location":"about-me/work-experience/tvbs/slide-events-data-pipeline/deal-with-data-latency/#result","title":"Result","text":""},{"location":"about-me/work-experience/tvbs/slide-iac/1/","title":"Challenges","text":"<ul> <li>1</li> <li>2</li> <li>3</li> </ul>"},{"location":"about-me/work-experience/tvbs/slide-iac/2/","title":"Goals","text":"<ul> <li>A</li> <li>B</li> <li>C</li> </ul>"},{"location":"about-me/work-experience/tvbs/slide-iac/3/","title":"Actions","text":""},{"location":"about-me/work-experience/tvbs/slide-iac/4/","title":"Results","text":""},{"location":"about-me/work-experience/tvbs/slide-modern-data-stack/1-gaps-between-teams/","title":"Challenges - Data and Upstream Teams Misaligned","text":""},{"location":"about-me/work-experience/tvbs/slide-modern-data-stack/1-gaps-within-team/","title":"Challenges - Analysts Rely on Engineers","text":""},{"location":"about-me/work-experience/tvbs/slide-modern-data-stack/1-legacy-data-stack/","title":"Challenges - Legacy Data Stack","text":""},{"location":"about-me/work-experience/tvbs/slide-modern-data-stack/1-summary/","title":"Challenges - Summary","text":""},{"location":"about-me/work-experience/tvbs/slide-modern-data-stack/1-summary/#technical","title":"Technical","text":"<ul> <li>A single VM taking on too many responsibilities</li> <li>Tightly coupled data pipelines</li> <li>Lack of software engineering best practices</li> <li>Scattered business logic across various codebases</li> </ul>"},{"location":"about-me/work-experience/tvbs/slide-modern-data-stack/1-summary/#organizational","title":"Organizational","text":"<ul> <li>Analysts rely on engineers \u2014 big technical gap</li> <li>Data and upstream teams misaligned \u2014 downstream needs ignored</li> <li>Shortage of data talent (2 members)</li> </ul>"},{"location":"about-me/work-experience/tvbs/slide-modern-data-stack/2/","title":"Goals","text":"<ul> <li>Empower more colleagues to interact more easily with a wider variety of data in more diverse ways.</li> <li>Drive down costs by streamlining workflows and reducing data friction.</li> </ul>"},{"location":"about-me/work-experience/tvbs/slide-modern-data-stack/3/","title":"Actions","text":"<ol> <li>Researched AWS-based Data Lake Architectures: Explored best practices using AWS Glue, Athena, S3, DMS, and open table formats like Iceberg, Hudi, and Delta Lake.</li> <li>Built a Minimum Viable Architecture on GCP: Delivered end-to-end analytics using Airbyte, dbt, BigQuery, and Looker Studio within a single sprint in order to deliver fast.</li> <li>Estimated and Managed Project Budget: Conducted budgeting calculations to ensure cost-effective data architecture planning and scaling.</li> <li>Rolled Out Solution to All: Successfully deployed the solution organization-wide to support broader data accessibility and insights.</li> </ol>"},{"location":"about-me/work-experience/tvbs/slide-modern-data-stack/4/","title":"Results","text":"<ol> <li>How to cut down 63%<ul> <li>Tableau -&gt; Looker Studio</li> <li>Cloud SQL -&gt; BigQuery</li> <li>VM -&gt; X</li> </ul> </li> </ol>"},{"location":"about-me/work-experience/ust/ust/","title":"Data Engineer, UST Global","text":""},{"location":"about-me/work-experience/ust/ust/#server-monitoring-solutions","title":"Server Monitoring Solutions","text":"<p>Abstract</p> <p>Built server monitoring solutions for Microsoft using Kusto (KQL) on Azure Data Explorer (ADX), Azure Monitor Log Analytics, and Azure Sentinel, delivering actionable insights via Power BI for operational excellence.</p> <p>About Me </p>"},{"location":"blog/","title":"Blog","text":""},{"location":"dataops-mlops/","title":"DataOps &amp; MLOps","text":""},{"location":"dataops-mlops/#dataops","title":"DataOps","text":"<ul> <li>Use dbt for data test, data transformation, documentation</li> </ul>"},{"location":"dataops-mlops/#mlops","title":"MLOps","text":"<ul> <li>Use feast for the feature store</li> <li>Use mlflow for model training tracking</li> <li>Use Kserve for feature serving</li> </ul>"},{"location":"dataops-mlops/#orchestration","title":"Orchestration","text":"<ul> <li>Use Airflow for model orchestrastion</li> </ul>"},{"location":"dataops-mlops/#course-notes","title":"Course Notes","text":"<pre><code>gcloud iam service-accounts create SERVICE_ACCOUNT_NAME \\\n  --description=\"DESCRIPTION\" \\\n  --display-name=\"DISPLAY_NAME\"\n\ngcloud projects add-iam-policy-binding udemy-mlops \\\n    --member=serviceAccount:vertexai-sa@udemy-mlops.iam.gserviceaccount.com \\\n    --role=roles/aiplatform.customCodeServiceAgent\n\ngcloud projects add-iam-policy-binding udemy-mlops \\\n    --member=serviceAccount:vertexai-sa@udemy-mlops.iam.gserviceaccount.com \\\n    --role=roles/aiplatform.admin\n\ngcloud projects add-iam-policy-binding udemy-mlops \\\n    --member=serviceAccount:vertexai-sa@udemy-mlops.iam.gserviceaccount.com \\\n    --role=roles/storage.objectAdmin\n</code></pre>"},{"location":"dataops-mlops/#hands-on-vertex-ai-custom-training-gcloud-cli","title":"hands-on Vertex AI Custom Training gcloud cli","text":"<ol> <li>Create a service account and grant the necessary permissions</li> </ol> <pre><code>gcloud iam service-accounts create SERVICE_ACCOUNT_NAME \\\n  --description=\"DESCRIPTION\" \\\n  --display-name=\"DISPLAY_NAME\"\n\ngcloud projects add-iam-policy-binding udemy-mlops \\\n    --member=serviceAccount:vertexai-sa@udemy-mlops.iam.gserviceaccount.com \\\n    --role=roles/aiplatform.customCodeServiceAgent\n\ngcloud projects add-iam-policy-binding udemy-mlops \\\n    --member=serviceAccount:vertexai-sa@udemy-mlops.iam.gserviceaccount.com \\\n    --role=roles/aiplatform.admin\n\ngcloud projects add-iam-policy-binding udemy-mlops \\\n    --member=serviceAccount:vertexai-sa@udemy-mlops.iam.gserviceaccount.com \\\n    --role=roles/storage.objectAdmin\n</code></pre> <ol> <li>Build the image locally</li> </ol> <pre><code>docker build -t vertex-bikeshare-model .\n</code></pre> <ol> <li>Tag the image locally</li> </ol> <pre><code>docker tag vertex-bikeshare-model gcr.io/udemy-mlops/vertex-bikeshare-model\n</code></pre> <ol> <li>Push the image to GCR</li> </ol> <pre><code>docker push gcr.io/udemy-mlops/vertex-bikeshare-model\n</code></pre> <ol> <li>Submit a custom model training job using gcloud</li> </ol> <pre><code>gcloud ai custom-jobs create --region=us-central1 \\\n--project=udemy-mlops \\\n--worker-pool-spec=replica-count=1,machine-type='n1-standard-4',container-image-uri='gcr.io/udemy-mlops/vertex-bikeshare-model' \\\n--service-account=SERVICE_ACCOUNT\n--display-name=bike-sharing-model-training\n</code></pre>"},{"location":"dataops-mlops/#section-5","title":"Section 5","text":"<ol> <li>Create bucket</li> <li>Upload dataset</li> <li>Enable Secret Manager API in order to create Cloud Builds v2 Repositories</li> <li>Create Cloud Build v2 Repositories connection and link repo</li> <li>Create Cloud Build Trigger</li> <li>create github repo</li> <li></li> </ol> <pre><code># Assign Service account user role to the service account \ngcloud projects add-iam-policy-binding udemy-mlops \\\n--member=serviceAccount:1090925531874@cloudbuild.gserviceaccount.com --role=roles/aiplatform.admin\n</code></pre> <p>Cloud Build</p> <ol> <li>Build Docker Image</li> <li>Push Docker Image To GCR</li> <li>Execute Tests</li> <li>Submit Custom Training Job and wait until succedded</li> <li>Upload Model to Model Registry</li> <li>Fetch Model ID</li> <li>Create Endpoint</li> <li>Deploy Model Endpoint</li> </ol>"},{"location":"dataops-mlops/#model_training_codepy","title":"model_training_code.py","text":"<ol> <li>load data</li> <li>preprocess data (rename columns, drop columns, one-hot-encodings)</li> <li>train test split</li> <li>train model</li> <li>dump model(\"gs://sid-vertex-mlops/bike-share-rf-regression-artifact/model.joblib\")</li> </ol>"},{"location":"dataops-mlops/#section-6-kubefloe-pipeline","title":"Section 6 Kubefloe Pipeline","text":"<ul> <li>Upload in-vehicle coupon recommendation file</li> <li>kubeflow <code>@component</code><ul> <li><code>validate_input_ds()</code></li> <li><code>custom_training_job_component()</code></li> </ul> </li> <li>kubeflow <code>@dsl.pipeline</code><ul> <li><code>pipeline()</code></li> </ul> </li> <li><code>compiler()</code> -&gt; compile.json</li> <li> <p><code>from google.cloud.aiplatform import pipeline_jobs</code></p> <ul> <li><code>pipeline_jobs.PipelineJob().run()</code></li> </ul> </li> <li> <p><code>kfp.dsl.Metrics</code>: An artifact for storing key-value scalar metrics.</p> </li> <li><code>kfp.dsl.Output</code></li> </ul>"},{"location":"dataops-mlops/cert-manager-introduction/","title":"Cert Manager","text":""},{"location":"dataops-mlops/dataset/","title":"Dataset","text":""},{"location":"dataops-mlops/dataset/#dataset","title":"Dataset","text":"<p>Bike Sharing</p> <p>This dataset contains the hourly and daily count of rental bikes between years 2011 and 2012 in Capital bikeshare system with the corresponding weather and seasonal information.</p> <p>Bike sharing systems are new generation of traditional bike rentals where whole process from membership, rental and return back has become automatic. Through these systems, user is able to easily rent a bike from a particular position and return back at another position. Currently, there are about over 500 bike-sharing programs around the world which is composed of over 500 thousands bicycles. Today, there exists great interest in these systems due to their important role in traffic, environmental and health issues. </p> <p>Apart from interesting real world applications of bike sharing systems, the characteristics of data being generated by these systems make them attractive for the research. Opposed to other transport services such as bus or subway, the duration of travel, departure and arrival position is explicitly recorded in these systems. This feature turns bike sharing system into a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that most of important events in the city could be detected via monitoring these data.</p> <ul> <li>number of features: 13</li> <li>number of records: 17389</li> </ul> <p>Variable Name   Role    Type    Description Units   Missing Values instant ID  Integer record index        no dteday  Feature Date    date        no season  Feature Categorical 1:winter, 2:spring, 3:summer, 4:fall        no yr  Feature Categorical year (0: 2011, 1: 2012)     no mnth    Feature Categorical month (1 to 12)     no hr  Feature Categorical hour (0 to 23)      no holiday Feature Binary  weather day is holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)     no weekday Feature Categorical day of the week     no workingday  Feature Binary  if day is neither weekend nor holiday is 1, otherwise is 0      no weathersit  Feature Categorical - 1: Clear, Few clouds, Partly cloudy, Partly cloudy        no temp    Feature Continuous  Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)   C   no atemp   Feature Continuous  Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)  C   no hum Feature Continuous  Normalized humidity. The values are divided to 100 (max)        no windspeed   Feature Continuous  Normalized wind speed. The values are divided to 67 (max)       no casual  Other   Integer count of casual users       no registered  Other   Integer count of registered users       no cnt Target  Integer count of total rental bikes including both casual and registered        no</p>"},{"location":"dataops-mlops/dbt-features/","title":"dbt Features","text":""},{"location":"dataops-mlops/dbt-implementation/","title":"dbt Implementation (Hands-on)","text":""},{"location":"dataops-mlops/dbt-introduction/","title":"dbt Introduction","text":""},{"location":"dataops-mlops/design-doc/","title":"Design Doc","text":""},{"location":"dataops-mlops/feast-introduction/","title":"Feast","text":"<p>Feast is an open source feature store that delivers structured data to AI and LLM applications at high scale during training and inference</p>"},{"location":"dataops-mlops/feast-introduction/#architecture-components","title":"Architecture &amp; Components","text":""},{"location":"dataops-mlops/feast-introduction/#feature-server","title":"Feature Server","text":"<p>The Feature Server is a core architectural component in Feast, designed to provide low-latency feature retrieval and updates for machine learning applications. It is a REST API server built using FastAPI and exposes a limited set of endpoints to serve features, push data, and support materialization operations.</p> <p>The Feature Server operates as a stateless service backed by two key components:</p> <ul> <li>Online Store: The primary data store used for low-latency feature retrieval.</li> <li>Registry: The metadata store that defines feature sets, feature views, and their relationships to entities.</li> </ul> <p>Key Features of Feature Server</p> <ul> <li>RESTful API: Provides standardized endpoints for feature retrieval and data pushing.</li> <li>CLI Integration: Easily managed through the Feast CLI with commands like feast serve.</li> <li>Flexible Deployment: Can be deployed locally, via Docker, or on Kubernetes using Helm charts.</li> <li>Scalability: Designed for distributed deployments to handle large-scale workloads.</li> <li>TLS Support: Ensures secure communication in production setups.</li> </ul> <ul> <li>Feast</li> <li>Feature Server | Feast</li> </ul>"},{"location":"dataops-mlops/feast-server-deployment/","title":"Deploy Feast Server on Cloud Run (Hands-on)","text":""},{"location":"dataops-mlops/feature-store-introduction/","title":"Feature Store Introduction","text":""},{"location":"dataops-mlops/feature-store-introduction/#what-is-feature-store","title":"What is Feature Store?","text":""},{"location":"dataops-mlops/feature-store-introduction/#why-feature-store","title":"Why Feature Store?","text":""},{"location":"dataops-mlops/feature-store-introduction/#whos-using-feature-store","title":"Who's using Feature Store?","text":""},{"location":"dataops-mlops/feature-store-introduction/#_1","title":"Feature Store Introduction","text":""},{"location":"dataops-mlops/istio-introduction/","title":"Istio","text":""},{"location":"dataops-mlops/istio-introduction/#what-is-istio","title":"What is Istio","text":"<ul> <li>Istio is an open source service mesh that layers transparently onto existing distributed applications.</li> <li>Istio extends Kubernetes to establish a programmable, application-aware network. Working with both Kubernetes and traditional workloads, Istio brings standard, universal traffic management, telemetry, and security to complex deployments.</li> <li>It gives you:<ul> <li>Secure service-to-service communication in a cluster with mutual TLS encryption, strong identity-based authentication and authorization</li> <li>Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic</li> <li>Fine-grained control of traffic behavior with rich routing rules, retries, failovers, and fault injection</li> <li>A pluggable policy layer and configuration API supporting access controls, rate limits and quotas</li> <li>Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress</li> </ul> </li> </ul>"},{"location":"dataops-mlops/istio-introduction/#how-it-works","title":"How it works?","text":"<ul> <li>The control plane takes your desired configuration, and its view of the services, and dynamically programs the proxy servers, updating them as the rules or the environment changes.</li> <li>The data plane is the communication between services. Without a service mesh, the network doesn\u2019t understand the traffic being sent over, and can\u2019t make any decisions based on what type of traffic it is, or who it is from or to. It supports two data planes:<ul> <li>sidecar mode, which deploys an Envoy proxy along with each pod that you start in your cluster, or running alongside services running on VMs.</li> <li>ambient mode, which uses a per-node Layer 4 proxy, and optionally a per-namespace Envoy proxy for Layer 7 features</li> </ul> </li> </ul>"},{"location":"dataops-mlops/istio-introduction/#sidecar-mode-vs-ambient-mode-in-istio","title":"Sidecar Mode vs. Ambient Mode in Istio","text":"Feature Sidecar Mode Ambient Mode Proxy location One per pod (sidecar) One per node (<code>ztunnel</code>), plus optional waypoint proxies Setup complexity Requires injection into each pod No sidecar; label namespace only Resource usage Higher Lower Feature maturity Very mature Newer, still evolving Transparency to app Yes Yes Traffic interception iptables per pod eBPF or iptables at node level"},{"location":"dataops-mlops/istio-introduction/#why-istio","title":"Why Istio?","text":"<p>Simple and powerful</p> <p>Istio \u4e4b\u6240\u4ee5\u88ab\u7a31\u70ba simple and powerful\uff0c\u662f\u56e0\u70ba\uff1a</p> <ul> <li>\u5b83\u529f\u80fd\u5b8c\u6574\uff0c\u5e7e\u4e4e\u6db5\u84cb\u6240\u6709 service mesh \u6240\u9700\u7684\u6cbb\u7406\u9700\u6c42</li> <li>\u4e00\u65e6\u5efa\u69cb\u5b8c\u6210\uff0c\u5b83\u8b93\u5fae\u670d\u52d9\u7684\u904b\u7dad\u8207\u6cbb\u7406\u8b8a\u5f97\u771f\u6b63\u7c21\u55ae</li> <li>\u900f\u904e\u6a19\u6e96\u5316\u4ee3\u7406\u5c64\u548c\u7b56\u7565\u5b9a\u7fa9\uff0c\u62bd\u8c61\u4e86\u5e95\u5c64\u8907\u96dc\u6027</li> <li>\u65b0\u63a8\u51fa\u7684 Ambient \u6a21\u5f0f\u66f4\u964d\u4f4e\u4e86\u90e8\u7f72\u8207\u5b78\u7fd2\u9580\u6abb</li> </ul> <p>\u5176\u5be6\u6211\u4e00\u958b\u59cb\u4e5f\u89ba\u5f97 Istio \u6709\u9ede\u8907\u96dc\uff0c\u7562\u7adf\u88e1\u9762\u540d\u8a5e\u4e00\u5806\u3001\u53c8\u6709\u4ec0\u9ebc sidecar\u3001control plane\u3001mTLS \u4e4b\u985e\u7684\u3002\u4f46\u5f8c\u4f86\u771f\u7684\u7406\u89e3\u5b83\u4e4b\u5f8c\uff0c\u6211\u89ba\u5f97\u5b83\u6703\u88ab\u8aaa\u6210\u300csimple and powerful\u300d\u4e0d\u662f\u6c92\u9053\u7406\u7684\u3002</p> <p>Powerful \u7684\u5730\u65b9\u8d85\u660e\u986f\uff0c\u5b83\u5e7e\u4e4e\u5e6b\u4f60\u5305\u8fa6\u4e86\u6240\u6709\u5fae\u670d\u52d9\u4e4b\u9593\u7684\u7db2\u8def\u6cbb\u7406\uff1a\u50cf\u662f TLS \u52a0\u5bc6\u3001\u96f6\u4fe1\u4efb\u67b6\u69cb\u3001\u6d41\u91cf\u5206\u6d41\u3001\u932f\u8aa4\u91cd\u8a66\u3001\u7570\u5e38\u6ce8\u5165\u3001A/B \u6e2c\u8a66\u9019\u4e9b\uff0c\u904e\u53bb\u8981\u81ea\u5df1\u4e00\u500b\u4e00\u500b\u958b\u767c\u3001\u6e2c\u8a66\u3001\u5e03\u7f72\uff0c\u4f46\u7528 Istio \u7684\u8a71\uff0c\u53ea\u8981\u5b9a\u7fa9\u4e00\u500b YAML \u6a94\u5c31\u641e\u5b9a\u4e86\u3002</p> <p>\u800c\u5b83\u88ab\u8aaa simple\uff0c\u5176\u5be6\u662f\u5f9e\u6574\u500b\u7cfb\u7d71\u5c64\u7d1a\u4f86\u770b\u3002\u4f60\u7528\u5b83\u628a service mesh \u67b6\u8d77\u4f86\u4e4b\u5f8c\uff0c\u771f\u7684\u6703\u89ba\u5f97\u300c\u7ba1\u7406\u670d\u52d9\u7684\u8907\u96dc\u5ea6\u300d\u88ab\u5927\u5e45\u964d\u4f4e\u3002\u7279\u5225\u662f\u73fe\u5728\u6709\u4e86 Ambient mode\uff0c\u4e0d\u9700\u8981\u518d\u6ce8\u5165 sidecar\uff0c\u4e5f\u4e0d\u7528\u4e00\u76f4\u6539 pod spec\uff0c\u8b93\u90e8\u7f72\u66f4\u4e7e\u6de8\u3001\u64f4\u5c55\u66f4\u5bb9\u6613\u3002</p> <p>\u6240\u4ee5\u8001\u5be6\u8aaa\uff0cIstio \u672c\u8eab\u53ef\u80fd\u4e0d\u662f\u771f\u7684\u300c\u5165\u9580\u8d85\u7c21\u55ae\u300d\uff0c\u4f46\u5b83\u53ef\u4ee5\u8b93\u4f60\u5f8c\u7e8c\u7dad\u904b\u8b8a\u5f97\u66f4\u7c21\u55ae\u3001\u66f4\u5b89\u5fc3\u3002</p> <p>The Envoy proxy</p> <p>Istio inherits all the power and flexibility of Envoy, including world-class extensibility using WebAssembly</p> <p>a high performance service proxy initially built by Lyft</p> <p>Envoy would go on to become the load balancer that powers Google Cloud as well as the proxy for almost every other service mesh platform.</p> <p>\u9996\u5148\uff0c\u5728\u6d41\u91cf\u63a7\u5236\u65b9\u9762\uff0cKubernetes \u672c\u8eab\u7684 Ingress \u53ea\u80fd\u8655\u7406\u5f9e\u5916\u90e8\u9032\u5165\u96c6\u7fa4\u7684\u8acb\u6c42\uff0c\u5c0d\u65bc\u670d\u52d9\u4e4b\u9593\u7684\u5167\u90e8\u901a\u8a0a\u6beb\u7121\u63a7\u5236\u80fd\u529b\u3002\u5b83\u7121\u6cd5\u6839\u64da\u7248\u672c\u3001Header\u3001\u4f7f\u7528\u8005\u8eab\u5206\u7b49\u689d\u4ef6\u4f86\u8abf\u6574\u8def\u7531\u908f\u8f2f\u3002Istio \u5247\u900f\u904e VirtualService \u548c DestinationRule \u9019\u5169\u7a2e\u8cc7\u6e90\uff0c\u8b93\u4f60\u80fd\u7cbe\u7d30\u5730\u6307\u5b9a\u6d41\u91cf\u8a72\u5982\u4f55\u5206\u6d41\uff0c\u6bd4\u5982\u91dd\u5c0d\u7279\u5b9a\u7528\u6236\u5c0e\u5411\u65b0\u7248\u672c\u3001\u5be6\u4f5c A/B \u6e2c\u8a66\u3001\u7070\u968e\u91cb\u51fa\u7b49\uff0c\u5728\u4e0d\u6539\u8b8a\u61c9\u7528\u7a0b\u5f0f\u7684\u60c5\u6cc1\u4e0b\u505a\u5230\u5f48\u6027\u8def\u7531\u3002</p> <p>\u63a5\u8457\u662f\u5b89\u5168\u6027\u554f\u984c\u3002Kubernetes \u96d6\u7136\u652f\u63f4 Role-Based Access Control\uff08RBAC\uff09\u8207 NetworkPolicy\uff0c\u537b\u7121\u6cd5\u78ba\u4fdd\u670d\u52d9\u4e4b\u9593\u7684\u901a\u8a0a\u662f\u52a0\u5bc6\u7684\uff0c\u4e5f\u7121\u6cd5\u9a57\u8b49\u901a\u8a0a\u96d9\u65b9\u7684\u8eab\u5206\u3002Istio \u5247\u85c9\u7531\u81ea\u52d5\u5316\u7684 mutual TLS\uff08mTLS\uff09\u6a5f\u5236\uff0c\u5728\u6bcf\u500b\u670d\u52d9\u4e4b\u9593\u5efa\u7acb\u52a0\u5bc6\u901a\u9053\uff0c\u4e26\u642d\u914d\u8eab\u4efd\u9a57\u8b49\u8207\u6388\u6b0a\u7b56\u7565\uff0c\u78ba\u4fdd\u53ea\u6709\u88ab\u6388\u6b0a\u7684\u670d\u52d9\u80fd\u4e92\u76f8\u901a\u8a0a\uff0c\u5be6\u73fe\u96f6\u4fe1\u4efb\u67b6\u69cb\u7684\u57fa\u672c\u539f\u5247\u3002</p> <p>\u518d\u4f86\u662f\u53ef\u89c0\u5bdf\u6027\u3002Kubernetes \u96d6\u7136\u53ef\u4ee5\u67e5\u770b Pod logs \u548c\u4e00\u4e9b\u57fa\u672c\u7684 metrics\uff0c\u4f46\u5c0d\u65bc\u5fae\u670d\u52d9\u4e4b\u9593\u7684\u8ffd\u8e64\u3001\u5ef6\u9072\u5206\u6790\u3001\u6d41\u91cf\u74f6\u9838\u7b49\u554f\u984c\uff0c\u539f\u751f\u529f\u80fd\u660e\u986f\u4e0d\u8db3\u3002Istio \u5728\u6bcf\u500b\u670d\u52d9\u908a\u7de3\u90e8\u7f72 proxy\uff0c\u9019\u8b93\u5b83\u53ef\u4ee5\u81ea\u52d5\u6536\u96c6\u8a73\u7d30\u7684 telemetry \u8cc7\u8a0a\uff0c\u5305\u62ec request-level tracing\u3001\u6d41\u91cf\u6307\u6a19\u8207\u932f\u8aa4\u7387\uff0c\u4e26\u6574\u5408\u5230\u50cf Prometheus\u3001Grafana\u3001Jaeger \u7b49\u5de5\u5177\u4e2d\uff0c\u5e7e\u4e4e\u4e0d\u9700\u8981\u4fee\u6539\u61c9\u7528\u7a0b\u5f0f\u5c31\u80fd\u505a\u5230\u5168\u9762\u76e3\u63a7\u3002</p> <p>\u6700\u5f8c\uff0c\u5728\u9748\u6d3b\u6027\u8207\u64f4\u5c55\u6027\u4e0a\uff0cKubernetes \u7121\u6cd5\u91dd\u5c0d\u500b\u5225\u670d\u52d9\u6ce8\u5165\u81ea\u8a02\u7684\u7db2\u8def\u8655\u7406\u908f\u8f2f\u3002Istio \u900f\u904e sidecar \u6a21\u5f0f\uff08\u6216\u8f03\u65b0\u7684 ambient \u6a21\u5f0f\uff09\u8b93\u6bcf\u500b\u670d\u52d9\u90fd\u64c1\u6709\u81ea\u5df1\u7684\u7db2\u8def\u4ee3\u7406\uff0c\u4e26\u652f\u63f4 WebAssembly \u63d2\u4ef6\uff0c\u4f60\u53ef\u4ee5\u52d5\u614b\u63d2\u5165\u8a8d\u8b49\u908f\u8f2f\u3001\u8cc7\u6599\u8f49\u63db\u751a\u81f3\u7570\u5e38\u6a21\u64ec\uff0c\u5c07\u7db2\u8def\u884c\u70ba\u8abf\u6574\u70ba\u7b26\u5408\u696d\u52d9\u9700\u6c42\u7684\u6a21\u6a23\u3002</p>"},{"location":"dataops-mlops/istio-introduction/#what-happened-after-installing-istio","title":"What happened after installing Istio","text":"<p>\u5728\u5b8c\u6210 Istio \u7684\u5b89\u88dd\u5f8c\uff0c\u7cfb\u7d71\u6703\u5728 Kubernetes \u4e2d\u5efa\u7acb\u4e00\u500b\u540d\u70ba istio-system \u7684\u547d\u540d\u7a7a\u9593\uff0c\u9019\u662f Istio \u63a7\u5236\u5e73\u9762\uff08Control Plane\uff09\u5143\u4ef6\u6240\u904b\u4f5c\u7684\u4e3b\u8981\u4f4d\u7f6e\u3002\u88e1\u9762\u6703\u81ea\u52d5\u90e8\u7f72\u591a\u500b\u6838\u5fc3\u5143\u4ef6\uff0c\u9019\u4e9b\u5143\u4ef6\u900f\u904e\u670d\u52d9\u8207 Pod \u7684\u5f62\u5f0f\u5448\u73fe\uff0c\u4e26\u4e14\u85c9\u7531 CRD \u8b93 Istio \u80fd\u64f4\u5c55\u51fa\u81ea\u5df1\u7684\u8cc7\u6e90\u6a21\u578b\u3002</p> <p>\u9996\u5148\uff0c\u6700\u91cd\u8981\u7684\u63a7\u5236\u5143\u4ef6\u662f istiod\uff0c\u5b83\u662f\u4e00\u500b\u6574\u5408\u578b\u7684\u63a7\u5236\u5e73\u9762\u5143\u4ef6\uff0c\u5f9e Istio 1.5 \u8d77\u958b\u59cb\u6574\u4f75\u4e86\u904e\u53bb\u7684 Pilot\u3001Mixer\u3001Citadel \u7b49\u5143\u4ef6\u3002\u9019\u500b Pod \u8ca0\u8cac\u7ba1\u7406 service discovery\u3001sidecar \u7684\u8a2d\u5b9a\u767c\u9001\uff08XDS\uff09\u3001mTLS \u6191\u8b49\u7c3d\u767c\u3001\u5b89\u5168\u6027\u7b56\u7565\u4e0b\u767c\u7b49\u5de5\u4f5c\u3002\u4f60\u6703\u5728 istio-system \u4e2d\u770b\u5230\u4e00\u500b\u53eb\u505a istiod-xxxx \u7684 Pod\uff0c\u5c0d\u61c9\u7684 Service \u5247\u901a\u5e38\u662f istiod \u6216 istio-pilot\uff0c\u53d6\u6c7a\u65bc\u7248\u672c\u8207\u5b89\u88dd\u65b9\u5f0f\u3002</p> <p>\u6b64\u5916\uff0c\u82e5\u4f60\u555f\u7528\u4e86 Ingress Gateway\uff08\u5927\u591a\u6578\u4eba\u6703\uff09\uff0c\u4f60\u9084\u6703\u770b\u5230\u4e00\u500b\u53eb\u505a istio-ingressgateway \u7684 Deployment\u3001Service \u548c Pod\u3002\u9019\u662f\u4e00\u500b\u4ee5 Envoy \u70ba\u6838\u5fc3\u7684\u8ca0\u8cac\u63a5\u6536\u5916\u90e8 HTTP/TCP \u6d41\u91cf\u7684\u4ee3\u7406\u4f3a\u670d\u5668\uff0c\u901a\u5e38\u6703\u662f NodePort \u6216 LoadBalancer \u578b\u5225\u7684 Service</p> <ul> <li>istiod\uff1a\u9019\u662f Istio \u7684\u63a7\u5236\u5e73\u9762\u6838\u5fc3\u3002\u5b83\u8ca0\u8cac\u7ba1\u7406\u670d\u52d9\u767c\u73fe\uff08service discovery\uff09\u3001\u767c\u9001\u4ee3\u7406\u8a2d\u5b9a\uff08xDS\uff09\u3001mTLS \u6191\u8b49\u7c3d\u767c\u8207\u5b89\u5168\u7b56\u7565\u4e0b\u767c\u3002\u5f9e Istio 1.5 \u958b\u59cb\uff0c\u9019\u500b\u5143\u4ef6\u6574\u5408\u4e86\u904e\u53bb\u7684 Pilot\u3001Citadel\u3001Galley \u7b49\u89d2\u8272\u3002</li> <li>istio-ingressgateway\uff1a\u9019\u662f\u4e00\u500b\u4f7f\u7528 Envoy \u5be6\u4f5c\u7684 Ingress Gateway\uff0c\u662f\u5916\u90e8\u6d41\u91cf\u9032\u5165\u670d\u52d9\u7db2\u683c\u7684\u5165\u53e3\uff0c\u901a\u5e38\u7528\u4f86\u63a5\u6536 HTTP\u3001HTTPS\u3001gRPC \u7b49\u5916\u90e8\u8acb\u6c42\uff0c\u4e26\u8f49\u767c\u81f3\u96c6\u7fa4\u5167\u90e8\u7684\u670d\u52d9\u3002</li> </ul> <p>\u9664\u4e86\u4e0a\u8ff0\u63a7\u5236\u5143\u4ef6\u4e4b\u5916\uff0cIstio \u5b89\u88dd\u4e5f\u6703\u9644\u5e36\u4e00\u4e9b\u64f4\u5145\u8cc7\u6e90\u3002\u6700\u660e\u986f\u7684\u662f\u4e00\u7cfb\u5217\u7684 CRD\uff0c\u50cf\u662f VirtualService\u3001DestinationRule\u3001Gateway\u3001PeerAuthentication\u3001AuthorizationPolicy \u7b49\u3002\u9019\u4e9b\u90fd\u662f\u4f60\u5728\u4f7f\u7528 Istio \u505a\u8def\u7531\u63a7\u5236\u3001\u6d41\u91cf\u5206\u6d41\u3001\u5b89\u5168\u7b56\u7565\u6642\u6703\u7528\u5230\u7684\u8cc7\u6e90\u3002\u9019\u4e9b CRD \u53ef\u4ee5\u900f\u904e kubectl get crd \u67e5\u770b\uff0c\u540d\u7a31\u901a\u5e38\u6703\u662f virtualservices.networking.istio.io \u9019\u6a23\u7684\u683c\u5f0f\u3002</p> <p>\u9019\u4e9b\u8cc7\u6e90\u69cb\u6210\u4e86 Istio \u7684\u6838\u5fc3\u57fa\u790e\uff0c\u8b93\u4f60\u5f97\u4ee5\u5728 Kubernetes \u4e0a\u5efa\u7acb\u4e00\u500b\u53ef\u63a7\u3001\u5b89\u5168\u4e14\u5177\u6709\u6d41\u91cf\u89c0\u5bdf\u80fd\u529b\u7684\u670d\u52d9\u7db2\u683c\u3002</p>"},{"location":"dataops-mlops/istio-introduction/#references","title":"References","text":"<ul> <li>What is Istio? | Istio</li> <li>Sidecar or ambient? | Istio</li> </ul>"},{"location":"dataops-mlops/knative-introduction/","title":"Knative","text":"<ul> <li>Knative is an Open-Source Enterprise-level solution to build Serverless and Event Driven Applications</li> <li>Why serverless containers?<ul> <li>Simpler Abstractions</li> <li>Autoscaling (Scale down to zero and up from zero)</li> <li>Progressive Rollouts</li> <li>Event Integrations</li> <li>Handle Events</li> <li>Plugable</li> </ul> </li> </ul> Knative Serving and Eventing <ul> <li>Knative has two main components that empower teams working with Kubernetes. Serving and Eventing work together to automate and manage tasks and applications.</li> <li>serving and eventing\u5404\u81ea\u7368\u7acb\uff0c\u53ef\u5206\u5225\u5b89\u88dd\u4f7f\u7528</li> <li>Knative Serving: Run serverless containers in Kubernetes with ease. Knative takes care of the details of networking, autoscaling (even to zero), and revision tracking. Teams can focus on core logic using any programming language.</li> <li>Knative Eventing: Universal subscription, delivery and management of events. Build modern apps by attaching compute to a data stream with declarative event connectivity and developer friendly object models.</li> </ul>"},{"location":"dataops-mlops/knative-introduction/#knative-history","title":"Knative History","text":"<ul> <li>Knative \u6700\u521d\u7531 Google \u65bc 2018 \u5e74 7 \u6708\u767c\u8d77\uff0c\u4e26\u8207 IBM\u3001Red Hat\u3001VMware \u548c SAP \u7b49\u516c\u53f8\u5bc6\u5207\u5408\u4f5c\u958b\u767c\u3002</li> <li>\u767c\u5c55\u91cc\u7a0b\u7891<ul> <li>2018 \u5e74 7 \u6708\uff1aKnative \u9996\u6b21\u516c\u958b\u767c\u5e03\u3002</li> <li>2019 \u5e74 3 \u6708\uff1aBuild \u7d44\u4ef6\u6f14\u9032\u70ba Tekton\uff0c\u5c08\u6ce8\u65bc CI/CD\u3002</li> <li>2019 \u5e74 9 \u6708\uff1aServing API \u9054\u5230 v1 \u7248\u672c\u3002</li> <li>2020 \u5e74 7 \u6708\uff1aEventing API \u9054\u5230 v1 \u7248\u672c\u3002</li> <li>2021 \u5e74 11 \u6708\uff1aKnative \u767c\u5e03 1.0 \u7248\u672c\uff0c\u6a19\u8a8c\u8457\u5176\u7a69\u5b9a\u6027\u548c\u5546\u696d\u53ef\u7528\u6027\u3002</li> <li>2022 \u5e74 3 \u6708\uff1aKnative \u6210\u70ba CNCF \u7684\u5b75\u5316\u5c08\u6848\u3002</li> </ul> </li> </ul>"},{"location":"dataops-mlops/knative-introduction/#knative_1","title":"Knative \u9069\u5408\u4f7f\u7528\u7684\u60c5\u5883","text":"<ul> <li>\u4e8b\u4ef6\u89f8\u767c\u578b\u61c9\u7528\uff08Event-Driven Applications\uff09\u7576\u61c9\u7528\u7a0b\u5f0f\u53ea\u9700\u8981\u5728\u7279\u5b9a\u4e8b\u4ef6\u767c\u751f\u6642\u57f7\u884c\uff0c\u4f8b\u5982\uff1a<ul> <li>GitHub webhook: \u6709 push \u4e8b\u4ef6\u6642\u89f8\u767c\u81ea\u52d5\u90e8\u7f72\u6d41\u7a0b</li> <li>Kafka message: \u6709\u8a0a\u606f\u9032\u5165\u7279\u5b9a topic \u6642\u555f\u52d5\u8655\u7406\u908f\u8f2f</li> <li>IoT \u8cc7\u6599\u4e0a\u50b3: \u8a2d\u5099\u56de\u50b3\u8cc7\u6599\u5c31\u8655\u7406\u4e00\u6b21</li> <li>\u5b9a\u6642\u5de5\u4f5c: \u6bcf10\u5206\u9418\u57f7\u884c\u4e00\u6b21\u8cc7\u6599\u6e05\u6d17\u4efb\u52d9</li> </ul> </li> <li>\u4e0d\u9700\u8981\u4e00\u76f4\u904b\u884c\u7684\u61c9\u7528 (Scale-to-zero)<ul> <li>\u958b\u767c\u74b0\u5883 API: \u4e0d\u5e38\u88ab\u8abf\u7528\uff0c\u4f46\u4e0d\u80fd\u4e0b\u7dda  </li> <li>\u81ea\u52a9\u5831\u8868\u7522\u751f: \u4f7f\u7528\u8005\u9ede\u9078\u6642\u624d\u555f\u52d5\u7522\u751f\u7a0b\u5f0f  </li> <li>\u4f7f\u7528\u8005\u89f8\u767c\u7684\u4efb\u52d9: \u4f8b\u5982\u8cc7\u6599\u5c0e\u5165\u3001\u8f49\u63db\u7b49\u81e8\u6642\u4efb\u52d9  </li> </ul> </li> <li>\u85cd\u7da0\u90e8\u7f72\u8207\u7070\u968e\u767c\u5e03 (Blue-Green / Canary Release)<ul> <li>\u767c\u5e03\u65b0\u7248\u672c: \u53ef\u5c07 10% \u6d41\u91cf\u5c0e\u5411\u65b0\u7248\u672c  </li> <li>\u9010\u6b65\u64f4\u5927\u6d41\u91cf: \u6839\u64da\u5065\u5eb7\u72c0\u6cc1\u6162\u6162\u8f49\u79fb\u6d41\u91cf  </li> <li>\u5feb\u901f\u56de\u9000: \u65b0\u7248\u6709\u554f\u984c\u6642\u7acb\u5373\u5207\u56de\u820a\u7248  </li> </ul> </li> <li>\u7121\u4f3a\u670d\u5668\u51fd\u6578 (FaaS) \u5e73\u53f0\u5efa\u8a2d<ul> <li>\u5efa\u7acb\u4f01\u696d\u5167\u90e8 FaaS: \u958b\u767c\u8005\u53ea\u9700\u63d0\u4f9b container \u6620\u50cf\u5373\u53ef  </li> <li>\u81ea\u5b9a\u7fa9\u89f8\u767c\u689d\u4ef6: \u53ef\u7d81\u5b9a Kafka\u3001Webhook\u3001Cron \u7b49  </li> <li>\u6a19\u6e96\u5316\u4e8b\u4ef6\u683c\u5f0f: \u652f\u63f4 CloudEvents \u6a19\u6e96\u683c\u5f0f  </li> </ul> </li> <li>\u7d50\u5408 DevOps / GitOps \u7684\u5feb\u901f\u4ea4\u4ed8\u5834\u666f<ul> <li>\u81ea\u52d5\u90e8\u7f72 pipeline: Git push \u2192 Tekton build \u2192 Knative deploy  </li> <li>GitOps Workflow: Git \u8a2d\u5b9a\u8b8a\u66f4 \u2192 \u81ea\u52d5\u66f4\u65b0 revision  </li> <li>\u591a\u7248\u672c\u63a7\u7ba1\u8207\u5207\u63db: \u53ef\u5feb\u901f\u5207\u63db\u7248\u672c\u3001\u6e2c\u8a66\u3001\u56de\u9000  </li> </ul> </li> <li>\u8cc7\u6e90\u654f\u611f\u578b\u7684\u5fae\u670d\u52d9\u67b6\u69cb<ul> <li>\u4f7f\u7528\u8005\u5831\u8868\u670d\u52d9: \u767d\u5929\u7528\u91cf\u9ad8\u3001\u665a\u4e0a\u5e7e\u4e4e\u6c92\u4eba\u7528  </li> <li>\u884c\u92b7\u6d3b\u52d5\u7cfb\u7d71: \u6d3b\u52d5\u671f\u9593\u9ad8\u5cf0\uff0c\u5e73\u5e38\u7121\u4eba\u4f7f\u7528  </li> <li>Edge/IoT \u7bc0\u9ede: \u9700\u7bc0\u7701 CPU/Memory \u4f7f\u7528\u91cf  </li> </ul> </li> </ul>"},{"location":"dataops-mlops/knative-introduction/#knative_2","title":"\u4e0d\u9069\u5408 Knative \u7684\u5834\u666f","text":"<ul> <li>\u9ad8\u541e\u5410\u3001\u4f4e\u5ef6\u9072\u5e38\u99d0\u670d\u52d9: scale-to-zero \u6703\u9020\u6210\u51b7\u555f\u52d5\u5ef6\u9072</li> <li>WebSocket / gRPC Streaming: \u4e0d\u652f\u63f4\u9577\u9023\u7dda\u5354\u5b9a</li> <li>\u975e HTTP \u5354\u8b70: Knative Serving \u76ee\u524d\u53ea\u652f\u63f4 HTTP-based \u8acb\u6c42</li> <li>\u6709\u72c0\u614b\u670d\u52d9: Knative \u50c5\u652f\u63f4 stateless container</li> </ul>"},{"location":"dataops-mlops/knative-introduction/#eventing","title":"Eventing","text":"<ul> <li>Knative Eventing is a powerful Kubernetes-based framework that enables event-driven application development. It allows developers to build loosely coupled, reactive services that respond to events from various sources. By decoupling producers and consumers of events, Knative Eventing makes it easier to scale, update, and maintain modern cloud-native applications, especially in serverless environments.</li> <li>Knative Eventing uses standard HTTP POST requests to send and receive events between event producers and sinks.</li> <li>These events conform to the CloudEvents specifications, which enables creating, parsing, sending, and receiving events in any programming language.</li> </ul>"},{"location":"dataops-mlops/knative-introduction/#event-mesh","title":"Event Mesh","text":"Knative Event Mesh <ul> <li>An Event Mesh is dynamic, interconnected infrastructure which is designed to simplify distributing events from senders to recipients.</li> <li>provides asynchronous (store-and-forward) delivery of messages which allows decoupling senders and recipients in time</li> <li>Event Meshes also simplify the routing concerns of senders and recipients by decoupling them from the underlying event transport infrastructure (which may be a federated set of solutions like Kafka, RabbitMQ, or cloud provider infrastructure)</li> <li>The mesh transports events from producers to consumers via a network of interconnected event brokers across any environment, and even between clouds in a seamless and loosely coupled way.</li> <li>Event producers can publish all events to the mesh, which can route events to interested subscribers without needing the application to subdivide events to channels</li> <li>Event consumers can use mesh configuration to receive events of interest using fine-grained filter expressions rather than needing to implement multiple subscriptions and application-level event filtering to select the events of interest.</li> <li>the Broker API offers a discoverable endpoint for event ingress and the Trigger API completes the offering with its event filtering and delivery capabilities</li> </ul> <p>\u7576\u6211\u7b2c\u4e00\u6b21\u63a5\u89f8\u5230 Knative Eventing \u7684 \u201cEvent Mesh\u201d \u6982\u5ff5\u6642\uff0c\u5176\u5be6\u6709\u9ede\u7591\u60d1\u3002\u7562\u7adf\u6211\u4ee5\u524d\u7fd2\u6163\u7528 Kafka \u9019\u7a2e\u6bd4\u8f03\u76f4\u63a5\u7684\u8a0a\u606f\u7cfb\u7d71\uff0cproducer \u628a\u8cc7\u6599\u4e1f\u5230\u67d0\u500b topic\uff0cconsumer \u8a02\u95b1\u90a3\u500b topic\uff0c\u5927\u5bb6\u5404\u53f8\u5176\u8077\u3001\u6e05\u695a\u660e\u77ad\u3002\u4e45\u4e86\u4e5f\u5c31\u7406\u6240\u7576\u7136\u5730\u63a5\u53d7\u4e86\u9019\u7a2e\u300c\u5927\u5bb6\u90fd\u8981\u77e5\u9053\u8a0a\u606f\u53bb\u54ea\u3001\u5f9e\u54ea\u4f86\u300d\u7684\u6a21\u5f0f\u3002</p> <p>\u4f46\u5f8c\u4f86\u6211\u958b\u59cb\u7406\u89e3 Event Mesh \u7684\u6642\u5019\uff0c\u8166\u4e2d\u6709\u4e00\u7a2e\u300c\u554a\uff0c\u539f\u4f86\u9084\u53ef\u4ee5\u9019\u6a23\u8a2d\u8a08\u300d\u7684\u611f\u89ba\u3002Event Mesh \u4e0d\u518d\u8981\u6c42 sender \u8ddf receiver \u90fd\u77e5\u9053\u8a0a\u606f\u901a\u904e\u4e86\u54ea\u689d\u901a\u9053\uff0c\u4e5f\u4e0d\u9700\u8981\u5927\u5bb6\u786c\u7d81\u5728\u540c\u4e00\u500b Kafka topic \u4e0a\u3002\u76f8\u53cd\u5730\uff0c\u5b83\u5f37\u8abf\u7684\u662f\u300c\u4e8b\u4ef6\u672c\u8eab\u300d\uff0c\u6bd4\u5982\u9019\u662f\u4e00\u500b\u4f86\u81ea\u67d0\u500b\u4f86\u6e90\u3001\u67d0\u7a2e\u985e\u578b\u3001\u767c\u751f\u5728\u67d0\u500b\u6642\u9593\u9ede\u7684\u4e8b\u4ef6\u2014\u2014\u9019\u4e9b\u5c6c\u6027\u624d\u662f\u5b83\u80fd\u4e0d\u80fd\u88ab\u8655\u7406\u7684\u95dc\u9375\u3002\u7cfb\u7d71\u6703\u6839\u64da\u9019\u4e9b\u5c6c\u6027\uff0c\u81ea\u52d5\u628a\u4e8b\u4ef6\u9001\u5230\u771f\u6b63\u9700\u8981\u5b83\u7684\u5730\u65b9\u3002\u9019\u4e2d\u9593\u7684\u8def\u600e\u9ebc\u8d70\uff0c\u4e0d\u518d\u662f\u61c9\u7528\u7a0b\u5f0f\u7684\u8cac\u4efb\uff0c\u800c\u662f Event Mesh \u5e6b\u4f60\u8655\u7406\u597d\u3002</p> <p>\u6700\u8b93\u6211\u9a5a\u8a1d\u7684\u662f\uff0c\u5b83\u751a\u81f3\u53ef\u4ee5\u5728\u80cc\u5f8c\u540c\u6642\u7528 Kafka\u3001RabbitMQ\uff0c\u751a\u81f3 cloud provider \u7684 Pub/Sub\uff0c\u4f5c\u70ba\u4e8b\u4ef6\u50b3\u8f38\u7684\u5e95\u5c64\u3002\u63db\u53e5\u8a71\u8aaa\uff0c\u4f60\u4e0d\u7528\u9078\u908a\u7ad9\uff0c\u4e5f\u4e0d\u9700\u8981\u5728\u8a2d\u8a08\u521d\u671f\u5c31\u7d81\u6b7b\u5728\u67d0\u500b\u6280\u8853\u4e0a\u3002\u53ea\u8981\u4e8b\u4ef6\u9001\u5f97\u51fa\u4f86\uff0c\u6709\u8208\u8da3\u7684\u670d\u52d9\u5c31\u6703\u6536\u5230\uff0c\u4e0d\u9700\u8981\u4e8b\u524d\u7d04\u597d topic\u3001\u4e5f\u4e0d\u9700\u8981\u7dad\u8b77\u4e00\u5806 subscriptions\u3002\u9019\u7a2e\u9b06\u8026\u5408\u7684\u8a2d\u8a08\u8b93\u7cfb\u7d71\u64f4\u5c55\u8d77\u4f86\u8f15\u9b06\u5f88\u591a\uff0c\u958b\u767c\u8d77\u4f86\u4e5f\u6bd4\u8f03\u81ea\u7531\uff0c\u7279\u5225\u9069\u5408\u5fae\u670d\u52d9\u6216 serverless \u67b6\u69cb\u3002</p> <p>\u5982\u679c\u4f60\u4e5f\u66fe\u7d93\u89ba\u5f97 Kafka \u7684 topic \u8a2d\u8a08\u5f88\u9748\u6d3b\u4f46\u8d8a\u4f86\u8d8a\u96e3\u7ba1\u7406\uff0c\u90a3\u6211\u771f\u7684\u5f88\u63a8\u85a6\u4f60\u770b\u770b Event Mesh \u7684\u505a\u6cd5\u3002\u5b83\u8b93\u4e8b\u4ef6\u6210\u70ba\u67b6\u69cb\u7684\u4e3b\u89d2\uff0c\u800c\u4e0d\u662f\u67d0\u500b\u5de5\u5177\u6216\u5e73\u53f0\u3002\u9019\u7a2e\u8f49\u8b8a\uff0c\u5c0d\u6211\u4f86\u8aaa\u4e0d\u53ea\u662f\u6280\u8853\u4e0a\u7684\u6f14\u9032\uff0c\u4e5f\u662f\u4e00\u7a2e\u601d\u7dad\u4e0a\u7684\u91cb\u653e\u3002</p> Event Sources <ul> <li>An event source is a Kubernetes custom resource (CR), created by a developer or cluster administrator, that acts as a link between an event producer and an event sink. A sink can be a k8s service, including Knative Services, a Channel, or a Broker that receives events from an event source.</li> <li>Apache Kafka, RabbitMQ, Amazon S3, Amazon SQS etc.</li> </ul> Brokers <p> Brokers are Kubernetes custom resources that define an event mesh for collecting a pool of events. Brokers provide a discoverable endpoint for event ingress, and use Triggers for event delivery. Event producers can send events to a broker by POSTing the event.</p>"},{"location":"dataops-mlops/knative-introduction/#triggers","title":"Triggers","text":"<p>A trigger represents a desire to subscribe to events from a specific broker.</p>"},{"location":"dataops-mlops/knative-introduction/#event-sinks","title":"Event Sinks","text":"<ul> <li>When you create an event source, you can specify a sink where events are sent to from the source. A sink is an Addressable or a Callable resource that can receive incoming events from other resources.</li> <li>Knative Services, Channels, and Brokers are all examples of sinks.</li> <li>Amazon S3, SNS, SQS, Kafka, Logger, Redis Sink,</li> </ul> <p>The core components of Knative Eventing include Event Sources, Brokers, Triggers and Sink. Event sources can originate from systems like GitHub (webhooks), Apache Kafka, CronJobs, Kubernetes API server events, or even custom containers that emit CloudEvents. A Broker acts as a central event mesh that receives and buffers incoming events. Triggers are routing rules that filter events from the Broker and forward them to services based on specific criteria. Currently, the most common event delivery mechanism is HTTP using the CloudEvents specification, typically in a push-based manner to HTTP endpoints such as Knative Services.</p> <p>For example, imagine you\u2019ve deployed Knative Eventing with a PingSource that emits an event every minute. This event is sent to a Broker, which acts as an event hub. A Trigger listens on that Broker and filters events based on attributes like the event type. When a matching event arrives, the Trigger forwards it to a Knative Service (a containerized HTTP handler). Behind the scenes, Knative handles service discovery, traffic routing, autoscaling (even from zero), and ensures that the container is activated just-in-time to handle the event. This creates a seamless, scalable, and efficient event-driven pipeline without needing to manage infrastructure manually.</p>"},{"location":"dataops-mlops/knative-introduction/#serving","title":"Serving","text":""},{"location":"dataops-mlops/knative-introduction/#resources","title":"Resources","text":"Knative Serving Resources: Services, Routes, Configurations, Revisions <ul> <li>Service: The main entry point that manages the full lifecycle of your app.</li> <li>Route: Sends traffic to specific revisions, with support for splitting and naming.</li> <li>Configuration: Stores deployment settings; changes create new revisions.</li> <li>Revision: A read-only snapshot of code and config that auto-scales with traffic.</li> </ul>"},{"location":"dataops-mlops/knative-introduction/#componenets","title":"Componenets","text":"Knative Serving Architecture <ul> <li>Activator:<ul> <li>It is responsible to queue incoming requests (if a Knative Service is scaled-to-zero)</li> <li>It communicates with the autoscaler to bring scaled-to-zero Services back up and forward the queued requests.</li> <li>Activator can also act as a request buffer to handle traffic bursts.</li> </ul> </li> <li>Autoscaler: scale the Knative Services based on configuration, metrics and incoming requests.</li> <li>Controller: manages the state of Knative resources within the cluster</li> <li>Queue-proxy:<ul> <li>The Queue-Proxy is a sidecar container in the Knative Service's Pod.</li> <li>collect metrics and enforcing the desired concurrency when forwarding requests to the user's container</li> <li>a queue if necessary, similar to the Activator.</li> </ul> </li> <li>Webhooks: validate and mutate Knative Resources.</li> </ul>"},{"location":"dataops-mlops/knative-introduction/#networking-layer","title":"Networking Layer","text":"<ul> <li>Knative Serving depends on a Networking Layer that fulfils the Knative Networking Specification.</li> <li>Knative Serving defines an internal <code>KIngress</code> resource, which acts as an abstraction for different multiple pluggable networking layers</li> <li>Currently, three networking layers are available and supported by the community:<ul> <li>net-istio</li> <li>net-kourier</li> <li>net-contour</li> </ul> </li> </ul> <p>How does the network traffic flow?</p> Knative Serving Network Traffic Flow <ul> <li>The <code>Ingress Gateway</code> is used to route requests to the activator (proxy mode) or directly to a Knative Service Pod (serve mode), depending on the mode (proxy/serve, see here for more details).</li> <li>Each networking layer has a controller that is responsible to watch the KIngress resources and configure the Ingress Gateway accordingly.</li> <li>For the Ingress Gateway to be reachable outside the cluster, it must be exposed using a Kubernetes Service of <code>type: LoadBalancer</code> or <code>type: NodePort</code></li> </ul>"},{"location":"dataops-mlops/knative-introduction/#references","title":"References","text":"<ul> <li>Knative Serving | Knative</li> <li>Knative Serving Architecture | Knative</li> <li>Knative Eventing | Knative</li> <li>Event Mesh | Knative</li> <li>Event Sources | Knative</li> <li>About Sinks | Knative</li> <li>About Brokers | Knative</li> <li>Using Triggers | Knative</li> </ul>"},{"location":"dataops-mlops/kserve-installation/","title":"Deploy Kserve on Serverless Mode (Hands-on)","text":"<ul> <li>Install Knative Serving v1.15.0</li> <li>Install Istio v1.22.8</li> </ul>"},{"location":"dataops-mlops/kserve-installation/#install-knative-serving","title":"Install Knative Serving","text":"<p>Install the required custom resources by running the command</p> <pre><code>kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.15.0/serving-crds.yaml\n</code></pre> Result <pre><code>customresourcedefinition.apiextensions.k8s.io/certificates.networking.internal.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/configurations.serving.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/clusterdomainclaims.networking.internal.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/domainmappings.serving.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/ingresses.networking.internal.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/metrics.autoscaling.internal.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/podautoscalers.autoscaling.internal.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/revisions.serving.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/routes.serving.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/serverlessservices.networking.internal.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/services.serving.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/images.caching.internal.knative.dev created\n</code></pre> <p>Install the core components of Knative Serving to you kubernetes cluster (namespace <code>knative-serving</code>) by running the command:</p> <pre><code>kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.15.0/serving-core.yaml\n</code></pre> Result <pre><code>namespace/knative-serving created\nrole.rbac.authorization.k8s.io/knative-serving-activator created\nclusterrole.rbac.authorization.k8s.io/knative-serving-activator-cluster created\nclusterrole.rbac.authorization.k8s.io/knative-serving-aggregated-addressable-resolver created\nclusterrole.rbac.authorization.k8s.io/knative-serving-addressable-resolver created\nclusterrole.rbac.authorization.k8s.io/knative-serving-namespaced-admin created\nclusterrole.rbac.authorization.k8s.io/knative-serving-namespaced-edit created\nclusterrole.rbac.authorization.k8s.io/knative-serving-namespaced-view created\nclusterrole.rbac.authorization.k8s.io/knative-serving-core created\nclusterrole.rbac.authorization.k8s.io/knative-serving-podspecable-binding created\nserviceaccount/controller created\nclusterrole.rbac.authorization.k8s.io/knative-serving-admin created\nclusterrolebinding.rbac.authorization.k8s.io/knative-serving-controller-admin created\nclusterrolebinding.rbac.authorization.k8s.io/knative-serving-controller-addressable-resolver created\nserviceaccount/activator created\nrolebinding.rbac.authorization.k8s.io/knative-serving-activator created\nclusterrolebinding.rbac.authorization.k8s.io/knative-serving-activator-cluster created\ncustomresourcedefinition.apiextensions.k8s.io/images.caching.internal.knative.dev unchanged\ncertificate.networking.internal.knative.dev/routing-serving-certs created\ncustomresourcedefinition.apiextensions.k8s.io/certificates.networking.internal.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/configurations.serving.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/clusterdomainclaims.networking.internal.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/domainmappings.serving.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/ingresses.networking.internal.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/metrics.autoscaling.internal.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/podautoscalers.autoscaling.internal.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/revisions.serving.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/routes.serving.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/serverlessservices.networking.internal.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/services.serving.knative.dev unchanged\nimage.caching.internal.knative.dev/queue-proxy created\nconfigmap/config-autoscaler created\nconfigmap/config-certmanager created\nconfigmap/config-defaults created\nconfigmap/config-deployment created\nconfigmap/config-domain created\nconfigmap/config-features created\nconfigmap/config-gc created\nconfigmap/config-leader-election created\nconfigmap/config-logging created\nconfigmap/config-network created\nconfigmap/config-observability created\nconfigmap/config-tracing created\nhorizontalpodautoscaler.autoscaling/activator created\npoddisruptionbudget.policy/activator-pdb created\ndeployment.apps/activator created\nservice/activator-service created\ndeployment.apps/autoscaler created\nservice/autoscaler created\ndeployment.apps/controller created\nservice/controller created\nhorizontalpodautoscaler.autoscaling/webhook created\npoddisruptionbudget.policy/webhook-pdb created\ndeployment.apps/webhook created\nservice/webhook created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/config.webhook.serving.knative.dev created\nmutatingwebhookconfiguration.admissionregistration.k8s.io/webhook.serving.knative.dev created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/validation.webhook.serving.knative.dev created\nsecret/webhook-certs created\n</code></pre>"},{"location":"dataops-mlops/kserve-installation/#install-networking-layer-istio","title":"Install Networking Layer - Istio","text":"<p>Install Istio 1.22.8</p> <pre><code>curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.22.8 sh -\n</code></pre> Result <pre><code>% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                Dload  Upload   Total   Spent    Left  Speed\n100   102  100   102    0     0    350      0 --:--:-- --:--:-- --:--:--   351\n100  5124  100  5124    0     0   6834      0 --:--:-- --:--:-- --:--:-- 13343\n\nDownloading istio-1.22.8 from https://github.com/istio/istio/releases/download/1.22.8/istio-1.22.8-osx-arm64.tar.gz ...\n\nIstio 1.22.8 download complete!\n\nThe Istio release archive has been downloaded to the istio-1.22.8 directory.\n\nTo configure the istioctl client tool for your workstation,\nadd the /Users/kcl/istio-1.22.8/bin directory to your environment path variable with:\n    export PATH=\"$PATH:/Users/kcl/istio-1.22.8/bin\"\n\nBegin the Istio pre-installation check by running:\n    istioctl x precheck \n\nTry Istio in ambient mode\n    https://istio.io/latest/docs/ambient/getting-started/\nTry Istio in sidecar mode\n    https://istio.io/latest/docs/setup/getting-started/\nInstall guides for ambient mode\n    https://istio.io/latest/docs/ambient/install/\nInstall guides for sidecar mode\n    https://istio.io/latest/docs/setup/install/\n\nNeed more information? Visit https://istio.io/latest/docs/\n</code></pre> <p>Add <code>istioctl</code> to the path</p> <pre><code>export PATH=\"$PATH:/User/kcl/istio-1.22.8/bin\"\n</code></pre> <p>You can easily install and customize your Istio installation with <code>istioctl</code>. It will deploy the resources to your kubernetes cluster in the namespace <code>istio-system</code></p> <pre><code>istioctl install -y\n</code></pre> Result <pre><code>WARNING: Istio 1.22.0 may be out of support (EOL) already: see https://istio.io/latest/docs/releases/supported-releases/ for supported releases\n\u2714 Istio core installed                       \n\u2714 Istiod installed                           \n\u2714 Ingress gateways installed\n\u2714 Installation complete\nMade this installation the default for injection and validation.\n</code></pre> <p>Check the versions</p> <pre><code>istioctl version\n</code></pre> Result <pre><code>client version: 1.22.8\ncontrol plane version: 1.22.8\ndata plane version: 1.22.8 (1 proxies)\n</code></pre> <p>Check all the deployed resources</p> <pre><code>kubectl get all -n istio-system\n</code></pre> Result <pre><code>NAME                                        READY   STATUS    RESTARTS   AGE\npod/istio-ingressgateway-5d4bc8b8c6-ql8tb   1/1     Running   0          34m\npod/istiod-6db4dfd884-k77x4                 1/1     Running   0          34m\n\nNAME                            TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)                                      AGE\nservice/istio-ingressgateway    LoadBalancer   10.97.200.129    &lt;pending&gt;     15021:31297/TCP,80:32665/TCP,443:30210/TCP   34m\nservice/istiod                  ClusterIP      10.110.251.34    &lt;none&gt;        15010/TCP,15012/TCP,443/TCP,15014/TCP        34m\nservice/knative-local-gateway   ClusterIP      10.111.160.103   &lt;none&gt;        80/TCP,443/TCP                               33m\n\nNAME                                   READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/istio-ingressgateway   1/1     1            1           34m\ndeployment.apps/istiod                 1/1     1            1           34m\n\nNAME                                              DESIRED   CURRENT   READY   AGE\nreplicaset.apps/istio-ingressgateway-5d4bc8b8c6   1         1         1       34m\nreplicaset.apps/istiod-6db4dfd884                 1         1         1       34m\n\nNAME                                                       REFERENCE                         TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\nhorizontalpodautoscaler.autoscaling/istio-ingressgateway   Deployment/istio-ingressgateway   &lt;unknown&gt;/80%   1         5         1          34m\nhorizontalpodautoscaler.autoscaling/istiod                 Deployment/istiod                 &lt;unknown&gt;/80%   1         5         1          34m\n</code></pre> <p>To integrate Istio with Knative Serving install the Knative Istio controller by running the command</p> <pre><code>kubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.15.0/net-istio.yaml\n</code></pre> Result <pre><code>clusterrole.rbac.authorization.k8s.io/knative-serving-istio created\ngateway.networking.istio.io/knative-ingress-gateway created\ngateway.networking.istio.io/knative-local-gateway created\nservice/knative-local-gateway created\nconfigmap/config-istio created\npeerauthentication.security.istio.io/webhook created\npeerauthentication.security.istio.io/net-istio-webhook created\ndeployment.apps/net-istio-controller created\ndeployment.apps/net-istio-webhook created\nsecret/net-istio-webhook-certs created\nservice/net-istio-webhook created\nmutatingwebhookconfiguration.admissionregistration.k8s.io/webhook.istio.networking.internal.knative.dev created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/config.webhook.istio.networking.internal.knative.dev created\n</code></pre> <p>Verify the installation</p> <pre><code>kubectl get pods -n knative-serving\n</code></pre> Result <pre><code>NAME                                    READY   STATUS    RESTARTS   AGE\nactivator-d87b89f55-kq9b7               1/1     Running   0          118m\nautoscaler-788d5c7f85-gl9kr             1/1     Running   0          118m\ncontroller-c55cbcc66-cr74s              1/1     Running   0          118m\nnet-istio-controller-85f48fbb75-kr8tp   1/1     Running   0          37m\nnet-istio-webhook-d4f6d7b45-l4kct       1/1     Running   0          37m\nwebhook-65fc6c7bd8-h47wc                1/1     Running   0          118m\n</code></pre>"},{"location":"dataops-mlops/kserve-installation/#configure-dns","title":"Configure DNS","text":"<p>You can configure DNS to prevent the need to run curl commands with a host header.</p> <p>Knative provides a Kubernetes Job called default-domain that configures Knative Serving to use sslip.io as the default DNS suffix. This will only work if the cluster LoadBalancer Service exposes an IPv4 address or hostname, so it will not work with IPv6 clusters or local setups like minikube unless <code>minikube tunnel</code> is running.</p> <pre><code>minikube tunnel\n</code></pre> Result <pre><code>\u2705  Tunnel successfully started\n\n\ud83d\udccc  NOTE: Please do not close this terminal as this process must stay alive for the tunnel to be accessible ...\n\n\u2757  The service/ingress istio-ingressgateway requires privileged ports to be exposed: [80 443]\n\ud83d\udd11  sudo permission will be asked for it.\n\ud83c\udfc3  Starting tunnel for service istio-ingressgateway.\n</code></pre> <pre><code>kubectl get svc istio-ingressgateway -n istio-system\n</code></pre> Result <pre><code>NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                      AGE\nistio-ingressgateway   LoadBalancer   10.97.200.129   127.0.0.1     15021:31297/TCP,80:32665/TCP,443:30210/TCP   71m\n</code></pre> <pre><code>kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.15.0/serving-default-domain.yaml\n</code></pre> Result <pre><code>job.batch/default-domain created\nservice/default-domain-service created\n</code></pre>"},{"location":"dataops-mlops/kserve-installation/#install-cert-manager","title":"Install Cert Manager","text":"<pre><code>kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.17.2/cert-manager.yaml\n</code></pre> Result <pre><code>namespace/cert-manager created\ncustomresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created\ncustomresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created\ncustomresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created\ncustomresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created\ncustomresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created\ncustomresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created\nserviceaccount/cert-manager-cainjector created\nserviceaccount/cert-manager created\nserviceaccount/cert-manager-webhook created\nclusterrole.rbac.authorization.k8s.io/cert-manager-cainjector created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created\nclusterrole.rbac.authorization.k8s.io/cert-manager-cluster-view created\nclusterrole.rbac.authorization.k8s.io/cert-manager-view created\nclusterrole.rbac.authorization.k8s.io/cert-manager-edit created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests created\nclusterrole.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created\nrole.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created\nrole.rbac.authorization.k8s.io/cert-manager:leaderelection created\nrole.rbac.authorization.k8s.io/cert-manager-tokenrequest created\nrole.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created\nrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created\nrolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection created\nrolebinding.rbac.authorization.k8s.io/cert-manager-cert-manager-tokenrequest created\nrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created\nservice/cert-manager-cainjector created\nservice/cert-manager created\nservice/cert-manager-webhook created\ndeployment.apps/cert-manager-cainjector created\ndeployment.apps/cert-manager created\ndeployment.apps/cert-manager-webhook created\nmutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created\n</code></pre>"},{"location":"dataops-mlops/kserve-installation/#install-kserve","title":"Install KServe","text":"<p>Install KServe CRDs</p> <pre><code>helm install kserve-crd oci://ghcr.io/kserve/charts/kserve-crd --version v0.14.1\n</code></pre> Result <pre><code>Pulled: ghcr.io/kserve/charts/kserve-crd:v0.14.1\nDigest: sha256:b5f4f22fae8fa747ef839e1b228e74e97a78416235eb5f35da49110d25b3d1e7\nNAME: kserve-crd\nLAST DEPLOYED: Thu May  1 16:44:05 2025\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <p>Install KServe Resources</p> <pre><code>helm install kserve oci://ghcr.io/kserve/charts/kserve --version v0.14.1\n</code></pre> <p>??? note \"Result?</p> <pre><code>```\nPulled: ghcr.io/kserve/charts/kserve:v0.14.1\nDigest: sha256:e65039d9e91b16d429f5fb56528e15a4695ff106a41eeae07f1f697abe974bd5\nNAME: kserve\nLAST DEPLOYED: Thu May  1 16:44:24 2025\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n```\n</code></pre>"},{"location":"dataops-mlops/kserve-installation/#references","title":"References","text":"<ul> <li>Serverless Deployment</li> </ul>"},{"location":"dataops-mlops/kserve-introduction/","title":"KServe","text":""},{"location":"dataops-mlops/kserve-model-deployment/","title":"KServe Model Deployment (Hands-on)","text":""},{"location":"dataops-mlops/kserve-model-deployment/#prerequisites","title":"Prerequisites","text":""},{"location":"dataops-mlops/kserve-model-deployment/#s3-credential","title":"S3 Credential","text":"secret.yaml sa.yaml"},{"location":"dataops-mlops/kserve-model-deployment/#install-grpcurl","title":"Install grpcurl","text":"<pre><code>brew install grpcurl\n</code></pre>"},{"location":"dataops-mlops/kserve-model-deployment/#deploy-model","title":"Deploy Model","text":"RESTgRPC inference-service-http.yaml <pre><code>kubectl apply -f inference-service-http.yaml\n</code></pre> inference-service-grpc.yaml<pre><code>\n</code></pre> <pre><code>kubectl apply -f inference-service-grpc.yaml\n</code></pre>"},{"location":"dataops-mlops/kserve-model-deployment/#test-endpoints","title":"Test Endpoints","text":"RESTgRPC <pre><code>curl -v \\\n-H \"Host: ${SERVICE_HOSTNAME}\" \\\n-H \"Content-Type: application/json\" \\\n-d @./input_example.json \\\nhttp://127.0.0.1:80/v2/models/mlflow-apple-demand/infer\n</code></pre> Result <pre><code>*   Trying 127.0.0.1:80...\n* Connected to 127.0.0.1 (127.0.0.1) port 80\n&gt; POST /v2/models/mlflow-apple-demand/infer HTTP/1.1\n&gt; Host: mlflow-apple-demand.default.127.0.0.1.sslip.io\n&gt; User-Agent: curl/8.7.1\n&gt; Accept: */*\n&gt; Content-Type: application/json\n&gt; Content-Length: 1089\n&gt; \n* upload completely sent off: 1089 bytes\n&lt; HTTP/1.1 200 OK\n&lt; ce-endpoint: mlflow-apple-demand\n&lt; ce-id: 9ddc841e-a8d4-405f-a7e4-73f7aa9bab09\n&lt; ce-inferenceservicename: mlserver\n&lt; ce-modelid: mlflow-apple-demand\n&lt; ce-namespace: default\n&lt; ce-requestid: 9ddc841e-a8d4-405f-a7e4-73f7aa9bab09\n&lt; ce-source: io.seldon.serving.deployment.mlserver.default\n&lt; ce-specversion: 0.3\n&lt; ce-type: io.seldon.serving.inference.response\n&lt; content-length: 240\n&lt; content-type: application/json\n&lt; date: Fri, 02 May 2025 04:06:58 GMT\n&lt; server: istio-envoy\n&lt; x-envoy-upstream-service-time: 247\n&lt; \n* Connection #0 to host 127.0.0.1 left intact\n{\"model_name\":\"mlflow-apple-demand\",\"id\":\"9ddc841e-a8d4-405f-a7e4-73f7aa9bab09\",\"parameters\":{\"content_type\":\"np\"},\"outputs\":[{\"name\":\"output-1\",\"shape\":[1,1],\"datatype\":\"FP32\",\"parameters\":{\"content_type\":\"np\"},\"data\":[1486.56298828125]}]}\n</code></pre> <pre><code>grpcurl \\\n  -vv \\\n  -plaintext \\\n  -proto ${PROTO_FILE} \\\n  -authority ${SERVICE_HOSTNAME} \\\n  -d @ \\\n  ${INGRESS_HOST}:${INGRESS_PORT} \\\n  inference.GRPCInferenceService.ModelInfer \\\n  &lt;&lt;&lt; $(cat \"$INPUT_PATH\")\n</code></pre>"},{"location":"dataops-mlops/kserve-model-deployment/#references","title":"References","text":"<ul> <li>Deploy MLflow models with InferenceService | KServe</li> <li>Deploy InferenceService with a saved model on S3 | KServe</li> <li>Deploy InferenceService with a saved model on GCS | KServe</li> <li>KServe Debugging Guide</li> <li>Develop ML model with MLflow and deploy to Kubernetes | MLflow</li> </ul>"},{"location":"dataops-mlops/minikube-introduction/","title":"Minikube","text":"<p>\u5728\u5b78\u7fd2 Kubernetes\uff08K8s\uff09\u4e4b\u524d\uff0c\u5f88\u591a\u4eba\u6703\u807d\u5230\u4e00\u500b\u5de5\u5177\u53eb\u505a Minikube\u3002\u9019\u7bc7\u6587\u7ae0\u6703\u544a\u8a34\u4f60\uff1a</p> <ul> <li>Minikube \u662f\u4ec0\u9ebc\uff1f</li> <li>\u5b83\u8207 Kubernetes \u548c Docker \u6709\u4ec0\u9ebc\u95dc\u4fc2\uff1f</li> <li>\u5982\u4f55\u5728 macOS \u4e0a\u5b89\u88dd Minikube\uff1f</li> <li>\u555f\u52d5\u5f8c\u767c\u751f\u4e86\u4ec0\u9ebc\u4e8b\uff1f</li> </ul>"},{"location":"dataops-mlops/minikube-introduction/#minikube_1","title":"\u4ec0\u9ebc\u662f Minikube\uff1f\u5b83\u662f\u505a\u4ec0\u9ebc\u7528\u7684\uff1f\u70ba\u4ec0\u9ebc\u9700\u8981\u5b83\uff1f","text":"<p>Minikube \u662f\u4e00\u500b\u8f15\u91cf\u7d1a\u7684\u5de5\u5177\uff0c\u7528\u4f86\u5728\u672c\u6a5f\uff08local\uff09\u96fb\u8166\u4e0a\u5efa\u7acb\u4e00\u500b\u5c0f\u578b\u7684 Kubernetes \u53e2\u96c6\uff08cluster\uff09\u3002\u5b83\u8b93\u4f60\u53ef\u4ee5\uff1a</p> <ul> <li>\u7df4\u7fd2\u548c\u5b78\u7fd2 Kubernetes \u6307\u4ee4\u8207\u6982\u5ff5</li> <li>\u672c\u6a5f\u6e2c\u8a66 Kubernetes \u90e8\u7f72\uff08\u4f8b\u5982 Ingress\u3001Pod\u3001Service\u3001Volume \u7b49\uff09</li> <li>\u5efa\u7acb\u958b\u767c\u74b0\u5883\uff0c\u6a21\u64ec\u96f2\u7aef\u90e8\u7f72\u524d\u7684\u64cd\u4f5c</li> </ul> <p>\u5982\u679c\u4f60\u6c92\u6709\u96f2\u7aef\u5e33\u865f\uff08\u50cf\u662f GCP\u3001AWS\uff09\uff0c\u6216\u8005\u4e0d\u60f3\u6bcf\u6b21\u6e2c\u8a66\u90fd\u4e0a\u96f2\u7aef\uff0cMinikube \u5c31\u662f\u4e00\u500b\u5f88\u597d\u7684\u9078\u64c7\u3002</p>"},{"location":"dataops-mlops/minikube-introduction/#minikubekubernetesdocker","title":"Minikube\u3001Kubernetes\u3001Docker \u4e4b\u9593\u7684\u95dc\u4fc2","text":"\u5de5\u5177\u540d\u7a31 \u529f\u80fd\u7c21\u4ecb \u8209\u4f8b\u8aaa\u660e Docker \u5bb9\u5668\u5de5\u5177\uff0c\u5e6b\u4f60\u6253\u5305\u61c9\u7528\u7a0b\u5f0f\u548c\u4f9d\u8cf4\u74b0\u5883 \u628a\u4e00\u500b Flask \u61c9\u7528\u5305\u6210 image Kubernetes \u5bb9\u5668\u7de8\u6392\u7cfb\u7d71\uff0c\u5e6b\u4f60\u81ea\u52d5\u5316\u7ba1\u7406\u5927\u91cf container \u904b\u884c\u72c0\u6cc1 \u81ea\u52d5\u5206\u914d\u8cc7\u6e90\u3001\u91cd\u555f\u7570\u5e38\u670d\u52d9 Minikube \u5728\u4f60\u672c\u6a5f\u4e0a\u6a21\u64ec\u4e00\u500b\u5c0f\u578b Kubernetes \u74b0\u5883 \u6e2c\u8a66 K8s \u8a2d\u5b9a\u662f\u5426\u6b63\u78ba <p>\u7c21\u55ae\u4f86\u8aaa\uff1a</p> <ul> <li>Docker \u662f\u88dd\u61c9\u7528\u7684\u300c\u5bb9\u5668\u300d\u3002</li> <li>Kubernetes \u662f\u8ca0\u8cac\u300c\u7ba1\u5bb9\u5668\u300d\u7684\u7cfb\u7d71\u3002</li> <li>Minikube \u5247\u662f\u8b93\u4f60\u300c\u5728\u81ea\u5df1\u96fb\u8166\u4e0a\u8dd1 Kubernetes\u300d\u7684\u5de5\u5177\u3002</li> </ul>"},{"location":"dataops-mlops/minikube-introduction/#macos-minikube","title":"\u5982\u4f55\u5728 macOS \u4e0a\u5b89\u88dd Minikube","text":"<ol> <li>\u5148\u78ba\u8a8d\u5b89\u88dd\u689d\u4ef6\uff1a</li> <li>macOS 10.13 \u4ee5\u4e0a</li> <li>\u5df2\u5b89\u88dd Homebrew</li> <li> <p>\u5df2\u5b89\u88dd\u865b\u64ec\u5316\u74b0\u5883\uff08\u5efa\u8b70\u4f7f\u7528 Docker Desktop\uff09</p> </li> <li> <p>\u4f7f\u7528 Homebrew \u5b89\u88dd Minikube\uff1a</p> <pre><code>brew install minikube\n</code></pre> </li> <li> <p>\u78ba\u8a8d\u5b89\u88dd\u5b8c\u6210\uff1a</p> <pre><code>minikube version\n</code></pre> </li> <li> <p>\u555f\u52d5 Minikube\uff1a</p> <pre><code>minikube start\n</code></pre> </li> </ol>"},{"location":"dataops-mlops/minikube-introduction/#minikube_2","title":"\u555f\u52d5 Minikube \u5f8c\u767c\u751f\u4e86\u4ec0\u9ebc\u4e8b\uff1f","text":"<p>\u7576\u4f60\u57f7\u884c <code>minikube start</code> \u6642\uff0c\u7cfb\u7d71\u6703\u505a\u4ee5\u4e0b\u5e7e\u4ef6\u4e8b\uff1a</p> <ol> <li>\u5efa\u7acb\u4e00\u500b VM \u6216 Container\uff1a</li> <li> <p>\u9810\u8a2d\u4f7f\u7528 Docker driver\uff0c\u5b83\u6703\u900f\u904e Docker \u5efa\u7acb\u4e00\u500b\u6a21\u64ec Kubernetes \u7684\u74b0\u5883\u3002</p> </li> <li> <p>\u4e0b\u8f09 Kubernetes \u6240\u9700\u5143\u4ef6\uff1a</p> </li> <li> <p>\u5305\u62ec kube-apiserver\u3001kubelet\u3001etcd \u7b49\uff0c\u6a21\u64ec\u4e00\u500b\u63a7\u5236\u5e73\u9762\u3002</p> </li> <li> <p>\u555f\u52d5 Kubernetes \u53e2\u96c6\uff1a</p> </li> <li> <p>Minikube \u5e6b\u4f60\u958b\u555f\u4e00\u500b\u300c\u55ae\u7bc0\u9ede\u300d\u7684 K8s \u53e2\u96c6\u3002</p> </li> <li> <p>\u8a2d\u5b9a kubectl \u6307\u5411\u672c\u6a5f\u53e2\u96c6\uff1a</p> </li> <li> <p>\u9019\u6a23\u4f60\u53ef\u4ee5\u76f4\u63a5\u7528 <code>kubectl</code> \u5c0d Minikube \u767c\u6307\u4ee4\u3002</p> </li> <li> <p>\u5efa\u7acb dashboard\uff08\u53ef\u9078\uff09\uff1a</p> <pre><code>minikube dashboard\n</code></pre> <p>\u9019\u6703\u958b\u555f\u700f\u89bd\u5668\u4ecb\u9762\uff0c\u8b93\u4f60\u8996\u89ba\u5316\u5730\u89c0\u5bdf Pod\u3001Service \u7b49\u72c0\u614b\u3002</p> </li> </ol>"},{"location":"dataops-mlops/minikube-introduction/#_1","title":"\u5c0f\u7d50","text":"<p>Minikube \u662f Kubernetes \u7684\u5b78\u7fd2\u597d\u5e6b\u624b\uff0c\u9069\u5408\u672c\u6a5f\u7df4\u7fd2\u8207\u958b\u767c\u3002\u4f60\u53ef\u4ee5\u900f\u904e\u5b83\u5feb\u901f\uff1a</p> <ul> <li>\u5efa\u7acb\u6e2c\u8a66\u74b0\u5883</li> <li>\u7df4\u7fd2 YAML \u6a94\u90e8\u7f72</li> <li>\u7df4\u7fd2 CI/CD \u6216 DevOps \u5de5\u4f5c\u6d41\u7a0b</li> </ul>"},{"location":"dataops-mlops/minio-introduction/","title":"MinIO","text":""},{"location":"dataops-mlops/mlflow-experiment-with-optuna/","title":"MLflow Experiment with Optuna (Hands-on)","text":""},{"location":"dataops-mlops/mlflow-installation/","title":"MLflow Installtion (Hands-on)","text":""},{"location":"dataops-mlops/mlflow-introduction/","title":"MLflow","text":""},{"location":"dataops-mlops/mlserver-introduction/","title":"MLServer","text":"<ul> <li>MLServer is an open source inference server for your machine learning models.</li> <li>MLServer aims to provide an easy way to start serving your machine learning models through a REST and gRPC interface</li> </ul>"},{"location":"dataops-mlops/mlserver-introduction/#inference-runtimes","title":"Inference Runtimes","text":"<ul> <li>Inference runtimes allow you to define how your model should be used within MLServer. You can think of them as the backend glue between MLServer and your machine learning framework of choice.</li> <li>Out of the box, MLServer comes with a set of pre-packaged runtimes which let you interact with a subset of common ML frameworks. This allows you to start serving models saved in these frameworks straight away.</li> </ul> MLServer Inference Runtime MLServer Supported Inference Runtime"},{"location":"dataops-mlops/mlserver-introduction/#openapi-support","title":"OpenAPI Support","text":"<ul> <li>OpenAPI spec: dataplane.json</li> <li>MLServer follows the Open Inference Protocol (previously known as the \u201cV2 Protocol\u201d). fully compliant with KServing\u2019s V2 Dataplane spec.</li> <li>Support Swagger UI: <ul> <li>The autogenerated Swagger UI can be accessed under the <code>/v2/docs</code> endpoint.</li> <li>MLServer will also autogenerate a Swagger UI tailored to individual models, showing the endpoints available for each one. under the following endpoints:<ul> <li><code>/v2/models/{model_name}/docs</code></li> <li><code>/v2/models/{model_name}/versions/{model_version}/docs</code></li> </ul> </li> </ul> </li> </ul> MLServer Swagger UI MLServer Model Swagger UI"},{"location":"dataops-mlops/mlserver-introduction/#parallel-inference","title":"Parallel Inference","text":"MLServer Parallel Inference <ul> <li>Python has some native issues</li> <li>The Global Interpreter Lock (GIL) is a mutex lock that exists in most Python interpreters (e.g. CPython). Its main purpose is to lock Python\u2019s execution so that it only runs on a single processor at the same time. This simplifies certain things to the interpreter. However, it also adds the limitation that a single Python process will never be able to leverage multiple cores.</li> <li>Out of the box, MLServer overcome the python native issue, to support to offload inference workloads to a pool of workers running in separate processes.</li> <li><code>parallel_workers</code> on the <code>settings.json</code> file</li> </ul>"},{"location":"dataops-mlops/mlserver-introduction/#multi-model-servingmms","title":"Multi-Model Serving(MMs)","text":"<ul> <li>within a single instance of MLServer, you can serve multiple models under different paths. This also includes multiple versions of the same model.</li> </ul>"},{"location":"dataops-mlops/mlserver-introduction/#adaptive-batching","title":"Adaptive Batching","text":"<ul> <li>MLServer includes support to batch requests together transparently on-the-fly. We refer to this as \u201cadaptive batching\u201d, although it can also be known as \u201cpredictive batching\u201d.</li> <li>Why? <ul> <li>Maximise resource usage</li> <li>Minimise any inference overhead</li> </ul> </li> <li>\u9700\u8981\u4ed4\u7d30\u8abf\u6574\uff0c\u56e0\u6b64MLServer won\u2019t enable by default adaptive batching on newly loaded models.</li> <li>Usage: <code>max_batch_size</code>, <code>max_batch_time</code> on the <code>model-settings.json</code> file</li> </ul>"},{"location":"dataops-mlops/mlserver-introduction/#metrics","title":"Metrics","text":"<ul> <li>Out-of-the-box, MLServer exposes a set of metrics that help you monitor your machine learning workloads in production.</li> <li>On top of these, you can also register and track your own custom metrics as part of your custom inference runtimes.</li> <li>Default Metrics<ul> <li><code>model_infer_request_success</code>: Number of successful inference requests.</li> <li><code>model_infer_request_failure</code>: Number of failed inference requests.</li> <li><code>batch_request_queue</code>: Queue size for the adaptive batching queue.</li> <li><code>parallel_request_queue</code>: Queue size for the inference workers queue.</li> </ul> </li> <li>REST Metrics<ul> <li><code>[rest_server]_requests</code>: Number of REST requests, labelled by endpoint and status code.</li> <li><code>[rest_server]_requests_duration_seconds</code>: Latency of REST requests.</li> <li><code>[rest_server]_requests_in_progress</code>: Number of in-flight REST requests.</li> </ul> </li> <li>gRPC Metrics<ul> <li><code>grpc_server_handled</code>: Number of gRPC requests, labelled by gRPC code and method.</li> <li><code>grpc_server_started</code>: Number of in-flight gRPC requests.</li> </ul> </li> </ul>"},{"location":"dataops-mlops/mlserver-introduction/#reference","title":"Reference","text":"<ul> <li>Inference Runtimes | MLServer</li> <li>Multi-Model Serving | MLServer</li> </ul>"},{"location":"dataops-mlops/sqlmesh-implementation/","title":"SQLMesh Implementation (Hands-on)","text":""},{"location":"dataops-mlops/sqlmesh-introduction/","title":"SQLMesh Introduciton","text":""},{"location":"dataops-mlops/vertex-ai-feature-store-implementation/","title":"Vertex AI Feature Store Implementation (Hands-on)","text":"main.yaml main.yaml main.yaml main.yaml main.yaml"},{"location":"dataops-mlops/vertex-ai-feature-store-introduction/","title":"Vertex AI Feature Store Introduction","text":""},{"location":"learning-plans/","title":"Learning Plan","text":"<p>Staying curious is at the core of how I grow. This is my personal learning roadmap\u2014what I'm currently exploring, skills I'm sharpening, and resources I've found useful. I believe in learning out loud, and I'm happy to share my journey.</p> <p>2025</p>"},{"location":"learning-plans/2025/","title":"2025","text":"MLOpsKafkaAI Workflow Automation <p>Action Plans</p> <ul> <li> Real-world End to End Machine Learning Ops on Google Cloud</li> <li> MLflow in Action - Master the art of MLOps using MLflow tool</li> </ul> <p>Action Plans</p> <ul> <li> Apache Kafka Series - Learn Apache Kafka for Beginners v3</li> </ul> <p>Action Plans</p> <ul> <li> n8n</li> <li> make</li> </ul>"},{"location":"side-projects/","title":"Side Projects","text":"<p>I love exploring new ideas and building things in my spare time. Here, you'll find a collection of side projects that showcase my interests, technical skills, and curiosity beyond work. Each project reflects my hands-on approach to learning and creating.</p> <ul> <li> <p> Data2ML Ops</p> <p>Tools and workflows for bridging data engineering and machine learning.</p> <p>\u2192 Data2ML Ops</p> </li> <li> <p> Trending Content Prediction</p> <p>Predicting popular content using machine learning and analytics.</p> <p>\u2192 Trending Content Prediction</p> </li> <li> <p> Restful APIs with Flask</p> <p>Building scalable APIs with Flask and Python for web applications.</p> <p>\u2192 Restful APIs with Flask</p> </li> <li> <p> Data Structures &amp; Algorithms</p> <p>Exploring foundational concepts for efficient problem-solving in code.</p> <p>\u2192 Data Structures &amp; Algorithms</p> </li> </ul>"},{"location":"side-projects/cross-cloud-unified-sql-data-pipelines/","title":"Cross Cloud Unified Sql Data Pipelines","text":""},{"location":"side-projects/sql-based-rag-application/","title":"SQL-based RAG Application","text":""},{"location":"side-projects/sql-based-rag-application/#create-a-bucket-in-gcs-and-upload-a-pdf-file","title":"Create a Bucket in GCS and upload a pdf file","text":""},{"location":"side-projects/sql-based-rag-application/#create-a-object-table-in-bigquery","title":"Create a Object Table in BigQuery","text":"<pre><code>create or replace external table `us_test2.pdf`\nwith connection `us.bg-object-tables`\noptions(\n  object_metadata = 'SIMPLE',\n  uris = ['gs://kcl-us-test/scf23.pdf']\n);\n</code></pre>"},{"location":"side-projects/sql-based-rag-application/#create-a-layout-parser-type-of-processor-in-document-ai-and-a-remote-model-corresponding-to-the-processor","title":"Create a Layout Parser type of Processor in Document AI and a Remote Model corresponding to the processor","text":"<pre><code>create or replace model `us_test2.doc_parser`\nremote with connection `us.document_ai`\noptions(\n  remote_service_type='CLOUD_AI_DOCUMENT_V1',\n  document_processor='ec023753643cb1be'\n);\n</code></pre>"},{"location":"side-projects/sql-based-rag-application/#create-a-embeddings-remote-model-in-bigquery","title":"Create a embeddings Remote Model in BigQuery","text":"<pre><code>create or replace model `us_test2.embedding_model`\nremote with connection `us.vertex_ai`\noptions (\n  endpoint='text-embedding-004'\n)\n</code></pre>"},{"location":"side-projects/sql-based-rag-application/#create-a-generative-text-remote-model-in-bigquery","title":"Create a generative text Remote Model in BigQuery","text":"<pre><code>create or replace model `us_test2.text_model`\nremote with connection `us.vertex_ai`\noptions(\n  endpoint = 'gemini-1.5-flash-002'\n)\n</code></pre>"},{"location":"side-projects/data-mesh/","title":"Data Mesh","text":"<p>Homepage</p>"},{"location":"side-projects/data-mesh/docs/org/","title":"Organization","text":""},{"location":"side-projects/dcard-hw/","title":"2020 Dcard Data Engineering Intern","text":"<p>Dcard is a popular social media platform in Taiwan, especially among college students and young adults. It was launched in 2011 as a university-only online forum, similar in spirit to how Facebook started within universities.</p> <p>This project is a pre-interview assignment for the 2020 Dcard Data Engineer Internship Program.</p> <p>On Dcard's app and website, there is an important section called \"Trending Posts,\" where users can find the hottest discussion topics on the platform. As data enthusiasts, we are also curious about which posts have the potential to become trending. If we consider this factor in our recommendations, we might help users discover great posts faster. Therefore, in this assignment, we aim to predict whether a post has the potential to appear in the \"Trending Posts\" section based on some data.</p> <p>To simplify the problem, we define a trending post as one that receives at least 1000 likes within 36 hours of being posted. During testing, we will calculate whether a post's like count exceeds 1000 within 36 hours to determine the ground truth or prediction benchmark.</p> <p>Abstract</p> <pre><code>$ tree\n.\n\u251c\u2500\u2500 requirements.txt: A list of required Python packages and their versions.\n\u251c\u2500\u2500 preprocessing.py: A shared utility script for database connections, preprocessing, and other common functions.\n\u251c\u2500\u2500 training.py: A utility script for training the model.\n\u251c\u2500\u2500 predict.py: A utility script for making predictions.\n\u251c\u2500\u2500 outputs\n\u2502   \u251c\u2500\u2500 best_model.h5: The best model obtained after training.\n\u2502   \u251c\u2500\u2500 cv_results.csv: Cross-validation results.\n\u2502   \u2514\u2500\u2500 output.csv: Prediction results for the public testing dataset.\n\u2514\u2500\u2500 eda_evaluation.ipynb: A Jupyter notebook used for generating visualizations.\n</code></pre> <p>The training dataset includes articles spanning from April 1, 2019, to the end of October 2019, covering approximately seven months. The dataset contains around 793,000 articles, of which about 2.32% (approximately 18,000 articles) are classified as popular. Through exploratory data analysis, we observed high correlations among variables. Additionally, the timing of article publication significantly influences the proportion of popular articles and the total number of likes within the first 36 hours of posting.</p> <p>We decided to use a \"binary classification model without considering sequential information\" as our primary approach, focusing on handling imbalanced datasets, tree-based ensemble models, and subsequent discussions. The training process was divided into three main stages:</p> <ol> <li>Resampling</li> <li>Feature Transformation</li> <li>Classification</li> </ol> <p>After experimentation, we opted to omit the \"Feature Transformation\" stage. In total, 108 combinations were tested using <code>GridSearchCV</code> with <code>cv=3</code> to find the optimal configuration.</p> <p>Using the f1-score as the evaluation metric, the best-performing model was an <code>AdaBoostClassifier</code> without any resampling. This model consisted of 100 decision trees, each limited to a depth of 2. The average f1-score from cross-validation was 0.56, while the f1-score on the public test set was 0.53. Key findings from the experiments include:</p> <ul> <li>Different resampling strategies significantly impact the f1-score.</li> <li>Resampling strategies can effectively identify genuinely popular articles. However, this comes at the cost of reduced trust in the model's predictions of popular articles.</li> <li>Under both \"SMOTE resampling\" and \"no resampling\" scenarios, the choice of classifier did not lead to substantial changes in the f1-score.</li> <li>The choice of classifier had a relatively minor impact on the f1-score.</li> </ul> <p>Finally, we discussed several potential future directions, including exploring other resampling techniques, alternative evaluation metrics, and incorporating sequential information.</p>"},{"location":"side-projects/dcard-hw/#training-dataset","title":"Training Dataset","text":"<p>The training dataset covers posts from April 1, 2019, to the end of October 2019, approximately 7 months. It contains around 794,000 posts, of which about 2.32% (approximately 18,000 posts) are trending.</p> <pre><code>posts_train                Contains 793,751 records and 3 columns\npost_shared_train          Contains 304,260 records and 3 columns\npost_comment_created_train Contains 2,372,228 records and 3 columns\npost_liked_train           Contains 3,395,903 records and 3 columns\npost_collected_train       Contains 1,235,126 records and 3 columns\n</code></pre> <p>Table: <code>posts_train</code></p> column_name data_type description <code>post_key</code> string Unique identifier of the post <code>created_at_hour</code> datetime The hour when the post was created <code>like_count_36_hour</code> integer Number of likes the post received within 36 hours (only in train table) <p>Table: <code>post_shared_train</code></p> column_name data_type description <code>post_key</code> string Unique identifier of the post <code>created_at_hour</code> datetime The hour of the sharing activity <code>count</code> integer Number of shares the post received in that hour <p>Table: <code>post_comment_created_train</code></p> column_name data_type description <code>post_key</code> string Unique identifier of the post <code>created_at_hour</code> datetime The hour of the comment activity <code>count</code> integer Number of comments the post received in that hour <p>Table: <code>post_liked_train</code></p> column_name data_type description <code>post_key</code> string Unique identifier of the post <code>created_at_hour</code> datetime The hour of the like activity <code>count</code> integer Number of likes the post received in that hour <p>Table: <code>post_collected_train</code></p> column_name data_type description <code>post_key</code> string Unique identifier of the post <code>created_at_hour</code> datetime The hour of the collection activity <code>count</code> integer Number of times the post was bookmarked in that hour"},{"location":"side-projects/dcard-hw/#testing-dataset","title":"Testing Dataset","text":"<pre><code>posts_test                 Contains 225,986 records and 3 columns\npost_shared_test           Contains 83,376 records and 3 columns\npost_comment_created_test  Contains 607,251 records and 3 columns\npost_liked_test            Contains 908,910 records and 3 columns\npost_collected_test        Contains 275,073 records and 3 columns\n</code></pre>"},{"location":"side-projects/dcard-hw/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>For offline evaluation, only the first 10 hours of data for each post will be used as input for prediction. The primary evaluation metric is the F1-score.</p>"},{"location":"side-projects/dcard-hw/#submission-requirements","title":"Submission Requirements","text":"<p>Upon completing the assignment, you must submit at least the following four files. Failure to include any of these will be considered incomplete.</p> <ol> <li><code>Report.pdf</code><ul> <li>Instructions on how to use your code</li> <li>Methods and rationale</li> <li>Evaluation results on the provided testing data</li> <li>Experimental observations</li> </ul> </li> <li><code>train.py</code></li> <li><code>predict.py</code></li> <li><code>requirements.txt</code> or Pipfile</li> <li>(Optional) If your prediction requires a model file, please include it (we will not train it for you) and explain how to use it in Report.pdf.</li> </ol> <p>We have some requirements for the program structure to facilitate testing:</p> <ul> <li> <p>Training</p> <ul> <li>The outermost layer should be wrapped in train.py.</li> <li>The program should be executable as <code>python train.py {database_host} {model_filepath}</code>.</li> <li>Example: <code>python train.py localhost:8080 ./model.h5</code></li> </ul> </li> <li> <p>Prediction</p> <ul> <li>The program should be executable as <code>python predict.py {database_host} {model_filepath} {output_filepath}</code>.</li> <li>Specify where your model_filepath is located.</li> <li>Example: <code>python predict.py localhost:8080 ./model.h5 ./sample_output.csv</code></li> <li>Your program must achieve the following during prediction:<ul> <li>Read data from the database. The data format will match the tables described in the next section. For evaluation, we will use our own test data.</li> <li>Use another database's xxx_test tables as the test set during actual testing. Your predict.py should use these tables as input.</li> <li>Output a CSV file with two columns as shown below, including a header (refer to the provided sample_output.csv):<ul> <li>post_key: string type</li> <li>is_trending: bool type</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"side-projects/dcard-hw/#usage-instructions","title":"Usage Instructions","text":"<p>Environment:</p> <ul> <li>Operating System: Ubuntu 18.04 LTS Desktop</li> <li>Python version: Python 3.6.8</li> <li>Required Python packages and their versions are listed in <code>requirements.txt</code>.</li> </ul>"},{"location":"side-projects/dcard-hw/#trainingpy","title":"<code>training.py</code>","text":"<p>The usage of <code>training.py</code> is as follows:</p> <pre><code>usage: training.py [-h] -u USERNAME -p PASSWORD --host HOSTNAME\n                   [--port PORTNUMBER] [--protocol PROTOCOL]\n                   DATABASE OUTPUT_PATH\n</code></pre> <p>At a minimum, you must provide five parameters: \"username,\" \"password,\" \"host IP address,\" \"database name,\" and \"output path.\" To train on the training set, use the following command:</p> <pre><code>python training.py -u \"USERNAME\"\\\n                   -p \"PASSWORD\"\\\n                   --host \"HOSTNAME\"\\\n                   \"DATABASE\"\\\n                   \"OUTPUT_PATH\"\n</code></pre> <p>By default, the program connects to a PostgreSQL database on port 5432. If needed, you can use the <code>--protocol</code> and <code>--port</code> options to connect to other databases, such as MySQL:</p> Note <pre><code>python training.py -u \"USERNAME\"\\\n                -p \"PASSWORD\"\\\n                --host \"HOSTNAME\"\\\n                --port \"3306\"\\\n                --protocol \"mysql\"\\\n                \"DATABASE\"\\\n                \"OUTPUT_PATH\"\n</code></pre> <p>Danger</p> <p>After training, the program generates two files: \"best model\" and \"cross-validation results.\" The default filenames are <code>best_model.h5</code> and <code>cv_results.csv</code> (these cannot be changed). Therefore, when specifying <code>OUTPUT_PATH</code>, only the folder name is required.</p> <p>For more details, use the <code>-h</code> or <code>--help</code> options:</p> Note <pre><code>$ python training.py -h\nusage: training.py [-h] -u USERNAME -p PASSWORD --host HOSTNAME\n                [--port PORTNUMBER] [--protocol PROTOCOL]\n                DATABASE OUTPUT_PATH\n\npositional arguments:\nDATABASE             (Required) Database to use when connecting to server.\nOUTPUT_PATH          (Required) Best prediction model and cross validation\n                    results outputs file path.\n\noptional arguments:\n-h, --help           show this help message and exit\n-u USERNAME          (Required) User for login if not current user.\n-p PASSWORD          (Required) Password to use when connecting to server.\n--host HOSTNAME      (Required) Host address to connect.\n--port PORTNUMBER    Port number to use for connection (default: 5432)\n--protocol PROTOCOL  Protocol to connect. (default: postgres)\n</code></pre>"},{"location":"side-projects/dcard-hw/#predictpy","title":"<code>predict.py</code>","text":"<p>The usage of <code>predict.py</code> is as follows:</p> <pre><code>usage: predict.py [-h] -u USERNAME -p PASSWORD --host HOSTNAME\n                  [--port PORTNUMBER] [--protocol PROTOCOL] [-n]\n                  DATABASE MODEL_NAME OUTPUT_PATH\n</code></pre> <p>Similar to <code>training.py</code>, you must provide five parameters, with an additional parameter for the \"model path\" used to predict trending posts. To predict on the public test set, use the following command:</p> <pre><code>python predict.py -u \"USERNAME\"\\\n                  -p \"PASSWORD\"\\\n                  --host \"HOSTNAME\"\\\n                  \"DATABASE\"\\\n                  \"MODELNAME\"\\\n                  \"OUTPUT_PATH\"\n</code></pre> <p>After execution, the program outputs <code>output.csv</code> (filename cannot be changed) to the specified folder. Note that the <code>MODEL_NAME</code> option must include the model file name, not the folder path.</p> <p>As mentioned in the \"Assignment Supplementary Notes and Corrections\" email, the <code>posts_test</code> table in the private test set does not include the <code>like_count_36_hour</code> column. Therefore, you must use the <code>-n</code> option to indicate that this column is absent. To predict on the private test set, use the following command:</p> <pre><code>python predict.py -u \"USERNAME\"\\\n                  -p \"PASSWORD\"\\\n                  --host \"HOSTNAME\"\\\n                  -n\\\n                  \"DATABASE\"\\\n                  \"MODELNAME\"\\\n                  \"OUTPUT_PATH\"\n</code></pre> <p>If needed, you can also use the <code>--port</code> and <code>--protocol</code> options to connect to other databases.</p> <p>For more details, use the <code>-h</code> or <code>--help</code> options:</p> Note <pre><code>$ python predict.py -h\nusage: predict.py [-h] -u USERNAME -p PASSWORD --host HOSTNAME\n                [--port PORTNUMBER] [--protocol PROTOCOL] [-n]\n                DATABASE MODEL_NAME OUTPUT_PATH\n\npositional arguments:\nDATABASE             (Required) Database to use when connecting to server.\nMODEL_NAME           (Required) Prediction model name. If it is not in the\n                    current directory, please specify where it is.\nOUTPUT_PATH          (Required) File path of predicted results.\n\noptional arguments:\n-h, --help           show this help message and exit\n-u USERNAME          (Required) User for login if not current user.\n-p PASSWORD          (Required) Password to use when connecting to server.\n--host HOSTNAME      (Required) Host address to connect.\n--port PORTNUMBER    Port number to use for connection (default: 5432)\n--protocol PROTOCOL  Protocol to connect. (default: postgres)\n-n                   No like_count_36_hour column when the option is given.\n</code></pre>"},{"location":"side-projects/dcard-hw/eda_evaluation/","title":"\u532f\u5165\u76f8\u95dc\u5957\u4ef6","text":"Table of Contents <ul><li>1\u00a0\u00a0\u532f\u5165\u76f8\u95dc\u5957\u4ef6</li><li>2\u00a0\u00a0\u4e8b\u524d\u6e96\u5099</li><li>3\u00a0\u00a0EDA</li><li>4\u00a0\u00a0Evaluation<ul><li>4.1\u00a0\u00a0Resampler</li><li>4.2\u00a0\u00a0Resampler + Classifier</li><li>4.3\u00a0\u00a0Classifier</li><li>4.4\u00a0\u00a0Classifier + n_estimator</li><li>4.5\u00a0\u00a0<code>AdaBoostClassifier</code> + <code>max_depth</code></li><li>4.6\u00a0\u00a0<code>GradientBoostingClassifier</code>, <code>XGBClassifier</code> + <code>learning_rate</code></li><li>4.7\u00a0\u00a0Best Model<ul><li>4.7.1\u00a0\u00a0f1-score</li><li>4.7.2\u00a0\u00a0balanced accuracy</li></ul></li></ul></li></ul> In\u00a0[1]: Copied! <pre># Import built-in packages\nfrom math import isnan\nfrom functools import reduce\n\n# Import 3-rd party packages\nimport sqlalchemy\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom plotnine import *\n</pre> # Import built-in packages from math import isnan from functools import reduce  # Import 3-rd party packages import sqlalchemy import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from plotnine import * In\u00a0[2]: Copied! <pre>def print_info(info, width=61, fillchar='='):\n    \"\"\"\n    \u5370\u51fa\u683c\u5f0f\u5316\u7684\u8cc7\u8a0a\n    \"\"\"\n    temp_width = width - (width-len(info))//2\n    print(info.rjust(temp_width, fillchar).ljust(width, fillchar))\n</pre> def print_info(info, width=61, fillchar='='):     \"\"\"     \u5370\u51fa\u683c\u5f0f\u5316\u7684\u8cc7\u8a0a     \"\"\"     temp_width = width - (width-len(info))//2     print(info.rjust(temp_width, fillchar).ljust(width, fillchar)) In\u00a0[3]: Copied! <pre>def get_connector(user, host, database, password=None, port='5432', protocol='postgres'):\n    \"\"\"\n    \u53d6\u5f97\u9023\u7dda\u5f15\u64ce\uff0c\u9810\u8a2d\u70ba\u9023\u7dda\u81f3 PostgreSQL\uff0c\u57e0\u865f\u9810\u8a2d\u70ba 5432\u3002\n    \"\"\"\n    print_info(\"GETTING CONNECTOR START!\")\n    user_info = f'{user}:{password}' if password else user\n    url = f'{protocol}://{user_info}@{host}:{port}/{database}'\n    engine = sqlalchemy.create_engine(url, client_encoding='utf-8')\n    print_info(\"DONE!\")\n    return engine\n</pre> def get_connector(user, host, database, password=None, port='5432', protocol='postgres'):     \"\"\"     \u53d6\u5f97\u9023\u7dda\u5f15\u64ce\uff0c\u9810\u8a2d\u70ba\u9023\u7dda\u81f3 PostgreSQL\uff0c\u57e0\u865f\u9810\u8a2d\u70ba 5432\u3002     \"\"\"     print_info(\"GETTING CONNECTOR START!\")     user_info = f'{user}:{password}' if password else user     url = f'{protocol}://{user_info}@{host}:{port}/{database}'     engine = sqlalchemy.create_engine(url, client_encoding='utf-8')     print_info(\"DONE!\")     return engine In\u00a0[4]: Copied! <pre>def get_tables(engine, table_names):\n    \"\"\"\n    \u4f9d\u7167 `tables_names` \u7684\u9806\u5e8f\uff0c\u53d6\u5f97 tables\uff0c\u4e26\u4f9d\u5e8f\u5132\u5b58\u65bc `list` \u7576\u4e2d\uff0c\u56de\u50b3\u578b\u614b\u70ba `list`\uff0c\u6bcf\u500b element \u70ba `DataFrame`\u3002\n    \"\"\"\n    print_info(\"GETTING TABLES START!\")\n    rslt = []\n    for tn in table_names:\n        query = f'SELECT * FROM {tn}'\n        exec(f'{tn} = pd.read_sql(query, engine)')\n        # exec(f\"{tn} = pd.read_csv('{tn}.csv', encoding='utf8')\") # from current working directory\n        print(\n            f'{format(tn, \"26s\")} \u7e3d\u5171\u6709 {eval(f\"{tn}.shape[0]\"):9,} \u7b46\u8cc7\u6599\u548c {eval(f\"{tn}.shape[1]\")} \u500b\u6b04\u4f4d')\n        exec(f'rslt.append({tn})')\n    print_info(\"DONE!\")\n    return rslt\n</pre> def get_tables(engine, table_names):     \"\"\"     \u4f9d\u7167 `tables_names` \u7684\u9806\u5e8f\uff0c\u53d6\u5f97 tables\uff0c\u4e26\u4f9d\u5e8f\u5132\u5b58\u65bc `list` \u7576\u4e2d\uff0c\u56de\u50b3\u578b\u614b\u70ba `list`\uff0c\u6bcf\u500b element \u70ba `DataFrame`\u3002     \"\"\"     print_info(\"GETTING TABLES START!\")     rslt = []     for tn in table_names:         query = f'SELECT * FROM {tn}'         exec(f'{tn} = pd.read_sql(query, engine)')         # exec(f\"{tn} = pd.read_csv('{tn}.csv', encoding='utf8')\") # from current working directory         print(             f'{format(tn, \"26s\")} \u7e3d\u5171\u6709 {eval(f\"{tn}.shape[0]\"):9,} \u7b46\u8cc7\u6599\u548c {eval(f\"{tn}.shape[1]\")} \u500b\u6b04\u4f4d')         exec(f'rslt.append({tn})')     print_info(\"DONE!\")     return rslt In\u00a0[5]: Copied! <pre>def merge_tables(tables, table_names, how):\n    \"\"\"\n    \u5408\u4f75\u6240\u6709 tables\uff0c\u56de\u50b3\u578b\u614b\u70ba `DataFrame`\u3002\n    \"\"\"\n    print_info(\"MERGING TABLES START!\")\n    # \u5206\u5225\u8655\u7406 post_{shared, comment_created, liked, collected}_{train, test} \u56db\u500b tables\n    # groupby \u6bcf\u7bc7\u6587\u7ae0\uff0c\u5c07\u524d\u5341\u5c0f\u6642\u7684\u5206\u4eab\u6578\u3001\u8a55\u8ad6\u6578\u3001\u611b\u5fc3\u6578\u3001\u6536\u85cf\u6578\u52a0\u7e3d\u8d77\u4f86\n    for idx, (table, tn) in enumerate(zip(tables, table_names)):\n        if len(tn.split('_'))==2: continue                  # for handling posts_{train, test} table\n        col_name = f\"{tn.split('_')[1]}_count\"              # tn.split('_')[1] is either {shared, comment, liked, collected}\n        mapper = {'count': col_name}\n        exec(f\"tables[{idx}] = table.groupby(['post_key'], as_index=False).sum().rename(columns=mapper)\")\n    # \u5c07 tables \u5408\u4f75\u8d77\u4f86\u4e26\u56de\u50b3\u3002\n    total_df = reduce(lambda left, right: pd.merge(left, right, on=['post_key'], how=how), tables)\n    print_info(\"DONE!\")\n    return total_df\n</pre> def merge_tables(tables, table_names, how):     \"\"\"     \u5408\u4f75\u6240\u6709 tables\uff0c\u56de\u50b3\u578b\u614b\u70ba `DataFrame`\u3002     \"\"\"     print_info(\"MERGING TABLES START!\")     # \u5206\u5225\u8655\u7406 post_{shared, comment_created, liked, collected}_{train, test} \u56db\u500b tables     # groupby \u6bcf\u7bc7\u6587\u7ae0\uff0c\u5c07\u524d\u5341\u5c0f\u6642\u7684\u5206\u4eab\u6578\u3001\u8a55\u8ad6\u6578\u3001\u611b\u5fc3\u6578\u3001\u6536\u85cf\u6578\u52a0\u7e3d\u8d77\u4f86     for idx, (table, tn) in enumerate(zip(tables, table_names)):         if len(tn.split('_'))==2: continue                  # for handling posts_{train, test} table         col_name = f\"{tn.split('_')[1]}_count\"              # tn.split('_')[1] is either {shared, comment, liked, collected}         mapper = {'count': col_name}         exec(f\"tables[{idx}] = table.groupby(['post_key'], as_index=False).sum().rename(columns=mapper)\")     # \u5c07 tables \u5408\u4f75\u8d77\u4f86\u4e26\u56de\u50b3\u3002     total_df = reduce(lambda left, right: pd.merge(left, right, on=['post_key'], how=how), tables)     print_info(\"DONE!\")     return total_df In\u00a0[6]: Copied! <pre>def preprocess_total_df(total_df):\n    \"\"\"\n    \u9810\u8655\u7406\u525b\u5408\u4f75\u597d\u7684 total_df \u4ee5\u7b26\u5408\u5f8c\u7e8c\u5efa\u6a21\u9700\u6c42\uff0c\u56de\u50b3\u578b\u614b\u70ba `DataFrame`\u3002\n    \"\"\"\n    print_info(\"PREPROCESSING TOTAL_DF START!\")\n    total_df.set_index('post_key', inplace=True)                                    # post_key \u6b04\u4f4d\u8a2d\u70ba\u7d22\u5f15\n    total_df['created_at_hour'] = pd.to_datetime(total_df['created_at_hour'])       # \u5c07 created_at_hour \u6b04\u4f4d\u8f49\u63db\u6210 datetime \u578b\u614b\n    total_df['weekday'] = total_df['created_at_hour'].dt.dayofweek                  # \u64f7\u53d6\u51fa\u767c\u6587\u7684 weekday\n    total_df['hour'] = total_df['created_at_hour'].dt.hour                          # \u64f7\u53d6\u51fa\u767c\u6587\u7684 hour\n    total_df.fillna(0, inplace=True)                                                # NaN \u503c\u88dc 0\n    total_df['is_trending'] = 0+(total_df['like_count_36_hour']&gt;=1000)              # \u8f49\u63db\u6210 is_trending \u985e\u5225\u6b04\u4f4d\n    total_df = total_df.drop(['created_at_hour'], axis=1)                           # drop \u6389\u4e0d\u5fc5\u8981\u7684\u6b04\u4f4d\n    # \u5c07\u8a08\u6b21\u6b04\u4f4d\u8f49\u63db\u6210 int \u578b\u614b\n    col_names = ['shared_count', 'comment_count', 'liked_count', 'collected_count']\n    for cn in col_names:\n        total_df[cn] = total_df[cn].astype(dtype='int')\n    print_info(\"DONE!\")\n    return total_df\n</pre> def preprocess_total_df(total_df):     \"\"\"     \u9810\u8655\u7406\u525b\u5408\u4f75\u597d\u7684 total_df \u4ee5\u7b26\u5408\u5f8c\u7e8c\u5efa\u6a21\u9700\u6c42\uff0c\u56de\u50b3\u578b\u614b\u70ba `DataFrame`\u3002     \"\"\"     print_info(\"PREPROCESSING TOTAL_DF START!\")     total_df.set_index('post_key', inplace=True)                                    # post_key \u6b04\u4f4d\u8a2d\u70ba\u7d22\u5f15     total_df['created_at_hour'] = pd.to_datetime(total_df['created_at_hour'])       # \u5c07 created_at_hour \u6b04\u4f4d\u8f49\u63db\u6210 datetime \u578b\u614b     total_df['weekday'] = total_df['created_at_hour'].dt.dayofweek                  # \u64f7\u53d6\u51fa\u767c\u6587\u7684 weekday     total_df['hour'] = total_df['created_at_hour'].dt.hour                          # \u64f7\u53d6\u51fa\u767c\u6587\u7684 hour     total_df.fillna(0, inplace=True)                                                # NaN \u503c\u88dc 0     total_df['is_trending'] = 0+(total_df['like_count_36_hour']&gt;=1000)              # \u8f49\u63db\u6210 is_trending \u985e\u5225\u6b04\u4f4d     total_df = total_df.drop(['created_at_hour'], axis=1)                           # drop \u6389\u4e0d\u5fc5\u8981\u7684\u6b04\u4f4d     # \u5c07\u8a08\u6b21\u6b04\u4f4d\u8f49\u63db\u6210 int \u578b\u614b     col_names = ['shared_count', 'comment_count', 'liked_count', 'collected_count']     for cn in col_names:         total_df[cn] = total_df[cn].astype(dtype='int')     print_info(\"DONE!\")     return total_df In\u00a0[7]: Copied! <pre># Get engine\nengine = get_connector(\n    user=\"candidate\",\n    password=\"dcard-data-intern-2020\",\n    host=\"35.187.144.113\",\n    database=\"intern_task\"\n)\n# Get tables from db\ntable_names_train = ['posts_train', 'post_shared_train', \n                     'post_comment_created_train', 'post_liked_train', 'post_collected_train']\ntables_train = get_tables(engine, table_names_train)\n# Merge tables\ntotal_df_train = merge_tables(tables_train, table_names_train, how='left')\n# Preprocess total_df\ntotal_df_train = preprocess_total_df(total_df_train)\n\nengine.dispose()\n</pre> # Get engine engine = get_connector(     user=\"candidate\",     password=\"dcard-data-intern-2020\",     host=\"35.187.144.113\",     database=\"intern_task\" ) # Get tables from db table_names_train = ['posts_train', 'post_shared_train',                       'post_comment_created_train', 'post_liked_train', 'post_collected_train'] tables_train = get_tables(engine, table_names_train) # Merge tables total_df_train = merge_tables(tables_train, table_names_train, how='left') # Preprocess total_df total_df_train = preprocess_total_df(total_df_train)  engine.dispose() <pre>===================GETTING CONNECTOR START!==================\n============================DONE!============================\n====================GETTING TABLES START!====================\nposts_train                \u7e3d\u5171\u6709   793,751 \u7b46\u8cc7\u6599\u548c 3 \u500b\u6b04\u4f4d\npost_shared_train          \u7e3d\u5171\u6709   304,260 \u7b46\u8cc7\u6599\u548c 3 \u500b\u6b04\u4f4d\npost_comment_created_train \u7e3d\u5171\u6709 2,372,228 \u7b46\u8cc7\u6599\u548c 3 \u500b\u6b04\u4f4d\npost_liked_train           \u7e3d\u5171\u6709 3,395,903 \u7b46\u8cc7\u6599\u548c 3 \u500b\u6b04\u4f4d\npost_collected_train       \u7e3d\u5171\u6709 1,235,126 \u7b46\u8cc7\u6599\u548c 3 \u500b\u6b04\u4f4d\n============================DONE!============================\n====================MERGING TABLES START!====================\n============================DONE!============================\n================PREPROCESSING TOTAL_DF START!================\n============================DONE!============================\n</pre> In\u00a0[8]: Copied! <pre>cv_results = pd.read_csv('./outputs/cv_results.csv')\n</pre> cv_results = pd.read_csv('./outputs/cv_results.csv') In\u00a0[9]: Copied! <pre>temp = total_df_train.drop(columns=['weekday', 'hour', 'is_trending'])\nsns.heatmap(temp.corr(), cmap='YlGnBu')\n</pre> temp = total_df_train.drop(columns=['weekday', 'hour', 'is_trending']) sns.heatmap(temp.corr(), cmap='YlGnBu') Out[9]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x23755717ba8&gt;</pre> In\u00a0[10]: Copied! <pre>mapper = dict(zip([0, 1, 2, 3, 4, 5, 6], ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']))\n</pre> mapper = dict(zip([0, 1, 2, 3, 4, 5, 6], ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])) In\u00a0[11]: Copied! <pre># \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\u7684\u767c\u6587\u6578\nnum_articles_heatmap_df = total_df_train.groupby(['weekday', 'hour']).size().reset_index().rename(columns={0:'count'})\nnum_articles_heatmap_df = num_articles_heatmap_df.pivot(index='weekday', columns='hour', values='count')\nnum_articles_heatmap_df = num_articles_heatmap_df.rename(mapper=mapper, axis=0)\nnum_articles_heatmap_df = num_articles_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday', \n                                                           'Thursday', 'Friday', 'Saturday', 'Sunday'])\nplt.figure(figsize=(20, 5))\nplt.title(f'Number of Articles by Day of Week / Hour of Day')\nsns.heatmap(num_articles_heatmap_df, cmap='OrRd', cbar=False)\n</pre> # \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\u7684\u767c\u6587\u6578 num_articles_heatmap_df = total_df_train.groupby(['weekday', 'hour']).size().reset_index().rename(columns={0:'count'}) num_articles_heatmap_df = num_articles_heatmap_df.pivot(index='weekday', columns='hour', values='count') num_articles_heatmap_df = num_articles_heatmap_df.rename(mapper=mapper, axis=0) num_articles_heatmap_df = num_articles_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday',                                                             'Thursday', 'Friday', 'Saturday', 'Sunday']) plt.figure(figsize=(20, 5)) plt.title(f'Number of Articles by Day of Week / Hour of Day') sns.heatmap(num_articles_heatmap_df, cmap='OrRd', cbar=False) Out[11]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x237490fb518&gt;</pre> In\u00a0[12]: Copied! <pre># \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\u7684\u71b1\u9580\u6587\u7ae0\u6bd4\u4f8b\nnum_pops_heatmap_df = total_df_train.groupby(['weekday', 'hour'])['is_trending'].sum().reset_index()\nnum_pops_heatmap_df = num_pops_heatmap_df.pivot(index='weekday', columns='hour', values='is_trending')\nnum_pops_heatmap_df = num_pops_heatmap_df.rename(mapper=mapper, axis=0)\nnum_pops_heatmap_df = num_pops_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday', \n                                                   'Thursday', 'Friday', 'Saturday', 'Sunday'])\npct_pops_heatmap_df = num_pops_heatmap_df/num_articles_heatmap_df\nplt.figure(figsize=(20, 5))\nplt.title(f'Percentage of Popular Articles by Day of Week / Hour of Day ')\nsns.heatmap(pct_pops_heatmap_df, cmap='Blues', cbar=False)\n</pre> # \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\u7684\u71b1\u9580\u6587\u7ae0\u6bd4\u4f8b num_pops_heatmap_df = total_df_train.groupby(['weekday', 'hour'])['is_trending'].sum().reset_index() num_pops_heatmap_df = num_pops_heatmap_df.pivot(index='weekday', columns='hour', values='is_trending') num_pops_heatmap_df = num_pops_heatmap_df.rename(mapper=mapper, axis=0) num_pops_heatmap_df = num_pops_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday',                                                     'Thursday', 'Friday', 'Saturday', 'Sunday']) pct_pops_heatmap_df = num_pops_heatmap_df/num_articles_heatmap_df plt.figure(figsize=(20, 5)) plt.title(f'Percentage of Popular Articles by Day of Week / Hour of Day ') sns.heatmap(pct_pops_heatmap_df, cmap='Blues', cbar=False) Out[12]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2375e9e5cc0&gt;</pre> In\u00a0[13]: Copied! <pre># \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\uff0c\u524d 10 \u5c0f\u6642\u611b\u5fc3\u5e73\u5747\u6578\nnum_likes_10_heatmap_df = total_df_train.groupby(['weekday', 'hour'])['liked_count'].mean().reset_index()\nnum_likes_10_heatmap_df = num_likes_10_heatmap_df.pivot(index='weekday', columns='hour', values='liked_count')\nnum_likes_10_heatmap_df = num_likes_10_heatmap_df.rename(mapper=mapper, axis=0)\nnum_likes_10_heatmap_df = num_likes_10_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday', \n                                                           'Thursday', 'Friday', 'Saturday', 'Sunday'])\nplt.figure(figsize=(20, 5))\nplt.title(f'Average Likes within 10 hours by Day of Week / Hour of Day')\nsns.heatmap(num_likes_10_heatmap_df, cmap='Purples', cbar=False)\n</pre> # \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\uff0c\u524d 10 \u5c0f\u6642\u611b\u5fc3\u5e73\u5747\u6578 num_likes_10_heatmap_df = total_df_train.groupby(['weekday', 'hour'])['liked_count'].mean().reset_index() num_likes_10_heatmap_df = num_likes_10_heatmap_df.pivot(index='weekday', columns='hour', values='liked_count') num_likes_10_heatmap_df = num_likes_10_heatmap_df.rename(mapper=mapper, axis=0) num_likes_10_heatmap_df = num_likes_10_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday',                                                             'Thursday', 'Friday', 'Saturday', 'Sunday']) plt.figure(figsize=(20, 5)) plt.title(f'Average Likes within 10 hours by Day of Week / Hour of Day') sns.heatmap(num_likes_10_heatmap_df, cmap='Purples', cbar=False) Out[13]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x237625a2ef0&gt;</pre> In\u00a0[14]: Copied! <pre># \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\uff0c\u524d 36 \u5c0f\u6642\u611b\u5fc3\u5e73\u5747\u6578\nnum_likes_36_heatmap_df = total_df_train.groupby(['weekday', 'hour'])['like_count_36_hour'].mean().reset_index()\nnum_likes_36_heatmap_df = num_likes_36_heatmap_df.pivot(index='weekday', columns='hour', values='like_count_36_hour')\nnum_likes_36_heatmap_df = num_likes_36_heatmap_df.rename(mapper=mapper, axis=0)\nnum_likes_36_heatmap_df = num_likes_36_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday', \n                                                           'Thursday', 'Friday', 'Saturday', 'Sunday'])\nplt.figure(figsize=(20, 5))\nplt.title(f'Average Likes within 36 hours by Day of Week / Hour of Day ')\nsns.heatmap(num_likes_36_heatmap_df, cmap='YlGn', cbar=False)\n</pre> # \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\uff0c\u524d 36 \u5c0f\u6642\u611b\u5fc3\u5e73\u5747\u6578 num_likes_36_heatmap_df = total_df_train.groupby(['weekday', 'hour'])['like_count_36_hour'].mean().reset_index() num_likes_36_heatmap_df = num_likes_36_heatmap_df.pivot(index='weekday', columns='hour', values='like_count_36_hour') num_likes_36_heatmap_df = num_likes_36_heatmap_df.rename(mapper=mapper, axis=0) num_likes_36_heatmap_df = num_likes_36_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday',                                                             'Thursday', 'Friday', 'Saturday', 'Sunday']) plt.figure(figsize=(20, 5)) plt.title(f'Average Likes within 36 hours by Day of Week / Hour of Day ') sns.heatmap(num_likes_36_heatmap_df, cmap='YlGn', cbar=False) Out[14]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2376129c400&gt;</pre> In\u00a0[15]: Copied! <pre># \u53bb\u9664\u7528\u4e0d\u5230\u7684\u6b04\u4f4d\u8cc7\u8a0a\ncv_results = cv_results.drop(columns=[col for col in cv_results.columns if 'split' in col])\n# \u5c07\u6b04\u4f4d\u5167\u5bb9\u5316\u7c21\ndef transform(elem, mapper):\n    if type(elem)==float and isnan(elem):\n        return elem\n    for sub_str in mapper:\n        if sub_str in elem:\n            return mapper[sub_str]\n    return elem\n# resampler\nmapper = {\n    'SMOTE': 'SMOTE',\n    'NearMiss': 'NearMiss'\n}\ncv_results['param_resampler'] = cv_results['param_resampler'].apply(transform, args=(mapper,))\n# classifier\nmapper = {\n    'AdaBoostClassifier': 'AdaBoostClassifier',\n    'XGBClassifier': 'XGBClassifier',\n    'GradientBoostingClassifier': 'GradientBoostingClassifier'\n}\ncv_results['param_classifier'] = cv_results['param_classifier'].apply(transform, args=(mapper,))\n# classifier__base_estimator\nmapper = {\n    'max_depth=1': 'DecisionTreeClassifier(max_depth=1)',\n    'max_depth=2': 'DecisionTreeClassifier(max_depth=2)',\n    'max_depth=3': 'DecisionTreeClassifier(max_depth=3)'\n}\ncv_results['param_classifier__base_estimator'] = cv_results['param_classifier__base_estimator'].apply(transform, args=(mapper,))\n</pre> # \u53bb\u9664\u7528\u4e0d\u5230\u7684\u6b04\u4f4d\u8cc7\u8a0a cv_results = cv_results.drop(columns=[col for col in cv_results.columns if 'split' in col]) # \u5c07\u6b04\u4f4d\u5167\u5bb9\u5316\u7c21 def transform(elem, mapper):     if type(elem)==float and isnan(elem):         return elem     for sub_str in mapper:         if sub_str in elem:             return mapper[sub_str]     return elem # resampler mapper = {     'SMOTE': 'SMOTE',     'NearMiss': 'NearMiss' } cv_results['param_resampler'] = cv_results['param_resampler'].apply(transform, args=(mapper,)) # classifier mapper = {     'AdaBoostClassifier': 'AdaBoostClassifier',     'XGBClassifier': 'XGBClassifier',     'GradientBoostingClassifier': 'GradientBoostingClassifier' } cv_results['param_classifier'] = cv_results['param_classifier'].apply(transform, args=(mapper,)) # classifier__base_estimator mapper = {     'max_depth=1': 'DecisionTreeClassifier(max_depth=1)',     'max_depth=2': 'DecisionTreeClassifier(max_depth=2)',     'max_depth=3': 'DecisionTreeClassifier(max_depth=3)' } cv_results['param_classifier__base_estimator'] = cv_results['param_classifier__base_estimator'].apply(transform, args=(mapper,)) In\u00a0[16]: Copied! <pre>temp = cv_results.groupby(['param_resampler'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean()\ntemp.reset_index(inplace=True)\n\n(ggplot(temp, aes(x='param_resampler', y='mean_test_f1_score', fill='param_resampler'))\n + geom_bar(stat=\"identity\")\n + ylim(0,1)\n + scale_fill_brewer('qualitative', 2)\n + geom_text(aes(label='mean_test_f1_score'), format_string='{:.2f}')\n + ggtitle(f'Average F1 Score by Resampler')\n + labs(fill=f'Resampler')\n + xlab('Resampler')\n + ylab(f'Average F1 score'))\n</pre> temp = cv_results.groupby(['param_resampler'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean() temp.reset_index(inplace=True)  (ggplot(temp, aes(x='param_resampler', y='mean_test_f1_score', fill='param_resampler'))  + geom_bar(stat=\"identity\")  + ylim(0,1)  + scale_fill_brewer('qualitative', 2)  + geom_text(aes(label='mean_test_f1_score'), format_string='{:.2f}')  + ggtitle(f'Average F1 Score by Resampler')  + labs(fill=f'Resampler')  + xlab('Resampler')  + ylab(f'Average F1 score')) Out[16]: <pre>&lt;ggplot: (-9223371884558871573)&gt;</pre> In\u00a0[17]: Copied! <pre>(ggplot(temp, aes(x='param_resampler', y='mean_test_recall', fill='param_resampler'))\n + geom_bar(stat=\"identity\")\n + ylim(0,1)\n + scale_fill_brewer('qualitative', 2)\n + geom_text(aes(label='mean_test_recall'), format_string='{:.2f}')\n + ggtitle(f'Average Recall by Resampler')\n + labs(fill=f'Resampler')\n + xlab('Resampler')\n + ylab(f'Average Recall'))\n</pre> (ggplot(temp, aes(x='param_resampler', y='mean_test_recall', fill='param_resampler'))  + geom_bar(stat=\"identity\")  + ylim(0,1)  + scale_fill_brewer('qualitative', 2)  + geom_text(aes(label='mean_test_recall'), format_string='{:.2f}')  + ggtitle(f'Average Recall by Resampler')  + labs(fill=f'Resampler')  + xlab('Resampler')  + ylab(f'Average Recall')) Out[17]: <pre>&lt;ggplot: (-9223371884558718762)&gt;</pre> In\u00a0[18]: Copied! <pre>(ggplot(temp, aes(x='param_resampler', y='mean_test_precision', fill='param_resampler'))\n + geom_bar(stat=\"identity\")\n + ylim(0,1)\n + scale_fill_brewer('qualitative', 2)\n + geom_text(aes(label='mean_test_precision'), format_string='{:.2f}')\n + ggtitle(f'Average Precision by Resampler')\n + labs(fill=f'Resampler')\n + xlab('Resampler')\n + ylab(f'Average Precision'))\n</pre> (ggplot(temp, aes(x='param_resampler', y='mean_test_precision', fill='param_resampler'))  + geom_bar(stat=\"identity\")  + ylim(0,1)  + scale_fill_brewer('qualitative', 2)  + geom_text(aes(label='mean_test_precision'), format_string='{:.2f}')  + ggtitle(f'Average Precision by Resampler')  + labs(fill=f'Resampler')  + xlab('Resampler')  + ylab(f'Average Precision')) Out[18]: <pre>&lt;ggplot: (152294750826)&gt;</pre> In\u00a0[19]: Copied! <pre>(ggplot(temp, aes(x='param_resampler', y='mean_test_balanced_accuracy', fill='param_resampler'))\n + geom_bar(stat=\"identity\")\n + ylim(0,1)\n + scale_fill_brewer('qualitative', 2)\n + geom_text(aes(label='mean_test_balanced_accuracy'), format_string='{:.2f}')\n + ggtitle(f'Average Balanced Accuracy by Resampler')\n + labs(fill=f'Resampler')\n + xlab('Resampler')\n + ylab(f'Average Balanced Accuracy'))\n</pre> (ggplot(temp, aes(x='param_resampler', y='mean_test_balanced_accuracy', fill='param_resampler'))  + geom_bar(stat=\"identity\")  + ylim(0,1)  + scale_fill_brewer('qualitative', 2)  + geom_text(aes(label='mean_test_balanced_accuracy'), format_string='{:.2f}')  + ggtitle(f'Average Balanced Accuracy by Resampler')  + labs(fill=f'Resampler')  + xlab('Resampler')  + ylab(f'Average Balanced Accuracy')) Out[19]: <pre>&lt;ggplot: (-9223371884547166951)&gt;</pre> In\u00a0[20]: Copied! <pre>temp = cv_results.groupby(['param_resampler', 'param_classifier'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean()\ntemp.reset_index(inplace=True)\n\n(ggplot(temp, aes(x='param_resampler', y='mean_test_f1_score', fill='param_classifier'))\n + geom_bar(position='dodge', stat=\"identity\")\n + ylim(0,1)\n + geom_text(aes(label='mean_test_f1_score'), position=position_dodge(width=0.9), format_string='{:.2f}')\n + ggtitle(f'Average F1 Score by Resampler and Classifier')\n + labs(fill=f'Classifier')\n + xlab('Resampler')\n + ylab(f'Average F1 score'))\n</pre> temp = cv_results.groupby(['param_resampler', 'param_classifier'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean() temp.reset_index(inplace=True)  (ggplot(temp, aes(x='param_resampler', y='mean_test_f1_score', fill='param_classifier'))  + geom_bar(position='dodge', stat=\"identity\")  + ylim(0,1)  + geom_text(aes(label='mean_test_f1_score'), position=position_dodge(width=0.9), format_string='{:.2f}')  + ggtitle(f'Average F1 Score by Resampler and Classifier')  + labs(fill=f'Classifier')  + xlab('Resampler')  + ylab(f'Average F1 score')) Out[20]: <pre>&lt;ggplot: (-9223371884550063342)&gt;</pre> In\u00a0[21]: Copied! <pre>temp = cv_results.groupby(['param_classifier'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean()\ntemp.reset_index(inplace=True)\n\n(ggplot(temp, aes(x='param_classifier', y='mean_test_f1_score', fill='param_classifier'))\n + geom_bar(stat=\"identity\")\n + ylim(0,1)\n + geom_text(aes(label='mean_test_f1_score'), format_string='{:.2f}')\n + ggtitle('Average F1 Score by Classifier')\n + labs(fill='Classifier')\n + xlab('Classifier')\n + ylab(f'Average F1 score'))\n</pre> temp = cv_results.groupby(['param_classifier'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean() temp.reset_index(inplace=True)  (ggplot(temp, aes(x='param_classifier', y='mean_test_f1_score', fill='param_classifier'))  + geom_bar(stat=\"identity\")  + ylim(0,1)  + geom_text(aes(label='mean_test_f1_score'), format_string='{:.2f}')  + ggtitle('Average F1 Score by Classifier')  + labs(fill='Classifier')  + xlab('Classifier')  + ylab(f'Average F1 score')) Out[21]: <pre>&lt;ggplot: (-9223371884545924818)&gt;</pre> In\u00a0[22]: Copied! <pre>temp = cv_results.groupby(['param_classifier', 'param_classifier__n_estimators'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean()\ntemp.reset_index(inplace=True)\n\n(ggplot(temp, aes(x='param_classifier__n_estimators', y='mean_test_f1_score', color='param_classifier'))\n + geom_line()\n + geom_point()\n + ylim(0,1)\n + ggtitle('Average F1 Score by Classifier and Number of Estimators')\n + labs(color='Classifier')\n + xlab('Number of Estimators')\n + ylab('Average F1 score'))\n</pre> temp = cv_results.groupby(['param_classifier', 'param_classifier__n_estimators'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean() temp.reset_index(inplace=True)  (ggplot(temp, aes(x='param_classifier__n_estimators', y='mean_test_f1_score', color='param_classifier'))  + geom_line()  + geom_point()  + ylim(0,1)  + ggtitle('Average F1 Score by Classifier and Number of Estimators')  + labs(color='Classifier')  + xlab('Number of Estimators')  + ylab('Average F1 score')) Out[22]: <pre>&lt;ggplot: (-9223371884546480871)&gt;</pre> In\u00a0[23]: Copied! <pre>cv_results[cv_results['param_resampler']=='passthrough'].groupby(['param_classifier', 'param_classifier__base_estimator'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean()\n</pre> cv_results[cv_results['param_resampler']=='passthrough'].groupby(['param_classifier', 'param_classifier__base_estimator'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean() Out[23]: mean_test_precision mean_test_recall mean_test_specificity mean_test_f1_score mean_test_balanced_accuracy param_classifier param_classifier__base_estimator AdaBoostClassifier DecisionTreeClassifier(max_depth=1) 0.738579 0.436288 0.996339 0.548524 0.716314 DecisionTreeClassifier(max_depth=2) 0.759336 0.443006 0.996670 0.559510 0.719838 DecisionTreeClassifier(max_depth=3) 0.755862 0.441223 0.996619 0.557159 0.718921 In\u00a0[24]: Copied! <pre>cv_results[cv_results['param_resampler']=='passthrough'].groupby(['param_classifier', 'param_classifier__learning_rate'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean()\n</pre> cv_results[cv_results['param_resampler']=='passthrough'].groupby(['param_classifier', 'param_classifier__learning_rate'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean() Out[24]: mean_test_precision mean_test_recall mean_test_specificity mean_test_f1_score mean_test_balanced_accuracy param_classifier param_classifier__learning_rate GradientBoostingClassifier 0.025 0.790585 0.395179 0.997518 0.526909 0.696348 0.050 0.780465 0.423388 0.997177 0.548966 0.710282 0.100 0.778204 0.434859 0.997062 0.557939 0.715961 XGBClassifier 0.025 0.754734 0.404196 0.996884 0.526422 0.700540 0.050 0.776283 0.406060 0.997226 0.533204 0.701643 0.100 0.783911 0.419787 0.997256 0.546763 0.708522 In\u00a0[25]: Copied! <pre>print(cv_results[cv_results['rank_test_f1_score']==1]['params'].values[0])\n</pre> print(cv_results[cv_results['rank_test_f1_score']==1]['params'].values[0]) <pre>{'classifier': AdaBoostClassifier(algorithm='SAMME.R',\n                   base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n                                                         class_weight=None,\n                                                         criterion='gini',\n                                                         max_depth=2,\n                                                         max_features=None,\n                                                         max_leaf_nodes=None,\n                                                         min_impurity_decrease=0.0,\n                                                         min_impurity_split=None,\n                                                         min_samples_leaf=1,\n                                                         min_samples_split=2,\n                                                         min_weight_fraction_leaf=0.0,\n                                                         presort='deprecated',\n                                                         random_state=None,\n                                                         splitter='best'),\n                   learning_rate=1.0, n_estimators=100, random_state=None), 'classifier__base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n                       max_depth=2, max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, presort='deprecated',\n                       random_state=None, splitter='best'), 'classifier__n_estimators': 100, 'resampler': 'passthrough'}\n</pre> In\u00a0[26]: Copied! <pre>temp = cv_results[cv_results['rank_test_f1_score']==1]\ntemp[['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy']]\n</pre> temp = cv_results[cv_results['rank_test_f1_score']==1] temp[['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy']] Out[26]: mean_test_precision mean_test_recall mean_test_specificity mean_test_f1_score mean_test_balanced_accuracy 15 0.759668 0.44419 0.996667 0.560527 0.720429 In\u00a0[27]: Copied! <pre>print(cv_results[cv_results['rank_test_balanced_accuracy']==1]['params'].values[0])\n</pre> print(cv_results[cv_results['rank_test_balanced_accuracy']==1]['params'].values[0]) <pre>{'classifier': GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n                           learning_rate=0.1, loss='deviance', max_depth=3,\n                           max_features=None, max_leaf_nodes=None,\n                           min_impurity_decrease=0.0, min_impurity_split=None,\n                           min_samples_leaf=1, min_samples_split=2,\n                           min_weight_fraction_leaf=0.0, n_estimators=100,\n                           n_iter_no_change=None, presort='deprecated',\n                           random_state=None, subsample=1.0, tol=0.0001,\n                           validation_fraction=0.1, verbose=0,\n                           warm_start=False), 'classifier__learning_rate': 0.025, 'classifier__n_estimators': 120, 'resampler': SMOTE(k_neighbors=5, n_jobs=None, random_state=None, sampling_strategy='auto')}\n</pre> In\u00a0[28]: Copied! <pre>temp = cv_results[cv_results['rank_test_balanced_accuracy']==1]\ntemp[['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy']]\n</pre> temp = cv_results[cv_results['rank_test_balanced_accuracy']==1] temp[['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy']] Out[28]: mean_test_precision mean_test_recall mean_test_specificity mean_test_f1_score mean_test_balanced_accuracy 46 0.199325 0.958312 0.908746 0.330003 0.933529"},{"location":"side-projects/dcard-hw/eda_evaluation/","title":"\u532f\u5165\u76f8\u95dc\u5957\u4ef6\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/","title":"\u4e8b\u524d\u6e96\u5099\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#eda","title":"EDA\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#evaluation","title":"Evaluation\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#resampler","title":"Resampler\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#resampler-classifier","title":"Resampler + Classifier\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#classifier","title":"Classifier\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#classifier-n_estimator","title":"Classifier + n_estimator\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#adaboostclassifier-max_depth","title":"<code>AdaBoostClassifier</code> + <code>max_depth</code>\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#gradientboostingclassifier-xgbclassifier-learning_rate","title":"<code>GradientBoostingClassifier</code>, <code>XGBClassifier</code> + <code>learning_rate</code>\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#best-model","title":"Best Model\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#f1-score","title":"f1-score\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#balanced-accuracy","title":"balanced accuracy\u00b6","text":""},{"location":"side-projects/dcard-hw/docs/1-eda/","title":"Exploratory Data Analysis (EDA)","text":"<p>The dataset is divided into training and testing sets. To avoid data leakage, only the training set is analyzed during EDA, leaving the testing set aside.</p> <p>When we first receive a dataset, the initial step is to examine its details, including the number of records and columns in each table. Below is the dataset information as of the update on 2020/04/13:</p> <pre><code>posts_train                Contains 793,751 records and 3 columns\npost_shared_train          Contains 304,260 records and 3 columns\npost_comment_created_train Contains 2,372,228 records and 3 columns\npost_liked_train           Contains 3,395,903 records and 3 columns\npost_collected_train       Contains 1,235,126 records and 3 columns\n</code></pre> <p>The training set covers posts from April 1, 2019, to the end of October 2019, spanning approximately seven months with around 793,000 posts. The goal is to build a predictive model that uses 10-hour post metrics (e.g., shares, comments, likes, and saves) to predict whether a post will receive 1,000 likes within 36 hours, classifying it as a \"popular post.\"</p> <p>Approximately 2.32% of the training posts are popular, equating to about 18,000 posts. This imbalance in the dataset necessitates techniques like over/undersampling during preprocessing and alternative evaluation metrics during model assessment.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#problem-definition","title":"Problem Definition","text":"<p>The task can be approached in four ways, based on \"whether sequence information is considered\" and \"whether the problem is framed as regression or binary classification\":</p> Regression Binary Classification With Sequence Info RNNs (e.g., GRU), traditional time series models (e.g., ARMA, ARIMA) Same as left Without Sequence Info Poisson regression, SVM, tree-based models, etc. Logistic regression, SVM, tree-based models, etc. <p>For simplicity and time constraints, we focus on \"without sequence info\" and \"binary classification,\" aggregating 10-hour metrics and building a binary classification model to predict popular posts. The focus will be on handling imbalanced data, tree-based models, and subsequent discussions.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#relationships-between-variables","title":"Relationships Between Variables","text":"<p>We simplify the dataset to include total shares, comments, likes, and saves within 10 hours and use a heatmap to observe their relationships with the total likes within 36 hours:</p> <p></p> <p>Info</p> <p>Key observations from the heatmap:</p> <ul> <li>Total likes within 36 hours moderately correlate with total likes within 10 hours (.58), shares (.36), and saves (.36), but weakly with comments (.17).</li> <li>Total likes within 10 hours moderately correlate with shares (.63) and saves (.61).</li> <li>Shares and saves within 10 hours moderately correlate (.48).</li> </ul> <p>In simple terms, posts with more likes within 10 hours tend to have more shares and saves. However, the strongest predictor of total likes within 36 hours is the likes within 10 hours. Comments show little correlation with total likes.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#heatmaps-of-key-metrics","title":"Heatmaps of Key Metrics","text":"<p>Danger</p> <p>To protect Dcard's proprietary information, color bars (<code>cbar=False</code>) are omitted, showing only relative relationships.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#total-posts-by-time","title":"Total Posts by Time","text":"<p>We examine whether the number of posts varies across different time periods:</p> <p></p> <p>The x-axis represents 24 hours, and the y-axis represents days of the week.</p> <p>Info</p> <p>Observations:</p> <ul> <li>Posts are concentrated during midday, afternoon, and evening (12:00\u201318:00), with weekdays slightly higher than weekends.</li> <li>The second-highest posting period is weekday mornings (05:00\u201312:00).</li> <li>Posts are relatively fewer during evenings (18:00\u201301:00) on both weekdays and weekends.</li> </ul> <p>These trends are reasonable, as students primarily post during the day. The relatively high number of early morning posts might be due to companies posting content before students wake up.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#popular-post-proportion-by-time","title":"Popular Post Proportion by Time","text":"<p>Next, we analyze whether certain time periods have a higher proportion of popular posts:</p> <p></p> <p>Info</p> <p>Observations:</p> <ul> <li>Posts during late-night and early-morning hours on weekends have a higher likelihood of being popular, likely due to increased user activity during these times.</li> <li>The heatmap confirms that the proportion of popular posts varies by time.</li> </ul>"},{"location":"side-projects/dcard-hw/docs/1-eda/#average-likes-within-10-hours-by-time","title":"Average Likes Within 10 Hours by Time","text":"<p>We then examine the average likes within 10 hours for posts made at different times:</p> <p></p> <p>Info</p> <p>Observations:</p> <ul> <li>Posts made between 21:00\u201311:00 generally receive more likes within 10 hours.</li> <li>Posts made between 11:00\u201321:00, especially during late afternoon and dinner hours, receive fewer likes on average.</li> </ul> <p>This difference might be because students are less active during late afternoon and dinner hours but more active during the evening. Early morning posts are also visible to students the next day.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#average-likes-within-36-hours-by-time","title":"Average Likes Within 36 Hours by Time","text":"<p>Finally, we analyze the average likes within 36 hours for posts made at different times:</p> <p></p> <p>The trends are consistent with the 10-hour analysis and are not elaborated further.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#summary","title":"Summary","text":"<p>Info</p> <p>Key takeaways:</p> <ul> <li>Variables are generally highly correlated. Polynomial transformations (<code>PolynomialFeatures</code>) may not yield significant improvements during feature engineering.</li> <li>Posting time significantly impacts the proportion of popular posts and the number of likes, and this information should be incorporated into the model.</li> </ul>"},{"location":"side-projects/dcard-hw/docs/2-training/","title":"Feature Engineering","text":"<p>After conducting exploratory data analysis (EDA), we gained a deeper understanding of the training dataset. Before diving into feature engineering, we first organize the training dataset into the following format:</p> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 793751 entries, 0 to 793750\nData columns (total 8 columns):\npost_key           793751 non-null object\nshared_count       793751 non-null int64\ncomment_count      793751 non-null int64\nlike_count         793751 non-null int64\ncollected_count    793751 non-null int64\nweekday            793751 non-null int64\nhour               793751 non-null int64\nis_trending        793751 non-null int64\ndtypes: bool(1), int64(6), object(1)\nmemory usage: 43.1+ MB\n</code></pre> <p>In this section, we will discuss the techniques and models used throughout the data pipeline, including over/undersampling, polynomial transformations, one-hot encoding, and tree-based models.</p> <p>Info</p> <p>The training process can be divided into three main stages:</p> <ol> <li>Resampling</li> <li>Column Transformation</li> <li>Classification</li> </ol> <p>These stages can be represented as the following <code>Pipeline</code> object:</p> <pre><code>cachedir = mkdtemp()\npipe = Pipeline(steps=[('resampler', 'passthrough'),\n                       # ('columntransformer', 'passthrough'),\n                       ('classifier', 'passthrough')],\n                memory=cachedir)\n</code></pre> <p>For each stage, we experiment with two to three different approaches and several hyperparameter settings to identify the optimal combination.</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#handling-imbalanced-datasets-stage-1","title":"Handling Imbalanced Datasets (STAGE 1)","text":"<p>In a binary classification problem, an imbalanced dataset refers to a scenario where the target variable (\\(y\\)) is predominantly of one class (majority) with only a small proportion belonging to the other class (minority). </p> <p>Training a model on such a dataset without addressing the imbalance often results in a biased model that predicts most samples as the majority class, ignoring valuable information from the minority class.</p> <p>A potential solution is resampling, which can be categorized into oversampling and undersampling:</p> <ul> <li>Oversampling: Increases the proportion of minority samples in the dataset.</li> <li>Undersampling: Reduces the proportion of majority samples in the dataset.</li> </ul> <p>Both methods help the model pay more attention to minority samples during training. The simplest approach is random sampling, where majority samples are removed, or minority samples are duplicated.</p> <p>The <code>imblearn</code> library provides implementations for various resampling techniques, including <code>RandomOverSampler</code> and <code>RandomUnderSampler</code>. Additionally, we utilize <code>SMOTE</code> and <code>NearMiss</code>. Below is a brief overview:</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#smote","title":"SMOTE","text":"<p>SMOTE (Synthetic Minority Oversampling Technique) is an oversampling method that synthesizes new minority samples between existing ones, increasing the proportion of the minority class. The following diagram illustrates this:</p> <p> Image Source</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#nearmiss","title":"NearMiss","text":"<p>NearMiss is an undersampling method with three versions. We focus on NearMiss-1, which calculates the average distance of all majority samples to their \\(k\\) nearest minority neighbors and removes the majority samples closest to the minority samples until the class ratio is 1:1. The following diagram illustrates this:</p> <p> Image Source</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#polynomial-transformation-and-one-hot-encoding-stage-2","title":"Polynomial Transformation and One-hot Encoding (STAGE 2)","text":"<p>Next, we use <code>sklearn</code>'s <code>PolynomialFeatures</code> and <code>OneHotEncoder</code> to transform specific features:</p> <ul> <li>For <code>shared_count</code>, <code>comment_count</code>, <code>liked_count</code>, and <code>collected_count</code>, we apply second-degree polynomial transformations to capture non-linear relationships and interactions between features.</li> <li>For the <code>weekday</code> feature, we convert integer values (<code>0</code> - <code>6</code>, representing Monday to Sunday) into one-hot encoded vectors, e.g., <code>[1, 0, 0, 0, 0, 0, 0]</code> for Monday.</li> </ul>"},{"location":"side-projects/dcard-hw/docs/2-training/#tree-based-ensemble-models-stage-3","title":"Tree-based Ensemble Models (STAGE 3)","text":"<p>For classification, we primarily use tree-based ensemble models, including <code>AdaBoostClassifier</code>, <code>GradientBoostingClassifier</code>, and <code>XGBClassifier</code>. These models are chosen for several reasons:</p> <ul> <li>They are invariant to monotonic transformations of features, reducing the need for extensive feature engineering.</li> <li>They offer high interpretability, making it easier to understand feature importance.</li> <li>They perform well on large and complex datasets and are often top performers in Kaggle competitions (e.g., <code>XGBoost</code>, <code>LightGBM</code>, <code>CatBoost</code>).</li> </ul> <p>Ensemble learning can be categorized into Bagging (bootstrap aggregating) and Boosting.</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#bagging","title":"Bagging","text":"<p>The most well-known Bagging application is Random Forest, which builds multiple decision trees using bootstrap sampling and random feature selection. Each tree learns a subset of features, and their predictions are aggregated for the final result.</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#boosting","title":"Boosting","text":""},{"location":"side-projects/dcard-hw/docs/2-training/#adaptive-boosting","title":"Adaptive Boosting","text":"<p>Adaptive Boosting (AdaBoost) sequentially builds \\(T\\) weak learners \\(h_t(x)\\), with each model focusing on samples misclassified by the previous one. Each model is assigned a weight \\(\\alpha_t\\) based on its performance:</p> <ul> <li>Higher weights indicate better performance.</li> <li>Lower weights indicate worse performance.</li> </ul> <p>The final model \\(H(x)\\) aggregates the predictions of all \\(T\\) weak learners. For more details, refer to this note.</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#gradient-boosting","title":"Gradient Boosting","text":"<p>Gradient Boosting builds \\(T\\) models \\(h_t(x)\\) sequentially, where each model predicts the gradient (pseudo-residuals) of the previous model's errors. The final model \\(H(x)\\) is the sum of all previous models. For mathematical derivations, refer to this note.</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#extreme-gradient-boosting","title":"Extreme Gradient Boosting","text":"<p>XGBoost is an optimized implementation of Gradient Boosting with enhancements like weighted quantile sketch, parallel learning, and cache-aware access. For more details, refer to this paper.</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#hyperparameter-optimization","title":"Hyperparameter Optimization","text":"<p>After understanding the techniques used in each stage, we define the possible methods and hyperparameter combinations for each stage as follows:</p> <p>Resampler - <code>passthrough</code>: No resampling. - <code>NearMiss</code>: Default parameters. - <code>SMOTE</code>: Default parameters.</p> <p>Column Transformer - <code>passthrough</code>: No feature transformation. - <code>col_trans</code>: Apply polynomial transformations and one-hot encoding.</p> <p>Classifier - <code>AdaBoostClassifier</code>: Default parameters with tree depth limited to <code>[1, 2, 3]</code>. - <code>GradientBoostingClassifier</code>, <code>XGBClassifier</code>: Default parameters with learning rates <code>[0.025, 0.05, 0.1]</code>.</p> <p>For all classifiers, we set the number of decision trees to <code>[90, 100, 110, 120]</code> and tune additional hyperparameters.</p> <p>Initially, there are 216 combinations to test, which is too many given time constraints. Experiments show that models with feature transformations generally perform worse, likely due to high feature correlations identified during EDA. As a result, we omit the \"Feature Transformation\" stage, reducing the combinations to 108. We use <code>GridSearchCV</code> with <code>cv=3</code> to find the best combination.</p> <p>The parameter grid is defined as follows:</p> <pre><code># poly_cols = ['shared_count', 'comment_count', 'liked_count', 'collected_count']\n# col_trans = make_column_transformer((OneHotEncoder(dtype='int'), ['weekday']),\n#                                     (PolynomialFeatures(include_bias=False), poly_cols),\n#                                     remainder='passthrough')\nparam_grid_ada = {\n    'resampler': ['passthrough', SMOTE(), NearMiss()],\n    # 'columntransformer': ['passthrough', col_trans],\n    'classifier': [AdaBoostClassifier()],\n    'classifier__n_estimators': [90, 100, 110, 120],\n    'classifier__base_estimator': [DecisionTreeClassifier(max_depth=1), \n                                   DecisionTreeClassifier(max_depth=2),\n                                   DecisionTreeClassifier(max_depth=3)]\n}\nparam_grid_gb = {\n    'resampler': ['passthrough', SMOTE(), NearMiss()],\n    # 'columntransformer': ['passthrough', col_trans],\n    'classifier': [GradientBoostingClassifier(), XGBClassifier()],\n    'classifier__n_estimators': [90, 100, 110, 120],\n    'classifier__learning_rate': [0.025, 0.05, 0.1]\n}\nparam_grid = [param_grid_ada, param_grid_gb]\n</code></pre>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/","title":"Results and Discussion","text":""},{"location":"side-projects/dcard-hw/docs/3-evaluation/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>Before discussing the results, let us revisit some commonly used metrics for binary classification, explained using this assignment as an example:</p> Actual\uff3cPredicted Negative Positive Negative \\(\\color{red}{\\text{TN}}\\) \\(\\color{blue}{\\text{FP}}\\) Positive \\(\\color{green}{\\text{FN}}\\) \\(\\color{orange}{\\text{TP}}\\) <p>\\(\\text{Precision}\\): Measures the proportion of articles predicted as popular that are actually popular. Higher values indicate greater trust in the model's predictions for popular articles. Formula: $$ \\text{Precision} = \\frac{\\color{orange}{\\text{TP}}}{\\color{blue}{\\text{FP}} + \\color{orange}{\\text{TP}}} $$</p> <p>\\(\\text{Recall}\\): Measures the proportion of actual popular articles that are correctly predicted by the model. Also known as True Positive Rate (TPR) or Sensitivity. Higher values indicate the model's ability to capture actual popular articles. Formula: $$ \\text{Recall} = \\dfrac{\\color{orange}{\\text{TP}}}{\\color{green}{\\text{FN}} + \\color{orange}{\\text{TP}}} $$ </p> <p>\\(\\text{Specificity}\\): Measures the proportion of actual non-popular articles that are correctly predicted by the model. Also known as True Negative Rate (TNR). Higher values indicate the model's ability to capture actual non-popular articles. Formula: $$ \\text{Specificity} = \\dfrac{\\color{red}{\\text{TN}}}{\\color{red}{\\text{TN}}+\\color{blue}{\\text{FP}}} $$</p> <p>\\(\\text{F1-score}\\): A harmonic mean of \\(\\text{Precision}\\) and \\(\\text{Recall}\\), ranging from \\(0\\) to \\(1\\), with higher values being better. Formula: $$ \\text{F1-score} = \\dfrac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$</p> <p>\\(\\text{Balanced Acc.}\\): A combined metric of \\(\\text{TPR}\\) and \\(\\text{TNR}\\), ranging from \\(0\\) to \\(1\\), with higher values being better. Formula: $$ \\text{Balanced Acc.} = \\dfrac{\\text{TNR} + \\text{TPR}}{2} $$</p> <p>When using <code>GridSearchCV</code> to find the best parameter combination, we record these five metrics and select the best combination based on the f1-score. Example code: <pre><code>scoring = {\n    'precision': 'precision',\n    'recall': 'recall',\n    'specificity': make_scorer(specificity_score),\n    'balanced_accuracy': 'balanced_accuracy',\n    'f1_score': 'f1',\n}\ngrid_search = GridSearchCV(pipe, param_grid=param_grid, scoring=scoring, refit='f1_score', \n                           n_jobs=-1, cv=3, return_train_score=True)\n</code></pre></p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#experimental-results","title":"Experimental Results","text":"<p>Info</p> <p>The best model is <code>AdaBoostClassifier</code> without any resampling, consisting of 100 decision trees with a maximum depth of 2. The average f1-score during cross-validation is 0.56, while the f1-score on the public test set is 0.53. Detailed prediction information is as follows:</p> <p></p> Note <pre><code>===================GETTING CONNECTOR START!==================\n============================DONE!============================\n====================GETTING TABLES START!====================\nposts_test                 Total:   225,986 rows, 3 columns\npost_shared_test           Total:    83,376 rows, 3 columns\npost_comment_created_test  Total:   607,251 rows, 3 columns\npost_liked_test            Total:   908,910 rows, 3 columns\npost_collected_test        Total:   275,073 rows, 3 columns\n============================DONE!============================\n====================MERGING TABLES START!====================\n============================DONE!============================\n================PREPROCESSING TOTAL_DF START!================\n============================DONE!============================\n==================PREDICTING TESTSET START!==================\nf1-score     = 0.53\nbalanced acc = 0.70\n\n            precision    recall  f1-score   support\n\n        0       0.99      1.00      0.99    221479\n        1       0.75      0.40      0.53      4507\n\n    accuracy                           0.99    225986\nmacro avg       0.87      0.70      0.76    225986\nweighted avg       0.98      0.99      0.98    225986\n\n============================DONE!============================\n</code></pre> <p>Now, let us analyze the experimental results. (All figures below are based on cross-validation results, not the entire training set or public test set.)</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#resampler","title":"Resampler","text":"<p>First, let us examine how different resampling strategies affect the f1-score: </p> <p>Info</p> <p>Different resampling strategies indeed affect the f1-score:</p> <ul> <li>NearMiss (undersampling) has the lowest f1-score, likely due to excessive removal of non-popular articles, losing too much majority class information.</li> <li>SMOTE (oversampling) achieves a moderate f1-score.</li> <li>No resampling achieves the highest f1-score.</li> </ul> <p>Next, we investigate how these resampling strategies impact precision and recall:</p> <p></p> <p>Info</p> <ul> <li>NearMiss and SMOTE significantly increase the model's focus on the minority class, resulting in excellent recall scores of 0.91 and 0.95, respectively. However, this comes at the cost of precision, which drops to 0.07 and 0.20, respectively.</li> <li>In other words, resampling strategies can capture actual popular articles but reduce the trustworthiness of the predicted popular articles.</li> </ul> <p>We further explore whether resampling strategies interact with different classifiers to influence the f1-score:</p> <p></p> <p>Info</p> <ul> <li>Under \"SMOTE\" and \"No Resampling\" strategies, different classifiers do not significantly affect the f1-score.</li> <li>However, under the NearMiss strategy, <code>XGBClassifier</code> achieves the highest f1-score (0.18), while <code>AdaBoostClassifier</code> has the lowest (0.07).<ul> <li><code>AdaBoostClassifier</code> performs poorly because it relies on weak classifiers, which struggle with limited majority class information.</li> <li><code>XGBClassifier</code> outperforms <code>GradientBoostingClassifier</code> due to its optimized GBDT implementation.</li> </ul> </li> </ul>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#classifier","title":"Classifier","text":"<p>Next, let us examine how different classifiers affect the f1-score:</p> <p></p> <p>Info</p> <ul> <li>Different classifiers have minimal impact on the f1-score. On average, <code>XGBClassifier</code> achieves the highest score (0.35), primarily due to its performance under the NearMiss strategy.</li> </ul> <p>Finally, we analyze whether the number of internal classifiers in ensemble models affects the f1-score:</p> <p></p> <p>Clearly, the number of classifiers has little impact. Similarly, the tree depth for <code>AdaBoostClassifier</code> and the learning rate for the other two models also have minimal impact on the f1-score (figures omitted).</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#future-directions","title":"Future Directions","text":"<p>The experimental results are summarized above. Due to time constraints, additional attempts were not included. Potential future directions are outlined below:</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#explore-other-resampling-techniques","title":"Explore Other Resampling Techniques","text":"<p>Resampling techniques can increase the model's focus on the minority class. Although the experimental results were not ideal, we can continue fine-tuning hyperparameters or exploring other resampling techniques. Refer to the \"Over-sampling\" and \"Under-sampling\" sections of the <code>imblearn</code> User Guide for potential directions.</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#consider-other-evaluation-metrics","title":"Consider Other Evaluation Metrics","text":"<p>The assignment requires using f1-score as the evaluation metric. However, if we use balanced accuracy instead, the best model would be a <code>GradientBoostingClassifier</code> trained with SMOTE, consisting of 120 classifiers and a learning rate of 0.1, achieving a balanced accuracy of 0.93.</p> <p>The impact of different resampling strategies on balanced accuracy is shown below:</p> <p></p> <p>Info</p> <p>SMOTE achieves the highest balanced accuracy. If the goal is to preliminarily identify potentially popular articles for subsequent workflows, balanced accuracy might be a better evaluation metric.</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#explore-other-feature-transformations-and-classifiers","title":"Explore Other Feature Transformations and Classifiers","text":"<p>The experiment only considered tree-based ensemble models, which require minimal feature transformation. However, we could explore logistic regression, support vector machines, Poisson regression, etc., combined with effective feature transformations. For example, converting <code>weekday</code> and <code>hour</code> into circular coordinates (refer to this post) could improve model performance.</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#incorporate-sequential-information","title":"Incorporate Sequential Information","text":"<p>The experiment ignored the \"time trends\" of shares, comments, likes, and collections within 10 hours of posting. One potential direction is to use recurrent neural networks (RNN, LSTM, GRU, etc.) to capture these trends and nonlinear relationships between variables.</p> <p>A simple approach is to combine the four count variables into a 4-dimensional vector (e.g., <code>[4, 23, 17, 0]</code> for 4 shares, 23 comments, etc.), with a sequence length of 10. Each article's sequential information would then be a <code>(10, 4)</code> matrix, which can be fed into the model for training.</p> <p>For details on LSTM models, refer to my notes.</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#explore-other-hyperparameter-optimization-methods","title":"Explore Other Hyperparameter Optimization Methods","text":"<p>The experiment used <code>GridSearchCV</code> for hyperparameter optimization. However, <code>RandomizedSearchCV</code> might be a better choice for optimizing a large number of hyperparameter combinations. Refer to this 2012 JMLR paper for details.</p> <p>Additionally, consider Bayesian optimization implementations provided by <code>optuna</code> or <code>hyperopt</code>. Watch this video for details, and compare the two libraries in this article.</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#incorporate-text-data-and-user-behavior","title":"Incorporate Text Data and User Behavior","text":"<p>The assignment does not include text data or user behavior. Since the ultimate goal is to \"recommend articles more accurately to users,\" consider incorporating Latent Dirichlet Allocation (LDA) topic modeling to enrich article topic information. Refer to my presentation for details on LDA.</p> <p>Additionally, combining user behavior data could enable more refined personalized text recommendations. Refer to this video and this paper for details.</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#tags-dcard","title":"tags: <code>dcard</code>","text":""},{"location":"side-projects/dcard-hw/docs/4-resampling/","title":"Resampling","text":""},{"location":"side-projects/dcard-hw/docs/4-resampling/#over-sampling","title":"Over Sampling","text":""},{"location":"side-projects/dcard-hw/docs/4-resampling/#under-sampling","title":"Under Sampling","text":""},{"location":"side-projects/restful-apis-with-flask/","title":"RESTful APIs with Flask","text":"<p>This repository documents my journey of learning how to build RESTful APIs using Flask. It includes step-by-step implementations of various concepts, from basic API design principles to advanced features like authentication, database integration, deployment, and third-party integrations. </p> <p>The content is based on two Udemy courses: \"REST APIs with Flask and Python\" and \"Advanced REST APIs with Flask and Python\". Each section highlights key topics, tools, and techniques, making it a comprehensive resource for anyone looking to learn Flask for API development.</p> <p></p> <p>Certificate</p> <p></p> <p>Certificate</p>"},{"location":"side-projects/restful-apis-with-flask/#about-this-project","title":"About This Project","text":"<p>The goal of this project is to: - Learn and experiment with Flask for building RESTful APIs. - Understand best practices for API design and implementation. - Explore integrations with databases, authentication, and other web technologies.</p>"},{"location":"side-projects/restful-apis-with-flask/#features","title":"Features","text":"<ul> <li>RESTful API Design: Follows REST principles for clean and scalable APIs.</li> <li>Flask Framework: Built using Flask for simplicity and flexibility.</li> <li>Database Integration: Includes examples of working with databases like SQLite or SQLAlchemy.</li> <li>Authentication: Demonstrates how to secure APIs with authentication mechanisms.</li> <li>Error Handling: Implements robust error handling for better user experience.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/#how-to-use-this-repository","title":"How to Use This Repository","text":"<p>Feel free to browse the code, read the documentation, and run the examples. If you're new to Flask or REST APIs, this project can serve as a learning resource.</p>"},{"location":"side-projects/restful-apis-with-flask/#getting-started","title":"Getting Started","text":"<ol> <li> <p>Clone the repository:     <pre><code>gh repo clone your-username/rest-apis-with-flask\ncd rest-apis-with-flask\n</code></pre></p> </li> <li> <p>Install dependencies:     <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Run the application:     <pre><code>python app.py\n</code></pre></p> </li> </ol> <p>Thank you for visiting, and I hope you find this project helpful!</p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/","title":"\u5c07 Flask \u61c9\u7528\u7a0b\u5f0f\u90e8\u7f72\u5728 Ubuntu 16.04 Server","text":"<p>\u9996\u5148\uff0c\u6211\u5011\u5fc5\u9808\u8a3b\u518a DigitalOcean \u5e33\u865f\u4e26\u5728\u4e0a\u9762\u79df\u7528\u4e00\u500b\u865b\u64ec\u4e3b\u6a5f\uff0c\u7248\u672c\u70ba Ubuntu 16.04 Server\u3002\u5982\u4f55\u4ee5 SSH \u9023\u7dda\u4e0d\u662f\u8ab2\u7a0b\u91cd\u9ede\uff0c\u56e0\u6b64\u6211\u5011\u5728\u6b64\u5148\u7565\u904e\u3002\u9023\u7dda\u5f8c\u7dca\u63a5\u8457\u9032\u884c\u5e7e\u9805\u4e8b\u524d\u6e96\u5099\uff1a</p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#_1","title":"\u66f4\u65b0\u5009\u5eab\u6e05\u55ae","text":"<pre><code># apt-get update\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#os","title":"\u5728 OS \u4e0a\u5275\u5efa\u65b0\u4f7f\u7528\u8005","text":"<pre><code># adduser &lt;username&gt;\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#super-user","title":"\u7d66\u4e88 super user \u6b0a\u9650","text":"<p>\u9032\u5165 <code>/etc/sudoers</code> \u6a94\u6848\uff1a <pre><code># visudo\n</code></pre></p> <p>\u5728 \"User privilege specification\" \u4e0b\u65b9\u66ff\u65b0\u4f7f\u7528\u8005\u52a0\u5165 super user \u7684\u6b0a\u9650\uff1a <pre><code>&lt;username&gt; ALL=(ALL:ALL) ALL\n</code></pre></p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#1-postresql","title":"1. \u5b89\u88dd\u4e26\u8a2d\u5b9a PostreSQL \u8cc7\u6599\u5eab","text":""},{"location":"side-projects/restful-apis-with-flask/basics/section9/#postgresql","title":"\u5b89\u88dd PostgreSQL","text":"<pre><code># apt-get install postgresql postgresql-contrib\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#postgres","title":"\u5207\u63db\u6210 <code>postgres</code> \u4f7f\u7528\u8005","text":"<pre><code># sudo -i -u postgres\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#postgresql_1","title":"\u66ff\u65b0\u4f7f\u7528\u8005\uff0c\u5275\u5efa PostgreSQL \u7576\u4e2d\u7684\u5e33\u865f\u548c\u8cc7\u6599\u5eab","text":"<pre><code>$ createuser &lt;username&gt; -P\n$ createdb &lt;username&gt;\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#postgresql_2","title":"\u5f37\u5236\u4ee5\u5bc6\u78bc\u767b\u5165 PostgreSQL","text":"<p>\u9032\u5165 <code>pg_hba.conf</code> \u914d\u7f6e\u6a94\uff1a <pre><code>$ nano /etc/postgresql/10/main/pg_hba.conf\n</code></pre></p> <p>\u5c07 <pre><code>local all all peer\n</code></pre> \u6539\u70ba <pre><code>local all all md5\n</code></pre></p> <p>\u5b8c\u6210\u5f8c\u8df3\u51fa\u914d\u7f6e\u6a94\uff0c\u63a5\u8457\u6539\u70ba\u65b0\u4f7f\u7528\u8005\u4f86\u64cd\u4f5c\u3002</p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#2-nginx","title":"2. \u5b89\u88dd\u4e26\u8a2d\u5b9a Nginx \u4f3a\u670d\u5668","text":""},{"location":"side-projects/restful-apis-with-flask/basics/section9/#nginx","title":"\u5b89\u88dd Nginx","text":"<pre><code>$ sudo apt-get install nginx\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#nginx-ssh","title":"\u958b\u555f\u9632\u706b\u7246\u4e26\u5141\u8a31 <code>nginx</code> \u548c <code>ssh</code>","text":"<pre><code>$ sudo ufw enable\n$ sudo ufw allow 'Nginx HTTP'\n$ sudo ufw allow ssh\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#flask-nginx","title":"\u66ff\u6211\u5011\u7684 Flask \u61c9\u7528\u7a0b\u5f0f\u52a0\u5165 Nginx \u914d\u7f6e\u6a94","text":"<p>\u5275\u5efa\u4e26\u9032\u5165 <code>items-rest.conf</code> \u914d\u7f6e\u6a94\uff1a <pre><code>$ sudo nano /etc/nginx/sites-available/items-rest.conf\n</code></pre></p> <p>\u8f38\u5165\u4ee5\u4e0b\u5167\u5bb9\uff1a <pre><code>server {\n    listen 80;\n    real_ip_header X-Forwarded-For;\n    set_real_ip_from 127.0.0.1;\n    server_name localhost;\n\n    location / {\n        include uwsgi_params;\n        uwsgi_pass unix:/var/www/html/items-rest/socket.sock;\n        uwsgi_modifier1 30;\n    }\n\n    error_page 404 /404.html;\n    location = 404.html {\n        root /usr/share/nginx/html;\n    }\n\n    error_page 500 502  503 504 50x.html;\n    location = /50x.html {\n        root /usr/share/nginx/html;\n    }\n}\n</code></pre></p> <p>\u5b8c\u6210\u5f8c\u8df3\u51fa\u914d\u7f6e\u6a94\uff0c\u63a5\u8457\u5efa\u7acb soft link\uff0c\u555f\u7528\u914d\u7f6e\uff1a <pre><code>$ sudo ln -s /etc/nginx/sites-available/items-rest.conf /etc/nginx/sites-enabled/\n</code></pre></p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#3-flask","title":"3. \u8a2d\u5b9a Flask \u61c9\u7528\u7a0b\u5f0f\u6240\u9700\u57f7\u884c\u74b0\u5883","text":""},{"location":"side-projects/restful-apis-with-flask/basics/section9/#_2","title":"\u5275\u5efa\u5c08\u6848\u76ee\u9304\u4e26\u7d66\u4e88\u9069\u7576\u6b0a\u9650","text":"<pre><code>$ sudo mkdir /var/www/html/items-rest\n$ sudo chown &lt;username&gt;:&lt;username&gt; /var/www/html/items-rest\n</code></pre> <p>\u5b8c\u6210\u5f8c\u9032\u5165\u8a72\u76ee\u9304\uff1a <pre><code>$ cd /var/www/html/items-rest\n</code></pre></p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#_3","title":"\u8a2d\u5b9a\u5c08\u6848\u6240\u9700\u74b0\u5883","text":"<p>\u4e0b\u8f09\u5c08\u6848\u5167\u5bb9\u4e26\u5275\u5efa\u65e5\u8a8c\u6a94\u76ee\u9304\uff1a <pre><code>$ git clone https://github.com/schoolofcode-me/stores-rest-api.git .\n$ mkdir log\n</code></pre></p> <p>\u5efa\u7acb\u865b\u64ec\u74b0\u5883\u4e26\u5b89\u88dd\u6240\u9700\u5957\u4ef6\uff1a <pre><code>$ sudo apt-get install python-pip python3-dev libpq-dev\n$ pip install virtualenv\n$ virtualenv venv --python=python3.6\n$ source venv/bin/activate\n(venv)$ pip install -r requirements.txt\n</code></pre></p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#4-uwsgi","title":"4. \u8a2d\u5b9a uWSGI \u4f3a\u670d\u5668","text":""},{"location":"side-projects/restful-apis-with-flask/basics/section9/#uwsgi_items_restservice-ubuntu","title":"\u5275\u5efa <code>uwsgi_items_rest.service</code> Ubuntu \u670d\u52d9","text":"<p>\u5275\u5efa\u4e26\u9032\u5165 <code>uwsgi_items_rest.service</code> \u6a94\uff1a <pre><code>$ sudo nano /etc/systemd/system/uwsgi_items_rest.service\n</code></pre></p> <p>\u8f38\u5165\u4ee5\u4e0b\u5167\u5bb9\uff1a <pre><code>[Unit]\nDescription=uWSGI items rest\n\n[Service]\nEnvironment=DATABASE_URL=postgres://jose:1234@localhost:5432/jose\nExecStart=/var/www/html/items-rest/venv/bin/uwsgi --master --emperor /var/www/html/items-rest/uwsgi.ini --die-on-term --uid jose --gid jose --logto /var/www/html/items-rest/log/emperor.log\nRestart=always\nKillSignal=SIGQUIT\nType=notify\nNotifyAccess=all\n\n[Install]\nWantedBy=multi-user.target\n</code></pre></p> <p>\u5b8c\u6210\u5f8c\u8df3\u51fa\u6a94\u6848\u3002</p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#uwsgiini","title":"\u4fee\u6539 <code>uwsgi.ini</code> \u914d\u7f6e\u6a94","text":"<p>\u9032\u5165\u5c08\u6848\u5167\u7684 <code>uwsgi.ini</code> \u914d\u7f6e\u6a94\uff1a <pre><code>$ nano uwsgi.ini\n</code></pre></p> <p>\u8f38\u5165\u4ee5\u4e0b\u5167\u5bb9\uff1a <pre><code>[uwsgi]\nbase = /var/www/html/items-rest\napp = run\nmodule = %(app)\n\nhome = %(base)/venv\npythonpath = %(base)\n\nsocket = %(base)/socket.sock\n\nchmod-socket = 777\n\nprocesses = 8\n\nthreads = 8\n\nharakiri = 15\n\ncallable = app\n\nlogto = /var/www/html/items-rest/log/%n.log\n</code></pre></p> <p>\u5b8c\u6210\u5f8c\u8df3\u51fa\u914d\u7f6e\u6a94\u3002</p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#5-flask","title":"5. \u555f\u52d5 Flask \u61c9\u7528\u7a0b\u5f0f","text":"<p>\u522a\u9664 Nginx \u9810\u8a2d\u914d\u7f6e\u6a94\uff0c\u907f\u514d\u8b80\u53d6\u932f\u8aa4\u7684\u914d\u7f6e\u6a94\uff0c\u63a5\u8457 reload \u4e26 restart\uff1a <pre><code>$ sudo rm /etc/nginx/sites-enabled/default\n$ sudo systemctl reload nginx \n$ sudo systemctl restart nginx\n</code></pre></p> <p>\u555f\u52d5 <code>uwsgi_items_rest</code> \u670d\u52d9\uff1a <pre><code>$ sudo systemctl start uwsgi_items_rest\n</code></pre></p> <p>\u5b8c\u6210\uff01</p>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/","title":"Advanced","text":""},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-1-course-preparations","title":"Section 1: Course Preparations","text":"<p>Preparations for the course:</p> <ul> <li>Simplified authentication mechanism.</li> <li>Added type hinting.</li> <li>Unified code style.</li> <li>Changed all <code>Resource</code> methods to class methods (using <code>@classmethod</code>).</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-2-marshmallow-integration","title":"Section 2: Marshmallow Integration","text":"<p>Introducing <code>marshmallow</code>, <code>flask-marshmallow</code>, and <code>marshmallow-sqlalchemy</code>:</p> <ul> <li>Simplified request parsing, <code>Model</code> object creation, and JSON responses by defining <code>Schema</code> for each <code>Resource</code>.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-3-email-verification","title":"Section 3: Email Verification","text":"<ul> <li>Implemented user email verification process (using Mailgun).</li> <li>Used <code>.env</code> files to store sensitive data.</li> <li>Returned <code>.html</code> files in <code>Flask-RESTful</code> using <code>make_response()</code> and <code>render_template()</code>.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-4-optimized-email-verification","title":"Section 4: Optimized Email Verification","text":"<p>Optimized the email verification process:</p> <ul> <li>Added expiration for verification and resend functionality.</li> <li>Refactored project structure by treating <code>confirmation</code> as a resource.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-6-secure-configuration-and-file-uploads","title":"Section 6: Secure Configuration and File Uploads","text":"<ul> <li>Configured the application more securely (using <code>from_object()</code> and <code>from_envvar()</code>).</li> <li>Learned the relationships between <code>WSGI</code>, <code>uwsgi</code>, <code>uWSGI</code>, and <code>Werkzeug</code>.</li> <li>Introduced <code>Flask-Uploads</code> for handling file uploads, downloads, and deletions (using <code>UploadSet</code>, <code>FileStorage</code>).</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-7-database-version-control","title":"Section 7: Database Version Control","text":"<ul> <li>Introduced <code>Flask-Migrate</code> for database version control, including adding, deleting, and modifying details.</li> <li>Common commands include <code>flask db init</code>, <code>flask db upgrade</code>, <code>flask db downgrade</code>.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-8-oauth-integration","title":"Section 8: OAuth Integration","text":"<ul> <li>Learned OAuth third-party login flow (e.g., GitHub), including authentication, authorization, and obtaining <code>access_token</code>.</li> <li>Introduced <code>Flask-OAuthlib</code>.</li> <li>Used Flask's <code>g</code> to store <code>access_token</code>.</li> <li>Allowed third-party login users to set passwords.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-9-payment-integration","title":"Section 9: Payment Integration","text":"<ul> <li>Integrated <code>Stripe</code> for third-party payment processing.</li> <li>Added an \"Order\" resource and implemented many-to-many relationships using <code>Flask-SQLAlchemy</code>.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/","title":"Basics","text":""},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-3-introduction-to-flask","title":"Section 3: Introduction to Flask","text":"<ul> <li>Introduction to the Flask web framework, using decorators to set up application routes.</li> <li>Understanding common HTTP request methods: GET, POST, PUT, DELETE.</li> <li>Understanding common HTTP status codes: 200, 201, 202, 401, 404.</li> <li>Understanding RESTful API design principles focusing on \"resources\" and statelessness.</li> <li>Implementing a RESTful API server application.</li> <li>Testing APIs using the Postman application.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-4-flask-restful-and-jwt","title":"Section 4: Flask-RESTful and JWT","text":"<ul> <li>Implementing RESTful API server applications using <code>Flask-RESTful</code>.</li> <li>Implementing JSON Web Token (JWT) authentication using <code>Flask-JWT</code>.</li> <li>Parsing user input JSON data using <code>RequestParser</code>.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-5-database-integration-with-sqlite","title":"Section 5: Database Integration with SQLite","text":"<ul> <li>Introducing <code>sqlite3</code> to store user and item information in a database.</li> <li>Implementing user registration functionality.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-6-database-integration-with-sqlalchemy","title":"Section 6: Database Integration with SQLAlchemy","text":"<ul> <li>Introducing <code>Flask-SQLAlchemy</code> to interact with the database using ORM.</li> <li>Adding store information with a one-to-many relationship to items.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-7-deploying-to-heroku","title":"Section 7: Deploying to Heroku","text":"<p>Deploying the Flask application to Heroku and using Heroku's PostgreSQL. Steps:</p> <ol> <li>Modify the project locally (e.g., add <code>Procfile</code>, <code>runtime.txt</code>, <code>uwsgi.ini</code>), then <code>commit</code> and <code>push</code> to the specified GitHub repo.</li> <li>Register on Heroku, create an application, connect it to the GitHub repo, and add the <code>heroku/python</code> buildpack and <code>Heroku Postgres</code> add-on.</li> <li>Install the Heroku CLI locally (see here) and log in using <code>heroku login</code>.</li> <li>Add a Heroku remote using <code>heroku git:remote -a &lt;app-name&gt;</code>.</li> <li>Deploy the project by pushing the <code>basics/section8</code> subdirectory to Heroku using <code>git subtree push --prefix basics/section8 heroku master</code>.</li> </ol> <p>Testing: Access here to retrieve all stores and their items in the database, returned in JSON format.</p>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-8-deploying-to-digitalocean","title":"Section 8: Deploying to DigitalOcean","text":"<p>Deploying the Flask application to a DigitalOcean Droplet. Steps:</p> <ol> <li>Register on DigitalOcean, create a Droplet with Ubuntu 16.04, set up SSH, and connect using PuTTY.</li> <li>Create a new user on the operating system.</li> <li>Install and configure PostgreSQL, including creating a new user and database with appropriate permissions.</li> <li>Install and configure the Nginx server, including firewall settings, error pages, and uwsgi parameters.</li> <li>Set up a Python virtual environment, install required packages, and clone the project from GitHub.</li> <li>Configure an Ubuntu service to run the uwsgi server, including log directories, processes, and threads.</li> </ol> <p>Testing: Access here (created on 2020/05/30) to retrieve all stores and their items in the database, returned in JSON format.</p>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-9-domain-and-https","title":"Section 9: Domain and HTTPS","text":"<p>Book</p> <ul> <li>Registering a domain and configuring DNS servers.</li> <li>Obtaining an SSL certificate for HTTPS communication and configuring Nginx.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-11-advanced-jwt-features","title":"Section 11: Advanced JWT Features","text":"<p>Introducing <code>Flask-JWT-Extended</code>:</p> <ul> <li>Implementing token-refreshing to improve user experience by avoiding frequent logins while requiring re-login for critical actions for security (using <code>@jwt_refresh_token_required</code>, <code>create_refresh_token()</code>, <code>create_access_token()</code>).</li> <li>Responding with appropriate data based on user roles (visitor, user, admin) using <code>@jwt.user_claims_loader</code>, <code>@jwt_optional</code>, <code>get_jwt_claims()</code>.</li> <li>Returning specific error messages for token-related issues using <code>@jwt.expired_token_loader</code>, <code>@jwt.invalid_token_loader</code>, <code>@jwt.needs_fresh_token_loader</code>.</li> <li>Implementing a logout mechanism using a blacklist (with <code>@jwt.token_in_blacklist_loader</code>, <code>get_raw_jwt()</code>).</li> </ul> <p>Book</p>"},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"}]}