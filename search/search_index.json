{"config":{"lang":["en","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to KC's Data &amp; Life Notes","text":"<p>Welcome! I'm KC (Kuan-Chou), a software engineer with a passion for data, AI, and continuous learning. This website is a space where I share my projects, learning journey, and experiences. Feel free to explore and get to know me better!</p> <ul> <li> <p> Side Projects</p> <p>Explore my personal projects in data, AI, and programming.</p> <p> Side Projects</p> </li> <li> <p> Learning in Progress</p> <p>What I\u2019m studying, building, and aiming For</p> <p> Learning Plans</p> </li> <li> <p> Blog</p> <p>Read my thoughts and insights on data, AI, books, and more.</p> <p> Blog</p> </li> <li> <p> About Me</p> <p>Learn more about my background, including education, work experience, public speaking, and other activities.</p> <p> About Me</p> </li> </ul>"},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#tag:the-lakehouse-series","title":"The Lakehouse Series","text":"<ul> <li>            The Lakehouse Series: OLTP vs. OLAP (A Parquet Primer)          </li> </ul>"},{"location":"tags/#tag:git-submodules","title":"git-submodules","text":"<ul> <li>            How to Use MkDocs to Integrate GitHub Actions and Git Submodule for Cross-repo Documentation          </li> </ul>"},{"location":"tags/#tag:github-actions","title":"github-actions","text":"<ul> <li>            How to Use MkDocs to Integrate GitHub Actions and Git Submodule for Cross-repo Documentation          </li> </ul>"},{"location":"tags/#tag:mkdocs","title":"mkdocs","text":"<ul> <li>            How to Use MkDocs to Integrate GitHub Actions and Git Submodule for Cross-repo Documentation          </li> </ul>"},{"location":"about-me/","title":"About Me","text":"<p>I'm a data-driven problem solver with experience in data engineering, machine learning, and public speaking. I enjoy turning complex ideas into actionable solutions and am always looking to grow\u2014both personally and professionally. Learn more about my background, skills, and what drives me.</p>"},{"location":"about-me/#work-experience","title":"Work Experience","text":"Period Role Company Type Link May 2024 - Sep 2024 Data Engineer UST Global Full-time, Remote Jun 2021 - Feb 2024 Data Engineer TVBS Media Inc. Full-time, Hybrid (Taipei, Taiwan) Jun 2020 - Sep 2020 ML Researcher NinoX Contract, Remote Jul 2018 - Aug 2018 Industry Analyst Intern ITRI Intern, On-site (Hsinchu, Taiwan)"},{"location":"about-me/#education","title":"Education","text":"Period Role Institution Location Sep 2019 - Feb 2020 Big Data Engineering Trainee III Taipei, Taiwan Jul 2017 - May 2023 M.B.A. NCCU Taipei, Taiwan Sep 2013 - Jun 2017 B.S. Psychology CCU Chiayi, Taiwan <p>Home </p>"},{"location":"about-me/public-speakings/","title":"Public Speakings","text":"<p>Sharing knowledge and ideas with others is something I value deeply. Here, you can find my past speaking engagements, talks, and presentations, along with topics I'm passionate about. I'm always open to new opportunities to connect and speak.</p>"},{"location":"about-me/public-speakings/#taipei-dbt-meetup-27","title":"Taipei dbt Meetup #27","text":"<p>In this talk, presented to an audience of around 100 attendees, I explored three key aspects of implementing dbt: people, process, and pipelines. From the people perspective, I discussed the challenges within data teams, such as the division of responsibilities between data engineers and analysts, which often creates bottlenecks. I also highlighted organizational challenges, like the lack of accountability for data quality in cross-functional teams, and how dbt helps address these issues by fostering collaboration and domain-oriented modeling.</p> <p>From the pipeline perspective, I shared how dbt simplifies complex workflows by leveraging tools like Jinja SQL and Google Cloud's BigQuery remote functions, enabling a unified and maintainable pipeline. Lastly, I touched on infrastructure challenges, particularly the need to manage separate repositories for Terraform and dbt, and how integrating these repositories can streamline workflows despite some trade-offs. Overall, the talk emphasized how dbt can transform data workflows and improve collaboration across teams.</p> <p>I'm thrilled to have had the opportunity to share and exchange ideas with like-minded individuals during this event. Engaging in discussions and learning from others is always an enriching experience.</p>"},{"location":"about-me/public-speakings/#devopsdays-taipei-2024","title":"DevOpsDays Taipei 2024","text":"<p>In this talk, presented to an audience of over 300 attendees, I shared how TVBS approaches DataOps and the foundational principles behind its architecture design: data democracy, bridging the gap between data engineers and analysts, and bringing code to data. </p> <p>I also discussed the four competitive advantages of our ELT approach: ease of maintenance, a focus on high-value activities, reducing technical gaps, and leveraging a large, mature community. </p> <p>For those interested in a deeper dive, I recommend exploring the following publications:</p> <ul> <li>TVBS\u6578\u64da\u67b6\u69cb\u5927\u89e3\u5bc6 (1) \u2014 \u524d\u4e16\u4eca\u751f</li> <li>TVBS\u6578\u64da\u67b6\u69cb\u5927\u89e3\u5bc6 (2) \u2014 \u73fe\u4ee3\u6578\u64da\u68e7(Modern Data Stack)</li> <li>TVBS\u6578\u64da\u67b6\u69cb\u5927\u89e3\u5bc6 (3) \u2014 Next Steps</li> </ul> <p>It was an incredible experience to share these insights and engage with such a large and enthusiastic audience. \u807d\u773e\u7684\u56de\u994b\u548c\u4e92\u52d5\u8b93\u9019\u6b21\u6f14\u8b1b\u66f4\u52a0\u5145\u5be6\u548c\u6709\u610f\u7fa9\u3002</p> Feedback <p>About Me </p>"},{"location":"about-me/work-experience/itri/itri/","title":"Industry Analyst Intern, ITRI","text":"<p>Summary</p> <ul> <li> Researched and analyzed the 7 key roles (DSP, SSP, DMP etc.) within the \"AdTech industry\", using \"eLand Information\", a leading AI &amp; Data Analytics company in Taiwan, as a case study to predict future industry trends and propose strategies.</li> <li> Analyzed technical trends within the cloud services industry, contributing to the authorship of 2 research articles on the subject.</li> <li> Systematized and summarized 100+ benchmark cases in the 4 major domains of IoT \u2013 water resources, air quality, earthquakes, and disaster prevention.</li> </ul> <p>The Industrial Technology Research Institute (ITRI), established in 1973 in Taiwan, is a leading applied research organization with over 6,500 experts driving industrial development and innovation. Known for pioneering advancements like integrated circuits and incubating companies such as TSMC and UMC, ITRI has shaped Taiwan's industrial landscape with over 30,000 patents. Focused on Smart Living, Quality Health, Sustainable Environment, and Resilient Society, ITRI addresses global challenges like digital transformation, demographic shifts, and climate change through its \"2035 Technology Strategy and Blueprint,\" fostering solutions that enhance lives and industries.</p> <p>During my time at ITRI, I was part of the interdisciplinary research team at the Industrial Economics and Knowledge Center (IEK). My work primarily revolved around studying the data economy, with a particular focus on the AdTech industry. I delved into the key roles within the ecosystem, such as Demand Side Platforms (DSP), Supply Side Platforms (SSP), and Data Management Platforms (DMP). Additionally, I explored technical trends in the IoT sector, analyzing over 100 benchmark cases across four major domains: water resources, air quality, earthquakes, and disaster prevention. These research efforts were aimed at predicting future industry trends and proposing actionable strategies.</p> <p>One of the most valuable lessons I learned at ITRI is that studying industry trends requires a holistic approach. It's not just about understanding the technology; you also need to consider business models, policies, and socio-cultural factors. This experience inspired me to dive deeper into the world of data science. While researching companies in the data-driven space, I realized how much I enjoyed working with data and uncovering insights. This journey has been both challenging and rewarding, and it has shaped my aspirations for the future.</p> <p>About Me </p>"},{"location":"about-me/work-experience/ninox/ninox/","title":"ML Researcher, NinoX","text":"<p>Summary</p> <ul> <li> Developed the Online Bayesian Inference (OBI) algorithm for the Collaborative Topic Regression (CTR) model\u2014a hybrid system combining Collaborative Filtering and Topic Modeling\u2014using Python with robust OOP design, while leveraging NumPy and SciPy for scalable, real-time recommendations.</li> <li> Achieved a 23% boost in computational efficiency by speeding up the MCMC sampling core with Cython.</li> <li> Established CI/CD pipelines with TravisCI for automated unit tests and a docs-as-code workflow using pytest and Sphinx, streamlining both development and documentation processes and ensuring consistent release quality</li> </ul> <p>NinoX initially focused on IoT sensors and logistics management. The team was exploring new business opportunities when they discovered my proof-of-concept project. This led to the idea of building and productizing a recommendation system.</p> <p>At NinoX, I developed the Online Bayesian Inference (OBI) algorithm, a hybrid system combining Collaborative Filtering and Topic Modeling. Using Python with a solid object-oriented design, I leveraged libraries like NumPy and SciPy to create a scalable, real-time recommendation engine. Additionally, I optimized the MCMC sampling core with Cython, achieving a 23% improvement in computational efficiency. To ensure smooth development and consistent quality, I established CI/CD pipelines with TravisCI, incorporating automated unit testing with pytest and a docs-as-code workflow using Sphinx.</p> <p>This role marked my first experience with object-oriented programming (OOP) design patterns and using Cython to accelerate Python code. I also gained insights into variational inference, a concept I hadn\u2019t encountered during my graduate studies. Beyond technical skills, I learned from my manager about algorithm development, CI/CD pipeline design, and the importance of efficient workflows. Most importantly, this experience gave me the confidence to tackle complex challenges and deliver impactful solutions.</p> <p>See here for the original project documentation.</p> CI/CD Pipeline for OBI Algorithm Development <p>About Me </p>"},{"location":"about-me/work-experience/tvbs/tvbs/","title":"Data Engineer, TVBS Media Inc.","text":"Organization Diagram <p>Summary</p> <ul> <li> Architected a cost-effective, scalable ETL / ELT Modern Data Stack (dbt, BigQuery, Airflow, Airbyte, Looker Studio, etc.) and introduced a streamlined DataOps workflow, processing 20M+ events daily at TB+ scale (300+ data models, 600+ daily quality checks), cutting cloud costs by 63%.</li> <li> Directed the organization-wide adoption of Data Mesh principles to strengthen data governance and improve data availability, empowering 7 domain teams through self-service reporting across 30+ data products, and achieving a previously unattainable holistic brand analysis through the expansion of data sources from 4 to 9+.</li> <li> Led IaC implementation with Terraform for over 500 cross-cloud data assets (AWS, GCP, dbt Cloud, etc.) and conducted internal DevOps workshops, slashing provisioning lead time from days to hours by integrating CI/CD pipelines with GitHub Actions and improving team IaC adoption by 80% within 6 months.</li> <li> Led the migration to GA4 and BigQuery to build a data lakehouse platform while maintaining a legacy event tracking pipeline (AWS Kinesis, MongoDB, PostgreSQL), saving $2M by retiring NoSQL database and ensuring real-time analytics for both anonymous and logged-in users.</li> <li> Championed an organization-wide experimentation mindset, engaged 60+ colleagues, and orchestrated 20+ A/B tests via Google Optimize and Firebase within 6 months, boosting mobile ad revenue by 27% and web pageviews by 6%.</li> </ul> <p>TVBS Media Inc. is a leading media company in Taiwan, known for its comprehensive news coverage and entertainment programming. The company operates multiple television channels, digital platforms, and mobile applications, reaching millions of viewers daily. TVBS is committed to delivering high-quality content and innovative media solutions, leveraging advanced technologies to enhance viewer engagement and experience.</p> <p>As a Data Engineer at TVBS Media Inc., I played a crucial role in transforming the company's data infrastructure and analytics capabilities. I architected a cost-effective, scalable ETL/ELT Modern Data Stack, enabling the processing of over 20 million events daily at TB+ scale. My efforts in implementing Data Mesh principles empowered domain teams with self-service reporting across multiple data products, significantly enhancing data governance and availability. I also led the migration to Google Analytics 4 (GA4) and BigQuery, ensuring real-time analytics while achieving substantial cost savings. Through my leadership in DevOps practices and experimentation initiatives, I drove significant improvements in operational efficiency and revenue growth.</p> <p>During my tenure at TVBS Media Inc., I gained extensive experience in architecting and implementing modern data solutions, particularly in the areas of ETL/ELT processes, Data Mesh principles, and cloud-based data platforms. I honed my skills in using tools like dbt, BigQuery, Airflow, and Looker Studio, and developed a deep understanding of data governance and self-service analytics. My leadership in DevOps practices, including Infrastructure as Code (IaC) with Terraform and CI/CD pipelines, significantly improved team efficiency and collaboration. Additionally, I learned the importance of fostering an experimentation culture within the organization, which led to measurable improvements in revenue and user engagement.</p> <p>About Me </p>"},{"location":"about-me/work-experience/ust/ust/","title":"Data Engineer, UST Global","text":"<p>Summary</p> <ul> <li> Built server monitoring solutions for Microsoft using Kusto (KQL) on Azure Data Explorer (ADX), Azure Monitor Log Analytics, and Azure Sentinel, delivering actionable insights via Power BI for operational excellence.</li> </ul> <p>UST Global is a leading digital transformation solutions provider, offering services such as data and analytics, SaaS, managed security, cloud transformation, product engineering, and business process automation. Headquartered in Aliso Viejo, California, UST operates in over 30 countries with more than 30,000 employees. The company is recognized for its commitment to innovation, diversity, and inclusion, and its philosophy of \"Fewer Clients, More Attention.\"</p> <p>As a Data Engineer at UST Global, I played a pivotal role in designing and implementing server monitoring solutions for Microsoft. This involved leveraging Kusto Query Language (KQL) on Azure Data Explorer (ADX), Azure Monitor Log Analytics, and Azure Sentinel to process and analyze large-scale telemetry data. I collaborated with cross-functional teams to create scalable data pipelines and developed Power BI dashboards to deliver actionable insights, driving operational excellence and informed decision-making.</p> <p>About Me </p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/cross-repo-docs-mkdocs-workflow/","title":"How to Use MkDocs to Integrate GitHub Actions and Git Submodule for Cross-repo Documentation","text":"<p>TLDR</p> <p>After reading this article, you will learn how to:</p> <ul> <li>Use Git Submodule to centrally manage documentation sources across multiple projects  </li> <li>Configure GitHub Actions for cross-project automation and integration workflows  </li> <li>Utilize Reusable Workflows to reuse CI/CD scripts and reduce maintenance costs  </li> <li>Leverage MkDocs Monorepo Plugin to merge documentation from multiple projects into a single website</li> </ul> <p>Ever tried hunting for that one crucial piece of documentation across five different repositories? It's like playing an exhausting game of hide-and-seek where the information you need is always in the other repo. Sound familiar?</p> <p>In many organizations, teams scatter their documentation across separate Git repositories like breadcrumbs in a forest. Frontend docs live here, backend knowledge sits there, and that critical ML pipeline explanation? Somewhere in yet another repository. While this approach keeps ownership clear and projects manageable, it creates a documentation nightmare that would make even the most patient developer pull their hair out.</p> <p>Picture this: a new team member joins your organization. Instead of a smooth onboarding experience, they embark on a treasure hunt across repositories, asking colleagues endless questions and trying to piece together how everything connects. It's like trying to assemble IKEA furniture with instructions scattered across different boxes \u2013 technically possible, but unnecessarily frustrating.</p> <p>I've been there too. While switching to a monorepo seemed tempting (like moving all your furniture to one room), we decided to keep our multi-repo structure for good reasons \u2013 authorization boundaries, deployment flexibility, and version independence. But I wasn't about to let scattered documentation continue being the thorn in our side.</p> <p>That's when I discovered the perfect recipe: combining Git Submodules (think of them as neat organizational folders), MkDocs Monorepo Plugin (the master chef that brings everything together), and GitHub Actions (your tireless automation assistant). The result? A centralized documentation platform that updates itself automatically. It's like having a personal librarian who keeps all your books organized and up-to-date!</p> <p></p> <p>Specifically, this setup involves three repositories:</p> <ul> <li><code>monorepo</code> (Main Repo) is the repository I use to build my personal website, utilizing MkDocs and deployed on GitHub Pages. It already has a predefined <code>publish-docs.yml</code> GitHub Actions Workflow.</li> <li><code>data2ml-ops</code> (Sub Repo) is my personal project for practicing DataOps and MLOps, which also uses MkDocs for documentation building.</li> <li><code>reusable-workflows</code> is used to store reusable workflows.</li> </ul> <p>My mission? Merge the learning notes from <code>data2ml-ops</code> into <code>monorepo</code> so I can showcase insights from different projects on one beautiful website. No more jumping between sites or wondering if documentation is outdated!</p> <p>The magic happens like this: whenever <code>data2ml-ops</code> gets updated documentation and pushes to GitHub, it automatically triggers the documentation build and deployment in <code>monorepo</code>. It's like having a loyal assistant who immediately updates your main presentation whenever you make changes to your notes. No more worrying about documentation drift!</p> <p>This article will explain step by step how I completed this integration, including implementation methods and considerations.</p> <ol> <li> <p>Add Submodule    Add <code>data2ml-ops</code> as a Git Submodule in <code>monorepo</code>.</p> </li> <li> <p>Create Documentation Deployment Workflow    Add a GitHub Actions Workflow in <code>monorepo</code> responsible for building and deploying documentation to GitHub Pages.</p> </li> <li> <p>Create Reusable Workflow    Create a Reusable Workflow in <code>reusable-workflows</code> responsible for triggering the documentation deployment process from step 2.</p> </li> <li> <p>Configure Sub-repo Workflow    Create a Workflow in <code>data2ml-ops</code> that uses the Reusable Workflow from step 3 to trigger the documentation deployment process in <code>monorepo</code>.</p> </li> <li> <p>Integrate MkDocs Monorepo Plugin    Add the MkDocs Monorepo Plugin to <code>monorepo</code> to integrate multiple documentation sources.</p> </li> <li> <p>Testing and Verification    Submit new changes, test whether the overall process works properly, and check if documentation is successfully deployed.</p> </li> </ol>","tags":["github-actions","git-submodules","mkdocs"]},{"location":"blog/cross-repo-docs-mkdocs-workflow/#1-add-submodule","title":"1. Add Submodule","text":"<p>kuanchoulai10/monorepo</p> <p>Think of Git Submodules as your project's way of having roommates \u2013 you can live together in the same house (repository) while keeping your personal belongings (version history) completely separate<sup>1</sup>. It's perfect for these scenarios:</p> <ol> <li>Modular management: Like having separate apartments for your frontend and backend teams, but with a shared lobby where they can meet and collaborate.</li> <li>Version independence: Your main repository acts like a strict landlord, locking onto specific versions of submodules. No surprise changes allowed!</li> <li>Reusability: That awesome utility library? Instead of copying it everywhere like a hoarder, just reference it as a submodule. Clean and efficient!</li> </ol> <p>Let's invite <code>data2ml-ops</code> to move into our <code>monorepo</code> using the digital equivalent of a lease agreement:</p> <pre><code>git submodule add https://github.com/kuanchoulai10/data2ml-ops.git data2ml-ops\n</code></pre> <pre><code>Cloning into '/Users/kcl/projects/monorepo/data2ml-ops'...\nremote: Enumerating objects: 197, done.\nremote: Counting objects: 100% (197/197), done.\nremote: Compressing objects: 100% (110/110), done.\nremote: Total 197 (delta 78), reused 183 (delta 66), pack-reused 0 (from 0)\nReceiving objects: 100% (197/197), 1.04 MiB | 1.94 MiB/s, done.\nResolving deltas: 100% (78/78), done.\n\n<pre><code>git status\n</code></pre>\n<pre><code>On branch main\nYour branch is up to date with 'origin/main'.\n\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n    modified:   .gitmodules\n    new file:   data2ml-ops\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n</code></pre>\n<p>Perfect! Git has thoughtfully created two essential files:</p>\n<ol>\n<li><code>.gitmodules</code> \u2013 Think of this as your address book, telling Git where to find each submodule:\n    <pre><code>[submodule \"data2ml-ops\"]\n  path = data2ml-ops\n  url = https://github.com/kuanchoulai10/data2ml-ops.git\n</code></pre></li>\n<li><code>data2ml-ops</code> \u2013 This is like a bookmark that points to exactly which version (commit) of the submodule we're using.</li>\n</ol>\n<p>Time to share our changes with the world! Commit and push these files to the remote repository:</p>\n<pre><code>git commit -m \"Add data2ml-ops as submodule\"\n</code></pre>\n<pre><code>git push\n</code></pre>\n<pre><code>Enumerating objects: 5, done.\nCounting objects: 100% (5/5), done.\nDelta compression using up to 8 threads\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 404 bytes | 404.00 KiB/s, done.\nTotal 3 (delta 2), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (2/2), completed with 2 local objects.\nTo https://github.com/kuanchoulai10/monorepo.git\n   6c36b76..c1e29ff  main -&gt; main\n</code></pre>\n<p>Now when you visit GitHub, you'll see something magical: <code>data2ml-ops @ 7369c16</code>. Click on it, and voil\u00e0! It teleports you to the actual repository. It's like having a portal in your house that leads directly to your friend's place \u2013 the reference is there, but you're not actually storing their stuff in your garage.</p>\n<p></p>","tags":["github-actions","git-submodules","mkdocs"]},{"location":"blog/cross-repo-docs-mkdocs-workflow/#2-create-documentation-deployment-workflow","title":"2. Create Documentation Deployment Workflow","text":"<p>kuanchoulai10/monorepo</p>\n\n<p>Time to set up our documentation deployment pipeline! Think of GitHub Actions as your personal automation butler \u2013 and the best part? It works for free on public repositories<sup>2</sup>. </p>\n<p>In the GitHub Actions universe, events are like doorbells that wake up your butler. Whether someone rings the <code>Push</code> doorbell, the <code>Pull Request</code> chime, or the <code>Merge</code> bell, each one triggers a workflow \u2013 basically a to-do list of automated tasks your butler follows religiously<sup>3</sup>.</p>\n<p>I won't bore you with every detail of writing deployment workflows (that's a whole other adventure), but let's focus on the Git submodules magic tricks.</p>\n<p>Our workflow has two ears, always listening for:</p>\n<ol>\n<li>New pushes to <code>monorepo</code> \u2013 When the main house gets updates, rebuild everything!</li>\n<li>Special <code>repository_dispatch</code> webhook events \u2013 Think of these as secret knock patterns that external repositories can use to wake up our workflow<sup>4</sup><sup>5</sup>. When other submodule repos update their docs, they'll send this special signal saying \"Hey, time to rebuild!\"</li>\n</ol>\n.github/workflows/publish-docs.yml<pre><code>on:\n  push:\n    branches:\n      - main\n  repository_dispatch:\n    types: [update-submodules]\n</code></pre>\n<p>Our workflow is like a well-organized recipe with one main job. The first step? Always get all the ingredients (checkout the codebase). But here's the crucial part \u2013 when using <code>actions/checkout@v4</code>, you absolutely must add <code>submodules: recursive</code>. Without this magical incantation, your submodule folders would be as empty as a diet soda's promise of satisfaction!</p>\n.github/workflows/publish-docs.yml<pre><code>      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n          submodules: recursive\n</code></pre>\n<p>Since our workflow might be awakened by updates from submodule repositories (like a helpful neighbor calling to say they've cleaned their yard), we need to be polite guests and:</p>\n<ol>\n<li>Update all submodules \u2013 Fetch the latest changes from our submodule neighbors</li>\n<li>Commit and push changes \u2013 If we found updates, we'll neatly file them away in our main repository</li>\n</ol>\n.github/workflows/publish-docs.yml<pre><code>      - name: Update Submodules\n        run: |\n          git submodule update --remote --merge\n      - name: Commit and Push Submodule Updates\n        run: |\n          git add .\n          if ! git diff --cached --quiet; then\n              git commit -m \"Update submodules to latest commit\"\n              git push origin main\n          else\n              echo \"No changes to commit\"\n          fi\n</code></pre>\n<p>This creates a beautiful synchronization dance \u2013 whenever a submodule updates, our documentation stays fresh and current. Don't forget to commit and push these changes when you're done!</p>","tags":["github-actions","git-submodules","mkdocs"]},{"location":"blog/cross-repo-docs-mkdocs-workflow/#3-create-reusable-workflow","title":"3. Create Reusable Workflow","text":"<p>kuanchoulai10/reusable-workflows</p>\n\n<p>Time to create our universal remote control! This reusable workflow is like that friend who's really good at getting everyone organized for group projects \u2013 simple, reliable, and saves everyone time.</p>\n<p>The logic is beautifully straightforward: use <code>curl</code> (the Swiss Army knife of web requests) to send a gentle tap on <code>monorepo</code>'s shoulder via the <code>repository_dispatch</code> webhook<sup>5</sup>. It's like sending a text message that says \"Hey, I updated my docs, could you refresh the website please?\" The message includes <code>event_type: update-submodules</code> so our main repo knows exactly what kind of help we need.</p>\n<p>Since this is a reusable workflow (the automation equivalent of a recipe you can share with friends), it listens for <code>workflow_call</code> events<sup>6</sup>:</p>\n.github/workflows/trigger-monorepo-to-build-doc.yml<pre><code>name: Trigger Monorepo to Build Docs\n\non:\n  workflow_call:\n    secrets:\n      PAT:\n        required: true\n        description: 'Personal Access Token with repo scope'\n\njobs:\n  trigger_monorepo:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Trigger update-submodule workflow\n        run: |\n          curl -L \\\n            -X POST \\\n            -H \"Accept: application/vnd.github+json\" \\\n            -H \"Authorization: Bearer ${{ secrets.PAT }}\" \\\n            -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n            https://api.github.com/repos/kuanchoulai10/monorepo/dispatches \\\n            -d '{\"event_type\":\"update-submodules\"}'\n</code></pre>\n<p>Why go through the trouble of creating reusable workflows? Imagine updating your automation process and having to visit each repository individually to make changes \u2013 it's like manually updating your address on every account when you move. With reusable workflows, you update once and everyone benefits. Smart and efficient!</p>\n<p>Remember to commit and push when you're done setting up this automation masterpiece.</p>","tags":["github-actions","git-submodules","mkdocs"]},{"location":"blog/cross-repo-docs-mkdocs-workflow/#4-configure-sub-repo-workflow","title":"4. Configure Sub-repo Workflow","text":"<p>kuanchoulai10/data2ml-ops</p>\n\n<p>Now comes the exciting part \u2013 teaching our sub-repository how to use that universal remote we just created! But first, we need to create a special key (Personal Access Token) that gives our automation the right permissions.</p>\n<p>Head over to GitHub's top-right corner: click your profile picture &gt; Settings &gt; Developer Settings &gt; Personal access tokens &gt; Fine-grained tokens. Think of this token as a VIP pass that allows reading metadata from the monorepo and pushing code with new submodule commits. Make sure to grant \"Contents\" repository permissions (write)<sup>5</sup> \u2013 it's like giving your automation assistant the keys to the filing cabinet.</p>\n<p></p>\n<p>Once you've created your token (and copied it \u2013 this is important!), head back to <code>data2ml-ops</code> and create a repository secret called <code>PAT</code>. Paste your token there like you're hiding a spare key under a digital doormat.</p>\n<p></p>\n<p>Now for the grand finale \u2013 creating the workflow that calls our reusable workflow<sup>8</sup>. It's surprisingly simple, like speed-dialing a friend. Just use <code>uses</code> in your job and point it to where your reusable workflow lives:</p>\n.github/workflows/trigger-monorepo-to-build-doc.yml<pre><code>name: Trigger Monorepo to Build Docs\n\non:\n  push:\n    paths:\n      - 'mkdocs.yml'\n      - 'README.md'\n      - 'docs/**'\n\njobs:\n  trigger_monorepo:\n    uses: kuanchoulai10/reusable-workflows/.github/workflows/trigger-monorepo-to-build-doc.yml@main\n    secrets:\n      PAT: ${{ secrets.PAT }}\n</code></pre>\n<p>Commit and push these changes, and you've just created your first cross-repository communication channel!</p>","tags":["github-actions","git-submodules","mkdocs"]},{"location":"blog/cross-repo-docs-mkdocs-workflow/#5-integrate-mkdocs-monorepo-plugin","title":"5. Integrate MkDocs Monorepo Plugin","text":"<p>kuanchoulai10/monorepo</p>\n\n<p>Almost there! Time to add the secret sauce that makes multiple documentation sources sing in harmony. Enter <code>mkdocs-monorepo-plugin</code> \u2013 the conductor of our documentation orchestra, crafted by the brilliant minds at Backstage.</p>\n<p>Quick backstory: Backstage is Spotify's gift to the developer world (it joined CNCF in 2020). Think of it as the ultimate dashboard for your engineering organization \u2013 like a mission control center where you can see all your microservices, documentation, CI/CD pipelines, and APIs in one beautiful, organized view<sup>9</sup>.</p>\n<p>First, let's invite this plugin to the party:</p>\n<pre><code>pip install mkdocs-monorepo-plugin\n</code></pre>\n<p>Once installed, the magic happens with just a few lines. Add <code>monorepo</code> to your plugins, and suddenly you have access to a powerful new syntax: <code>!include</code>. It's like having a magic wand that can summon documentation from other repositories:</p>\n<pre><code>...\n\nnav:\n  - Data2ML Ops: \"!include ./data2ml-ops/mkdocs.yml\"\n\n...\n\nplugins:\n  - monorepo\n</code></pre>\n<p>This elegant little configuration tells MkDocs: \"Hey, go grab the navigation structure from that submodule and weave it seamlessly into our main site.\" It's documentation magic at its finest!</p>\n<p>Don't forget to commit and push these changes \u2013 we're almost ready for the big reveal!</p>","tags":["github-actions","git-submodules","mkdocs"]},{"location":"blog/cross-repo-docs-mkdocs-workflow/#6-testing-and-verification","title":"6. Testing and Verification","text":"<p>kuanchoulai10/data2ml-ops</p>\n\n<p>The moment of truth has arrived! Time to put our automation masterpiece to the test. Let's make some updates to the documentation in <code>data2ml-ops</code> and push them to GitHub, then sit back and watch our creation come to life.</p>\n<p>Check out the GitHub Actions workflow doing its thing:</p>\n<p></p>\n<p><code>data2ml-ops</code> run history</p>\n<p>Behind the scenes, our workflow is using that reusable workflow like a well-oiled machine. It sends a polite <code>curl</code> request to the GitHub API, creating a <code>repository_dispatch</code> event in <code>monorepo</code>. This digital tap on the shoulder then wakes up the <code>publish-docs.yml</code> workflow \u2013 it's like watching a perfectly choreographed dance!</p>\n<p>Switch over to the <code>monorepo</code> page, and you'll see our documentation deployment process has indeed sprung into action:</p>\n<p></p>\n<p><code>monorepo</code> run history</p>\n<p>Look at that beautiful update! Our submodule reference has smoothly transitioned from <code>data2ml-ops @ 7369c16</code> to <code>data2ml-ops @ 887a9a0</code>. It's like watching your bookmark automatically update to point to the latest chapter of your favorite book.</p>\n<p></p>\n<p>The final proof? Visit the actual website, and there it is \u2013 the documentation from my <code>data2ml-ops</code> project, freshly deployed and beautifully integrated! It's like watching all the pieces of a puzzle click into place.</p>\n<p></p>\n<p>And just like that, we've created a self-updating documentation ecosystem that keeps everything in sync without manual intervention. No more documentation archaeology expeditions or wondering if what you're reading is current. Your cross-repository documentation now flows as smoothly as a well-conducted symphony!</p>","tags":["github-actions","git-submodules","mkdocs"]},{"location":"blog/cross-repo-docs-mkdocs-workflow/#references","title":"References","text":"<ol>\n<li>\n<p>Git Submodules Basic Explanation | gitaarik GitHub\u00a0\u21a9</p>\n</li>\n<li>\n<p>About billing for GitHub Actions | GitHub Docs\u00a0\u21a9</p>\n</li>\n<li>\n<p>The components of GitHub Actions | GitHub Docs\u00a0\u21a9</p>\n</li>\n<li>\n<p><code>repository_dispatch</code> Event | GitHub Docs\u00a0\u21a9</p>\n</li>\n<li>\n<p>Create a repository dispatch event | GitHub Docs\u00a0\u21a9\u21a9\u21a9</p>\n</li>\n<li>\n<p>Creating a reusable workflow | GitHub Docs\u00a0\u21a9</p>\n</li>\n<li>\n<p>Creating a fine-grained personal access token | GitHub Docs\u00a0\u21a9</p>\n</li>\n<li>\n<p>Calling a reusable workflow | GitHub Docs\u00a0\u21a9</p>\n</li>\n<li>\n<p>backstage/mkdocs-monorepo-plugin | GitHub\u00a0\u21a9</p>\n</li>\n</ol>","tags":["github-actions","git-submodules","mkdocs"]},{"location":"blog/oltp-vs-olap-a-parquet-primer/","title":"The Lakehouse Series: OLTP vs. OLAP (A Parquet Primer)","text":"<p>TLDR</p> <p>After reading this article, you will learn:</p> <ul> <li>The key differences between OLTP and OLAP workloads, and why storage format matters</li> <li>How Parquet organizes data internally and optimizes data storage using compression techniques like dictionary encoding and RLE</li> <li>Where Parquet falls short in today's data landscape</li> </ul>","tags":["The Lakehouse Series"]},{"location":"blog/oltp-vs-olap-a-parquet-primer/#oltp-vs-olap-the-data-processing-showdown","title":"OLTP vs. OLAP: The Data Processing Showdown","text":"<p>Picture this: you're at a bustling coffee shop. The barista (OLTP) takes your order, processes payment, and updates inventory \u2014 all in seconds. Meanwhile, the shop owner (OLAP) sits in the back office, analyzing months of sales data to figure out which pastries sell best on rainy Tuesdays. Same data, completely different games!</p>","tags":["The Lakehouse Series"]},{"location":"blog/oltp-vs-olap-a-parquet-primer/#what-is-oltp-the-speed-demon","title":"What is OLTP? (The Speed Demon)","text":"<p>OLTP stands for Online Transaction Processing \u2014 think of it as your data's personal speed trainer. It's obsessed with handling individual transactions faster than you can say \"latte with oat milk.\"</p> <p>OLTP is like that friend who finishes your sentences before you're halfway through. It specializes in real-time data updates and retrievals with the patience of a caffeinated hummingbird. Here's where it shines:</p> <ul> <li>Banking Scenario: When you withdraw cash, OLTP updates your balance faster than you can pocket the money. It's also the invisible hero when you update your address for that credit card application \u2014 boom, done, next!</li> <li>E-commerce Scenario: Adding items to your cart, completing purchases, or deleting that embarrassing profile from your teenage years \u2014 OLTP handles it all with sub-second precision.</li> </ul> <p>The magic? OLTP focuses on processing individual rows of data, making it the perfect sprinter for workloads that demand instant gratification and rock-solid reliability.</p>","tags":["The Lakehouse Series"]},{"location":"blog/oltp-vs-olap-a-parquet-primer/#what-is-olap-the-deep-thinker","title":"What is OLAP? (The Deep Thinker)","text":"<p>OLAP stands for Online Analytical Processing \u2014 imagine a brilliant detective who loves crunching numbers more than solving crimes. While OLTP races around handling transactions, OLAP sits in a cozy chair, analyzing patterns like Sherlock Holmes with a spreadsheet addiction.</p> <p>OLAP is your go-to when you need to analyze massive datasets and extract insights. It's less interested in individual records and more fascinated by the big picture across specific columns. Check out its superpowers:</p> <ul> <li>Banking Scenario: Need to know the average age of customers across different branches? OLAP dives into thousands of records, groups them by location, and serves up insights that would make a marketing manager weep with joy.</li> <li>E-commerce Scenario: Those beautiful dashboards showing sales trends, customer demographics, and year-over-year growth? That's OLAP flexing its analytical muscles.</li> </ul> <p>The secret sauce? OLAP excels at processing subsets of columns rather than individual rows, making it the marathon champion of data aggregation and business intelligence.</p>","tags":["The Lakehouse Series"]},{"location":"blog/oltp-vs-olap-a-parquet-primer/#why-storage-formats-matter","title":"Why Storage Formats Matter","text":"<p>Choosing the right data format is like picking the perfect tool for a job \u2014 use a hammer when you need a screwdriver, and you'll have a very bad time!</p> <p>Before we dive into the storage format showdown, let's map out the data landscape. Imagine data formats on a spectrum:</p> <p>At one end, we have unstructured data \u2014 the wild west of information. Think images, videos, and that chaotic folder of memes on your desktop. No rules, no schema, just pure digital chaos.</p> <p>At the other end sits structured data \u2014 the neat freak of the data world. CSV files fall here, with their obsessive love for rows and columns, following rules stricter than a Swiss train schedule.</p> <p>Hanging out in the middle is semi-structured data like JSON and XML \u2014 the cool kids who have some structure but refuse to be completely boxed in. They're like organized chaos, with key-value pairs that make sense but don't follow traditional table manners.</p> <p>Now, let's get specific about structured data layouts. Imagine we have a <code>users</code> table with 5 rows and 4 columns (age, gender, country, and average order value) \u2014 our guinea pig for this storage format experiment:</p> <p>We can classify this logical structured data into two categories based on how the data is physically stored on disk.</p> <p>Row-Based Storage: The Transaction Speedster</p> <p>Row-based formats store data like reading a book \u2014 line by line, left to right. It's how traditional databases like PostgreSQL and MySQL organize their data, and for good reason!</p> <p>Since data is stored row by row, it's incredibly efficient for typical OLTP operations. Want to insert a new user? Easy \u2014 just append a new row. Need to update someone's profile? Find the row offset and boom, mission accomplished. Deleting a user? Same deal.</p> <p>See how elegant it is? To delete the second row, we just need its offset \u2014 like having the exact address of a house. But here's the catch: if you want to analyze the average age of all users, you'd have to knock on every door in the neighborhood. Not exactly efficient!</p> <p>Column-Based Storage: The Analytics Powerhouse</p> <p>Column-based formats flip the script entirely \u2014 instead of reading like a book, they read like scanning a newspaper column. Each column lives together, creating some serious analytical superpowers.</p> <p>This layout is compression heaven. That gender column with mostly \"male\" and \"female\" values? Column storage can compress it down to almost nothing, like vacuum-packing your winter clothes.</p> <p>Need the average age? Just grab the age column and you're done \u2014 no need to wade through irrelevant data. But deleting a specific user? Now you're playing hide-and-seek across multiple columns. Not exactly OLTP's cup of tea!</p>","tags":["The Lakehouse Series"]},{"location":"blog/oltp-vs-olap-a-parquet-primer/#enter-parquet-the-best-of-both-worlds","title":"Enter Parquet: The Best of Both Worlds","text":"<p>What if I told you there's a storage format that's like having a sports car that's also an SUV? Meet Parquet \u2014 the overachiever of the data storage world!</p> <p>Parquet emerged from the brilliant minds at Twitter and Cloudera back in 2012. By 2013, it had graduated to become a top-level Apache project, and the data world hasn't been the same since.</p> <p>Here's the genius: Parquet organizes data into row groups (like small neighborhoods) that are then divided into columns (like sorting each neighborhood by house type). In our example, we have 2 row groups \u2014 the first contains rows 1-3, the second contains rows 4-5. Within each row group, columns live separately, enabling both row-wise convenience and column-wise efficiency.</p> <p>It's like having the best of both worlds \u2014 you can quickly find a specific user (row-wise operations) or analyze age patterns across thousands of users (column-wise operations) without breaking a sweat!</p> <p>Parquet's Anatomy: A Peek Under the Hood</p> <p>A Parquet file is like a well-organized filing cabinet with some seriously smart features:</p> <ol> <li>Magic Number: Every Parquet file starts with \"PAR1\" \u2014 its digital signature that screams \"I'm special!\"</li> <li>Row Groups: The main organizing principle, like having separate drawers for different categories of data</li> <li>Column Chunks: Within each row group, columns get their own dedicated space \u2014 think individual folders within each drawer</li> <li>Pages: The smallest storage unit, like individual documents within each folder</li> <li>Footer: The master index at the end that contains all the metadata \u2014 like having a detailed table of contents that tells you exactly where everything is without opening every drawer</li> </ol> <p>This structure enables lightning-fast metadata reading and incredibly efficient data retrieval. It's like having a librarian who knows exactly where every book is without checking the shelves!</p> <p>Efficient Compression in Parquet</p> <p>Efficient compression is a cornerstone of Parquet's design, enabling it to store massive datasets while minimizing disk usage and maximizing query performance. By leveraging techniques like dictionary encoding, run-length encoding (RLE), and bit-packing, Parquet achieves remarkable compression ratios without sacrificing speed. Let's explore how these methods work together to optimize columnar data storage.</p> <p>The image illustrates how Parquet efficiently compresses columnar data using dictionary encoding, followed by run-length encoding (RLE) and bit-packing. In the original column <code>[\"US\", \"TW\", \"FR\", \"JP\", \"JP\", \"KR\", \"CA\", \"CA\", \"CA\"]</code>, dictionary encoding first replaces each unique string with a unique integer ID. For example, <code>\"US\"</code> becomes <code>0</code>, <code>\"TW\"</code> \u2192 <code>1</code>, ..., <code>\"CA\"</code> \u2192 <code>5</code>, resulting in the encoded sequence: <code>[0, 1, 2, 3, 3, 4, 5, 5, 5]</code>.</p> <p>To further reduce storage, Parquet applies RLE and bit-packing to the integer sequence. Run-length encoding compresses consecutive repeated values, such as <code>\"JP\"</code> <code>(3, 3)</code> and <code>\"CA\"</code> <code>(5, 5, 5)</code>, into pairs like <code>3,2</code> (meaning value 3 repeated 2 times) and <code>5,3</code>. Bit-packing then ensures each integer is stored using the minimum number of bits necessary (in this case, 3 bits for up to 6 dictionary values). This layered approach dramatically reduces data size and speeds up scan performance by enabling efficient decoding and skipping of unneeded values.</p>","tags":["The Lakehouse Series"]},{"location":"blog/oltp-vs-olap-a-parquet-primer/#beyond-parquet-the-next-evolution","title":"Beyond Parquet: The Next Evolution","text":"<p>Even superheroes have kryptonite, and Parquet is no exception. Despite being the Swiss Army knife of data storage, it does have some Achilles' heels that can make you pull your hair out:</p> <ul> <li>Subsecond latency: Parquet struggles with workloads requiring microsecond updates, such as high-frequency trading.</li> <li>High-concurrency read-write: Maintaining consistency during concurrent operations is challenging.</li> </ul> <p>Parquet shines in OLAP systems where batch processing and read-heavy workloads dominate. However, as modern data applications grow more interactive and collaborative, expectations have shifted. Today's data platforms increasingly require features like:</p> <ul> <li>ACID transactions: Ensuring data reliability during concurrent operations.</li> <li>Time travel queries: Accessing historical data snapshots.</li> <li>Concurrent reads and writes: Supporting high-performance, multi-user environments.</li> </ul> <p>To address these needs, open formats like Apache Hudi, Iceberg, and Delta Lake have emerged. Built on top of Parquet, these formats extend its functionality to support advanced features, making them ideal for modern lakehouse architectures.</p> <p>Stay tuned for the next chapter in our Lakehouse series, where we'll explore how these formats are revolutionizing data processing!</p>","tags":["The Lakehouse Series"]},{"location":"blog/oltp-vs-olap-a-parquet-primer/#references","title":"References","text":"<p>I particularly enjoyed this talk by Boudewijn Braams, a senior software engineer at Databricks. He provided an insightful and engaging explanation of Parquet's internal structure and compression techniques, using vivid analogies that left a lasting impression. This presentation is not only suitable for beginners but also offers valuable insights and optimization tips for experienced data engineers.</p> <p>For official documentation, you can refer to the Parquet File Format documentation, which provides a comprehensive overview of its structure and features.</p>","tags":["The Lakehouse Series"]},{"location":"interview/data-lakehouse/","title":"Data Lakehouse Technical Interview Questions","text":""},{"location":"interview/data-lakehouse/#1-what-is-a-data-lakehouse-and-how-does-it-differ-from-traditional-data-lakes-and-data-warehouses","title":"1. What is a data lakehouse and how does it differ from traditional data lakes and data warehouses?","text":"<p>Answer: So a data lakehouse is essentially trying to get the best of both worlds - the flexibility of a data lake with the performance and ACID guarantees of a data warehouse. Traditional data lakes are great for storing massive amounts of raw data cheaply, but they lack schema enforcement and transaction support. Data warehouses give you that structure and performance but are expensive and not great with unstructured data. </p> <p>A lakehouse sits on top of your data lake storage - think S3 or ADLS - but adds a metadata layer that brings ACID transactions, schema evolution, and time travel capabilities. You're basically getting warehouse-like features while keeping your data in open formats like Parquet. The key is that metadata layer - it tracks all your table versions, handles concurrent writes, and maintains data quality without moving data around.</p>"},{"location":"interview/data-lakehouse/#2-explain-delta-lake-and-its-core-features","title":"2. Explain Delta Lake and its core features.","text":"<p>Answer: Delta Lake is probably the most popular lakehouse implementation right now. At its core, it's an open-source storage layer that brings reliability to data lakes. The magic happens through transaction logs - every operation gets recorded in JSON files that act like a database log.</p> <p>The big features are ACID transactions, so you can have multiple writers without corruption; time travel, where you can query any previous version of your data; and schema evolution, so you can safely add or modify columns. It also handles small file problems through automatic compaction and supports DML operations like updates and deletes, which raw Parquet can't do efficiently. The transaction log is really the secret sauce - it's how Delta knows what files belong to what version of the table and ensures consistency.</p>"},{"location":"interview/data-lakehouse/#3-how-does-the-medallion-architecture-work-in-a-lakehouse","title":"3. How does the medallion architecture work in a lakehouse?","text":"<p>Answer: The medallion architecture is a design pattern with three layers - bronze, silver, and gold. Think of it as a data refinement pipeline. Bronze is your raw ingestion layer - you're basically landing data as-is from source systems, minimal transformation, just getting it into your lakehouse quickly. This gives you that safety net where you never lose the original data.</p> <p>Silver is where you start cleaning and conforming - deduplication, data quality checks, maybe some light transformations. You're creating a cleaner, more reliable dataset but still keeping it fairly atomic. Gold is your business-ready layer - aggregated metrics, joined datasets, whatever your downstream consumers need.</p> <p>The beauty is each layer serves different use cases. Data scientists might work in silver for exploration, while executives get their dashboards from gold. And since it's all in the lakehouse, you're not duplicating data or managing complex ETL between systems.</p>"},{"location":"interview/data-lakehouse/#4-what-are-the-challenges-of-implementing-acid-transactions-in-a-data-lake-environment","title":"4. What are the challenges of implementing ACID transactions in a data lake environment?","text":"<p>Answer: The biggest challenge is that traditional data lakes use object storage like S3, which doesn't have built-in transactional capabilities. You can't just lock files like you would in a database. So you need to build consistency on top of eventually consistent storage, which is tricky.</p> <p>Concurrent writes are a nightmare - if two processes try to modify the same table simultaneously, you can end up with corrupted state or lost data. That's why solutions like Delta Lake use optimistic concurrency control with transaction logs. But even then, you're dealing with retry logic and conflict resolution.</p> <p>Performance is another issue - maintaining transaction logs and metadata can add overhead, especially for small frequent writes. And schema evolution gets complex when you're trying to maintain backwards compatibility across different versions of your data. The key is having a robust metadata layer that can handle all this complexity while keeping the interface simple for users.</p>"},{"location":"interview/data-lakehouse/#5-how-do-you-handle-schema-evolution-in-a-lakehouse-architecture","title":"5. How do you handle schema evolution in a lakehouse architecture?","text":"<p>Answer: Schema evolution in a lakehouse is all about managing changes without breaking downstream consumers. The metadata layer tracks schema versions, so when someone adds a new column, it doesn't invalidate existing data or queries.</p> <p>There are different types of evolution - additive changes like new columns are usually safe, but removing or renaming columns can break things. Good lakehouse implementations support schema merging, where new data with additional fields gets automatically incorporated. You also get type evolution, like promoting integers to longs.</p> <p>The transaction log is crucial here - it records not just data changes but schema changes too. Tools like Delta Lake let you enforce schema on write or evolve schema automatically. Time travel becomes super valuable because you can always go back to a previous schema version if something breaks. The key is having clear governance around who can make schema changes and testing downstream impacts before deploying.</p>"},{"location":"interview/data-lakehouse/#6-what-is-the-role-of-metadata-management-in-a-lakehouse","title":"6. What is the role of metadata management in a lakehouse?","text":"<p>Answer: Metadata is absolutely critical - it's what turns a data swamp into a usable lakehouse. You've got technical metadata like schema, partitioning, and file locations, but also business metadata like data lineage, ownership, and quality metrics.</p> <p>The metadata layer needs to handle discovery - users should be able to find relevant datasets without knowing exactly where they live. It tracks data lineage so you understand dependencies and can assess impact of changes. Version control is huge too - knowing what changed, when, and who changed it.</p> <p>Modern lakehouses use catalogs like Hive Metastore, AWS Glue, or Unity Catalog to centralize this. But you also need governance - data classification, access controls, retention policies. The metadata layer essentially makes your lakehouse self-documenting and governable. Without it, you're back to the data lake problem where data exists but nobody knows what it means or if they can trust it.</p>"},{"location":"interview/data-lakehouse/#7-how-do-you-optimize-query-performance-in-a-data-lakehouse","title":"7. How do you optimize query performance in a data lakehouse?","text":"<p>Answer: Query optimization in a lakehouse starts with smart partitioning - organizing your data by frequently filtered columns like date or region. This eliminates unnecessary file scanning. File sizing is crucial too - too many small files kill performance, so you need compaction strategies.</p> <p>Columnar formats like Parquet are essential for analytical workloads since you only read the columns you need. Adding statistics and zone maps helps query engines skip irrelevant files entirely. Some lakehouses support bloom filters for high-cardinality lookups.</p> <p>Caching is huge - both result caching and data caching. Tools like Databricks have disk caching that persists between queries. Indexing is evolving too - Delta Lake recently added liquid clustering which is like dynamic partitioning.</p> <p>The compute engine matters a lot - vectorized execution, code generation, predicate pushdown. And don't forget about data layout optimization - techniques like Z-ordering can dramatically improve performance for multi-dimensional queries. The key is understanding your query patterns and optimizing accordingly.</p>"},{"location":"interview/data-lakehouse/#8-what-are-the-main-data-ingestion-patterns-for-lakehouses","title":"8. What are the main data ingestion patterns for lakehouses?","text":"<p>Answer: There are several ingestion patterns depending on your needs. Batch ingestion is still common - think daily ETL jobs pulling from operational systems. You're usually landing data in bronze first, then processing through your medallion layers.</p> <p>Real-time streaming is becoming huge though. Tools like Kafka, Kinesis, or Pulsar feed directly into your lakehouse. The trick is handling late-arriving data and maintaining exactly-once semantics. Change data capture is really powerful here - capturing database changes and streaming them in near real-time.</p> <p>Micro-batching is a middle ground - processing small batches frequently instead of large batches daily. And don't forget about API ingestion for SaaS systems or file-based ingestion for things like CSV uploads.</p> <p>The architecture usually involves landing zones for raw data, then transformation layers. Auto-loader patterns are popular where new files trigger processing automatically. The key is designing for both throughput and reliability while maintaining data quality throughout the pipeline.</p>"},{"location":"interview/data-lakehouse/#9-how-do-you-ensure-data-quality-and-governance-in-a-lakehouse-environment","title":"9. How do you ensure data quality and governance in a lakehouse environment?","text":"<p>Answer: Data quality in a lakehouse requires a multi-layered approach. First, you implement schema enforcement at ingestion - reject or quarantine data that doesn't match expected formats. Great Expectations or similar frameworks can validate data against business rules before it enters your silver layer.</p> <p>Lineage tracking is crucial - you need to know where data came from and where it's going. This helps with impact analysis and root cause investigation when quality issues arise. Data contracts between teams help set clear expectations about data formats and SLAs.</p> <p>Access controls are fundamental - use role-based permissions and attribute-based access control to ensure people only see what they should. Tools like Unity Catalog provide fine-grained security down to the column level. Data classification helps identify sensitive information automatically.</p> <p>Monitoring is key - track things like data freshness, completeness, and statistical distributions. Set up alerts for anomalies. And don't forget about data retention policies and right-to-be-forgotten compliance. The goal is building trust through transparency and reliability.</p>"},{"location":"interview/data-lakehouse/#10-what-are-the-key-considerations-when-choosing-between-different-lakehouse-platforms","title":"10. What are the key considerations when choosing between different lakehouse platforms?","text":"<p>Answer: Platform choice really depends on your existing ecosystem and requirements. If you're already in AWS, something like Lake Formation with Delta Lake on EMR might make sense. Azure users often go with Synapse Analytics or Databricks on Azure.</p> <p>Consider your compute needs - do you need auto-scaling for variable workloads? How important is multi-language support? Some teams need SQL-only interfaces while others want Python and Scala support. Performance requirements matter too - some platforms optimize better for specific query patterns.</p> <p>Open standards are increasingly important - can you avoid vendor lock-in? Delta Lake, Iceberg, and Hudi are all open source, which gives you portability. But proprietary features might offer better performance or ease of use.</p> <p>Cost structure varies significantly - some charge by compute time, others by storage, some have complex pricing models. Factor in not just raw costs but operational overhead. How much engineering effort will it take to maintain? What's the learning curve for your team? Sometimes paying more for a managed service saves money in the long run.</p>"},{"location":"learning-in-progress/","title":"Learning in Progress","text":"<p>Staying curious is at the core of how I grow. I embrace challenges as chances to learn, document progress, and connect with others who share a passion for discovery.</p> <p>This roadmap keeps me accountable while inspiring others to start their own learning journeys. By sharing tools and insights, I aim to contribute to a community of lifelong learners eager to grow together.</p>"},{"location":"learning-in-progress/202505/","title":"May 2025","text":""},{"location":"learning-in-progress/202505/#highlight-of-the-month","title":"Highlight of the Month","text":"<p>Summarize my biggest breakthrough, project, or insight in this month:</p> <p>This month I mainly focused on my side project Fraud Detection: From DataOps to MLOps, and successfully integrated lots of interesting tools I've never tried before, like Feast, MLflow, Ray and KServe! It helped me better understand how to connect data pipelines with real-time model serving and got so much fun during the process.</p>"},{"location":"learning-in-progress/202505/#what-i-consumed","title":"What I Consumed","text":"<p>A list of articles, papers, courses, or videos I read/watched/completed:</p>"},{"location":"learning-in-progress/202505/#read","title":"Read","text":"<ul> <li> Real-time Fraud Detection on GCP with Feast</li> <li> Hyperparameter Tuning with MLflow and Optuna</li> <li> Deploy MLflow models with InferenceService</li> <li> Running Tune experiments with Optuna</li> <li> ihower's Facebook Post about MCP</li> <li> Python Tooling at Scale: LlamaIndex\u2019s Monorepo Overhaul</li> <li> Using uv in Docker</li> <li> Docker \u6559\u5b78\uff1a\u7528 Multi-stage build \u5efa\u7acb Poetry \u865b\u64ec\u74b0\u5883</li> <li> Python \u5957\u4ef6\u7ba1\u7406\u5668\u2014\u2014Poetry \u5b8c\u5168\u5165\u9580\u6307\u5357</li> </ul>"},{"location":"learning-in-progress/202505/#watched","title":"Watched","text":"<ul> <li> Claude 4 ADVANCED AI Coding: How I PARALLELIZE Claude Code with Git Worktrees</li> <li> Code with Claude Opening Keynote</li> </ul>"},{"location":"learning-in-progress/202505/#completed-courses","title":"Completed Courses","text":"<ul> <li> GitHub Copilot \u5354\u4f5c\u958b\u767c\u5be6\u6230</li> <li> MLflow in Action - Master the art of MLOps using MLflow tool</li> <li> Real-world End to End Machine Learning Ops on Google Cloud</li> </ul>"},{"location":"learning-in-progress/202505/#what-i-created-or-tried","title":"What I Created or Tried","text":"<p>What I built, experimented with, or wrote:</p> <ul> <li> Set up an end-to-end Data2MLOps pipeline using dbt, Feast, MLflow, Ray and KServe.</li> <li> Published a series of posts on my side project Fraud Detection: From DataOps to MLOps.</li> <li> Tried out multi-stage Docker builds with <code>uv</code> to optimize my Python environment setup.</li> <li> Experimented with MinIO as an S3-compatible object store on Kubernetes.</li> <li> Tried out GitHub Copilot's configuraiton best practices and its agent mode for more efficient coding.</li> </ul>"},{"location":"learning-in-progress/202505/#what-i-learned","title":"What I Learned","text":"<p>Short reflections on what I actually learned or became more confident in:</p> <ul> <li> Grasped how multi-stage Docker builds work and how they can significantly reduce image size and build time</li> <li> Deepened my understanding of how Ray Tune integrates with Optuna to perform distributed hyperparameter tuning, enhancing both speed and efficiency in machine learning tasks</li> <li> Learned the pros and cons between REST API and gRPC for model serving, and how to use KServe to deploy models with both protocols</li> </ul>"},{"location":"learning-in-progress/202505/#reflections-beyond-just-tech","title":"Reflections - Beyond Just Tech","text":"<p>Soft-skill insights or workflow/communication/process reflections:</p> <ul> <li> Recognized that in the AI era, mastering individual tools is easy, but engineers add value by excelling in multi-tool integration and architectural design.</li> <li> Realized that relying solely on LLMs for poorly documented tools can lead to inefficiencies. It's crucial to combine LLM assistance with thorough manual exploration and testing.</li> <li> Noticed that GitHub Copilot can significantly speed up coding, but it requires careful management to avoid code quality issues.</li> </ul>"},{"location":"learning-in-progress/202505/#goals-for-next-month","title":"Goals for Next Month","text":"<p>Set 2\u20133 simple goals to stay focused and accountable:</p> <ul> <li> Explore Airflow 3.0 and its new features.</li> <li> Try building my first MCP server.</li> <li> Accelarate my developer experience with GitHub Copilot and its agent mode.</li> <li> Write and publish my thoughts on why I use <code>mkdocs-material</code> for my blog and how I set it up.</li> </ul>"},{"location":"learning-in-progress/202506/","title":"June 2025","text":""},{"location":"learning-in-progress/202506/#highlight-of-the-month","title":"Highlight of the Month","text":"<p>Summarize my biggest breakthrough, project, or insight in this month:</p>"},{"location":"learning-in-progress/202506/#what-i-consumed","title":"What I Consumed","text":"<p>A list of articles, papers, courses, or videos I read/watched/completed:</p>"},{"location":"learning-in-progress/202506/#read","title":"Read","text":"<ul> <li>[] An Introduction to the Hudi and Flink Integration</li> <li> Building a Real-time Data Lake with Flink CDC</li> <li> Evolution to the Data Lakehouse</li> <li> What is a data lakehouse? | Databricks Docs</li> <li> What Is a Lakehouse? | Databricks Blog</li> <li> What\u2019s New in Apache Iceberg Format Version 3?</li> <li> Apache Iceberg\u2122 v3: Moving the Ecosystem Towards Unification</li> <li> 12-Factor Agents</li> <li> Practical Guide for Model Selection for Real\u2011World Use Cases</li> <li> \u611b\u597d AI Engineer \u96fb\u5b50\u5831 \ud83d\ude80 \u6a21\u578b\u4e0a\u4e0b\u6587\u5354\u5b9a MCP \u61c9\u7528\u958b\u767c #27<ul> <li>I really liked how the author described two different ways of building agents: one that relies on a customizable framework, and another that's more lightweight and built using just the core features of the programming language. It instantly reminded me of the old debates between TensorFlow 1.0 and PyTorch.</li> <li>After reading this article, I realized that the strength of senior engineers lies in their ability to quickly pick up new technologies and analyze different approaches logically with their own keen insights. This is a skill that I aspire to develop.</li> </ul> </li> <li> Featurestore at Agoda: How We Optimized Dragonfly for High-Performance Caching</li> <li> How Agoda manages 1.8 trillion Events per day on Kafka<ul> <li>2-step logging approach.</li> <li>Multiple smaller Kafka clusters instead of 1 Large Kafka cluster per Data Center</li> <li>Agoda employs a robust Kafka auditing system by aggregating message counts via background threads in client libraries, routing audits to a dedicated Kafka cluster, and implementing monitoring and alerting mechanisms for audit messages.</li> <li>Agoda calculates cluster capacity by comparing each resource\u2019s usage against its upper limit and taking the highest percentage to represent the dominant constraint at that moment.</li> <li>Agoda attributes cost back to teams, which transformed team mindsets, driving proactive cost management and accountability across Agoda</li> <li>The new auth system empowers the Kafka team to control access, manage credentials, and protect sensitive data through fine-grained ACLs</li> <li>Operational scalability is ensured through automated tooling that streamlines and simplifies system management.</li> </ul> </li> <li> Scaling Kafka to Support PayPal\u2019s Data Growth<ul> <li>Cluster Management: Kafka Config Service, ACLs, PayPal Kafka Libraries, QA Environment</li> <li>Monitoring and Alerting</li> <li>Configuration Management</li> <li>Enhancements and Automation: Patching security vulnerabilities, Security Enhancements, Topic Onboarding, MirrorMaker Onboarding, Repartition Assignment Enhancements, </li> </ul> </li> <li> Revolutionizing Real-Time Streaming Processing: 4 Trillion Events Daily at LinkedIn</li> <li> Pyright \u4e0a\u624b\u6307\u5357\uff1aPython \u578b\u5225\u6aa2\u67e5\u7684\u65b0\u9078\u64c7</li> <li> Data versioning as your \u2018Get out of jail\u2019 card \u2013 DVC vs. Git-LFS vs. dolt vs. lakeFS</li> <li> Unity Catalog | GitHub</li> <li> Exploring the Architecture of Apache Iceberg, Delta Lake, and Apache Hudi</li> <li> Hudi vs Iceberg vs Delta Lake: Data Lake Table Formats Compared</li> <li> Big Metadata: When Metadata is Big Data</li> <li> Vortex: A Stream-oriented Storage Engine For Big Data Analytics</li> <li> DuckLake: SQL as a Lakehouse Format<ul> <li>It simplifies lakehouses by using a standard SQL database for all metadata while still storing data in open formats like Parquet, just like BigQuery with Spanner and Snowflake with FoundationDB.</li> </ul> </li> <li> GitHub MCP Exploited: Accessing private repositories via MCP</li> </ul>"},{"location":"learning-in-progress/202506/#watched","title":"Watched","text":"<ul> <li> How I build Agentic MCP Servers for Claude Code (Prompts CHANGE Everything)</li> <li> Apache Iceberg V3 and Beyond</li> <li> Apache Iceberg V3 Ahead</li> <li> Architecting an Iceberg Lakehouse</li> <li> Introducing Pyrefly: A new type checker and IDE experience for Python</li> <li> Tampa Bay DE Meetup: The Who, What and Why of Data Lake Table Formats (Iceberg, Hudi, Delta Lake)</li> <li> Watch a Complete NOOB Try DuckDB and DuckLake for the first time</li> <li> Introducing DuckLake<ul> <li>Next Steps: the ability to import and export from existing lakehouse formats like Iceberg and the ability to talk to more databases.</li> </ul> </li> <li> Why build Event-Driven AI systems?</li> <li> Why MCP really is a big deal.<ul> <li>MCP offers pluggable, discoverable, and composable solutions that simplify complex integrations.</li> </ul> </li> <li> Why Everyone\u2019s Talking About MCP?<ul> <li>It addresses the issue faced by \\(M\\) AI vendors, where implementing \\(N\\) tools results in an \\(M \\times N\\) complexity problem. Instead, it simplifies the problem to an \\(M+N\\) complexity solution.</li> <li>Five primitives of MCP: resources, tools, prompts, roots and sampling.</li> </ul> </li> </ul>"},{"location":"learning-in-progress/202506/#completed-courses","title":"Completed Courses","text":"<ul> <li> GitHub Copilot \u9032\u968e\u958b\u767c\u5be6\u6230<ul> <li> Customize chat responses in VS Code<ul> <li>Instruction files and Prompt files are used to customize the chat responses in VS Code.</li> </ul> </li> <li> Prompt engineering for Copilot Chat</li> </ul> </li> <li> MCP Course</li> </ul>"},{"location":"learning-in-progress/202506/#what-i-created-or-tried","title":"What I Created or Tried","text":"<p>What I built, experimented with, or implemented:</p> <ul> <li> Completed a side project: A Unified SQL-based Data Pipeline</li> <li> Published a blog post: The Lakehouse Series: DuckLake \u2014 The Next Big Thing?</li> <li> Published a blog post: The Lakehouse Series: Hudi vs Iceberg vs Delta Lake \u2014 Format Wars Begin</li> <li> Published a blog post: The Lakehouse Series: OLTP vs. OLAP (A Parquet Primer)</li> <li> Published a blog post: How to Use MkDocs to Integrate GitHub Actions and Git Submodule for Cross-repo Documentation</li> <li> Experimented with Pyrefly</li> <li> Experimented with Speechify<ul> <li>I really like the ability to listen to articles and papers while doing other tasks. It helps me consume more content without feeling overwhelmed. What I like the most is that they have Snoop Dogg as a voice option, which adds a fun twist to the experience! Could you imagine listening to a data lakehouse article narrated by Snoop Dogg? \u2620\ufe0f</li> </ul> </li> <li> Experimented with DuckDB and DuckLake</li> <li> Experimented with FastMCP v2</li> <li> Experimented with <code>kubectl-ai</code></li> </ul>"},{"location":"learning-in-progress/202506/#what-i-learned","title":"What I Learned","text":"<p>Short reflections on what I actually learned or became more confident in:</p>"},{"location":"learning-in-progress/202506/#reflections-beyond-just-tech","title":"Reflections \u2013 Beyond Just Tech","text":"<p>Soft-skill insights or workflow/communication/process reflections:</p>"},{"location":"learning-in-progress/202506/#goals-for-next-month","title":"Goals for Next Month","text":"<p>Set 2\u20133 simple goals to stay focused and accountable:</p> <ul> <li>MindsDB</li> <li>Data Catalog<ul> <li>Comprehensive Data Catalog Comparison</li> <li>Introducing BigQuery metastore, a unified metadata service with Apache Iceberg support</li> </ul> </li> <li>Trino<ul> <li>Data Lake at Wise powered by Trino and Iceberg</li> <li>Running Trino as exabyte-scale data warehouse</li> <li>Empowering self-serve data analytics with a text-to-SQL assistant at LinkedIn</li> <li>Best practices and insights when migrating to Apache Iceberg for data engineers</li> <li>Many clusters and only one gateway - Starburst, Naver, and Bloomberg at Trino Summit 2023</li> <li>Visualizing Trino with Superset - Preset at Trino Summit 2023</li> <li>Trino workload management - Airbnb at Trino Summit 2023</li> </ul> </li> </ul>"},{"location":"learning-in-progress/template/","title":"May 2025","text":""},{"location":"learning-in-progress/template/#highlight-of-the-month","title":"Highlight of the Month","text":"<p>Summarize my biggest breakthrough, project, or insight in this month:</p>"},{"location":"learning-in-progress/template/#what-i-consumed","title":"What I Consumed","text":"<p>A list of articles, papers, courses, or videos I read/watched/completed:</p>"},{"location":"learning-in-progress/template/#read","title":"Read","text":""},{"location":"learning-in-progress/template/#watched","title":"Watched","text":""},{"location":"learning-in-progress/template/#completed-courses","title":"Completed Courses","text":""},{"location":"learning-in-progress/template/#what-i-created-or-tried","title":"What I Created or Tried","text":"<p>What I built, experimented with, or implemented:</p> <ul> <li> Set up ____</li> <li> Experimented with ____</li> <li> Published a blog post: ____</li> </ul>"},{"location":"learning-in-progress/template/#what-i-learned","title":"What I Learned","text":"<p>Short reflections on what I actually learned or became more confident in:</p> <ul> <li> Grasped the pros and cons of ____</li> <li> Recognized how ____</li> <li> Learned ____</li> <li> Understood ____</li> </ul>"},{"location":"learning-in-progress/template/#reflections-beyond-just-tech","title":"Reflections \u2013 Beyond Just Tech","text":"<p>Soft-skill insights or workflow/communication/process reflections:</p> <ul> <li> Realized that ____</li> <li> Noticed that ____</li> <li> Started ____</li> </ul>"},{"location":"learning-in-progress/template/#goals-for-next-month","title":"Goals for Next Month","text":"<p>Set 2\u20133 simple goals to stay focused and accountable:</p> <ul> <li> Explore ____</li> <li> Write and publish ____</li> <li> Try ____</li> </ul>"},{"location":"side-projects/","title":"Side Projects","text":"<p>I love exploring new ideas and building things in my spare time. Here, you'll find a collection of side projects that showcase my interests, technical skills, and curiosity beyond work. Each project reflects my hands-on approach to learning and creating.</p> <ul> <li> <p> Data2ML Ops</p> <p>Tools and workflows for bridging data engineering and machine learning.</p> <p>\u2192 Data2ML Ops</p> </li> <li> <p> Trending Content Prediction</p> <p>Predicting popular content using machine learning and analytics.</p> <p>\u2192 Trending Content Prediction</p> </li> <li> <p> Restful APIs with Flask</p> <p>Building scalable APIs with Flask and Python for web applications.</p> <p>\u2192 Restful APIs with Flask</p> </li> </ul>"},{"location":"side-projects/cross-cloud-unified-sql-data-pipelines/","title":"Cross Cloud Unified Sql Data Pipelines","text":""},{"location":"side-projects/sql-based-rag-application/","title":"SQL-based RAG Application","text":""},{"location":"side-projects/sql-based-rag-application/#create-a-bucket-in-gcs-and-upload-a-pdf-file","title":"Create a Bucket in GCS and upload a pdf file","text":""},{"location":"side-projects/sql-based-rag-application/#create-a-object-table-in-bigquery","title":"Create a Object Table in BigQuery","text":"<pre><code>create or replace external table `us_test2.pdf`\nwith connection `us.bg-object-tables`\noptions(\n  object_metadata = 'SIMPLE',\n  uris = ['gs://kcl-us-test/scf23.pdf']\n);\n</code></pre>"},{"location":"side-projects/sql-based-rag-application/#create-a-layout-parser-type-of-processor-in-document-ai-and-a-remote-model-corresponding-to-the-processor","title":"Create a Layout Parser type of Processor in Document AI and a Remote Model corresponding to the processor","text":"<pre><code>create or replace model `us_test2.doc_parser`\nremote with connection `us.document_ai`\noptions(\n  remote_service_type='CLOUD_AI_DOCUMENT_V1',\n  document_processor='ec023753643cb1be'\n);\n</code></pre>"},{"location":"side-projects/sql-based-rag-application/#create-a-embeddings-remote-model-in-bigquery","title":"Create a embeddings Remote Model in BigQuery","text":"<pre><code>create or replace model `us_test2.embedding_model`\nremote with connection `us.vertex_ai`\noptions (\n  endpoint='text-embedding-004'\n)\n</code></pre>"},{"location":"side-projects/sql-based-rag-application/#create-a-generative-text-remote-model-in-bigquery","title":"Create a generative text Remote Model in BigQuery","text":"<pre><code>create or replace model `us_test2.text_model`\nremote with connection `us.vertex_ai`\noptions(\n  endpoint = 'gemini-1.5-flash-002'\n)\n</code></pre>"},{"location":"side-projects/data2ml-ops/","title":"Fraud Detection: from DataOps to MLOps","text":""},{"location":"side-projects/data2ml-ops/#highlights","title":"\ud83d\udca1 Highlights","text":"Data &amp; FeaturesAutoMLOrchestration &amp; InfrastructureExperiment TrackingReal-Time Inference <p>Highlights</p> <ul> <li> Built modular, testable SQL pipelines with dbt, enabling reproducible and version-controlled feature generation</li> <li> Registered features to Feast (open source feature store) for consistent usage in both batch training and real-time serving</li> <li> Enabled feature backfilling and time-travel queries, supporting point-in-time correctness for fraud detection models</li> </ul> <p>Highlights</p> <ul> <li> Performed distributed Bayesian hyperparameter optimization using Ray Tune + Optuna, accelerating tuning efficiency at scale</li> <li> Handled imbalanced datasets using imbalanced-learn to dynamically apply over- and under-sampling strategies, improving model prediction performance</li> <li> Ensured reproducibility by tracking fixed random seeds, stratified sampling, and consistent data splits across trials</li> </ul> <p>Highlights</p> <ul> <li> Deployed the entire pipeline on Kubernetes, enabling scalable, containerized execution of distributed services</li> <li> (WIP) Orchestrated pipeline stages with Airflow, improving automation, observability, and task dependency management</li> <li> Integrated MinIO (S3-compatible) storage for storing intermediate features and trained models across components</li> </ul> <p>Highlights</p> <ul> <li> Integrated MLflow to auto-log training parameters, metrics, and artifacts, enabling experiment reproducibility and traceability</li> <li> Versioned models and experiments using MLflow\u2019s tracking server, enabling full auditability and rollback</li> <li> Stored model artifacts in remote object storage (MinIO), making them accessible for downstream deployment</li> </ul> <p>Highlights</p> <ul> <li> Deployed models as gRPC and REST endpoints using KServe, supporting diverse integration requirements</li> <li> Ensured compatibility between training-time and serving-time features via Feast\u2019s online store integration</li> <li> Enabled autoscaling and scale-to-zero, optimizing cost for infrequently used models</li> <li> Configured A/B testing traffic split, allowing controlled experimentation in production deployments</li> </ul> <p>In real-world machine learning projects, managing the workflow from data transformation to model deployment is often fragmented, error-prone, and hard to scale.</p> <p>This project demonstrates how to streamline and automate the entire lifecycle\u2014from feature engineering to hyperparameter tuning, model tracking, and deployment\u2014using modern open source tools and running fully on Kubernetes.</p> <p>The use case for this project is fraud detection, a high-impact and time-sensitive problem where real-time inference is critical. It serves as a practical demo of how to operationalize machine learning pipelines that are version-controlled, reproducible, and ready for production.</p> <p>It\u2019s designed to be:</p> <ul> <li> Reproducible \u2013 All data transformations, features, and models are versioned via dbt, Feast, and MLflow</li> <li> Scalable \u2013 Built on Kubernetes, enabling distributed training and resource orchestration across services</li> <li> Modular \u2013 Each stage is decoupled and replaceable, promoting clear responsibility and reuse</li> <li> Open Source \u2013 Fully built on open source tools like dbt, Feast, Ray, Optuna, MLflow, and KServe</li> <li> Portable \u2013 Easily adapted to other use cases beyond fraud detection</li> </ul> <p>Whether you're a data engineer, ML practitioner, or platform builder, this project offers a clear, working example of how to bridge DataOps and MLOps on a scalable, production-ready foundation.</p>"},{"location":"side-projects/data2ml-ops/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"Architecture (Click to Enlarge) dbtSQLMeshFeastAirflowRayMLflowKServe <p>Highlights</p> <ul> <li> Developed incremental models to process data in minibatches, improving pipeline efficiency and reducing compute cost</li> <li> Implemented test coverage and schema validation, ensuring data quality across transformations</li> <li> Generated documentation automatically from dbt models, enhancing maintainability and team collaboration</li> </ul> <p>Highlights</p> <p>Work In Progress</p> <p>Highlights</p> <ul> <li> Materialized online features to Redis, enabling real-time feature retrieval for low-latency inference</li> <li> Supported both batch and online inference by separating offline and online stores</li> <li> Enabled time-travel and point-in-time feature retrieval, ensuring training-serving consistency for fraud detection</li> </ul> <p>Highlights</p> <p>Work In Progress</p> <p>Highlights</p> <ul> <li> Performed distributed Bayesian hyperparameter tuning using Ray Tune and Optuna, accelerating model search and training time</li> <li> Integrated imbalanced-learn to automatically select appropriate over- and under-sampling strategies, improving performance on imbalanced datasets</li> <li> Scaled training across nodes on Kubernetes, leveraging Ray cluster for efficient resource utilization</li> </ul> <p>Highlights</p> <ul> <li> Integrated MLflow to auto-log parameters, metrics, and artifacts during training, enabling experiment tracking and auditability</li> <li> Logged final model as a versioned artifact, facilitating reproducible deployment and rollback</li> <li> Enabled reproducibility across environments by centralizing tracking and storage in a MinIO-based S3-compatible backend</li> </ul> <p>Highlights</p> <ul> <li> Deployed models as gRPC and REST endpoints using KServe, supporting diverse integration requirements</li> <li> Ensured compatibility between training-time and serving-time features via Feast\u2019s online store integration</li> <li> Enabled autoscaling and scale-to-zero, optimizing cost for infrequently used models</li> <li> Configured A/B testing traffic split, allowing controlled experimentation in production deployments</li> </ul>"},{"location":"side-projects/data2ml-ops/#whats-inside","title":"\ud83d\uddc2\ufe0f What's Inside?","text":"<pre><code>.\n\u251c\u2500\u2500 dbt/       - Transform raw data into feature tables\n\u251c\u2500\u2500 sqlmesh/   - (Work In Progress)\n\u251c\u2500\u2500 feast/     - Define and manage features with Feast\n\u251c\u2500\u2500 airflow/   - (Work In Progress)\n\u251c\u2500\u2500 ray/       - Run distributed hyperparameter tuning with Ray and Optuna\n\u251c\u2500\u2500 mlflow/    - Track experiments and log models with MLflow\n\u251c\u2500\u2500 kserve/    - Deploy trained models using KServe\n\u251c\u2500\u2500 minio/     - Configure MinIO (S3-compatible) for model/data storage\n</code></pre>"},{"location":"side-projects/data2ml-ops/courese-notes/","title":"Courese notes","text":""},{"location":"side-projects/data2ml-ops/courese-notes/#course-notes","title":"Course Notes","text":"<pre><code>gcloud iam service-accounts create SERVICE_ACCOUNT_NAME \\\n  --description=\"DESCRIPTION\" \\\n  --display-name=\"DISPLAY_NAME\"\n\ngcloud projects add-iam-policy-binding udemy-mlops \\\n    --member=serviceAccount:vertexai-sa@udemy-mlops.iam.gserviceaccount.com \\\n    --role=roles/aiplatform.customCodeServiceAgent\n\ngcloud projects add-iam-policy-binding udemy-mlops \\\n    --member=serviceAccount:vertexai-sa@udemy-mlops.iam.gserviceaccount.com \\\n    --role=roles/aiplatform.admin\n\ngcloud projects add-iam-policy-binding udemy-mlops \\\n    --member=serviceAccount:vertexai-sa@udemy-mlops.iam.gserviceaccount.com \\\n    --role=roles/storage.objectAdmin\n</code></pre>"},{"location":"side-projects/data2ml-ops/courese-notes/#hands-on-vertex-ai-custom-training-gcloud-cli","title":"hands-on Vertex AI Custom Training gcloud cli","text":"<ol> <li>Create a service account and grant the necessary permissions</li> </ol> <pre><code>gcloud iam service-accounts create SERVICE_ACCOUNT_NAME \\\n  --description=\"DESCRIPTION\" \\\n  --display-name=\"DISPLAY_NAME\"\n\ngcloud projects add-iam-policy-binding udemy-mlops \\\n    --member=serviceAccount:vertexai-sa@udemy-mlops.iam.gserviceaccount.com \\\n    --role=roles/aiplatform.customCodeServiceAgent\n\ngcloud projects add-iam-policy-binding udemy-mlops \\\n    --member=serviceAccount:vertexai-sa@udemy-mlops.iam.gserviceaccount.com \\\n    --role=roles/aiplatform.admin\n\ngcloud projects add-iam-policy-binding udemy-mlops \\\n    --member=serviceAccount:vertexai-sa@udemy-mlops.iam.gserviceaccount.com \\\n    --role=roles/storage.objectAdmin\n</code></pre> <ol> <li>Build the image locally</li> </ol> <pre><code>docker build -t vertex-bikeshare-model .\n</code></pre> <ol> <li>Tag the image locally</li> </ol> <pre><code>docker tag vertex-bikeshare-model gcr.io/udemy-mlops/vertex-bikeshare-model\n</code></pre> <ol> <li>Push the image to GCR</li> </ol> <pre><code>docker push gcr.io/udemy-mlops/vertex-bikeshare-model\n</code></pre> <ol> <li>Submit a custom model training job using gcloud</li> </ol> <pre><code>gcloud ai custom-jobs create --region=us-central1 \\\n--project=udemy-mlops \\\n--worker-pool-spec=replica-count=1,machine-type='n1-standard-4',container-image-uri='gcr.io/udemy-mlops/vertex-bikeshare-model' \\\n--service-account=SERVICE_ACCOUNT\n--display-name=bike-sharing-model-training\n</code></pre>"},{"location":"side-projects/data2ml-ops/courese-notes/#section-5","title":"Section 5","text":"<ol> <li>Create bucket</li> <li>Upload dataset</li> <li>Enable Secret Manager API in order to create Cloud Builds v2 Repositories</li> <li>Create Cloud Build v2 Repositories connection and link repo</li> <li>Create Cloud Build Trigger</li> <li>create github repo</li> <li></li> </ol> <pre><code># Assign Service account user role to the service account \ngcloud projects add-iam-policy-binding udemy-mlops \\\n--member=serviceAccount:1090925531874@cloudbuild.gserviceaccount.com --role=roles/aiplatform.admin\n</code></pre> <p>Cloud Build</p> <ol> <li>Build Docker Image</li> <li>Push Docker Image To GCR</li> <li>Execute Tests</li> <li>Submit Custom Training Job and wait until succedded</li> <li>Upload Model to Model Registry</li> <li>Fetch Model ID</li> <li>Create Endpoint</li> <li>Deploy Model Endpoint</li> </ol>"},{"location":"side-projects/data2ml-ops/courese-notes/#model_training_codepy","title":"model_training_code.py","text":"<ol> <li>load data</li> <li>preprocess data (rename columns, drop columns, one-hot-encodings)</li> <li>train test split</li> <li>train model</li> <li>dump model(\"gs://sid-vertex-mlops/bike-share-rf-regression-artifact/model.joblib\")</li> </ol>"},{"location":"side-projects/data2ml-ops/courese-notes/#section-6-kubefloe-pipeline","title":"Section 6 Kubefloe Pipeline","text":"<ul> <li>Upload in-vehicle coupon recommendation file</li> <li>kubeflow <code>@component</code><ul> <li><code>validate_input_ds()</code></li> <li><code>custom_training_job_component()</code></li> </ul> </li> <li>kubeflow <code>@dsl.pipeline</code><ul> <li><code>pipeline()</code></li> </ul> </li> <li><code>compiler()</code> -&gt; compile.json</li> <li> <p><code>from google.cloud.aiplatform import pipeline_jobs</code></p> <ul> <li><code>pipeline_jobs.PipelineJob().run()</code></li> </ul> </li> <li> <p><code>kfp.dsl.Metrics</code>: An artifact for storing key-value scalar metrics.</p> </li> <li><code>kfp.dsl.Output</code></li> </ul>"},{"location":"side-projects/data2ml-ops/prerequisites/","title":"Prerequisites","text":"<p>Before getting started, ensure your environment is properly set up with the following installations:</p> <ul> <li>Homebrew</li> <li>Minikube</li> <li>Docker Desktop</li> <li>kubectl</li> </ul> <pre><code>brew --version\n\nHomebrew 4.5.1\n</code></pre> <pre><code>minikube version\n\nminikube version: v1.33.1\ncommit: 248d1ec5b3f9be5569977749a725f47b018078ff\n</code></pre> <pre><code>docker version\n\nClient:\n Version:           28.1.1\n API version:       1.49\n Go version:        go1.23.8\n Git commit:        4eba377\n Built:             Fri Apr 18 09:49:45 2025\n OS/Arch:           darwin/arm64\n Context:           desktop-linux\n\nServer: Docker Desktop 4.41.1 (191279)\n Engine:\n  Version:          28.1.1\n  API version:      1.49 (minimum version 1.24)\n  Go version:       go1.23.8\n  Git commit:       01f442b\n  Built:            Fri Apr 18 09:52:08 2025\n  OS/Arch:          linux/arm64\n  Experimental:     false\n containerd:\n  Version:          1.7.27\n  GitCommit:        05044ec0a9a75232cad458027ca83437aae3f4da\n runc:\n  Version:          1.2.5\n  GitCommit:        v1.2.5-0-g59923ef\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n</code></pre> <pre><code>kubectl version\n\nClient Version: v1.30.2\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\nServer Version: v1.30.0\n</code></pre> <p>This series of articles provides detailed explanations and step-by-step guides. If you prefer to focus on the practical implementation, follow the sequence below to build the architecture step by step.</p> <ul> <li>Modeling Data</li> <li>Modeling Features</li> <li>Deploy Feast on Kubernetes</li> <li>Deploy MinIO on Kubernetes</li> <li>Deploy MLflow on Kubernetes</li> <li>Deploy Ray Cluster on Kubernetes Using KubeRay</li> <li>Integrate Ray Tune with Optuna, Imblearn, MLflow and MinIO</li> <li>Submit a Ray Tune Job to Your Ray Cluster</li> <li>Install KServe in Serverless Mode</li> <li>Feast Transformer</li> <li>Deploy Your Model on Kubernetes Using Kserve InferenceService</li> </ul>"},{"location":"side-projects/data2ml-ops/dbt/how-it-works/","title":"How It Works?","text":"<p>__</p>"},{"location":"side-projects/data2ml-ops/dbt/how-it-works/#core-concepts","title":"Core Concepts","text":""},{"location":"side-projects/data2ml-ops/dbt/how-it-works/#architecture-components","title":"Architecture Components","text":""},{"location":"side-projects/data2ml-ops/dbt/how-it-works/#key-modules-and-their-roles","title":"Key Modules and Their Roles","text":""},{"location":"side-projects/data2ml-ops/dbt/how-it-works/#behind-the-scenes","title":"Behind the Scenes","text":""},{"location":"side-projects/data2ml-ops/dbt/in-the-bigger-picture/","title":"In the Bigger Picture","text":""},{"location":"side-projects/data2ml-ops/dbt/in-the-bigger-picture/#history-current-status","title":"History &amp; Current Status","text":"<p>dbt (data build tool) is an open-source command-line tool that enables data analysts and engineers to transform data within their warehouses using SQL. Developed by dbt Labs (formerly Fishtown Analytics), the project was initiated in 2016 at RJMetrics and open-sourced in 2018. dbt focuses on the transformation component of the ELT (Extract, Load, Transform) process, allowing users to write modular, version-controlled SQL queries that can be tested and documented. It supports various data warehouses, including Snowflake, BigQuery, and Redshift.  \ufffc \ufffc \ufffc</p> <p>As of May 2025, dbt has over 10,800 stars on GitHub, with more than 325 contributors.  The dbt Community has grown to over 80,000 members, featuring active Slack channels, community forums, and regular meetups in 26 countries.  dbt is utilized by over 25,000 companies, including Microsoft, JetBlue, GitLab, and Sequoia Capital, highlighting its widespread adoption in the industry.  </p>"},{"location":"side-projects/data2ml-ops/dbt/in-the-bigger-picture/#alternatives","title":"Alternatives","text":""},{"location":"side-projects/data2ml-ops/dbt/in-the-bigger-picture/#integration-points","title":"Integration Points","text":""},{"location":"side-projects/data2ml-ops/dbt/in-the-bigger-picture/#kubernetes","title":"Kubernetes","text":""},{"location":"side-projects/data2ml-ops/dbt/in-the-bigger-picture/#cicd","title":"CI/CD","text":""},{"location":"side-projects/data2ml-ops/dbt/modeling-data/","title":"Modeling Data","text":""},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#installation","title":"Installation","text":"<p>Install with pip | dbt Docs</p> <pre><code>python3.10 -m venv dbt\n</code></pre> <pre><code>source ~/.venvs/dbt/bin/activate\n</code></pre> requirements.txt<pre><code>dbt-core==1.9.4\ndbt-bigquery==1.9.1\nsqlfluff==3.4.0\n</code></pre> <pre><code>pip install -r requirements.txt\n</code></pre> <pre><code>dbt --version\nCore:\n  - installed: 1.9.4\n  - latest:    1.9.4 - Up to date!\n\nPlugins:\n  - bigquery: 1.9.1 - Up to date!\n</code></pre>"},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#sources","title":"Sources","text":"_sources.yml<pre><code>version: 2\n\nsources:\n  - name: feast\n    description: &gt;\n      This source contains the raw data for the fraud detection project. It includes \n      transaction records, user account features, and labels indicating whether a \n      transaction was fraudulent.\n    project: feast-oss\n    dataset: fraud_tutorial\n    meta:\n      domain: fraud\n      owner: \"@kclai\"\n    tags:\n      - fraud\n      - pii\n    tables:\n      - name: transactions\n      - name: user_account_features\n      - name: user_has_fraudulent_transactions\n</code></pre>"},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#staging","title":"Staging","text":"<ul> <li>naming convention: <code>stg__&lt;src&gt;__&lt;tbl&gt;</code></li> </ul>"},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#stg__feast__transactions","title":"stg__feast__transactions","text":"stg__feast__transactions.sql<pre><code>select\n  src_account as src_account_id,\n  dest_account as dest_account_id,\n  amount,\n  case\n    when is_fraud = 1 then true\n    when is_fraud = 0 then false\n    else null\n  end as is_fraud,\n  timestamp as created_at\nfrom {{ source('feast', 'transactions') }}\n</code></pre> stg__feast__transactions.yml<pre><code>version: 2\n\nmodels:\n  - name: stg__feast__transactions\n    description: &gt;\n      This staging model standardizes raw transaction data for downstream fraud analysis. \n      It includes source and destination accounts, transaction amount, timestamp, and \n      a binary fraud label. Used as a foundational layer for detecting fraudulent behavior.\n    config:\n      event_time: created_at\n      contract:\n        enforced: true\n    columns:\n      - name: src_account_id\n        data_type: string\n        description: &gt;\n          The unique identifier of the source account that initiated the transaction.\n\n      - name: dest_account_id\n        data_type: string\n        description: &gt;\n          The unique identifier of the destination account that received the transaction.\n\n      - name: amount\n        data_type: float\n        description: &gt;\n          The monetary amount of the transaction. Assumes a consistent currency.\n\n      - name: is_fraud\n        data_type: boolean\n        description: &gt;\n          A boolean flag indicating whether the transaction was identified as fraudulent.\n          `true` indicates a confirmed fraudulent transaction.\n\n      - name: created_at\n        data_type: timestamp\n        description: &gt;\n          The timestamp when the transaction was created. Used for time-based analysis and \n          feature extraction (e.g., fraud trends over time).\n</code></pre>"},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#stg__feast__acct_fraud_7d","title":"stg__feast__acct_fraud_7d","text":"stg__feast__acct_fraud_7d.sql<pre><code>select\n  user_id as account_id,\n  case\n    when user_has_fraudulent_transactions_7d = 1 then true\n    when user_has_fraudulent_transactions_7d = 0 then false\n    else null\n  end as has_fraud_7d,\n  feature_timestamp\nfrom {{ source('feast', 'user_has_fraudulent_transactions') }}\n</code></pre> stg__feast__acct_fraud_7d.yml<pre><code>version: 2\n\nmodels:\n  - name: stg__feast__acct_fraud_7d\n    description: &gt;\n      This staging model identifies whether a user has been involved in any fraudulent transactions\n      within a specified time window (7 days). It is typically used as a feature in fraud detection\n      pipelines or real-time inference systems.\n    config:\n      contract:\n        enforced: true\n    columns:\n      - name: account_id\n        data_type: string\n        description: &gt;\n          The unique identifier of the user. This is used as the primary key to track fraud status\n          per user.\n\n      - name: has_fraud_7d\n        data_type: boolean\n        description: &gt;\n          A binary flag (true or false) indicating whether the user has had any fraudulent transactions \n          in the past 7 days. Can be used as a feature in ML models.\n\n      - name: feature_timestamp\n        data_type: timestamp\n        description: &gt;\n          The timestamp representing the point-in-time when the feature was calculated. Useful for\n          point-in-time correctness in feature stores or backtesting.\n</code></pre>"},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#stg__feast__acct_profiles","title":"stg__feast__acct_profiles","text":"stg__feast__acct_profiles.sql<pre><code>select\n  user_id as account_id,\n  credit_score,\n  account_age_days,\n  case\n    when user_has_2fa_installed = 1 then true\n    when user_has_2fa_installed = 0 then false\n    else null\n  end as has_2fa_installed,\n  feature_timestamp\nfrom {{ source('feast', 'user_account_features') }}\n</code></pre> stg__feast__acct_profiles.yml<pre><code>version: 2\n\nmodels:\n  - name: stg__feast__acct_profiles\n    description: \"\"\n    config:\n      contract:\n        enforced: true\n    columns:\n      - name: account_id\n        data_type: string\n        description: \"\"\n\n      - name: credit_score\n        data_type: int\n        description: \"\"\n\n      - name: account_age_days\n        data_type: int\n        description: \"\"\n\n      - name: has_2fa_installed\n        data_type: boolean\n        description: \"\"\n\n      - name: feature_timestamp\n        data_type: timestamp\n        description: \"\"\n</code></pre>"},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#intermediate","title":"Intermediate","text":""},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#int__feast__acct_num_txns","title":"int__feast__acct_num_txns","text":"int__feast__acct_num_txns.sql<pre><code>{{ config(\n    materialized='incremental',\n    incremental_strategy='microbatch',\n    event_time='transaction_date',\n    begin='2025-04-20',\n    batch_size='day',\n    partition_by={\n      \"field\": \"transaction_date\",\n      \"data_type\": \"date\",\n      \"granularity\": \"day\"\n    },\n    on_schema_change='append_new_columns'\n) }}\n\nwith transactions as (\n\n    -- this ref will be auto-filtered\n    select\n      {{ dbt_utils.star(\n        ref('fraud', 'stg__feast__transactions')\n      ) }}\n    from {{ ref('fraud', 'stg__feast__transactions') }}\n\n)\n\nselect\n  src_account_id as account_id,\n  date(created_at) as transaction_date,\n  count(*) as num_transactions,\n  sum(amount) as total_amount,\n  sum(case when amount &gt; 0 then amount else 0 end) as total_deposits,\nfrom transactions\ngroup by account_id, date(created_at)\n</code></pre>"},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#int__feast__acct_num_txns__extented","title":"int__feast__acct_num_txns__extented","text":"int__feast__acct_num_txns__extented.sql<pre><code>select\n  *,\n  sum(num_transactions) over (\n    partition by account_id\n    order by unix_date(transaction_date)\n    range between 6 preceding\n    and current row\n  ) as num_transactions_7d\nfrom {{ ref('fraud', 'int__feast__acct_num_txns') }}\n</code></pre>"},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#marts","title":"Marts","text":""},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#feat_acct_num_txns_7d","title":"feat_acct_num_txns_7d","text":"feat_acct_num_txns_7d.sql<pre><code>select\n  account_id as entity_id,\n  num_transactions_7d,\n  cast(transaction_date as timestamp) as feature_timestamp\nfrom {{ ref(\"fraud\", \"int__feast__acct_num_txns__extented\") }}\n</code></pre>"},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#feat_acct_fraud_7d","title":"feat_acct_fraud_7d","text":"feat_acct_fraud_7d.sql<pre><code>select\n  account_id as entity_id,\n  has_fraud_7d,\n  feature_timestamp\nfrom {{ ref('fraud', 'stg__feast__acct_fraud_7d') }}\n</code></pre>"},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#feat_acct_profiles","title":"feat_acct_profiles","text":"feat_acct_profiles.sql<pre><code>select\n  account_id as entity_id,\n  credit_score,\n  account_age_days,\n  has_2fa_installed,\n  feature_timestamp\nfrom {{ ref('fraud', 'stg__feast__acct_profiles') }}\n</code></pre>"},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#exposures","title":"Exposures","text":"exposures.yml<pre><code>version: 2\n\nexposures:\n  - name: account_features\n    description:\n      This exposure contains features related to account activity and fraud detection.\n    type: ml\n    maturity: medium\n    owner:\n      name: KC Lai\n      email: kuanchoulai10@gmail.com\n    label: Account-related features\n    url: https://example.com\n    depends_on:\n      - ref('feat_acct_fraud_7d')\n      - ref('feat_acct_num_txns_7d')\n      - ref('feat_acct_profiles')\n</code></pre>"},{"location":"side-projects/data2ml-ops/dbt/what-why-when/","title":"What, Why, When","text":""},{"location":"side-projects/data2ml-ops/dbt/what-why-when/#what-is-x","title":"What is X?","text":""},{"location":"side-projects/data2ml-ops/dbt/what-why-when/#why-x","title":"Why X?","text":"<ul> <li>Data Engineer</li> <li>Data Analyst</li> <li>Data Scientist</li> <li>Machine Learning Engineer</li> </ul>"},{"location":"side-projects/data2ml-ops/dbt/what-why-when/#when-to-use-x","title":"When to Use X?","text":""},{"location":"side-projects/data2ml-ops/feast/deployment/","title":"Deploy Feast on Kubernetes","text":"<ul> <li>Offline Store: BigQuery</li> <li>Registry: postgresql</li> <li>Online Store: Redis</li> </ul> Architecture (Click to Enlarge) <pre><code>minikube start --cpus=4 --memory=7000 --driver=docker\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/deployment/#build-feast-docker-image-locally","title":"Build Feast Docker Image Locally","text":"<p>go to <code>feast/</code> folder. \u5c31\u6703\u770b\u5230\u4ee5\u4e0b\u6a94\u6848</p> Dockerfile<pre><code># Use Python 3.10 slim as the base image\nFROM python:3.10-slim\n\n# Set the working directory\nWORKDIR /app\n\n# Install Python dependencies\nCOPY requirements.txt /app/requirements.txt\nRUN pip install --no-cache-dir -r requirements.txt\nCOPY entrypoint.sh /app/entrypoint.sh\nRUN chmod +x entrypoint.sh\n\nCOPY feature_store.yaml /app/feature_store.yaml\nCOPY account_features.py /app/account_features.py\n\n# Set the entrypoint to run Feast\nENTRYPOINT [\"./entrypoint.sh\"]\n</code></pre> feature_store.yaml<pre><code>project: test\nregistry: # https://docs.feast.dev/reference/registries/sql\n  registry_type: sql\n  path: postgresql+psycopg://postgres:password@registry.feast.svc.cluster.local:5432\n  cache_ttl_seconds: 60\n  sqlalchemy_config_kwargs:\n    echo: false\n    pool_pre_ping: true\nentity_key_serialization_version: 3\nprovider: local \nonline_store: # https://docs.feast.dev/reference/online-stores/redis\n  type: redis\n  connection_string: \"online-store.feast.svc.cluster.local:6379\"\noffline_store: # https://docs.feast.dev/reference/offline-stores/bigquery\n  type: bigquery\n  dataset: feast\n  project_id: mlops-437709\n  billing_project_id: mlops-437709\n  location: US\n</code></pre> entrypoint.sh<pre><code>#!/bin/bash\nset -e\n\necho \"Running feast apply...\"\nfeast apply\n\necho \"Starting feast server...\"\nexec feast serve --host 0.0.0.0 --port 8080\n</code></pre> <p>Build image</p> <pre><code>docker buildx build \\\n  --platform linux/amd64 \\\n  -t feast:v0.1.0 \\\n  -t feast:latest \\\n  .\n</code></pre> <p>\u6211\u662f\u4f7f\u7528MacOS\uff0c\u6211\u5011\u4f7f\u7528<code>docker buildx</code>\uff0cfor amd64\u67b6\u69cb</p> <p>\u78ba\u8a8d\u662f\u5426\u5efa\u7f6e\u6210\u529f</p> <pre><code>docker images --filter=reference=\"feast*\"\n</code></pre> <pre><code>REPOSITORY   TAG       IMAGE ID       CREATED          SIZE\nfeast        v0.1.0    436abcf371e9   55 seconds ago   596MB\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/deployment/#load-image-into-minikube-cluster","title":"Load Image into Minikube Cluster","text":"<p>\u5728Local\u9032\u5165minikube</p> <pre><code>minikube ssh\n</code></pre> <p>\u67e5\u770b\u76ee\u524dminikube\u88e1\u6709\u7684images\uff0c\u6c92\u6709feast <pre><code>docker@minikube:~$ docker images\n</code></pre></p> <p>Expected Output</p> <pre><code>REPOSITORY                                TAG        IMAGE ID       CREATED         SIZE\nregistry.k8s.io/kube-apiserver            v1.30.0    181f57fd3cdb   12 months ago   112MB\nregistry.k8s.io/kube-controller-manager   v1.30.0    68feac521c0f   12 months ago   107MB\nregistry.k8s.io/kube-proxy                v1.30.0    cb7eac0b42cc   12 months ago   87.9MB\nregistry.k8s.io/kube-scheduler            v1.30.0    547adae34140   12 months ago   60.5MB\nregistry.k8s.io/etcd                      3.5.12-0   014faa467e29   15 months ago   139MB\nregistry.k8s.io/coredns/coredns           v1.11.1    2437cf762177   21 months ago   57.4MB\nregistry.k8s.io/pause                     3.9        829e9de338bd   2 years ago     514kB\ngcr.io/k8s-minikube/storage-provisioner   v5         ba04bb24b957   4 years ago     29MB\ndocker@minikube:~$ \n</code></pre> <p>\u5728local\u958b\u555f\u53e6\u500bterminal\uff0c\u5c07local\u7684docker image\u8f09\u5165\u5230minikube cluster\u88e1</p> <pre><code>minikube image load feast:v0.1.0\n</code></pre> <p>\u56de\u5230minikube\u88e1\uff0c\u67e5\u770bimages\uff0c\u6709feast\u4e86</p> <pre><code>docker@minikube:~$ docker images\n</code></pre> <p>Expected Output</p> <pre><code>REPOSITORY                                TAG        IMAGE ID       CREATED          SIZE\nfeast                                     v0.1.0     436abcf371e9   33 minutes ago   596MB\nregistry.k8s.io/kube-apiserver            v1.30.0    181f57fd3cdb   12 months ago    112MB\nregistry.k8s.io/kube-controller-manager   v1.30.0    68feac521c0f   12 months ago    107MB\nregistry.k8s.io/kube-scheduler            v1.30.0    547adae34140   12 months ago    60.5MB\nregistry.k8s.io/kube-proxy                v1.30.0    cb7eac0b42cc   12 months ago    87.9MB\nregistry.k8s.io/etcd                      3.5.12-0   014faa467e29   15 months ago    139MB\nregistry.k8s.io/coredns/coredns           v1.11.1    2437cf762177   21 months ago    57.4MB\nregistry.k8s.io/pause                     3.9        829e9de338bd   2 years ago      514kB\ngcr.io/k8s-minikube/storage-provisioner   v5         ba04bb24b957   4 years ago      29MB\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/deployment/#k8s","title":"K8S","text":""},{"location":"side-projects/data2ml-ops/feast/deployment/#registry","title":"Registry","text":"registry.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: registry\n  namespace: feast\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: registry\n  template:\n    metadata:\n      labels:\n        app: registry\n    spec:\n      containers:\n        - name: registry\n          image: postgres\n          env:\n            - name: POSTGRES_DB\n              value: feast\n            - name: POSTGRES_USER\n              value: postgres\n            - name: POSTGRES_PASSWORD\n              value: password\n          ports:\n            - containerPort: 5432\n          volumeMounts:\n            - name: storage\n              mountPath: /var/lib/postgresql/data\n      volumes:\n        - name: storage\n          hostPath:\n            path: /home/docker/data/feast/registry\n            type: DirectoryOrCreate          \n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: registry\n  namespace: feast\nspec:\n  selector:\n    app: registry\n  type: ClusterIP\n  ports:\n    - port: 5432\n      targetPort: 5432\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/deployment/#online-store","title":"Online Store","text":"online-store.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: online-store\n  namespace: feast\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: online-store\n  template:\n    metadata:\n      labels:\n        app: online-store\n    spec:\n      containers:\n        - name: online-store\n          image: redis\n          ports:\n            - containerPort: 6379\n              protocol: TCP\n          volumeMounts:\n            - name: storage\n              mountPath: /data\n      restartPolicy: Always\n      volumes:\n        - name: storage\n          hostPath:\n            path: /home/docker/data/feast/online-store\n            type: DirectoryOrCreate\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: online-store\n  namespace: feast\nspec:\n  selector:\n    app: online-store\n  type: ClusterIP\n  ports:\n    - port: 6379\n      targetPort: 6379\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/deployment/#secret","title":"Secret","text":""},{"location":"side-projects/data2ml-ops/feast/deployment/#create-gcp-service-account-key","title":"Create GCP Service Account Key","text":"<p>\u78ba\u8a8dProject id</p> <pre><code>gcloud config get-value project\n</code></pre> <p>Expected Output</p> <pre><code>mlops-437709\n</code></pre> <p>\u5efa\u7acbService Account</p> <pre><code>gcloud iam service-accounts create feast-sa \\\n    --display-name \"Feast Service Account\"\n</code></pre> <p>Expected Output</p> <pre><code>Reauthentication required.\nPlease enter your password:\nReauthentication successful.\nCreated service account [feast-sa].\n</code></pre> <p>\u66ffService Account\u52a0\u4e0aBigQuery\u6b0a\u9650</p> <pre><code>gcloud projects add-iam-policy-binding mlops-437709 \\\n    --member=\"serviceAccount:feast-sa@mlops-437709.iam.gserviceaccount.com\" \\\n    --role=\"roles/bigquery.admin\"\n</code></pre> Expected Output <pre><code>[1] EXPRESSION=request.time &lt; timestamp(\"2025-04-09T07:42:05.596Z\"), TITLE=cloudbuild-connection-setup\n[2] None\n[3] Specify a new condition\nThe policy contains bindings with conditions, so specifying a condition is required when adding a binding. Please specify a condition.:  2\n\nUpdated IAM policy for project [mlops-437709].\nbindings:\n- members:\n  - serviceAccount:service-362026176730@gcp-sa-aiplatform-cc.iam.gserviceaccount.com\n  role: roles/aiplatform.customCodeServiceAgent\n- members:\n  - serviceAccount:service-362026176730@gcp-sa-vertex-op.iam.gserviceaccount.com\n  role: roles/aiplatform.onlinePredictionServiceAgent\n- members:\n  - serviceAccount:service-362026176730@gcp-sa-aiplatform.iam.gserviceaccount.com\n  role: roles/aiplatform.serviceAgent\n- members:\n  - serviceAccount:service-362026176730@gcp-sa-artifactregistry.iam.gserviceaccount.com\n  role: roles/artifactregistry.serviceAgent\n- members:\n  - serviceAccount:feast-sa@mlops-437709.iam.gserviceaccount.com\n  role: roles/bigquery.admin\n- members:\n  - serviceAccount:362026176730@cloudbuild.gserviceaccount.com\n  role: roles/cloudbuild.builds.builder\n- members:\n  - serviceAccount:service-362026176730@gcp-sa-cloudbuild.iam.gserviceaccount.com\n  role: roles/cloudbuild.serviceAgent\n- members:\n  - serviceAccount:service-362026176730@containerregistry.iam.gserviceaccount.com\n  role: roles/containerregistry.ServiceAgent\n- members:\n  - serviceAccount:362026176730-compute@developer.gserviceaccount.com\n  role: roles/editor\n- members:\n  - serviceAccount:service-362026176730@gcp-sa-firestore.iam.gserviceaccount.com\n  role: roles/firestore.serviceAgent\n- members:\n  - serviceAccount:362026176730@cloudbuild.gserviceaccount.com\n  role: roles/iam.serviceAccountUser\n- members:\n  - serviceAccount:service-362026176730@cloud-ml.google.com.iam.gserviceaccount.com\n  role: roles/ml.serviceAgent\n- members:\n  - user:edison@kcl10.com\n  role: roles/owner\n- members:\n  - serviceAccount:service-362026176730@gcp-sa-pubsub.iam.gserviceaccount.com\n  role: roles/pubsub.serviceAgent\n- members:\n  - serviceAccount:362026176730@cloudbuild.gserviceaccount.com\n  role: roles/run.admin\n- members:\n  - serviceAccount:service-362026176730@serverless-robot-prod.iam.gserviceaccount.com\n  role: roles/run.serviceAgent\n- condition:\n    expression: request.time &lt; timestamp(\"2025-04-09T07:42:05.596Z\")\n    title: cloudbuild-connection-setup\n  members:\n  - serviceAccount:service-362026176730@gcp-sa-cloudbuild.iam.gserviceaccount.com\n  role: roles/secretmanager.admin\netag: BwY0his6v7Y=\nversion: 3\n</code></pre> <p>\u5efa\u7acbKey</p> <pre><code>gcloud iam service-accounts keys create feast-gcp-key.json \\\n    --iam-account=feast-sa@mlops-437709.iam.gserviceaccount.com\n</code></pre> <p>Expected Output</p> <pre><code>created key [a2609fffff05f5fdf311de233f1a2e1e89288ab5] of type [json] as [feast-gcp-key.json] for [feast-sa@mlops-437709.iam.gserviceaccount.com]\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/deployment/#create-k8s-secret","title":"Create K8S Secret","text":"<pre><code>cat feast-gcp-key.json | base64\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: feast-gcp-key\n  namespace: feast\ntype: Opaque\ndata:\n  key.json: &lt;base64 encoded string&gt;\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/deployment/#online-feature-server","title":"Online Feature Server","text":"online-feature-server.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: online-feature-server\n  namespace: feast\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: online-feature-server\n  template:\n    metadata:\n      labels:\n        app: online-feature-server\n    spec:\n      containers:\n        - name: online-feature-server\n          image: feast:v0.1.8\n          ports:\n            - containerPort: 8080\n          env:\n            - name: FEAST_FEATURE_SERVER_CONFIG_PATH\n              value: /app/config/feature_store.yaml\n            - name: GOOGLE_APPLICATION_CREDENTIALS\n              value: /var/secrets/google/key.json\n          volumeMounts:\n            - name: gcp-sa-key\n              mountPath: /var/secrets/google\n              readOnly: true\n      volumes:\n        - name: gcp-sa-key\n          secret:\n            secretName: gcp-sa-key\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: online-feature-server\n  namespace: feast\nspec:\n  selector:\n    app: online-feature-server\n  type: NodePort\n  ports:\n    - port: 8080\n      targetPort: 8080\n      nodePort: 30000\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/deployment/#deploy-your-feast-architecture","title":"Deploy Your Feast Architecture","text":"install.sh<pre><code>#!/bin/bash\n\necho \"Step 0: Create feast Namespace\"\nkubectl create ns feast\n\necho \"Step 1: Deploy Online Store\"\nkubectl apply -f online-store.yaml\nkubectl rollout status deployment/online-store -n feast\n\necho \"Step 2: Deploy Registry\"\nkubectl apply -f registry.yaml\nkubectl rollout status deployment/registry -n feast\n\necho \"Step 3: Deploy Feast Online Feature Server\"\nkubectl apply -f secret.yaml\nkubectl apply -f online-feature-server.yaml\nkubectl rollout status deployment/online-feature-server -n feast\n</code></pre> <pre><code>./install.sh\n</code></pre> <p>Expected Output</p> <pre><code>Step 0: Create feast Namespace\nnamespace/feast created\n\nStep 1: Deploy Online Store\ndeployment.apps/online-store created\nservice/online-store created\nWaiting for deployment \"online-store\" rollout to finish: 0 of 1 updated replicas are available...\ndeployment \"online-store\" successfully rolled out\n\nStep 2: Deploy Registry\ndeployment.apps/registry created\nservice/registry created\nWaiting for deployment \"registry\" rollout to finish: 0 of 1 updated replicas are available...\ndeployment \"registry\" successfully rolled out\n\nStep 3: Deploy Feast Online Feature Server\nsecret/gcp-sa-key created\ndeployment.apps/online-feature-server created\nservice/online-feature-server created\nWaiting for deployment \"online-feature-server\" rollout to finish: 0 of 1 updated replicas are available...\ndeployment \"online-feature-server\" successfully rolled out\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/deployment/#test-your-feast-online-feature-server","title":"Test Your Feast Online Feature Server","text":"<p>\u9996\u5148\u8981\u5148\u5c07service \u900f\u904e<code>minikube service</code> output\u7d66Local\u4f7f\u7528</p> <pre><code>minikube service online-feature-server -n feast\n</code></pre> <p>Expected Output</p> <pre><code>|-----------|-----------------------|-------------|---------------------------|\n| NAMESPACE |         NAME          | TARGET PORT |            URL            |\n|-----------|-----------------------|-------------|---------------------------|\n| feast     | online-feature-server |        8080 | http://192.168.49.2:30000 |\n|-----------|-----------------------|-------------|---------------------------|\n\ud83c\udfc3  Starting tunnel for service online-feature-server.\n|-----------|-----------------------|-------------|------------------------|\n| NAMESPACE |         NAME          | TARGET PORT |          URL           |\n|-----------|-----------------------|-------------|------------------------|\n| feast     | online-feature-server |             | http://127.0.0.1:52316 |\n|-----------|-----------------------|-------------|------------------------|\n\ud83c\udf89  Opening service feast/online-feature-server in default browser...\n\u2757  Because you are using a Docker driver on darwin, the terminal needs to be open to run it.\n</code></pre> <p><code>http://127.0.0.1:52316</code>\u5c31\u662f\u6211\u5011\u5728localhost\u53ef\u4ee5access\u5f97\u5230\u7684URL\uff0c\u5f85\u6703\u6e2c\u8a66\u6642\u90fd\u6703\u9700\u8981\u6253\u9019\u500bURL</p>"},{"location":"side-projects/data2ml-ops/feast/deployment/#materialize-features-to-online-store","title":"Materialize Features to Online Store","text":""},{"location":"side-projects/data2ml-ops/feast/deployment/#materialize","title":"<code>/materialize</code>","text":"<p>Materialize features within a specified time range</p> <pre><code>curl -X POST http://127.0.0.1:52316/materialize \\\n     -H \"Content-Type: application/json\" \\\n     -d @request-materialize.json\n</code></pre> <p>where <code>request-materialize.json</code> is</p> <pre><code>{\"start_ts\": \"2024-08-07T00:00:00Z\", \"end_ts\": \"2024-08-08T00:00:00Z\"}\n</code></pre> <p>by running <code>kubectl logs pod</code>, you can see the progress of materialization:</p> <p>Expected Output</p> <pre><code>Materializing 3 feature views from 2024-08-07 00:00:00+00:00 to 2024-08-08 00:00:00+00:00 into the redis online store.\n\nacct_fraud_7d:\n0it [00:00, ?it/s]\nacct_profiles:\n0it [00:00, ?it/s]\nacct_num_txns_7d:\n0it [00:00, ?it/s]\n10.244.0.1:39638 - \"POST /materialize HTTP/1.1\" 200\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/deployment/#materialize-incremental","title":"<code>/materialize-incremental</code>","text":"<p>Incrementally materialize features up to a specified timestamp</p> <pre><code>curl -X POST http://127.0.0.1:52316/materialize-incremental \\\n     -H \"Content-Type: application/json\" \\\n     -d @request-materialize-incremental.json\n</code></pre> <p>where <code>request-materialize-incremental.json</code> is</p> <pre><code>{\"end_ts\": \"2025-05-27T07:00:00\"}\n</code></pre> <p>by running <code>kubectl logs pod</code>, you can see the progress of materialization:</p> <p>Expected Output</p> <pre><code>acct_fraud_7d from 2025-05-11 11:56:07+00:00 to 2025-05-27 07:00:00+00:00:\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9944/9944 [00:02&lt;00:00, 3984.00it/s]\nacct_profiles from 2025-05-11 11:56:07+00:00 to 2025-05-27 07:00:00+00:00:\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9944/9944 [00:00&lt;00:00, 14760.64it/s]\nacct_num_txns_7d from 2025-05-11 11:56:07+00:00 to 2025-05-27 07:00:00+00:00:\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9944/9944 [00:00&lt;00:00, 17575.82it/s]\n10.244.0.1:23621 - \"POST /materialize-incremental HTTP/1.1\" 200\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/deployment/#get-online-features","title":"Get Online Features","text":""},{"location":"side-projects/data2ml-ops/feast/deployment/#get-online-features_1","title":"<code>/get-online-features</code>","text":"<p>Get online features from the feature store:</p> <pre><code>curl -X POST http://127.0.0.1:52316/get-online-features \\\n     -H \"Content-Type: application/json\" \\\n     -d @request-get-online-features.json | jq\n</code></pre> <p>where <code>request-get-online-features.json</code> is </p> <pre><code>{\n  \"feature_service\": \"fraud_detection_v1\",\n  \"entities\": {\n    \"entity_id\": [\"v5zlw0\", \"000q95\"]\n  }\n}\n</code></pre> <p>Expected Output</p> <pre><code>{\n  \"metadata\": {\n    \"feature_names\": [\n      \"entity_id\",\n      \"has_fraud_7d\",\n      \"num_transactions_7d\",\n      \"account_age_days\",\n      \"credit_score\",\n      \"has_2fa_installed\"\n    ]\n  },\n  \"results\": [\n    {\n      \"values\": [\n        \"v5zlw0\",\n        \"000q95\"\n      ],\n      \"statuses\": [\n        \"PRESENT\",\n        \"PRESENT\"\n      ],\n      \"event_timestamps\": [\n        \"1970-01-01T00:00:00Z\",\n        \"1970-01-01T00:00:00Z\"\n      ]\n    },\n    {\n      \"values\": [\n        false,\n        false\n      ],\n      \"statuses\": [\n        \"PRESENT\",\n        \"PRESENT\"\n      ],\n      \"event_timestamps\": [\n        \"2025-05-26T22:00:37Z\",\n        \"2025-05-26T22:00:37Z\"\n      ]\n    },\n    {\n      \"values\": [\n        7,\n        6\n      ],\n      \"statuses\": [\n        \"PRESENT\",\n        \"PRESENT\"\n      ],\n      \"event_timestamps\": [\n        \"2025-05-26T00:00:00Z\",\n        \"2025-05-26T00:00:00Z\"\n      ]\n    },\n    {\n      \"values\": [\n        655,\n        236\n      ],\n      \"statuses\": [\n        \"PRESENT\",\n        \"PRESENT\"\n      ],\n      \"event_timestamps\": [\n        \"2025-05-19T22:00:27Z\",\n        \"2025-05-19T22:00:27Z\"\n      ]\n    },\n    {\n      \"values\": [\n        480,\n        737\n      ],\n      \"statuses\": [\n        \"PRESENT\",\n        \"PRESENT\"\n      ],\n      \"event_timestamps\": [\n        \"2025-05-19T22:00:27Z\",\n        \"2025-05-19T22:00:27Z\"\n      ]\n    },\n    {\n      \"values\": [\n        true,\n        true\n      ],\n      \"statuses\": [\n        \"PRESENT\",\n        \"PRESENT\"\n      ],\n      \"event_timestamps\": [\n        \"2025-05-19T22:00:27Z\",\n        \"2025-05-19T22:00:27Z\"\n      ]\n    }\n  ]\n}\n</code></pre> <p>https://docs.feast.dev/reference/feature-servers/python-feature-server</p>"},{"location":"side-projects/data2ml-ops/feast/how-it-works/","title":"How It Works?","text":"<p>__</p>"},{"location":"side-projects/data2ml-ops/feast/how-it-works/#core-concepts","title":"Core Concepts","text":""},{"location":"side-projects/data2ml-ops/feast/how-it-works/#architecture-components","title":"Architecture Components","text":""},{"location":"side-projects/data2ml-ops/feast/how-it-works/#key-modules-and-their-roles","title":"Key Modules and Their Roles","text":""},{"location":"side-projects/data2ml-ops/feast/how-it-works/#behind-the-scenes","title":"Behind the Scenes","text":""},{"location":"side-projects/data2ml-ops/feast/in-the-bigger-picture/","title":"In the Bigger Picture","text":""},{"location":"side-projects/data2ml-ops/feast/in-the-bigger-picture/#history-current-status","title":"History &amp; Current Status","text":"<p>Feast (Feature Store) is an open-source feature store for machine learning, originally developed by Gojek in collaboration with Google Cloud. The project was open-sourced in 2019, aiming to provide a centralized platform for managing and serving machine learning features in production environments. Feast has since evolved into a widely adopted solution, with contributions from a growing community of developers and organizations.</p> <p>As of May 2025, Feast has garnered over 6,000 stars on GitHub, reflecting its popularity and active development. The community includes more than 1,000 contributors and over 5,500 members on Slack . Feast hosts biweekly community calls and annual events, such as the Feast Summit, to foster collaboration and share advancements. Notable adopters of Feast include companies like Robinhood, NVIDIA, Discord, Cloudflare, Walmart, Shopify, Salesforce, Twitter, IBM, Capital One, Red Hat, Expedia, HelloFresh, Adyen, and SeatGeek.</p>"},{"location":"side-projects/data2ml-ops/feast/in-the-bigger-picture/#alternatives","title":"Alternatives","text":""},{"location":"side-projects/data2ml-ops/feast/in-the-bigger-picture/#integration-points","title":"Integration Points","text":""},{"location":"side-projects/data2ml-ops/feast/in-the-bigger-picture/#kubernetes","title":"Kubernetes","text":""},{"location":"side-projects/data2ml-ops/feast/in-the-bigger-picture/#cicd","title":"CI/CD","text":""},{"location":"side-projects/data2ml-ops/feast/modeling-features/","title":"Modeling Features","text":"<ul> <li>three data sources</li> <li>one entity</li> <li>three feature views</li> <li>one feature service</li> </ul>"},{"location":"side-projects/data2ml-ops/feast/modeling-features/#prerequisites","title":"Prerequisites","text":"<p>Virtual environment prepared</p> requirements.txt<pre><code>feast[gcp,redis]==0.49.0\npsycopg[binary]==3.2.7\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/modeling-features/#packages","title":"Packages","text":"account_features.py<pre><code>from datetime import timedelta\n\nfrom feast import (BigQuerySource, Entity, FeatureService, FeatureView,\n                   ValueType, Field)\nfrom feast.types import String, Int64, Bool\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/modeling-features/#data-sources","title":"Data Sources","text":"account_features.py<pre><code># Data Sources\n# https://rtd.feast.dev/en/latest/index.html#feast.infra.offline_stores.bigquery_source.BigQuerySource\nds_acct_fraud_7d = BigQuerySource(\n    table=f\"mlops-437709.dbt_kclai.feat_acct_fraud_7d\",\n    timestamp_field=\"feature_timestamp\"\n)\n\nds_acct_num_txns_7d = BigQuerySource(\n    table=f\"mlops-437709.dbt_kclai.feat_acct_num_txns_7d\",\n    timestamp_field=\"feature_timestamp\"\n)\n\nds_acct_profiles = BigQuerySource(\n    table=f\"mlops-437709.dbt_kclai.feat_acct_profiles\",\n    timestamp_field=\"feature_timestamp\"\n)\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/modeling-features/#entity","title":"Entity","text":"account_features.py<pre><code># Entity\naccount_entity = Entity(\n    name=\"Account\",\n    description=\"A user that has executed a transaction or received a transaction\",\n    value_type=ValueType.STRING,\n    join_keys=[\"entity_id\"]\n)\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/modeling-features/#feature-views","title":"Feature Views","text":"account_features.py<pre><code># Feature Views\nfv_acct_fraud_7d = FeatureView(\n    name=\"acct_fraud_7d\",\n    entities=[account_entity],\n    schema=[\n        Field(name=\"has_fraud_7d\", dtype=Bool)\n    ],\n    ttl=timedelta(weeks=52),\n    source=ds_acct_fraud_7d\n)\n\n\nfv_acct_num_txns_7d = FeatureView(\n    name=\"acct_num_txns_7d\",\n    entities=[account_entity],\n    schema=[\n        Field(name=\"num_transactions_7d\", dtype=Int64)\n    ],\n    ttl=timedelta(weeks=1),\n    source=ds_acct_num_txns_7d\n)\n\nfv_acct_profiles = FeatureView(\n    name=\"acct_profiles\",\n    entities=[account_entity],\n    schema=[\n        Field(name=\"credit_score\", dtype=Int64),\n        Field(name=\"account_age_days\", dtype=Int64),\n        Field(name=\"has_2fa_installed\", dtype=Bool)\n    ],\n    ttl=timedelta(weeks=52),\n    source=ds_acct_profiles\n)\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/modeling-features/#feature-services","title":"Feature Services","text":"account_features.py<pre><code># Feature Services\n# Versioning features that power ML models:\n# https://docs.feast.dev/master/how-to-guides/running-feast-in-production#id-3.2-versioning-features-that-power-ml-models\nfs_fraud_detection_v1 = FeatureService(\n    name=\"fraud_detection_v1\",\n    features=[\n        fv_acct_fraud_7d,\n        fv_acct_num_txns_7d[[\"num_transactions_7d\"]],\n        fv_acct_profiles\n    ]\n)\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/vertex-ai-feature-store-implementation/","title":"Vertex AI Feature Store Implementation (Hands-on)","text":"main.yaml<pre><code># Feature Online Store\nresource \"google_vertex_ai_feature_online_store\" \"demo_store\" {\n  name   = \"demo\"\n  region = \"us-central1\"\n\n  bigtable {\n    auto_scaling {\n      min_node_count         = 1\n      max_node_count         = 1\n      cpu_utilization_target = 70\n    }\n  }\n}\n</code></pre> main.yaml<pre><code># Template for feature group and features\nlocals {\n  feature_groups = {\n    credit_request = [\n      \"credit_amount\", \"credit_duration\", \"installment_commitment\", \"credit_score\"\n    ]\n    customer_financial_profile = [\n      \"checking_balance\", \"savings_balance\", \"existing_credits\", \"job_history\"\n    ]\n    credit_context = [\n      \"purpose\", \"other_parties\", \"credit_standing\", \"assets\", \"other_payment_plans\"\n    ]\n    customer_demographics = [\n      \"age\", \"num_dependents\", \"residence_since\", \"sex\"\n    ]\n  }\n}\n</code></pre> main.yaml<pre><code># Feature Groups\nresource \"google_vertex_ai_feature_group\" \"feature_groups\" {\n  for_each    = local.feature_groups\n  name        = each.key\n  region      = \"us-central1\"\n  description = \"Feature group for ${each.key}\"\n\n  big_query {\n    big_query_source {\n      input_uri = \"bq://mlops-437709.featurestore_demo.credit_files_with_timestamp\"\n    }\n    entity_id_columns = [\"credit_request_id\"]\n  }\n}\n</code></pre> main.yaml<pre><code># Feature Group - Features\nresource \"google_vertex_ai_feature_group_feature\" \"credit_request_features\" {\n  for_each = toset(local.feature_groups.credit_request)\n\n  name          = each.value\n  region        = \"us-central1\"\n  feature_group = google_vertex_ai_feature_group.feature_groups[\"credit_request\"].name\n  description   = \"Feature for ${each.value}\"\n}\n\nresource \"google_vertex_ai_feature_group_feature\" \"customer_financial_profile_features\" {\n  for_each = toset(local.feature_groups.customer_financial_profile)\n\n  name          = each.value\n  region        = \"us-central1\"\n  feature_group = google_vertex_ai_feature_group.feature_groups[\"customer_financial_profile\"].name\n  description   = \"Feature for ${each.value}\"\n}\n\nresource \"google_vertex_ai_feature_group_feature\" \"credit_context_features\" {\n  for_each = toset(local.feature_groups.credit_context)\n\n  name          = each.value\n  region        = \"us-central1\"\n  feature_group = google_vertex_ai_feature_group.feature_groups[\"credit_context\"].name\n  description   = \"Feature for ${each.value}\"\n}\n\nresource \"google_vertex_ai_feature_group_feature\" \"customer_demographics_features\" {\n  for_each = toset(local.feature_groups.customer_demographics)\n\n  name          = each.value\n  region        = \"us-central1\"\n  feature_group = google_vertex_ai_feature_group.feature_groups[\"customer_demographics\"].name\n  description   = \"Feature for ${each.value}\"\n}\n</code></pre> main.yaml<pre><code># Feature Online Store FeatureView\nresource \"google_vertex_ai_feature_online_store_featureview\" \"main\" {\n  name                 = \"main\"\n  region               = \"us-central1\"\n  feature_online_store = google_vertex_ai_feature_online_store.demo_store.name\n  sync_config {\n    cron = \"*/10 * * * *\"\n  }\n  feature_registry_source {\n\n    feature_groups { \n        feature_group_id = google_vertex_ai_feature_group.feature_groups[\"credit_request\"].name\n        feature_ids      = [google_vertex_ai_feature_group_feature.credit_request_features[\"credit_amount\"].name]\n       }\n  }\n}\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/vertex-ai-feature-store-introduction/","title":"Vertex AI Feature Store Introduction","text":""},{"location":"side-projects/data2ml-ops/feast/what-why-when/","title":"What, Why, When","text":""},{"location":"side-projects/data2ml-ops/feast/what-why-when/#what-is-x","title":"What is X?","text":""},{"location":"side-projects/data2ml-ops/feast/what-why-when/#why-x","title":"Why X?","text":"<ul> <li>Data Engineer</li> <li>Data Analyst</li> <li>Data Scientist</li> <li>Machine Learning Engineer</li> </ul>"},{"location":"side-projects/data2ml-ops/feast/what-why-when/#when-to-use-x","title":"When to Use X?","text":""},{"location":"side-projects/data2ml-ops/kserve/deployment-with-transformer/","title":"Deploy Your Model on Kubernetes with Feast Transformer","text":""},{"location":"side-projects/data2ml-ops/kserve/deployment/","title":"Deploy Your Model on Kubernetes","text":""},{"location":"side-projects/data2ml-ops/kserve/deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li>MinIO must be running and contain a bucket named <code>mlflow</code>.  <ul> <li>If you haven\u2019t set it up, see MinIO Deployment for instructions.</li> </ul> </li> <li>A Ray Tune job must have completed and logged the best model to MLflow, which was stored in MinIO. If you haven\u2019t done this yet, refer to the following guides:<ul> <li>MLflow Deployment: Set up the MLflow tracking server and configure MinIO as the artifact store.</li> <li>Ray Deployment: Deploy a Ray cluster on Kubernetes.</li> <li>Ray Tune Integration Guide: Learn how to integrate Ray Tune with MLflow, Optuna imbalanced-learn, XGBoost and MinIO.</li> <li>Ray Tune Job Submission: Run the tuning job and log the best model to MLflow.</li> </ul> </li> <li><code>gRPCurl</code> installed     <pre><code>brew install grpcurl\n</code></pre></li> </ul> <p>This guide walks you through how to deploy the model you previously trained with Ray and logged to MinIO via MLflow. You'll learn how to serve it using KServe with both REST and gRPC endpoints, and enable autoscaling\u2014including scale-to-zero support.</p>"},{"location":"side-projects/data2ml-ops/kserve/deployment/#grant-kserve-permission-to-load-models-from-minio","title":"Grant KServe Permission to Load Models from MinIO <sup>1</sup>","text":"<p>To allow KServe to pull models from MinIO (or any S3-compatible storage), you'll need to provide access credentials via a Kubernetes <code>Secret</code> and bind it to a <code>ServiceAccount</code>. Then, reference that service account in your <code>InferenceService</code>.</p> <p>Start by creating a <code>Secret</code> that holds your S3 credentials. This secret should include your access key and secret key, and be annotated with the MinIO endpoint and settings to disable HTTPS if you're working in a local or test environment.</p> secret.yaml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: s3creds\n  annotations:\n     serving.kserve.io/s3-endpoint: minio-api.minio.svc.cluster.local:9000\n     serving.kserve.io/s3-usehttps: \"0\" # by default 1, if testing with minio you can set to 0\ntype: Opaque\nstringData: # This is for raw credential string. For base64 encoded string, use `data` instead\n  AWS_ACCESS_KEY_ID: minio_user\n  AWS_SECRET_ACCESS_KEY: minio_password\n</code></pre> <p>These values should match what you specified when deploying MinIO on Kubernetes. For more details, refer to the configuration section below or revisit this article.</p> Info minio.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio\n  namespace: minio\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n        - name: minio\n          image: minio/minio\n          args:\n            - server\n            - /data\n            - --console-address\n            - :9001\n          env:\n            - name: MINIO_ROOT_USER\n              value: minio_user\n            - name: MINIO_ROOT_PASSWORD\n              value: minio_password\n          ports:\n            - containerPort: 9000\n              protocol: TCP\n            - containerPort: 9001\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /minio/health/live\n              port: 9000\n            initialDelaySeconds: 30\n            periodSeconds: 20\n            timeoutSeconds: 15\n            failureThreshold: 6\n          readinessProbe:\n            httpGet:\n              path: /minio/health/ready\n              port: 9000\n            initialDelaySeconds: 15\n            periodSeconds: 10\n            timeoutSeconds: 10\n            failureThreshold: 3\n          volumeMounts:\n            - name: storage\n              mountPath: /data\n      volumes:\n        - name: storage\n          hostPath:\n            path: /home/docker/data/minio\n            type: DirectoryOrCreate\n      restartPolicy: Always\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: minio-console\n  namespace: minio\nspec:\n  selector:\n    app: minio\n  type: NodePort\n  ports:\n    - name: console\n      port: 9001\n      targetPort: 9001\n      nodePort: 30901\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: minio-api\n  namespace: minio\nspec:\n  selector:\n    app: minio\n  type: ClusterIP\n  ports:\n    - name: api\n      port: 9000\n      targetPort: 9000\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: minio-create-bucket\n  namespace: minio\nspec:\n  backoffLimit: 6\n  completions: 1\n  template:\n    metadata:\n      labels:\n        job: minio-create-bucket\n    spec:\n      initContainers:\n        - name: wait-for-minio\n          image: busybox\n          command:\n            - sh\n            - -c\n            - |\n              until nc -z minio-api.minio.svc.cluster.local 9000; do\n                echo \"Waiting for MinIO...\"\n                sleep 2\n              done\n              echo \"MinIO is ready!\"\n      containers:\n        - name: minio-create-buckets\n          image: minio/mc\n          command:\n            - sh\n            - -c\n            - |\n              mc alias set minio http://minio-api.minio.svc.cluster.local:9000 minio_user minio_password &amp;&amp;\n              for bucket in mlflow dbt sqlmesh ray; do\n                if ! mc ls minio/$bucket &gt;/dev/null 2&gt;&amp;1; then\n                  echo \"Creating bucket: $bucket\"\n                  mc mb minio/$bucket\n                  echo \"Bucket created: $bucket\"\n                else\n                  echo \"Bucket already exists: $bucket\"\n                fi\n              done\n      restartPolicy: OnFailure\n      terminationGracePeriodSeconds: 30\n</code></pre> <p>Next, create a <code>ServiceAccount</code> that references the secret. This will allow KServe to inject the credentials when pulling models.</p> sa.yaml<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: sa\nsecrets:\n- name: s3creds\n</code></pre> <p>Finally, define an <code>InferenceService</code> that uses the <code>ServiceAccount</code> and points to the model artifact stored in MinIO. In this example, we are deploying a model saved in MLflow format using the v2 inference protocol.</p> RESTgRPC inference-service-rest-v2.yaml<pre><code>apiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"fraud-detection-rest\"\nspec:\n  predictor:\n    minReplicas: 0\n    scaleTarget: 1\n    scaleMetric: qps\n    model:\n      modelFormat:\n        name: mlflow\n      protocolVersion: v2\n      storageUri: s3://miflow/2/5ccd7dcabc1f49c1bc45f1f94d945dd6/artifacts/model\n    serviceAccountName: sa\n</code></pre> inference-service-grpc-v2.yaml<pre><code>apiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"fraud-detection-grpc\"\nspec:\n  predictor:\n    minReplicas: 0\n    scaleTarget: 1\n    scaleMetric: qps\n    model:\n      modelFormat:\n        name: mlflow\n      protocolVersion: v2\n      storageUri: s3://miflow/2/5ccd7dcabc1f49c1bc45f1f94d945dd6/artifacts/model\n      ports:\n        - containerPort: 9000\n          name: h2c\n          protocol: TCP\n    serviceAccountName: sa\n</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/deployment/#deploy-the-fraud-detection-mlflow-model-with-inferenceservice","title":"Deploy the Fraud Detection MLflow Model with InferenceService<sup>2</sup><sup>3</sup>","text":"<p>This example shows how to deploy your trained MLflow model on KServe using both the REST and gRPC protocol. The <code>InferenceService</code> configuration specifies the model format (<code>mlflow</code>), the s2 inference protocol, and the S3 URI where the model is stored. The <code>serviceAccountName</code> field allows KServe to access the model stored in MinIO using the credentials provided earlier.</p> RESTgRPC inference-service-rest-v2.yaml<pre><code>apiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"fraud-detection-rest\"\nspec:\n  predictor:\n    minReplicas: 0\n    scaleTarget: 1\n    scaleMetric: qps\n    model:\n      modelFormat:\n        name: mlflow\n      protocolVersion: v2\n      storageUri: s3://miflow/2/5ccd7dcabc1f49c1bc45f1f94d945dd6/artifacts/model\n    serviceAccountName: sa\n</code></pre> <p>Apply the configuration using <code>kubectl</code>:</p> <pre><code>kubectl apply -f inference-service-rest-v2.yaml\n</code></pre> <p>Once deployed, KServe will expose a REST endpoint where you can send inference requests. You can verify the service status using:</p> <pre><code>kubectl get inferenceservice fraud-detection-http\n</code></pre> inference-service-grpc-v2.yaml<pre><code>apiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"fraud-detection-grpc\"\nspec:\n  predictor:\n    minReplicas: 0\n    scaleTarget: 1\n    scaleMetric: qps\n    model:\n      modelFormat:\n        name: mlflow\n      protocolVersion: v2\n      storageUri: s3://miflow/2/5ccd7dcabc1f49c1bc45f1f94d945dd6/artifacts/model\n      ports:\n        - containerPort: 9000\n          name: h2c\n          protocol: TCP\n    serviceAccountName: sa\n</code></pre> <p>Apply the configuration using <code>kubectl</code>:</p> <pre><code>kubectl apply -f inference-service-grpc-v2.yaml\n</code></pre> <p>Once deployed, KServe will expose a gRPC endpoint where you can send inference requests. You can verify the service status using: </p> <pre><code>kubectl get inferenceservice fraud-detection-grpc\n</code></pre> <p>Make sure the <code>storageUri</code> matches the path where your model is saved.</p> <p>During deployment, you may encounter a few common issues:</p> <ol> <li>The model fails to load inside the pod during the storage initialization phase<sup>4</sup>. This is usually a permission issue\u2014make sure your access credentials are correctly configured as shown in the section above.</li> <li>Sometimes the model loads successfully into the model server, but inference requests still fail. This could be due to:<ul> <li>A mismatch between the model version and the model server runtime. In this case, try explicitly setting the <code>runtimeVersion</code><sup>5</sup>.</li> <li>Incorrect port settings, which prevent the server from responding properly.</li> <li>Architecture mismatch\u2014for example, if you trained the model on a Mac (ARM64) but are using an x86-based KServe runtime image.</li> <li>Deployment in a control plane namespace. Namespaces labeled with <code>control-plane</code> are skipped by KServe\u2019s webhook to avoid privilege escalation. This prevents the storage initializer from being injected into the pod, leading to errors like: <code>No such file or directory: '/mnt/models'</code>.</li> </ul> </li> </ol>"},{"location":"side-projects/data2ml-ops/kserve/deployment/#test-the-endpoints","title":"Test the Endpoints","text":"<p>Determine the ingress IP and port:</p> <pre><code>export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\nexport INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\"http2\")].port}')\n</code></pre> <p>This step retrieves the external IP address of the Istio Ingress Gateway and stores it in <code>INGRESS_HOST</code>, and extracts the port named <code>http2</code> to set as <code>INGRESS_PORT</code>, allowing you to construct the full service endpoint for sending inference requests.</p> RESTgRPC <p>Set the required environment variables for the HTTP inference request:</p> <pre><code>export INPUT_PATH=input-example-rest-v2.json\nexport SERVICE_HOSTNAME=$(kubectl get inferenceservice fraud-detection-rest -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n</code></pre> input-example-rest-v2.json input-example-rest-v2.json<pre><code>{\n  \"parameters\": {\n    \"content_type\": \"pd\"\n  },\n  \"inputs\": [\n    {\n      \"name\": \"has_fraud_7d\",\n      \"shape\": [2],\n      \"datatype\": \"BOOL\",\n      \"data\": [false, false]\n    },\n    {\n      \"name\": \"num_transactions_7d\",\n      \"shape\": [2],\n      \"datatype\": \"INT64\",\n      \"data\": [7, 6]\n    },\n    {\n      \"name\": \"account_age_days\",\n      \"shape\": [2],\n      \"datatype\": \"INT64\",\n      \"data\": [655, 236]\n    },\n    {\n      \"name\": \"credit_score\",\n      \"shape\": [2],\n      \"datatype\": \"INT64\",\n      \"data\": [680, 737]\n    },\n    {\n      \"name\": \"has_2fa_installed\",\n      \"shape\": [2],\n      \"datatype\": \"BOOL\",\n      \"data\": [true, true]\n    }\n  ]\n}\n</code></pre> test-commands.txt<pre><code># -v                       Enable verbose output for debugging\n# -H \"Host:...\"            Set the Host header to route through the ingress gateway\n# -H \"Content-Type:...\"    Specify the request content type as JSON\n# -d @...                  Provide the input payload from the specified JSON file\n# http://${INGRESS_HOST}... Target the model's inference endpoint\ncurl -v \\\n  -H \"Host: ${SERVICE_HOSTNAME}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d @${INPUT_PATH} \\\n  http://${INGRESS_HOST}:${INGRESS_PORT}/v2/models/fraud-detection-http/infer\n</code></pre> <p>Expected Output</p> <pre><code>*   Trying 127.0.0.1:80...\n* Connected to 127.0.0.1 (127.0.0.1) port 80\n&gt; POST /v2/models/mlflow-apple-demand/infer HTTP/1.1\n&gt; Host: mlflow-apple-demand.default.127.0.0.1.sslip.io\n&gt; User-Agent: curl/8.7.1\n&gt; Accept: */*\n&gt; Content-Type: application/json\n&gt; Content-Length: 1089\n&gt; \n* upload completely sent off: 1089 bytes\n&lt; HTTP/1.1 200 OK\n&lt; ce-endpoint: mlflow-apple-demand\n&lt; ce-id: 9ddc841e-a8d4-405f-a7e4-73f7aa9bab09\n&lt; ce-inferenceservicename: mlserver\n&lt; ce-modelid: mlflow-apple-demand\n&lt; ce-namespace: default\n&lt; ce-requestid: 9ddc841e-a8d4-405f-a7e4-73f7aa9bab09\n&lt; ce-source: io.seldon.serving.deployment.mlserver.default\n&lt; ce-specversion: 0.3\n&lt; ce-type: io.seldon.serving.inference.response\n&lt; content-length: 240\n&lt; content-type: application/json\n&lt; date: Fri, 02 May 2025 04:06:58 GMT\n&lt; server: istio-envoy\n&lt; x-envoy-upstream-service-time: 247\n&lt; \n* Connection #0 to host 127.0.0.1 left intact\n{\"model_name\":\"mlflow-apple-demand\",\"id\":\"9ddc841e-a8d4-405f-a7e4-73f7aa9bab09\",\"parameters\":{\"content_type\":\"np\"},\"outputs\":[{\"name\":\"output-1\",\"shape\":[1,1],\"datatype\":\"FP32\",\"parameters\":{\"content_type\":\"np\"},\"data\":[1486.56298828125]}]}\n</code></pre> <p>Download the <code>open_inference_grpc.proto</code> file:</p> <pre><code>curl -O https://raw.githubusercontent.com/kserve/open-inference-protocol/main/specification/protocol/open_inference_grpc.proto\n</code></pre> open_inference_grpc.proto open_inference_grpc.proto<pre><code>// Copyright 2023 The KServe Authors.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//    http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\nsyntax = \"proto3\";\npackage inference;\n\n// Inference Server GRPC endpoints.\nservice GRPCInferenceService\n{\n  // The ServerLive API indicates if the inference server is able to receive \n  // and respond to metadata and inference requests.\n  rpc ServerLive(ServerLiveRequest) returns (ServerLiveResponse) {}\n\n  // The ServerReady API indicates if the server is ready for inferencing.\n  rpc ServerReady(ServerReadyRequest) returns (ServerReadyResponse) {}\n\n  // The ModelReady API indicates if a specific model is ready for inferencing.\n  rpc ModelReady(ModelReadyRequest) returns (ModelReadyResponse) {}\n\n  // The ServerMetadata API provides information about the server. Errors are \n  // indicated by the google.rpc.Status returned for the request. The OK code \n  // indicates success and other codes indicate failure.\n  rpc ServerMetadata(ServerMetadataRequest) returns (ServerMetadataResponse) {}\n\n  // The per-model metadata API provides information about a model. Errors are \n  // indicated by the google.rpc.Status returned for the request. The OK code \n  // indicates success and other codes indicate failure.\n  rpc ModelMetadata(ModelMetadataRequest) returns (ModelMetadataResponse) {}\n\n  // The ModelInfer API performs inference using the specified model. Errors are\n  // indicated by the google.rpc.Status returned for the request. The OK code \n  // indicates success and other codes indicate failure.\n  rpc ModelInfer(ModelInferRequest) returns (ModelInferResponse) {}\n}\n\nmessage ServerLiveRequest {}\n\nmessage ServerLiveResponse\n{\n  // True if the inference server is live, false if not live.\n  bool live = 1;\n}\n\nmessage ServerReadyRequest {}\n\nmessage ServerReadyResponse\n{\n  // True if the inference server is ready, false if not ready.\n  bool ready = 1;\n}\n\nmessage ModelReadyRequest\n{\n  // The name of the model to check for readiness.\n  string name = 1;\n\n  // The version of the model to check for readiness. If not given the\n  // server will choose a version based on the model and internal policy.\n  string version = 2;\n}\n\nmessage ModelReadyResponse\n{\n  // True if the model is ready, false if not ready.\n  bool ready = 1;\n}\n\nmessage ServerMetadataRequest {}\n\nmessage ServerMetadataResponse\n{\n  // The server name.\n  string name = 1;\n\n  // The server version.\n  string version = 2;\n\n  // The extensions supported by the server.\n  repeated string extensions = 3;\n}\n\nmessage ModelMetadataRequest\n{\n  // The name of the model.\n  string name = 1;\n\n  // The version of the model to check for readiness. If not given the\n  // server will choose a version based on the model and internal policy.\n  string version = 2;\n}\n\nmessage ModelMetadataResponse\n{\n  // Metadata for a tensor.\n  message TensorMetadata\n  {\n    // The tensor name.\n    string name = 1;\n\n    // The tensor data type.\n    string datatype = 2;\n\n    // The tensor shape. A variable-size dimension is represented\n    // by a -1 value.\n    repeated int64 shape = 3;\n  }\n\n  // The model name.\n  string name = 1;\n\n  // The versions of the model available on the server.\n  repeated string versions = 2;\n\n  // The model's platform. See Platforms.\n  string platform = 3;\n\n  // The model's inputs.\n  repeated TensorMetadata inputs = 4;\n\n  // The model's outputs.\n  repeated TensorMetadata outputs = 5;\n\n  // Optional Model Properties\n  map&lt;string, string&gt; properties = 6;\n}\n\nmessage ModelInferRequest\n{\n  // An input tensor for an inference request.\n  message InferInputTensor\n  {\n    // The tensor name.\n    string name = 1;\n\n    // The tensor data type.\n    string datatype = 2;\n\n    // The tensor shape.\n    repeated int64 shape = 3;\n\n    // Optional inference input tensor parameters.\n    map&lt;string, InferParameter&gt; parameters = 4;\n\n    // The tensor contents using a data-type format. This field must\n    // not be specified if \"raw\" tensor contents are being used for\n    // the inference request.\n    InferTensorContents contents = 5;\n  }\n\n  // An output tensor requested for an inference request.\n  message InferRequestedOutputTensor\n  {\n    // The tensor name.\n    string name = 1;\n\n    // Optional requested output tensor parameters.\n    map&lt;string, InferParameter&gt; parameters = 2;\n  }\n\n  // The name of the model to use for inferencing.\n  string model_name = 1;\n\n  // The version of the model to use for inference. If not given the\n  // server will choose a version based on the model and internal policy.\n  string model_version = 2;\n\n  // Optional identifier for the request. If specified will be\n  // returned in the response.\n  string id = 3;\n\n  // Optional inference parameters.\n  map&lt;string, InferParameter&gt; parameters = 4;\n\n  // The input tensors for the inference.\n  repeated InferInputTensor inputs = 5;\n\n  // The requested output tensors for the inference. Optional, if not\n  // specified all outputs produced by the model will be returned.\n  repeated InferRequestedOutputTensor outputs = 6;\n\n  // The data contained in an input tensor can be represented in \"raw\"\n  // bytes form or in the repeated type that matches the tensor's data\n  // type. To use the raw representation 'raw_input_contents' must be\n  // initialized with data for each tensor in the same order as\n  // 'inputs'. For each tensor, the size of this content must match\n  // what is expected by the tensor's shape and data type. The raw\n  // data must be the flattened, one-dimensional, row-major order of\n  // the tensor elements without any stride or padding between the\n  // elements. Note that the FP16 and BF16 data types must be represented as\n  // raw content as there is no specific data type for a 16-bit float type.\n  //\n  // If this field is specified then InferInputTensor::contents must\n  // not be specified for any input tensor.\n  repeated bytes raw_input_contents = 7;\n}\n\nmessage ModelInferResponse\n{\n  // An output tensor returned for an inference request.\n  message InferOutputTensor\n  {\n    // The tensor name.\n    string name = 1;\n\n    // The tensor data type.\n    string datatype = 2;\n\n    // The tensor shape.\n    repeated int64 shape = 3;\n\n    // Optional output tensor parameters.\n    map&lt;string, InferParameter&gt; parameters = 4;\n\n    // The tensor contents using a data-type format. This field must\n    // not be specified if \"raw\" tensor contents are being used for\n    // the inference response.\n    InferTensorContents contents = 5;\n  }\n\n  // The name of the model used for inference.\n  string model_name = 1;\n\n  // The version of the model used for inference.\n  string model_version = 2;\n\n  // The id of the inference request if one was specified.\n  string id = 3;\n\n  // Optional inference response parameters.\n  map&lt;string, InferParameter&gt; parameters = 4;\n\n  // The output tensors holding inference results.\n  repeated InferOutputTensor outputs = 5;\n\n  // The data contained in an output tensor can be represented in\n  // \"raw\" bytes form or in the repeated type that matches the\n  // tensor's data type. To use the raw representation 'raw_output_contents'\n  // must be initialized with data for each tensor in the same order as\n  // 'outputs'. For each tensor, the size of this content must match\n  // what is expected by the tensor's shape and data type. The raw\n  // data must be the flattened, one-dimensional, row-major order of\n  // the tensor elements without any stride or padding between the\n  // elements. Note that the FP16 and BF16 data types must be represented as\n  // raw content as there is no specific data type for a 16-bit float type.\n  //\n  // If this field is specified then InferOutputTensor::contents must\n  // not be specified for any output tensor.\n  repeated bytes raw_output_contents = 6;\n}\n\n// An inference parameter value. The Parameters message describes a \n// \u201cname\u201d/\u201dvalue\u201d pair, where the \u201cname\u201d is the name of the parameter\n// and the \u201cvalue\u201d is a boolean, integer, or string corresponding to \n// the parameter.\nmessage InferParameter\n{\n  // The parameter value can be a string, an int64, a boolean\n  // or a message specific to a predefined parameter.\n  oneof parameter_choice\n  {\n    // A boolean parameter value.\n    bool bool_param = 1;\n\n    // An int64 parameter value.\n    int64 int64_param = 2;\n\n    // A string parameter value.\n    string string_param = 3;\n\n    // A double parameter value.\n    double double_param = 4;\n\n    // A uint64 parameter value.\n    uint64 uint64_param = 5;\n  }\n}\n\n// The data contained in a tensor represented by the repeated type\n// that matches the tensor's data type. Protobuf oneof is not used\n// because oneofs cannot contain repeated fields.\nmessage InferTensorContents\n{\n  // Representation for BOOL data type. The size must match what is\n  // expected by the tensor's shape. The contents must be the flattened,\n  // one-dimensional, row-major order of the tensor elements.\n  repeated bool bool_contents = 1;\n\n  // Representation for INT8, INT16, and INT32 data types. The size\n  // must match what is expected by the tensor's shape. The contents\n  // must be the flattened, one-dimensional, row-major order of the\n  // tensor elements.\n  repeated int32 int_contents = 2;\n\n  // Representation for INT64 data types. The size must match what\n  // is expected by the tensor's shape. The contents must be the\n  // flattened, one-dimensional, row-major order of the tensor elements.\n  repeated int64 int64_contents = 3;\n\n  // Representation for UINT8, UINT16, and UINT32 data types. The size\n  // must match what is expected by the tensor's shape. The contents\n  // must be the flattened, one-dimensional, row-major order of the\n  // tensor elements.\n  repeated uint32 uint_contents = 4;\n\n  // Representation for UINT64 data types. The size must match what\n  // is expected by the tensor's shape. The contents must be the\n  // flattened, one-dimensional, row-major order of the tensor elements.\n  repeated uint64 uint64_contents = 5;\n\n  // Representation for FP32 data type. The size must match what is\n  // expected by the tensor's shape. The contents must be the flattened,\n  // one-dimensional, row-major order of the tensor elements.\n  repeated float fp32_contents = 6;\n\n  // Representation for FP64 data type. The size must match what is\n  // expected by the tensor's shape. The contents must be the flattened,\n  // one-dimensional, row-major order of the tensor elements.\n  repeated double fp64_contents = 7;\n\n  // Representation for BYTES data type. The size must match what is\n  // expected by the tensor's shape. The contents must be the flattened,\n  // one-dimensional, row-major order of the tensor elements.\n  repeated bytes bytes_contents = 8;\n}\n</code></pre> <p>Downloading this <code>.proto</code> file gives you the standard gRPC interface definition for the Open Inference Protocol. It allows your client or server to communicate with ML model servers using a unified API.</p> <p>Set the required environment variables for the gRPC inference request:</p> <pre><code>export PROTO_FILE=open_inference_grpc.proto\nexport INPUT_PATH=input-example-grpc-v2.json\nexport SERVICE_HOSTNAME=$(kubectl get inferenceservice fraud-detection-grpc -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n</code></pre> <p>These variables specify the protobuf schema for gRPC, the input payload to send, and the target hostname for routing the request through the ingress gateway.</p> input-example-grpc-v2.json input-example-grpc-v2.json<pre><code>{\n  \"model_name\": \"fraud-detection-grpc\",\n  \"inputs\": [\n    {\n      \"name\": \"has_fraud_7d\",\n      \"shape\": [2],\n      \"datatype\": \"BOOL\",\n      \"contents\": {\n        \"bool_contents\": [false, false]\n      }\n    },\n    {\n      \"name\": \"num_transactions_7d\",\n      \"shape\": [2],\n      \"datatype\": \"INT64\",\n      \"contents\": {\n        \"int64_contents\": [7, 6]\n      }\n    },\n    {\n      \"name\": \"account_age_days\",\n      \"shape\": [2],\n      \"datatype\": \"INT64\",\n      \"contents\": {\n        \"int64_contents\": [655, 236]\n      }\n    },\n    {\n      \"name\": \"credit_score\",\n      \"shape\": [2],\n      \"datatype\": \"INT64\",\n      \"contents\": {\n        \"int64_contents\": [680, 737]\n      }\n    },\n    {\n      \"name\": \"has_2fa_installed\",\n      \"shape\": [2],\n      \"datatype\": \"BOOL\",\n      \"contents\": {\n        \"bool_contents\": [true, true]\n      }\n    }\n  ]\n}\n</code></pre> test-commands.txt<pre><code># -vv                Verbose output for debugging\n# -plaintext         Use plaintext (non-TLS) connection\n# -proto             Path to the .proto file describing the gRPC service\n# -authority         Sets the HTTP/2 authority header (useful for ingress routing)\n# -d                 Read the request body from stdin\n# ${INGRESS_HOST}... Target host and port of the gRPC server\n# inference.GRPC...  Fully-qualified gRPC method to call\n# &lt;&lt;&lt;...             Provide JSON request body from file as stdin\ngrpcurl -vv \\\n  -plaintext \\\n  -proto ${PROTO_FILE} \\\n  -authority ${SERVICE_HOSTNAME} \\\n  -d @ \\\n  ${INGRESS_HOST}:${INGRESS_PORT} \\\n  inference.GRPCInferenceService.ModelInfer \\\n  &lt;&lt;&lt; $(cat \"$INPUT_PATH\")\n</code></pre> <p>Expected Output</p> <pre><code>TODO: Add expected output for gRPC inference request\n</code></pre> <ol> <li> <p>Deploy InferenceService with a saved model on S3 | KServe \u21a9</p> </li> <li> <p>Deploy MLflow models with InferenceService | KServe \u21a9</p> </li> <li> <p>Develop ML model with MLflow and deploy to Kubernetes | MLflow \u21a9</p> </li> <li> <p>Storage Initializer fails to download model | KServe Debugging Guide \u21a9</p> </li> <li> <p>Explicitly Specify a Runtime Version \u21a9</p> </li> </ol>"},{"location":"side-projects/data2ml-ops/kserve/feast-transformer/","title":"Write a Custom KServe Transformer with Feast Integration","text":"<p>This guide demonstrates how to integrate Feast with KServe, enabling feature retrieval and transformation during inference.</p>"},{"location":"side-projects/data2ml-ops/kserve/feast-transformer/#extend-the-kserve-model-class","title":"Extend the KServe Model Class","text":"<p><code>KServe.Model</code> base class mainly defines three handlers <code>preprocess</code>, <code>predict</code> and <code>postprocess</code>, these handlers are executed in sequence where the output of the <code>preprocess</code> handler is passed to the <code>predict</code> handler as the input.</p> <p>To create a custom transformer that retrieves features from Feast, you will extend the <code>KServe.Model</code> class and implement the <code>preprocess</code> method. This method will dynamically fetch features based on the input data and append them to the inference request.</p> feast_transformer.py<pre><code>class FeastTransformer(kserve.Model):\n    \"\"\"A class object for the data handling activities of driver ranking\n    Task and returns a KServe compatible response.\n\n    Args:\n        kserve (class object): The Model class from the KServe\n        module is passed here.\n    \"\"\"\n\n    def __init__(\n        self,\n        feast_url: str,\n        feast_entity_id: str,\n        feature_service: str,\n        model_name: str,\n        predictor_host: str,\n        predictor_protocol: str,\n    ):\n        \"\"\"Initialize the model name, predictor host, Feast serving URL,\n           entity IDs, and feature references\n\n        Args:\n            feast_url (str): URL for the Feast online feature server, in the name of &lt;host_name&gt;:&lt;port&gt;.\n            feast_entity_id (str): Name of the entity ID key for feature store lookups.\n            feature_service (str): Name of the feature service to retrieve from the feature store.\n            model_name (str): Name of the model used on the endpoint path (default is \"model\").\n            predictor_host (str): Hostname for calling the predictor from the transformer (default is None).\n            predictor_protocol (str): Inference protocol for the predictor (default is \"v1\").\n        \"\"\"\n        super().__init__(\n            name=model_name,\n            predictor_config=PredictorConfig(\n                predictor_host=predictor_host,\n                predictor_protocol=predictor_protocol,\n            )\n        )\n        self.feast_url = feast_url\n        self.feast_entity_id = feast_entity_id\n        self.feature_service = feature_service\n</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/feast-transformer/#logic-of-the-preprocess-method","title":"Logic of the <code>preprocess()</code> Method","text":"<p>The <code>preprocess()</code> method plays a pivotal role in the inference pipeline by preparing the input data for prediction. It begins by parsing the payload to extract entity IDs (<code>extract_entity_ids()</code>), which are essential for querying the Feast feature store.</p> <p>Once the entity IDs are identified, the method sends a request to the Feast online feature server to fetch the corresponding features. </p> <p>Finally, it processes the response from Feast, transforming the retrieved features into a format that aligns with the requirements of the predictor (<code>create_inference_request</code>). This ensures a seamless flow of data, enabling accurate and efficient predictions.</p> <p>Since the transformer is designed to handle multiple protocols, the <code>preprocess()</code> method checks the request type and processes it accordingly. It supports REST v1, REST v2, and gRPC protocols, ensuring compatibility with various KServe deployments. Additionally, the method processes the response received from Feast based on the protocol used by the predictor. By tailoring both the incoming request handling and the outgoing response formatting, this Feast transformer becomes a highly versatile and adaptable component within the KServe ecosystem.</p> feast_transformer.py<pre><code>    def preprocess(\n        self, payload: Union[Dict, InferRequest], headers: Dict[str, str] = None\n    ) -&gt; Union[Dict, InferRequest]:\n        \"\"\"Pre-process activity of the driver input data.\n\n        Args:\n            payload (Dict|InferRequest): Body of the request, v2 endpoints pass InferRequest.\n            headers (Dict): Request headers.\n\n        Returns:\n            output (Dict|InferRequest): Transformed payload to ``predict`` handler or return InferRequest for predictor call.\n        \"\"\"\n        logger.info(f\"Headers: {headers}\")\n        logger.info(f\"Type of payload: {type(payload)}\")\n        logger.info(f\"Payload: {payload}\")\n\n        # Prepare and send a request for the Feast online feature server\n        entites = self.extract_entity_ids(payload)\n        logger.info(f\"Extracted entities: {entites}\")\n        feast_response = requests.post(\n            f\"{self.feast_url}/get-online-features\",\n            data=json.dumps({\n                \"feature_service\": self.feature_service,\n                \"entities\": entites\n            }),\n            headers={\n                \"Content-Type\": \"application/json\",\n                \"Accept\": \"application/json\"\n            }\n        )\n\n        # Parse the response from the Feast online feature server\n        if feast_response.status_code != 200:\n            logger.error(\n                f\"Error fetching features from Feast: {feast_results}\")\n            raise Exception(\n                f\"Error fetching features from Feast: {feast_results}\")\n        feast_results = feast_response.json()\n        logger.info(f\"Feast results: {feast_results}\")\n\n        output = self.create_inference_request(feast_results)\n        logger.info(f\"Type of output: {type(output)}\")\n        logger.info(f\"Output of preprocess: {output}\")\n        return output\n</code></pre> feast_transformer.py<pre><code>    def extract_entity_ids(self, payload: Union[Dict, InferRequest]) -&gt; Dict:\n        \"\"\"Extract entity IDs from the input payload.\n\n        This method processes the input payload to extract entity IDs based on the \n        protocol (REST v1, REST v2, or gRPC v2) and returns them in a dictionary format.\n\n        Args:\n            payload (Dict|InferRequest): The input payload containing entity IDs.\n\n        Returns:\n            entites (Dict): A dictionary with the extracted entity IDs. For example:\n            {\n            \"entity_id\": [\"v5zlw0\", \"000q95\"]\n            }\n        \"\"\"\n        # The protocol here refers to the protocol used by the transformer deployment\n        # v2\n        if isinstance(payload, InferRequest):\n            infer_input = payload.inputs[0]\n            entity_ids = [\n                # Decode each element based on the protocol: gRPC uses raw bytes, REST uses base64-encoded strings\n                d.decode(\n                    'utf-8') if payload.from_grpc else base64.b64decode(d).decode('utf-8')\n                for d in infer_input.data\n            ]\n        # REST v1, type(payload) = Dict\n        else:\n            entity_ids = [\n                instance[self.feast_entity_id]\n                for instance in payload[\"instances\"]\n            ]\n\n        return {self.feast_entity_id: entity_ids}\n</code></pre> feast_transformer.py<pre><code>    def create_inference_request(self, feast_results: Dict) -&gt; Union[Dict, InferRequest]:\n        \"\"\"Create the inference request for all entities and return it as a dict.\n\n        Args:\n            feast_results (Dict): entity feast_results extracted from the feature store\n\n        Returns:\n            output (Dict|InferRequest): Returns the entity ids with feast_results\n        \"\"\"\n        feature_names = feast_results[\"metadata\"][\"feature_names\"]\n        results = feast_results[\"results\"]\n        num_datapoints = len(results[0][\"values\"])\n        num_features = len(feature_names)\n\n        # for v1 predictor protocol, we can directly pass the dict\n        if self.protocol == PredictorProtocol.REST_V1.value:\n            output = {\n                \"instances\": [\n                    {\n                        feature_names[j]: results[j]['values'][i] for j in range(num_features) if feature_names[j] != \"entity_id\"\n                    }\n                    for i in range(num_datapoints)\n                ]\n            }\n        # for v2 predictor protocol, we need to build an InferRequest\n        else:\n            # TODO: find a way to not hardcode the data types\n            type_map = {\n                \"has_fraud_7d\": \"BOOL\",\n                \"num_transactions_7d\": \"INT64\",\n                \"credit_score\": \"INT64\",\n                \"account_age_days\": \"INT64\",\n                \"has_2fa_installed\": \"BOOL\",\n            }\n            map_datatype = lambda feature_name: type_map.get(feature_name, \"BYTES\")\n\n            output = InferRequest(\n                model_name=self.name,\n                parameters={\n                    \"content-type\": \"pd\"\n                },\n                infer_inputs=[\n                    InferInput(\n                        name=feature_names[j],\n                        datatype=map_datatype(feature_names[j]),\n                        shape=[num_datapoints],\n                        data=[\n                            results[j][\"values\"][i]\n                            for i in range(num_datapoints)\n                        ]\n                    )\n                    for j in range(num_features)\n                    if feature_names[j] != \"entity_id\"\n                ]\n            )\n\n        return output\n</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/feast-transformer/#package-the-custom-transformer","title":"Package the Custom Transformer","text":"<p>Packaging the custom transformer is essential to ensure it can be easily deployed and reused across different environments. By organizing the code into a well-defined structure and containerizing it, we create a portable and self-contained solution that can be seamlessly integrated into any KServe deployment. This approach not only simplifies dependency management but also ensures consistency and reliability, making it easier to scale and maintain the transformer in production.</p> <p>First, create a directory structure for your custom transformer:</p> <pre><code>.\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 __main__.py\n\u2514\u2500\u2500 feast_transformer.py\n</code></pre> <p>Then, extend the <code>__main__.py</code> file to include custom arguments that allow us to easily inject Feast-related information into the KServe transformer. This makes it convenient to specify details like the Feast URL, entity ID, and feature service directly in the inference service YAML file. By doing so, we ensure that the transformer remains flexible and configurable, adapting seamlessly to different deployment environments.</p> __main__.py<pre><code>parser = argparse.ArgumentParser(parents=[kserve.model_server.parser])\nparser.add_argument(\n    \"--feast_url\",\n    type=str,\n    help=\"URL for the Feast online feature server, in the name of &lt;host_name&gt;:&lt;port&gt;\",\n    required=True,\n)\nparser.add_argument(\n    \"--feast_entity_id\",\n    type=str,\n    help=\"Name of the entity ID key for feature store lookups.\",\n    required=True,\n)\nparser.add_argument(\n    \"--feature_service\",\n    type=str,\n    help=\"Name of the feature service to retrieve from the feature store.\",\n    required=True,\n)\n</code></pre> <p>When you run the transformer, you can specify the Feast URL, entity ID, and feature service as command-line arguments. This allows the transformer to dynamically connect to the Feast feature store and retrieve the necessary features for inference.</p> <pre><code>python -m feast_transformer \\\n  --feast_url http://example.com:6565 \\\n  --feast_entity_id entity_id \\\n  --feature-service fraud_detection_v1\n</code></pre> <p>After the arguments are parsed, the <code>main()</code> function initializes the KServe model with the provided Feast configuration and starts the model server.</p> __main__.py<pre><code>if __name__ == \"__main__\":\n    if args.configure_logging:\n        logging.configure_logging(args.log_config_file)\n    transformer = FeastTransformer(\n        feast_url=args.feast_url,\n        feast_entity_id=args.feast_entity_id,\n        feature_service=args.feature_service,\n        model_name=args.model_name,\n        predictor_host=args.predictor_host,\n        predictor_protocol=args.predictor_protocol,\n    )\n    server = kserve.ModelServer()\n    server.start(models=[transformer])\n</code></pre> <p>Here's a sequence diagram when we start the KServe Model Server:</p> <pre><code>sequenceDiagram\n    participant User as User\n    participant ModelServer as ModelServer\n    participant ModelRepo as Model Repository\n    participant RESTServer as REST Server\n    participant GRPCServer as GRPC Server\n    participant EventLoop as Event Loop\n    participant SignalHandler as Signal Handler\n\n    User-&gt;&gt;ModelServer: Instantiate ModelServer\n    ModelServer-&gt;&gt;ModelRepo: Initialize Model Repository\n    ModelServer-&gt;&gt;EventLoop: Setup Event Loop\n    ModelServer-&gt;&gt;SignalHandler: Register Signal Handlers\n\n    User-&gt;&gt;ModelServer: Call start(models)\n    ModelServer-&gt;&gt;ModelRepo: Register Models\n    alt At least one model is ready\n        ModelServer-&gt;&gt;RESTServer: Start REST Server\n        RESTServer--&gt;&gt;ModelServer: REST Server Running\n        ModelServer-&gt;&gt;GRPCServer: Start GRPC Server\n        GRPCServer--&gt;&gt;ModelServer: GRPC Server Running\n    else No models are ready\n        ModelServer--&gt;&gt;User: Raise NoModelReady Exception\n    end\n    ModelServer-&gt;&gt;EventLoop: Run Event Loop\n\n    User-&gt;&gt;ModelServer: Call stop(sig)\n    ModelServer-&gt;&gt;RESTServer: Stop REST Server\n    RESTServer--&gt;&gt;ModelServer: REST Server Stopped\n    ModelServer-&gt;&gt;GRPCServer: Stop GRPC Server\n    GRPCServer--&gt;&gt;ModelServer: GRPC Server Stopped\n    ModelServer-&gt;&gt;ModelRepo: Unload Models\n    ModelRepo--&gt;&gt;ModelServer: Models Unloaded\n    ModelServer--&gt;&gt;User: Server Stopped</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/feast-transformer/#containerize-the-transformer","title":"Containerize the Transformer","text":"<pre><code>kserve/docker\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 uv.lock\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 .dockerignore\n\u251c\u2500\u2500 feast_transformer\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 __main__.py\n\u2514   \u2514\u2500\u2500 feast_transformer.py\n</code></pre> <p>First, create a <code>pyproject.toml</code> file to define the dependencies for your custom transformer. This file will be used by the <code>uv</code> package manager to install the necessary libraries.</p> pyproject.toml<pre><code>[project]\nname = \"feast-transformer\"\nversion = \"0.1.0\"\ndescription = \"Feast Transformer for KServe\"\nrequires-python = \"~=3.11\"\nauthors = [\n    { email = \"kuanchoulai10@gmail.com\" }\n]\nreadme = \"README.md\"\n\ndependencies = [\n    \"kserve==0.15.1\",\n    \"requests&gt;=2.22.0\",\n    \"numpy&gt;=1.16.3\"\n]\n\n\n[build-system]\nrequires = [\"setuptools&gt;=80.0.0\"]\nbuild-backend = \"setuptools.build_meta\"\n</code></pre> <p>Next, use the <code>uv</code> package manager to install the dependencies and generate a lock file. The lock file ensures that the same versions of the dependencies are used across different environments, providing consistency and reliability.</p> <pre><code>uv lock\n</code></pre> <p>Then, create a <code>Dockerfile</code> to build the custom transformer image. This Dockerfile uses a multi-stage build process to ensure that the final image is lightweight and contains only the necessary components.</p> Dockerfile<pre><code># Stage 1: Dependency installation using uv\n# https://docs.astral.sh/uv/guides/integration/docker/#available-images\n# https://github.com/astral-sh/uv-docker-example/blob/main/multistage.Dockerfile\nFROM ghcr.io/astral-sh/uv:python3.11-bookworm-slim AS builder\n\nENV UV_COMPILE_BYTECODE=1 UV_LINK_MODE=copy UV_PYTHON_DOWNLOADS=0\n\nWORKDIR /app\n\n# Install build tools required for psutil\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\\n    gcc python3-dev &amp;&amp; \\\n    apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=uv.lock,target=uv.lock \\\n    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \\\n    uv sync --locked --no-install-project --no-dev\nCOPY . /app\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv sync --locked --no-dev\n\n# Stage 2: Final image\nFROM python:3.11-slim-bookworm\n\nWORKDIR /app\n\nCOPY --from=builder --chown=app:app /app /app\n\nENV PATH=\"/app/.venv/bin:$PATH\"\n\n# Set the entry point for the container\nENTRYPOINT [\"python\", \"-m\", \"feast_transformer\"]\n</code></pre> <p>The first stage (<code>builder</code>) uses the <code>uv</code> package manager to install all dependencies defined in the <code>pyproject.toml</code> file. It also caches dependencies to speed up subsequent builds and compiles Python bytecode for better performance. Additionally, it installs build tools like <code>gcc</code> and <code>python3-dev</code> to handle dependencies requiring compilation, such as <code>psutil</code>.</p> <p>The <code>uv sync</code> command is run twice in this stage. The first <code>uv sync</code> installs only the dependencies without the project files, ensuring that the dependency installation is cached and reused across builds. The second <code>uv sync</code> installs the project files and finalizes the environment. This two-step process minimizes rebuild times by leveraging cached dependencies while still ensuring the application code is up-to-date.</p> <p>The second stage creates the final image. It copies the application and its dependencies from the <code>builder</code> stage into a minimal Python base image. This ensures the final image is small and optimized for production use. The <code>PATH</code> environment variable is updated to include the virtual environment created by <code>uv</code>, and the entry point is set to run the transformer using the <code>python -m feast_transformer</code> command.</p> <p>This approach ensures a clean, efficient, and portable container image for deploying the Feast transformer.</p> <p>Finally, build the Docker image and load it into Minikube. This step is crucial for deploying the transformer in a local Kubernetes environment, allowing you to test and validate the integration with KServe and Feast.</p> <pre><code>docker buildx build \\\n  --platform linux/amd64 \\\n  -t feast-transformer:v0.1.0 \\\n  .\n</code></pre> <pre><code>minikube image load feast-transformer:v0.1.0\n</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/how-it-works/","title":"How It Works?","text":"<p>KServe can be installed in two modes:</p> <ol> <li>Serverless Mode (Recommended): Powered by Knative and Istio, this mode offers benefits such as automatic scaling, enhanced security, simplified traffic management, and seamless integration with serverless workflows.<sup>1</sup></li> <li>RawDeployment Mode: Utilizes native Kubernetes resources like Deployment, Service, Ingress, and Horizontal Pod Autoscaler, providing a more traditional approach to model serving.<sup>2</sup></li> </ol> KServe Architecture on Serverless Mode<sup>3</sup>"},{"location":"side-projects/data2ml-ops/kserve/how-it-works/#architecture-components","title":"Architecture Components","text":""},{"location":"side-projects/data2ml-ops/kserve/how-it-works/#control-plane","title":"Control Plane","text":"KServe Controller <p>Handles the creation of services, ingress resources, model server containers, and model agent containers to facilitate request/response logging, batching, and model retrieval.<sup>4</sup></p> Ingress Gateway <p>Acts as an entry point for directing external or internal traffic to the appropriate services.<sup>4</sup></p> <p>If operating in Serverless mode, the following additional components are included:</p> Knative Serving Controller <p>Manages service revisions, sets up network routing configurations, and provisions serverless containers with a queue proxy to handle traffic metrics and enforce concurrency limits.<sup>4</sup></p> Knative Activator <p>Responsible for reviving pods that have been scaled down to zero and routing incoming requests to them.<sup>4</sup></p> Knative Autoscaler (KPA) <p>Monitors application traffic and dynamically adjusts the number of replicas based on predefined metrics.<sup>4</sup></p>"},{"location":"side-projects/data2ml-ops/kserve/how-it-works/#data-plane","title":"Data Plane","text":"InferenceService <p>An InferenceService in KServe is a Kubernetes custom resource designed to simplify the deployment and management of machine learning models for inference. It integrates components such as predictors, transformers, and explainers, offering capabilities like autoscaling, version control, and traffic splitting to optimize model serving in production.<sup>5</sup></p> Predictor <p>The predictor is the core component of the InferenceService, responsible for hosting the model and exposing it through a network endpoint for inference requests.<sup>5</sup></p> Explainer <p>The explainer provides an optional feature that generates model explanations alongside predictions, offering insights into the model's decision-making process.<sup>5</sup></p> Transformer <p>The transformer allows users to define custom pre-processing and post-processing steps, enabling data transformations before predictions or explanations are generated.<sup>5</sup></p>"},{"location":"side-projects/data2ml-ops/kserve/how-it-works/#serving-runtimes","title":"Serving Runtimes","text":"<p>KServe utilizes two types of Custom Resource Definitions (CRDs) to define model serving environments: <code>ServingRuntimes</code> and <code>ClusterServingRuntimes</code>. The primary distinction between them is their scope\u2014<code>ServingRuntimes</code> are namespace-scoped, while <code>ClusterServingRuntimes</code> are cluster-scoped.</p> ServingRuntime / ClusterServingRuntime <p>These CRDs specify templates for Pods capable of serving one or more specific model formats. Each ServingRuntime includes essential details such as the runtime's container image and the list of supported model formats<sup>8</sup>.</p> <p>KServe provides several pre-configured ClusterServingRuntimes, enabling users to deploy popular model formats without the need to manually define the runtimes.</p> Support Model Formats<sup>8</sup><sup>9</sup>"},{"location":"side-projects/data2ml-ops/kserve/how-it-works/#core-concepts","title":"Core Concepts","text":""},{"location":"side-projects/data2ml-ops/kserve/how-it-works/#inference-service","title":"Inference Service","text":"<p>The following example demonstrates the minimum setup required to deploy an InferenceService in KServe<sup>10</sup>:</p> <pre><code>apiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"sklearn-iris\"\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: sklearn\n      storageUri: \"gs://kfserving-examples/models/sklearn/1.0/model\"\n</code></pre> <p>Predictor:</p> <ul> <li>Model Format: Specifies the framework or format of the model being served. In this example, the model format is <code>sklearn</code>.</li> <li>Storage URI: Indicates the location of the model file. Here, the model is stored in a Google Cloud Storage bucket at <code>gs://kfserving-examples/models/sklearn/1.0/model</code>.</li> </ul> <p>To ensure that the pod can successfully load the model, proper permissions must be configured.<sup>16</sup><sup>17</sup></p> <p>Once applied, your InferenceService will be successfully deployed:</p> <pre><code>kubectl get inferenceservices sklearn-iris -n kserve-test\n</code></pre> <pre><code>NAME           URL                                                 READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                    AGE\nsklearn-iris   http://sklearn-iris.kserve-test.example.com         True           100                              sklearn-iris-predictor-default-47q2g   7d23h\n</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/how-it-works/#inference-protocol","title":"Inference Protocol","text":"<p>KServe's data plane protocol provides a framework-agnostic inference API that works seamlessly across various ML/DL frameworks and model servers. It supports two versions:</p> <ul> <li>v1: Offers support exclusively for REST APIs.<sup>6</sup></li> <li>v2: Extends support to both REST and gRPC APIs.<sup>7</sup></li> </ul> <p>To use v2 REST protocol for inference with the deployed model, you set the <code>protocolVersion</code> field to v2<sup>11</sup>:</p> <pre><code>apiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"mlflow-v2-wine-classifier\"\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: mlflow\n      protocolVersion: v2\n      storageUri: \"gs://kfserving-examples/models/mlflow/wine\"\n</code></pre> <p>Not all server runtimes support v2 inference protocol and gRPC protocol, you should check here<sup>9</sup> for more information.</p> Supported Protocols for Each Server Runtime<sup>9</sup> <p>To use v2 gRPC protocol for inference with the deployed model, you set the container port to be <code>8081</code> and the name of port to be <code>h2c</code><sup>12</sup>(this setup is not for TensorFlow and PyTorch, which have their own settings).</p> <pre><code>apiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"sklearn-v2-iris-grpc\"\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: sklearn\n      protocolVersion: v2\n      runtime: kserve-sklearnserver\n      storageUri: \"gs://kfserving-examples/models/sklearn/1.0/model\"\n      ports:\n        - name: h2c     # knative expects grpc port name to be 'h2c'\n          protocol: TCP\n          containerPort: 8081\n</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/how-it-works/#autoscaling","title":"Autoscaling","text":"<p>Set up based on what condition to scale up the predictor using <code>scaleTarget</code> and  <code>scaleMetric</code>.<sup>18</sup></p> <pre><code>apiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"flowers-sample\"\nspec:\n  predictor:\n    scaleTarget: 1\n    scaleMetric: concurrency # \"qps\" is an option as well\n    model:\n      modelFormat:\n        name: tensorflow\n      storageUri: \"gs://kfserving-examples/models/tensorflow/flowers\"\n</code></pre> <p>Scale to zero using <code>minReplicas: 0</code>.<sup>18</sup></p> <pre><code>apiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"flowers-sample\"\nspec:\n  predictor:\n    minReplicas: 0\n    model:\n      modelFormat:\n        name: tensorflow\n      storageUri: \"gs://kfserving-examples/models/tensorflow/flowers\"\n</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/how-it-works/#canary-rollout","title":"Canary Rollout","text":"<p>KServe supports canary rollout, a deployment strategy that allows you to gradually shift traffic between different versions of a model. This approach minimizes risks by enabling you to test new versions (canary models) with a small percentage of traffic before fully rolling them out.</p> <p>Promote the canary model or roll back to the previous model using the <code>canaryTrafficPercent</code> field. In addition, you can use the <code>serving.kserve.io/enable-tag-routing</code> annotation to route traffic explicitly. This allows you to direct traffic to the canary model (model v2) or the previous model (model v1) by including a tag in the request URL.<sup>13</sup></p>"},{"location":"side-projects/data2ml-ops/kserve/how-it-works/#inference-graph","title":"Inference Graph","text":"<p>Modern ML inference systems are increasingly complex, often requiring multiple models to generate a single prediction. KServe simplifies this process by supporting <code>InferenceGraph</code>, allowing users to define and deploy intricate ML inference pipelines in a declarative and scalable manner for production use.<sup>14</sup></p> Inference Graph<sup>14</sup> InferenceGraph <p>Composed of routing <code>Nodes</code>, each containing a series of routing <code>Steps</code>. Each <code>Step</code> can direct traffic to either an InferenceService or another <code>Node</code> within the graph, making the <code>InferenceGraph</code> highly modular. The <code>InferenceGraph</code> supports four types of Routing <code>Nodes</code>: Sequence, Switch, Ensemble, and Splitter.<sup>14</sup></p> Sequence Node <p>Enables users to define a series of <code>Steps</code> where each <code>Step</code> routes to an <code>InferenceService</code> or another <code>Node</code> in a sequential manner. The output of one <code>Step</code> can be configured to serve as the input for the next <code>Step</code>.<sup>14</sup></p> Ensemble Node <p>Facilitates model ensembles by running multiple models independently and combining their outputs into a single prediction. Various methods, such as majority voting for classification or averaging for regression, can be used to aggregate the results.<sup>14</sup></p> Splitter Node <p>Distributes traffic across multiple targets based on a specified weighted distribution.<sup>14</sup></p> Switch Node <p>Allows users to specify routing conditions to determine which <code>Step</code> to execute. The response is returned as soon as a condition is met. If no conditions are satisfied, the graph returns the original request.<sup>14</sup></p> <p>Example<sup>15</sup></p> <pre><code>apiVersion: \"serving.kserve.io/v1alpha1\"\nkind: \"InferenceGraph\"\nmetadata:\n  name: \"dog-breed-pipeline\"\nspec:\n  nodes:\n    root:\n      routerType: Sequence\n      steps:\n      - serviceName: cat-dog-classifier\n        name: cat_dog_classifier # step name\n      - serviceName: dog-breed-classifier\n        name: dog_breed_classifier\n        data: $request\n        condition: \"[@this].#(predictions.0==\\\"dog\\\")\"\n  resources:\n    requests:\n      cpu: 100m\n      memory: 256Mi\n    limits:\n      cpu: 1\n      memory: 1Gi\n</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/how-it-works/#behind-the-scenes","title":"Behind the Scenes","text":"<p>When you apply an <code>InferenceService</code> using <code>kubectl apply</code>, the following steps occur behind the scenes (in serverless mode):</p> <ol> <li>The KServe Controller receives the request and deploys a Knative Service.</li> <li>A Knative Revision is prepared to manage versioning and traffic routing.</li> <li>The Transformer and Predictor Pods are deployed, with autoscaling configurations set up via the Knative Autoscaler.</li> <li>The Predictor Pod uses an InitContainer (Storage Initializer) to fetch the model from a storage location (e.g., GCS, S3).</li> <li>Once the model is retrieved, the Predictor Pod deploys the model using the specified Server Runtime.</li> <li>The Predictor Pod exposes its endpoint through a Queue Proxy, which handles traffic metrics and concurrency limits. The endpoint is then made accessible externally via a Service.</li> <li>The Transformer Pod, which handles pre-processing and post-processing logic, does not require a storage initializer. It simply deploys the transformer container.</li> <li>Similar to the Predictor Pod, the Transformer Pod exposes its endpoint through a Queue Proxy, making it accessible externally via a Service.</li> <li>Finally, the backend of your AI application can call the <code>InferenceService</code> endpoints to execute pre-processing, prediction, and post-processing. The system dynamically scales up or down based on the configured autoscaling metrics.</li> </ol> <p>This seamless orchestration ensures efficient and scalable model serving for your AI applications.</p> <ol> <li> <p>Serverless Installation Guide \u21a9</p> </li> <li> <p>Kubernetes Deployment Installation Guide \u21a9</p> </li> <li> <p>KServe Docs \u21a9</p> </li> <li> <p>Control Plane \u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Data Plane \u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>V1 Inference Protocol \u21a9</p> </li> <li> <p>V2 Inference Protocol \u21a9</p> </li> <li> <p>Serving Runtimes | Concepts \u21a9\u21a9</p> </li> <li> <p>Model Serving Runtimes | Supported Model Frameworks/Formats \u21a9\u21a9\u21a9</p> </li> <li> <p>First InferenceService \u21a9</p> </li> <li> <p>Deploy MLflow models with InferenceService\u00b6 \u21a9</p> </li> <li> <p>Deploy Scikit-learn models with InferenceService \u21a9</p> </li> <li> <p>Canary Rollout Example \u21a9</p> </li> <li> <p>Inference Graph \u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Deploy Image Processing Inference pipeline with InferenceGraph \u21a9</p> </li> <li> <p>Deploy InferenceService with a saved model on S3 \u21a9</p> </li> <li> <p>Deploy InferenceService with a saved model on GCS \u21a9</p> </li> <li> <p>Autoscale InferenceService with Knative Autoscaler \u21a9\u21a9</p> </li> </ol>"},{"location":"side-projects/data2ml-ops/kserve/in-the-bigger-picture/","title":"In the Bigger Picture","text":""},{"location":"side-projects/data2ml-ops/kserve/in-the-bigger-picture/#history-current-status","title":"History &amp; Current Status","text":"<p>KServe is an open-source, serverless machine learning inference platform designed to run on Kubernetes. Originally developed in 2019 by teams from Google, IBM, Bloomberg, NVIDIA, and Seldon, it was initially released as KFServing. In September 2021, the project was rebranded to KServe and transitioned to an independent GitHub organization. KServe is currently incubated under the LF AI &amp; Data Foundation.  \ufffc \ufffc \ufffc</p> <p>As of May 2025, KServe has over 4,000 stars on GitHub and more than 1,000 contributors. The community is active, hosting biweekly public meetings and maintaining comprehensive documentation. KServe is utilized by organizations such as Amazon Web Services, Bloomberg, Gojek, IBM, NVIDIA, and Samsung SDS.</p>"},{"location":"side-projects/data2ml-ops/kserve/in-the-bigger-picture/#alternatives","title":"Alternatives","text":"<p>We have researched the alternatives in the previous article. See here for more.</p>"},{"location":"side-projects/data2ml-ops/kserve/installation/","title":"Install Kserve in Serverless Mode","text":"<p>To install KServe in serverless mode, you need to first install three components: Knative Serving, Networking Layer, and Cert Manager.</p> <p>KServe provides a recommended version matrix for Knative Serving and Istio based on your Kubernetes version.</p> Recommended Version Matrix<sup>1</sup> <p>Since my Kubernetes version is 1.30, I will install the following versions:</p> <ul> <li>Knative Serving v1.15.0</li> <li>Istio v1.22.8</li> <li>Cert Manager v1.17.2</li> <li>KServe v0.15.1</li> </ul>"},{"location":"side-projects/data2ml-ops/kserve/installation/#install-knative-serving","title":"Install Knative Serving<sup>2</sup>","text":"<p>Before install, nothing</p> <pre><code>kubectl get ns\n</code></pre> <p>Expected Output</p> <pre><code>NAME              STATUS   AGE\ndefault           Active   15d\nkube-node-lease   Active   15d\nkube-public       Active   15d\nkube-system       Active   15d\n</code></pre> <p>Install the required custom resources by running the command:</p> <pre><code>kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.15.0/serving-crds.yaml\n</code></pre> Expected Output <pre><code>customresourcedefinition.apiextensions.k8s.io/certificates.networking.internal.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/configurations.serving.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/clusterdomainclaims.networking.internal.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/domainmappings.serving.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/ingresses.networking.internal.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/metrics.autoscaling.internal.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/podautoscalers.autoscaling.internal.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/revisions.serving.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/routes.serving.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/serverlessservices.networking.internal.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/services.serving.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/images.caching.internal.knative.dev created\n</code></pre> <p>Deploy the core components of Knative Serving to you Kubernetes cluster (namespace <code>knative-serving</code>) by running the command:</p> <pre><code>kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.15.0/serving-core.yaml\n</code></pre> Expected Output <pre><code>namespace/knative-serving created\nrole.rbac.authorization.k8s.io/knative-serving-activator created\nclusterrole.rbac.authorization.k8s.io/knative-serving-activator-cluster created\nclusterrole.rbac.authorization.k8s.io/knative-serving-aggregated-addressable-resolver created\nclusterrole.rbac.authorization.k8s.io/knative-serving-addressable-resolver created\nclusterrole.rbac.authorization.k8s.io/knative-serving-namespaced-admin created\nclusterrole.rbac.authorization.k8s.io/knative-serving-namespaced-edit created\nclusterrole.rbac.authorization.k8s.io/knative-serving-namespaced-view created\nclusterrole.rbac.authorization.k8s.io/knative-serving-core created\nclusterrole.rbac.authorization.k8s.io/knative-serving-podspecable-binding created\nserviceaccount/controller created\nclusterrole.rbac.authorization.k8s.io/knative-serving-admin created\nclusterrolebinding.rbac.authorization.k8s.io/knative-serving-controller-admin created\nclusterrolebinding.rbac.authorization.k8s.io/knative-serving-controller-addressable-resolver created\nserviceaccount/activator created\nrolebinding.rbac.authorization.k8s.io/knative-serving-activator created\nclusterrolebinding.rbac.authorization.k8s.io/knative-serving-activator-cluster created\ncustomresourcedefinition.apiextensions.k8s.io/images.caching.internal.knative.dev unchanged\ncertificate.networking.internal.knative.dev/routing-serving-certs created\ncustomresourcedefinition.apiextensions.k8s.io/certificates.networking.internal.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/configurations.serving.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/clusterdomainclaims.networking.internal.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/domainmappings.serving.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/ingresses.networking.internal.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/metrics.autoscaling.internal.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/podautoscalers.autoscaling.internal.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/revisions.serving.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/routes.serving.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/serverlessservices.networking.internal.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/services.serving.knative.dev unchanged\nimage.caching.internal.knative.dev/queue-proxy created\nconfigmap/config-autoscaler created\nconfigmap/config-certmanager created\nconfigmap/config-defaults created\nconfigmap/config-deployment created\nconfigmap/config-domain created\nconfigmap/config-features created\nconfigmap/config-gc created\nconfigmap/config-leader-election created\nconfigmap/config-logging created\nconfigmap/config-network created\nconfigmap/config-observability created\nconfigmap/config-tracing created\nhorizontalpodautoscaler.autoscaling/activator created\npoddisruptionbudget.policy/activator-pdb created\ndeployment.apps/activator created\nservice/activator-service created\ndeployment.apps/autoscaler created\nservice/autoscaler created\ndeployment.apps/controller created\nservice/controller created\nhorizontalpodautoscaler.autoscaling/webhook created\npoddisruptionbudget.policy/webhook-pdb created\ndeployment.apps/webhook created\nservice/webhook created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/config.webhook.serving.knative.dev created\nmutatingwebhookconfiguration.admissionregistration.k8s.io/webhook.serving.knative.dev created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/validation.webhook.serving.knative.dev created\nsecret/webhook-certs created\n</code></pre> <p>Once the deployment finished, you can see <code>knative-serving</code> namespace created and all the resources inside:</p> <pre><code>kubectl get ns\n</code></pre> <p>Expected Output</p> <pre><code>NAME              STATUS   AGE\ndefault           Active   15d\nknative-serving   Active   17s\nkube-node-lease   Active   15d\nkube-public       Active   15d\nkube-system       Active   15d\n</code></pre> <pre><code>kubectl get all -n knative-serving\n</code></pre> <p>Expected Output</p> <pre><code>kubectl get all -n knative-serving\nNAME                              READY   STATUS    RESTARTS   AGE\npod/activator-bccd57594-l5n5m     1/1     Running   0          3m16s\npod/autoscaler-7c6d8b8456-lwn6b   1/1     Running   0          3m16s\npod/controller-6458dc4845-bv7st   1/1     Running   0          3m16s\npod/webhook-68b5b4c69-8jv7q       1/1     Running   0          3m16s\n\nNAME                                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                   AGE\nservice/activator-service            ClusterIP   10.110.50.161    &lt;none&gt;        9090/TCP,8008/TCP,80/TCP,81/TCP,443/TCP   3m16s\nservice/autoscaler                   ClusterIP   10.105.23.244    &lt;none&gt;        9090/TCP,8008/TCP,8080/TCP                3m16s\nservice/autoscaler-bucket-00-of-01   ClusterIP   10.108.159.205   &lt;none&gt;        8080/TCP                                  2m55s\nservice/controller                   ClusterIP   10.110.203.67    &lt;none&gt;        9090/TCP,8008/TCP                         3m16s\nservice/webhook                      ClusterIP   10.104.187.107   &lt;none&gt;        9090/TCP,8008/TCP,443/TCP                 3m16s\n\nNAME                         READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/activator    1/1     1            1           3m16s\ndeployment.apps/autoscaler   1/1     1            1           3m16s\ndeployment.apps/controller   1/1     1            1           3m16s\ndeployment.apps/webhook      1/1     1            1           3m16s\n\nNAME                                    DESIRED   CURRENT   READY   AGE\nreplicaset.apps/activator-bccd57594     1         1         1       3m16s\nreplicaset.apps/autoscaler-7c6d8b8456   1         1         1       3m16s\nreplicaset.apps/controller-6458dc4845   1         1         1       3m16s\nreplicaset.apps/webhook-68b5b4c69       1         1         1       3m16s\n\nNAME                                            REFERENCE              TARGETS               MINPODS   MAXPODS   REPLICAS   AGE\nhorizontalpodautoscaler.autoscaling/activator   Deployment/activator   cpu: &lt;unknown&gt;/100%   1         20        1          3m16s\nhorizontalpodautoscaler.autoscaling/webhook     Deployment/webhook     cpu: &lt;unknown&gt;/100%   1         5         1          3m16s\n</code></pre> <p>\u90e8\u7f72\u5b8c\u6210\u5f8c\uff0c\u53ef\u4ee5\u770b\u5230\u6709\u4ee5\u4e0b\u9019\u56db\u500bpods\uff0c\u5206\u5225\u8ca0\u8cac</p> <ul> <li>activator</li> <li>autoscaler</li> <li>controller</li> <li>webhook</li> </ul>"},{"location":"side-projects/data2ml-ops/kserve/installation/#install-networking-layer-istio","title":"Install Networking Layer - Istio<sup>3</sup>","text":"<p>Install Istio 1.22.8 on the home directory</p> <pre><code>cd $HOME\ncurl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.22.8 sh -\n</code></pre> Expected Output <pre><code>% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                Dload  Upload   Total   Spent    Left  Speed\n100   102  100   102    0     0    350      0 --:--:-- --:--:-- --:--:--   351\n100  5124  100  5124    0     0   6834      0 --:--:-- --:--:-- --:--:-- 13343\n\nDownloading istio-1.22.8 from https://github.com/istio/istio/releases/download/1.22.8/istio-1.22.8-osx-arm64.tar.gz ...\n\nIstio 1.22.8 download complete!\n\nThe Istio release archive has been downloaded to the istio-1.22.8 directory.\n\nTo configure the istioctl client tool for your workstation,\nadd the /Users/kcl/istio-1.22.8/bin directory to your environment path variable with:\n    export PATH=\"$PATH:/Users/kcl/istio-1.22.8/bin\"\n\nBegin the Istio pre-installation check by running:\n    istioctl x precheck \n\nTry Istio in ambient mode\n    https://istio.io/latest/docs/ambient/getting-started/\nTry Istio in sidecar mode\n    https://istio.io/latest/docs/setup/getting-started/\nInstall guides for ambient mode\n    https://istio.io/latest/docs/ambient/install/\nInstall guides for sidecar mode\n    https://istio.io/latest/docs/setup/install/\n\nNeed more information? Visit https://istio.io/latest/docs/\n</code></pre> <p>Add the <code>$HOME/istio-1.22.8/bin</code> directory to your environment path variable</p> <pre><code>export PATH=\"$PATH:$HOME/istio-1.22.8/bin\"\n</code></pre> <p>Check the versions</p> <pre><code>istioctl version\n</code></pre> <p>Expected Output</p> <pre><code>client version: 1.22.8\ncontrol plane version: 1.22.8\ndata plane version: 1.22.8 (1 proxies)\n</code></pre> <p>You can easily install and customize your Istio installation with <code>istioctl</code>. It will deploy the resources to your kubernetes cluster in the namespace <code>istio-system</code></p> <pre><code>istioctl install -y\n</code></pre> <p>Expected Output</p> <pre><code>WARNING: Istio 1.22.0 may be out of support (EOL) already: see https://istio.io/latest/docs/releases/supported-releases/ for supported releases\n\u2714 Istio core installed                       \n\u2714 Istiod installed                           \n\u2714 Ingress gateways installed\n\u2714 Installation complete\nMade this installation the default for injection and validation.\n</code></pre> <p>Check all the deployed resources</p> <pre><code>kubectl get ns\n</code></pre> <p>Expected Output</p> <pre><code>NAME              STATUS   AGE\ndefault           Active   15d\nistio-system      Active   62s\nknative-serving   Active   18m\nkube-node-lease   Active   15d\nkube-public       Active   15d\nkube-system       Active   15d\n</code></pre> <pre><code>kubectl get all -n istio-system\n</code></pre> <p>Expected Output</p> <pre><code>NAME                                        READY   STATUS    RESTARTS   AGE\npod/istio-ingressgateway-767ff7b4b6-7wxzk   1/1     Running   0          5m52s\npod/istiod-7bc77d764b-vh66z                 1/1     Running   0          6m18s\n\nNAME                           TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                      AGE\nservice/istio-ingressgateway   LoadBalancer   10.97.252.211   &lt;pending&gt;     15021:32213/TCP,80:31540/TCP,443:30462/TCP   5m52s\nservice/istiod                 ClusterIP      10.96.206.177   &lt;none&gt;        15010/TCP,15012/TCP,443/TCP,15014/TCP        6m18s\n\nNAME                                   READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/istio-ingressgateway   1/1     1            1           5m52s\ndeployment.apps/istiod                 1/1     1            1           6m19s\n\nNAME                                              DESIRED   CURRENT   READY   AGE\nreplicaset.apps/istio-ingressgateway-767ff7b4b6   1         1         1       5m52s\nreplicaset.apps/istiod-7bc77d764b                 1         1         1       6m18s\n\nNAME                                                       REFERENCE                         TARGETS              MINPODS   MAXPODS   REPLICAS   AGE\nhorizontalpodautoscaler.autoscaling/istio-ingressgateway   Deployment/istio-ingressgateway   cpu: &lt;unknown&gt;/80%   1         5         1          5m52s\nhorizontalpodautoscaler.autoscaling/istiod                 Deployment/istiod                 cpu: &lt;unknown&gt;/80%   1         5         1          6m18s\n</code></pre> <p>\u8aaa\u660e</p> <ul> <li>istio-ingressgateway</li> <li>istiod</li> <li>external-ip pending</li> </ul>"},{"location":"side-projects/data2ml-ops/kserve/installation/#integrate-istio-with-knative-serving","title":"Integrate Istio with Knative Serving<sup>3</sup>","text":"<p>To integrate Istio with Knative Serving install the Knative Istio controller by running the command</p> <pre><code>kubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.15.0/net-istio.yaml\n</code></pre> <p>Expected Output</p> <pre><code>clusterrole.rbac.authorization.k8s.io/knative-serving-istio created\ngateway.networking.istio.io/knative-ingress-gateway created\ngateway.networking.istio.io/knative-local-gateway created\nservice/knative-local-gateway created\nconfigmap/config-istio created\npeerauthentication.security.istio.io/webhook created\npeerauthentication.security.istio.io/net-istio-webhook created\ndeployment.apps/net-istio-controller created\ndeployment.apps/net-istio-webhook created\nsecret/net-istio-webhook-certs created\nservice/net-istio-webhook created\nmutatingwebhookconfiguration.admissionregistration.k8s.io/webhook.istio.networking.internal.knative.dev created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/config.webhook.istio.networking.internal.knative.dev created\n</code></pre> <p>Verify the integration</p> <pre><code>kubectl get all -n knative-serving\n</code></pre> <p>Expected Output</p> <pre><code>NAME                                        READY   STATUS    RESTARTS   AGE\npod/activator-bccd57594-l5n5m               1/1     Running   0          28m\npod/autoscaler-7c6d8b8456-lwn6b             1/1     Running   0          28m\npod/controller-6458dc4845-bv7st             1/1     Running   0          28m\npod/net-istio-controller-6b847d477f-5vtcd   1/1     Running   0          102s\npod/net-istio-webhook-856498bfc7-tswxz      1/1     Running   0          102s\npod/webhook-68b5b4c69-8jv7q                 1/1     Running   0          28m\n\nNAME                                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                   AGE\nservice/activator-service            ClusterIP   10.110.50.161    &lt;none&gt;        9090/TCP,8008/TCP,80/TCP,81/TCP,443/TCP   28m\nservice/autoscaler                   ClusterIP   10.105.23.244    &lt;none&gt;        9090/TCP,8008/TCP,8080/TCP                28m\nservice/autoscaler-bucket-00-of-01   ClusterIP   10.108.159.205   &lt;none&gt;        8080/TCP                                  27m\nservice/controller                   ClusterIP   10.110.203.67    &lt;none&gt;        9090/TCP,8008/TCP                         28m\nservice/net-istio-webhook            ClusterIP   10.97.168.104    &lt;none&gt;        9090/TCP,8008/TCP,443/TCP                 102s\nservice/webhook                      ClusterIP   10.104.187.107   &lt;none&gt;        9090/TCP,8008/TCP,443/TCP                 28m\n\nNAME                                   READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/activator              1/1     1            1           28m\ndeployment.apps/autoscaler             1/1     1            1           28m\ndeployment.apps/controller             1/1     1            1           28m\ndeployment.apps/net-istio-controller   1/1     1            1           102s\ndeployment.apps/net-istio-webhook      1/1     1            1           102s\ndeployment.apps/webhook                1/1     1            1           28m\n\nNAME                                              DESIRED   CURRENT   READY   AGE\nreplicaset.apps/activator-bccd57594               1         1         1       28m\nreplicaset.apps/autoscaler-7c6d8b8456             1         1         1       28m\nreplicaset.apps/controller-6458dc4845             1         1         1       28m\nreplicaset.apps/net-istio-controller-6b847d477f   1         1         1       102s\nreplicaset.apps/net-istio-webhook-856498bfc7      1         1         1       102s\nreplicaset.apps/webhook-68b5b4c69                 1         1         1       28m\n\nNAME                                            REFERENCE              TARGETS               MINPODS   MAXPODS   REPLICAS   AGE\nhorizontalpodautoscaler.autoscaling/activator   Deployment/activator   cpu: &lt;unknown&gt;/100%   1         20        1          28m\nhorizontalpodautoscaler.autoscaling/webhook     Deployment/webhook     cpu: &lt;unknown&gt;/100%   1         5         1          28m\n</code></pre> <p>Verify the installation for Istio</p> <pre><code>kubectl get all -n istio-system\n</code></pre> <p>Expected Output</p> <pre><code>kubectl get all -n istio-system\nNAME                                        READY   STATUS    RESTARTS   AGE\npod/istio-ingressgateway-767ff7b4b6-7wxzk   1/1     Running   0          12m\npod/istiod-7bc77d764b-vh66z                 1/1     Running   0          13m\n\nNAME                            TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                      AGE\nservice/istio-ingressgateway    LoadBalancer   10.97.252.211   &lt;pending&gt;     15021:32213/TCP,80:31540/TCP,443:30462/TCP   12m\nservice/istiod                  ClusterIP      10.96.206.177   &lt;none&gt;        15010/TCP,15012/TCP,443/TCP,15014/TCP        13m\nservice/knative-local-gateway   ClusterIP      10.101.124.46   &lt;none&gt;        80/TCP,443/TCP                               4m27s\n\nNAME                                   READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/istio-ingressgateway   1/1     1            1           12m\ndeployment.apps/istiod                 1/1     1            1           13m\n\nNAME                                              DESIRED   CURRENT   READY   AGE\nreplicaset.apps/istio-ingressgateway-767ff7b4b6   1         1         1       12m\nreplicaset.apps/istiod-7bc77d764b                 1         1         1       13m\n\nNAME                                                       REFERENCE                         TARGETS              MINPODS   MAXPODS   REPLICAS   AGE\nhorizontalpodautoscaler.autoscaling/istio-ingressgateway   Deployment/istio-ingressgateway   cpu: &lt;unknown&gt;/80%   1         5         1          12m\nhorizontalpodautoscaler.autoscaling/istiod                 Deployment/istiod                 cpu: &lt;unknown&gt;/80%   1         5         1          13m\n</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/installation/#configure-dns","title":"Configure DNS<sup>4</sup>","text":"<p>You can configure DNS to prevent the need to run <code>curl</code> commands with a host header. Knative provides a Kubernetes Job called default-domain that configures Knative Serving to use sslip.io as the default DNS suffix. This will only work if the cluster LoadBalancer Service exposes an IPv4 address or hostname, so it will not work with IPv6 clusters or local setups like minikube unless <code>minikube tunnel</code> is running.</p> <p>First, run the <code>minikube tunnel</code> command:</p> <pre><code>minikube tunnel\n</code></pre> <p>Expected Output</p> <pre><code>\u2705  Tunnel successfully started\n\n\ud83d\udccc  NOTE: Please do not close this terminal as this process must stay alive for the tunnel to be accessible ...\n\n\u2757  The service/ingress istio-ingressgateway requires privileged ports to be exposed: [80 443]\n\ud83d\udd11  sudo permission will be asked for it.\n\ud83c\udfc3  Starting tunnel for service istio-ingressgateway.\n</code></pre> <p>You can see that the Istio Ingress Gayeway now has external IP:</p> <pre><code>kubectl get svc istio-ingressgateway -n istio-system\n</code></pre> <p>Expected Output</p> <pre><code>NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                      AGE\nistio-ingressgateway   LoadBalancer   10.97.200.129   127.0.0.1     15021:31297/TCP,80:32665/TCP,443:30210/TCP   71m\n</code></pre> <p>Then we run the <code>default-domain</code> job to configure Knative Serving to use sslip.io as the default DNS suffix</p> <pre><code>kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.15.0/serving-default-domain.yaml\n</code></pre> <p>Expected Output</p> <pre><code>job.batch/default-domain created\nservice/default-domain-service created\n</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/installation/#install-cert-manager","title":"Install Cert Manager<sup>5</sup>","text":"<p>\u5b89\u88dd\u524d\uff0cnamespaces\u88e1\u6c92\u6709<code>cert-manager</code></p> <pre><code>kubectl get ns\n</code></pre> <pre><code>NAME              STATUS   AGE\ndefault           Active   15d\nistio-system      Active   35m\nknative-serving   Active   53m\nkube-node-lease   Active   15d\nkube-public       Active   15d\nkube-system       Active   15d\n</code></pre> <p>\u90e8\u7f72Cert Manager</p> <pre><code>kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.17.2/cert-manager.yaml\n</code></pre> Expected Output <pre><code>namespace/cert-manager created\ncustomresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created\ncustomresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created\ncustomresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created\ncustomresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created\ncustomresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created\ncustomresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created\nserviceaccount/cert-manager-cainjector created\nserviceaccount/cert-manager created\nserviceaccount/cert-manager-webhook created\nclusterrole.rbac.authorization.k8s.io/cert-manager-cainjector created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created\nclusterrole.rbac.authorization.k8s.io/cert-manager-cluster-view created\nclusterrole.rbac.authorization.k8s.io/cert-manager-view created\nclusterrole.rbac.authorization.k8s.io/cert-manager-edit created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests created\nclusterrole.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created\nrole.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created\nrole.rbac.authorization.k8s.io/cert-manager:leaderelection created\nrole.rbac.authorization.k8s.io/cert-manager-tokenrequest created\nrole.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created\nrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created\nrolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection created\nrolebinding.rbac.authorization.k8s.io/cert-manager-cert-manager-tokenrequest created\nrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created\nservice/cert-manager-cainjector created\nservice/cert-manager created\nservice/cert-manager-webhook created\ndeployment.apps/cert-manager-cainjector created\ndeployment.apps/cert-manager created\ndeployment.apps/cert-manager-webhook created\nmutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created\n</code></pre> <p>\u4f86\u770b\u770b\u90e8\u7f72\u4e86\u54ea\u4e9b\u6771\u897f</p> <pre><code>kubectl get ns\n</code></pre> <p>Expected Output</p> <pre><code>NAME              STATUS   AGE\ncert-manager      Active   2m59s\ndefault           Active   15d\nistio-system      Active   40m\nknative-serving   Active   58m\nkube-node-lease   Active   15d\nkube-public       Active   15d\nkube-system       Active   15d\n</code></pre> <pre><code>kubectl get all -n cert-manager\n</code></pre> <p>Expected Output</p> <pre><code>NAME                                           READY   STATUS    RESTARTS   AGE\npod/cert-manager-6687d8765c-v8zfd              1/1     Running   0          3m55s\npod/cert-manager-cainjector-764498cfc8-m7rs2   1/1     Running   0          3m55s\npod/cert-manager-webhook-74c74b87d7-dsz9x      1/1     Running   0          3m55s\n\nNAME                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)            AGE\nservice/cert-manager              ClusterIP   10.96.219.58     &lt;none&gt;        9402/TCP           3m55s\nservice/cert-manager-cainjector   ClusterIP   10.109.144.231   &lt;none&gt;        9402/TCP           3m55s\nservice/cert-manager-webhook      ClusterIP   10.107.111.91    &lt;none&gt;        443/TCP,9402/TCP   3m55s\n\nNAME                                      READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/cert-manager              1/1     1            1           3m55s\ndeployment.apps/cert-manager-cainjector   1/1     1            1           3m55s\ndeployment.apps/cert-manager-webhook      1/1     1            1           3m55s\n\nNAME                                                 DESIRED   CURRENT   READY   AGE\nreplicaset.apps/cert-manager-6687d8765c              1         1         1       3m55s\nreplicaset.apps/cert-manager-cainjector-764498cfc8   1         1         1       3m55s\nreplicaset.apps/cert-manager-webhook-74c74b87d7      1         1         1       3m55s\n</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/installation/#install-kserve","title":"Install KServe<sup>6</sup>","text":"<pre><code>kubectl create ns kserve\n</code></pre> <p>Expected Output</p> <pre><code>namespace/kserve created\n</code></pre> <p>Install KServe CRDs</p> <pre><code>helm install kserve-crd oci://ghcr.io/kserve/charts/kserve-crd \\\n  --version v0.15.1 \\\n  -n kserve\n</code></pre> <p>Expected Output</p> <pre><code>Pulled: ghcr.io/kserve/charts/kserve-crd:v0.15.1\nDigest: sha256:b5f4f22fae8fa747ef839e1b228e74e97a78416235eb5f35da49110d25b3d1e7\nNAME: kserve-crd\nLAST DEPLOYED: Thu May 22 22:02:37 2025\nNAMESPACE: kserve\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <p>Deploy KServe</p> <pre><code>helm install kserve oci://ghcr.io/kserve/charts/kserve \\\n  --version v0.15.1 \\\n  -n kserve\n</code></pre> <p>Expected Output</p> <pre><code>Pulled: ghcr.io/kserve/charts/kserve:v0.15.1\nDigest: sha256:e65039d9e91b16d429f5fb56528e15a4695ff106a41eeae07f1f697abe974bd5\nNAME: kserve\nLAST DEPLOYED: Thu May 22 22:02:52 2025\nNAMESPACE: kserve\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <pre><code>kubectl get all -n kserve\n</code></pre> <p>Expected Output</p> <pre><code>NAME                                             READY   STATUS    RESTARTS   AGE\npod/kserve-controller-manager-6cb87dcc55-2zrgm   2/2     Running   0          2m38s\npod/modelmesh-controller-6f5bdb97db-878bb        1/1     Running   0          2m38s\n\nNAME                                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE\nservice/kserve-controller-manager-service   ClusterIP   10.97.113.121    &lt;none&gt;        8443/TCP                     2m38s\nservice/kserve-webhook-server-service       ClusterIP   10.103.116.193   &lt;none&gt;        443/TCP                      2m38s\nservice/modelmesh-serving                   ClusterIP   None             &lt;none&gt;        8033/TCP,8008/TCP,2112/TCP   2m4s\nservice/modelmesh-webhook-server-service    ClusterIP   10.97.175.193    &lt;none&gt;        9443/TCP                     2m38s\n\nNAME                                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/kserve-controller-manager          1/1     1            1           2m38s\ndeployment.apps/modelmesh-controller               1/1     1            1           2m38s\ndeployment.apps/modelmesh-serving-mlserver-1.x     0/0     0            0           2m4s\ndeployment.apps/modelmesh-serving-ovms-1.x         0/0     0            0           2m4s\ndeployment.apps/modelmesh-serving-torchserve-0.x   0/0     0            0           2m4s\ndeployment.apps/modelmesh-serving-triton-2.x       0/0     0            0           2m4s\n\nNAME                                                          DESIRED   CURRENT   READY   AGE\nreplicaset.apps/kserve-controller-manager-6cb87dcc55          1         1         1       2m38s\nreplicaset.apps/modelmesh-controller-6f5bdb97db               1         1         1       2m38s\nreplicaset.apps/modelmesh-serving-mlserver-1.x-57d65d9fdd     0         0         0       2m4s\nreplicaset.apps/modelmesh-serving-ovms-1.x-5488c8f4f9         0         0         0       2m4s\nreplicaset.apps/modelmesh-serving-torchserve-0.x-67f9485cb9   0         0         0       2m4s\nreplicaset.apps/modelmesh-serving-triton-2.x-66756bc646       0         0         0       2m4s\n</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/installation/#summary","title":"Summary","text":"Install and Uninstall KServe Scripts install.sh<pre><code>#!/bin/bash\n\nset -e\n\necho \"\ud83d\ude80 Installing Knative + Istio + KServe...\"\n\n# Install Knative Serving CRDs and Core\nkubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.15.0/serving-crds.yaml\nkubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.15.0/serving-core.yaml\n\n# Download and install Istio\ncurl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.22.8 DESTDIR=$HOME sh -\nexport PATH=\"$PATH:$HOME/istio-1.22.8/bin\"\n\n# Add Istio to .zshrc if not already present\ngrep -qxF 'export PATH=\"$PATH:$HOME/istio-1.22.8/bin\"' ~/.zshrc || echo 'export PATH=\"$PATH:$HOME/istio-1.22.8/bin\"' &gt;&gt; ~/.zshrc\n\n# Install Istio components\n$HOME/istio-1.22.8/bin/istioctl install -y\n\n# Install Knative net-istio integration\nkubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.15.0/net-istio.yaml\n\n# Install cert-manager\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.17.2/cert-manager.yaml\n\n# Create namespace for KServe (if not already exists)\nkubectl create ns kserve --dry-run=client -o yaml | kubectl apply -f -\n\n# Install KServe CRDs and components\nhelm install kserve-crd oci://ghcr.io/kserve/charts/kserve-crd \\\n  --version v0.15.1 \\\n  -n kserve\n\nhelm install kserve oci://ghcr.io/kserve/charts/kserve \\\n  --version v0.15.1 \\\n  -n kserve\n\necho \"\u2705 Installation complete. You may want to run:\"\necho \"   source ~/.zshrc\"\n</code></pre> uninstall.sh<pre><code>#!/bin/bash\n\nset -e\n\necho \"Uninstalling KServe, Knative, Istio, and Cert-Manager...\"\n\n# Uninstall KServe\nhelm uninstall kserve -n kserve || echo \"kserve not found\"\nhelm uninstall kserve-crd -n kserve || echo \"kserve-crd not found\"\nkubectl delete ns kserve --ignore-not-found\n\n# Uninstall cert-manager\nkubectl delete -f https://github.com/cert-manager/cert-manager/releases/download/v1.17.2/cert-manager.yaml || echo \"cert-manager not installed\"\nkubectl delete ns cert-manager --ignore-not-found\n\n# Uninstall Knative net-istio and serving\nkubectl delete -f https://github.com/knative/net-istio/releases/download/knative-v1.15.0/net-istio.yaml || echo \"net-istio not installed\"\nkubectl delete -f https://github.com/knative/serving/releases/download/knative-v1.15.0/serving-core.yaml || echo \"knative-core not installed\"\nkubectl delete -f https://github.com/knative/serving/releases/download/knative-v1.15.0/serving-crds.yaml || echo \"knative-crds not installed\"\n\n# Uninstall Istio\n$HOME/istio-1.22.8/bin/istioctl uninstall --purge -y || echo \"istioctl uninstall failed\"\nkubectl delete ns istio-system --ignore-not-found\n\n# Optional: Remove istioctl binary and path from .zshrc\necho \"Cleaning up istioctl binary and .zshrc entry...\"\nsed -i '' '/istio-1.22.8\\/bin/d' ~/.zshrc 2&gt;/dev/null || true\nrm -rf $HOME/istio-1.22.8\n\necho \"Uninstallation completed.\"\n</code></pre> <ol> <li> <p>Serverless Installation Guide \u21a9</p> </li> <li> <p>Installing Knative Serving using YAML files \u21a9</p> </li> <li> <p>Installing Istio for Knative \u21a9\u21a9</p> </li> <li> <p>Configure DNS \u21a9</p> </li> <li> <p>Install Cert Manager \u21a9</p> </li> <li> <p>Install KServe using Helm \u21a9</p> </li> </ol>"},{"location":"side-projects/data2ml-ops/kserve/transformer-advantages/","title":"Move Feature Retrieval to KServe Transformer","text":"<p>In real-time fraud detection, response time and consistency are critical. When serving machine learning models on Kubernetes, the way you integrate feature lookups into your inference flow can have significant architectural implications. Let\u2019s explore two setups and highlight the benefits of offloading feature retrieval to KServe\u2019s built-in Transformer component by using the Feast online feature server.</p>"},{"location":"side-projects/data2ml-ops/kserve/transformer-advantages/#two-approaches-to-feature-retrieval","title":"Two Approaches to Feature Retrieval","text":""},{"location":"side-projects/data2ml-ops/kserve/transformer-advantages/#option-1-feature-lookup-in-backend-no-transformer","title":"Option 1: Feature Lookup in Backend (No Transformer)","text":"<p>In this setup, the Backend is responsible for querying the Online Feature Server (OFS), retrieving features based on an input ID (e.g., <code>entity_id</code>), and passing the features to the predictor.</p> <pre><code>sequenceDiagram\n    participant User as User\n    participant FE as Frontend\n    participant BE as Backend\n    participant Predictor as InferenceService:Predictor\n    participant OFS as Online Feature Server\n\n    User-&gt;&gt;FE: Trigger prediction\n    FE-&gt;&gt;BE: Send request (entity_id)\n    BE-&gt;&gt;OFS: Lookup features by entity_id (PREPROCESSING)\n    OFS--&gt;&gt;BE: Return features (PREPROCESSING)\n    BE-&gt;&gt;Predictor: Send features for prediction (PREDICTION)\n    Predictor--&gt;&gt;BE: Return prediction result (PREDICTION)\n    BE--&gt;&gt;FE: Return prediction\n    FE--&gt;&gt;User: Display result</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/transformer-advantages/#option-2-feature-lookup-via-transformer","title":"Option 2: Feature Lookup via Transformer","text":"<p>Here, the backend simply passes the raw ID to the InferenceService. The Transformer performs the feature lookup before passing features to the predictor.</p> <pre><code>sequenceDiagram\n    participant User as User\n    participant FE as Frontend\n    participant BE as Backend\n    participant Transformer as InferenceService:Transformer\n    participant Predictor as InferenceService:Predictor\n    participant OFS as Online Feature Server\n\n    User-&gt;&gt;FE: Trigger prediction\n    FE-&gt;&gt;BE: Send request (entity_id)\n    BE-&gt;&gt;Transformer: Send request (entity_id)\n    Transformer-&gt;&gt;OFS: Lookup features by entity_id (PREPROCESSING)\n    OFS--&gt;&gt;Transformer: Return features (PREPROCESSING)\n    Transformer-&gt;&gt;Predictor: Send features for prediction (PREDICTION)\n    Predictor--&gt;&gt;Transformer: Return prediction result (PREDICTION)\n    Transformer--&gt;&gt;BE: Return prediction result\n    BE--&gt;&gt;FE: Return prediction\n    FE--&gt;&gt;User: Display result</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/transformer-advantages/#why-move-feature-retrieval-logic-from-backend-to-transformer","title":"Why Move Feature Retrieval Logic from Backend to Transformer?","text":"<p>Switching to the second architecture\u2014embedding the feature lookup within KServe\u2019s inference pipeline using its Transformer\u2014brings several advantages:</p>"},{"location":"side-projects/data2ml-ops/kserve/transformer-advantages/#simpler-backend-logic","title":"Simpler Backend Logic","text":"<p>In the first setup, the backend must:</p> <ul> <li>Know which features to request.</li> <li>Understand how to call the online feature server.</li> <li>Deal with errors or retries.</li> </ul> <p>In the second setup, the backend becomes a simple pass-through: it just sends a entity_id. The InferenceService encapsulates all feature logic, making the backend more lightweight and maintainable.</p>"},{"location":"side-projects/data2ml-ops/kserve/transformer-advantages/#better-reusability-and-portability","title":"Better Reusability and Portability","text":"<p>Decoupling the feature logic from the backend means multiple clients (mobile, web, partner APIs) can use the same InferenceService without duplicating feature retrieval logic. You can also deploy the same model in different environments without rewriting backend code.</p>"},{"location":"side-projects/data2ml-ops/kserve/transformer-advantages/#stronger-data-security-in-backend","title":"Stronger Data Security in Backend","text":"<p>By shifting feature retrieval logic to the KServe transformer, you ensure that the backend never directly accesses or handles sensitive feature data. This separation minimizes data exposure risks. For example, in fraud detection, features like historical average transaction amounts may be sensitive and should be kept within the model serving infrastructure.</p>"},{"location":"side-projects/data2ml-ops/kserve/transformer-advantages/#summary","title":"Summary","text":"<p>Moving feature retrieval from the backend to a transformer in KServe\u2019s InferenceService creates a cleaner, more consistent, and scalable architecture for real-time fraud detection. With Feast and KServe, it\u2019s easier than ever to centralize feature logic and productionize your models efficiently.</p>"},{"location":"side-projects/data2ml-ops/kserve/what-why-when/","title":"What, Why, When","text":""},{"location":"side-projects/data2ml-ops/kserve/what-why-when/#what-is-kserve","title":"What is KServe?","text":"<p>KServe is an open-source, Kubernetes-native platform designed to streamline the deployment and management of machine learning (ML) models at scale. It provides a standardized interface for serving models across various ML frameworks, including TensorFlow, PyTorch, XGBoost, scikit-learn, ONNX, and even large language models (LLMs)<sup>1</sup>.</p> <p>Built upon Kubernetes and Knative, KServe offers serverless capabilities such as autoscaling (including scaling down to zero)<sup>2</sup>, canary rollouts<sup>3</sup>, and model versioning. This architecture abstracts the complexities of infrastructure management, allowing data scientists and ML engineers to focus on developing and deploying models without delving into the intricacies of Kubernetes configurations.</p> <p>For a comprehensive introduction to KServe, consider watching the following video:</p>"},{"location":"side-projects/data2ml-ops/kserve/what-why-when/#why-kserve","title":"Why KServe?","text":"<p>KServe caters to various roles within the ML lifecycle, offering tailored benefits:</p> <p>For Data Scientists, With KServe's standardized APIs and support for multiple ML frameworks, data scientists can deploy models without worrying about the underlying infrastructure. Features like model explainability<sup>4</sup> and inference graphs aid in understanding and refining model behavior.</p> <p>For ML Engineers, KServe provides advanced deployment strategies, including canary rollouts and traffic splitting<sup>3</sup>, facilitating safe and controlled model updates. Its integration with monitoring tools like Prometheus and Grafana ensures observability and performance tracking<sup>5</sup><sup>6</sup>.</p> <p>For MLOps Teams, By leveraging Kubernetes' scalability and KServe's serverless capabilities, MLOps teams can manage model deployments efficiently across different environments, ensuring high availability and reliability.</p>"},{"location":"side-projects/data2ml-ops/kserve/what-why-when/#when-to-use-kserve","title":"When to Use KServe?","text":""},{"location":"side-projects/data2ml-ops/kserve/what-why-when/#deploying-models-across-diverse-frameworks","title":"Deploying Models Across Diverse Frameworks","text":"<p>When working with a variety of ML frameworks, KServe's standardized serving interface<sup>7</sup> allows for consistent deployment practices, reducing the overhead of managing different serving solutions.</p>"},{"location":"side-projects/data2ml-ops/kserve/what-why-when/#scaling-inference-services-based-on-demand","title":"Scaling Inference Services Based on Demand","text":"<p>For applications with fluctuating traffic patterns, KServe's autoscaling features, including scaling down to zero during idle periods, ensure cost-effective resource utilization while maintaining responsiveness<sup>2</sup>.</p>"},{"location":"side-projects/data2ml-ops/kserve/what-why-when/#implementing-safe-and-controlled-model-updates","title":"Implementing Safe and Controlled Model Updates","text":"<p>In scenarios requiring gradual model rollouts, KServe's support for canary deployments and traffic splitting enables testing new model versions with a subset of traffic before full-scale deployment<sup>3</sup>.</p>"},{"location":"side-projects/data2ml-ops/kserve/what-why-when/#managing-complex-inference-pipelines","title":"Managing Complex Inference Pipelines","text":"<p>When dealing with intricate inference workflows involving preprocessing, postprocessing<sup>8</sup>, or chaining multiple models, KServe's inference graph<sup>9</sup> feature allows for the composition of such pipelines, enhancing modularity and maintainability.</p> <ol> <li> <p>Model Serving Runtimes | KServe Docs \u21a9</p> </li> <li> <p>KServe GitHub Repository \u21a9\u21a9</p> </li> <li> <p>Exploring ML Model Serving with KServe (YouTube) \u21a9\u21a9\u21a9</p> </li> <li> <p>KServe: Highly Scalable Machine Learning Deployment with Kubernetes \u21a9</p> </li> <li> <p>KServe: Streamlining Machine Learning Model Serving in Kubernetes \u21a9</p> </li> <li> <p>Grafana Dashboards \u21a9</p> </li> <li> <p>Open Inference Protocol (V2 Inference Protocol) \u21a9</p> </li> <li> <p>How to write a custom transformer \u21a9</p> </li> <li> <p>Inference Graph \u21a9</p> </li> </ol>"},{"location":"side-projects/data2ml-ops/minio/deployment/","title":"Deploy MinIO on Kubernetes","text":"Architecture (Click to Enlarge)"},{"location":"side-projects/data2ml-ops/minio/deployment/#deployment","title":"Deployment","text":"minio.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio\n  namespace: minio\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n        - name: minio\n          image: minio/minio\n          args:\n            - server\n            - /data\n            - --console-address\n            - :9001\n          env:\n            - name: MINIO_ROOT_USER\n              value: minio_user\n            - name: MINIO_ROOT_PASSWORD\n              value: minio_password\n          ports:\n            - containerPort: 9000\n              protocol: TCP\n            - containerPort: 9001\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /minio/health/live\n              port: 9000\n            initialDelaySeconds: 30\n            periodSeconds: 20\n            timeoutSeconds: 15\n            failureThreshold: 6\n          readinessProbe:\n            httpGet:\n              path: /minio/health/ready\n              port: 9000\n            initialDelaySeconds: 15\n            periodSeconds: 10\n            timeoutSeconds: 10\n            failureThreshold: 3\n          volumeMounts:\n            - name: storage\n              mountPath: /data\n      volumes:\n        - name: storage\n          hostPath:\n            path: /home/docker/data/minio\n            type: DirectoryOrCreate\n      restartPolicy: Always\n</code></pre>"},{"location":"side-projects/data2ml-ops/minio/deployment/#job","title":"Job","text":"minio.yaml<pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: minio-create-bucket\n  namespace: minio\nspec:\n  backoffLimit: 6\n  completions: 1\n  template:\n    metadata:\n      labels:\n        job: minio-create-bucket\n    spec:\n      initContainers:\n        - name: wait-for-minio\n          image: busybox\n          command:\n            - sh\n            - -c\n            - |\n              until nc -z minio-api.minio.svc.cluster.local 9000; do\n                echo \"Waiting for MinIO...\"\n                sleep 2\n              done\n              echo \"MinIO is ready!\"\n      containers:\n        - name: minio-create-buckets\n          image: minio/mc\n          command:\n            - sh\n            - -c\n            - |\n              mc alias set minio http://minio-api.minio.svc.cluster.local:9000 minio_user minio_password &amp;&amp;\n              for bucket in mlflow dbt sqlmesh ray; do\n                if ! mc ls minio/$bucket &gt;/dev/null 2&gt;&amp;1; then\n                  echo \"Creating bucket: $bucket\"\n                  mc mb minio/$bucket\n                  echo \"Bucket created: $bucket\"\n                else\n                  echo \"Bucket already exists: $bucket\"\n                fi\n              done\n      restartPolicy: OnFailure\n      terminationGracePeriodSeconds: 30\n</code></pre>"},{"location":"side-projects/data2ml-ops/minio/deployment/#services","title":"Services","text":"minio.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: minio-console\n  namespace: minio\nspec:\n  selector:\n    app: minio\n  type: NodePort\n  ports:\n    - name: console\n      port: 9001\n      targetPort: 9001\n      nodePort: 30901\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: minio-api\n  namespace: minio\nspec:\n  selector:\n    app: minio\n  type: ClusterIP\n  ports:\n    - name: api\n      port: 9000\n      targetPort: 9000\n</code></pre>"},{"location":"side-projects/data2ml-ops/minio/what-why-when/","title":"What, Why, When","text":""},{"location":"side-projects/data2ml-ops/mlflow/deployment/","title":"Deploy MLflow on Kubernetes","text":"Architecture (Click to Enlarge)"},{"location":"side-projects/data2ml-ops/mlflow/deployment/#artifact-store-minio","title":"Artifact Store (MinIO)","text":"<p>First deploy an S3-compatible object store - MinIO for our MLflow artifact store to store artifacts like figures, models, reports, etc. See here for deploying MinIO.</p> <p>After deploying MinIO and the <code>mlflow</code> bucket created, in MLflow's helm chart, we could specify artifact store's configuration</p> values.yaml<pre><code>artifactStore:\n  name: minio-api # API Service name for MinIO\n  namespace: minio\n  user: minio_user\n  password: minio_password\n  apiPort: 9000\n  bucketName: mlflow\n  hostPath: /home/docker/data/minio\n  mountPath: /data\n</code></pre>"},{"location":"side-projects/data2ml-ops/mlflow/deployment/#backend-store","title":"Backend Store","text":"values.yaml<pre><code>backendStore:\n  name: backend-store\n  db: mlflow\n  user: user\n  password: password\n  host: postgres\n  port: 5432\n  hostPath: /home/docker/data/mlflow/backend-store\n  mountPath: /var/lib/postgresql/data\n</code></pre> backend-store.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ .Values.backendStore.name }}\n  namespace: {{ .Release.Namespace }}\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: {{ .Values.backendStore.name }}\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: {{ .Values.backendStore.name }}\n    spec:\n      containers:\n        - name: {{ .Values.backendStore.name }}\n          image: postgres:latest\n          env:\n            - name: POSTGRES_DB\n              value: {{ .Values.backendStore.db }}\n            - name: POSTGRES_USER\n              value: {{ .Values.backendStore.user }}\n            - name: POSTGRES_PASSWORD\n              value: {{ .Values.backendStore.password }}\n          ports:\n            - containerPort: {{ .Values.backendStore.port }}\n              protocol: TCP\n          volumeMounts:\n            - name: storage\n              mountPath: {{ .Values.backendStore.mountPath }}\n      restartPolicy: Always\n      volumes:\n        - name: storage\n          hostPath:\n            path: {{ .Values.backendStore.hostPath }}\n            type: DirectoryOrCreate\n</code></pre> backend-store.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: {{ .Values.backendStore.name }}\n  namespace: {{ .Release.Namespace }}\nspec:\n  selector:\n    app: {{ .Values.backendStore.name }}\n  type: ClusterIP\n  ports:\n    - port: {{ .Values.backendStore.port }}\n      targetPort: {{ .Values.backendStore.port }}\n</code></pre>"},{"location":"side-projects/data2ml-ops/mlflow/deployment/#tracking-server","title":"Tracking Server","text":"values.yaml<pre><code>trackingServer:\n  name: tracking-server\n  host: 0.0.0.0\n  port: 5000\n</code></pre> tracking-server.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ .Values.trackingServer.name }}\n  namespace: {{ .Release.Namespace }}\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: {{ .Values.trackingServer.name }}\n  template:\n    metadata:\n      labels:\n        app: {{ .Values.trackingServer.name }}\n    spec:\n      initContainers:\n        - name: wait-for-backend-store\n          image: busybox\n          command:\n            - sh\n            - -c\n            - |\n              until nc -z {{ .Values.backendStore.name }}.{{ .Release.Namespace }}.svc.cluster.local {{ .Values.backendStore.port }}; do\n                echo \"Waiting for backend store...\"\n                sleep 2\n              done\n              echo \"Backend store is ready!\"\n        - name: wait-for-artifact-store\n          image: busybox\n          command:\n            - sh\n            - -c\n            - |\n              until nc -z {{ .Values.artifactStore.name }}.{{ .Values.artifactStore.namespace }}.svc.cluster.local {{ .Values.artifactStore.apiPort }}; do\n                echo \"Waiting for artifact store...\"\n                sleep 2\n              done\n              echo \"Artifact store is ready!\"\n      containers:\n        - name: {{ .Values.trackingServer.name }}\n          image: bitnami/mlflow:2.22.0\n          env:\n            - name: MLFLOW_S3_ENDPOINT_URL\n              value: http://{{ .Values.artifactStore.name }}.{{ .Values.artifactStore.namespace }}.svc.cluster.local:{{ .Values.artifactStore.apiPort }}\n            - name: AWS_ACCESS_KEY_ID\n              value: {{ .Values.artifactStore.user }}\n            - name: AWS_SECRET_ACCESS_KEY\n              value: {{ .Values.artifactStore.password }}\n            - name: MLFLOW_S3_IGNORE_TLS\n              value: \"true\"\n          command: [\"mlflow\"]\n          args:\n            [\n              \"server\",\n              \"--backend-store-uri\", \"postgresql://{{ .Values.backendStore.user }}:{{ .Values.backendStore.password }}@{{ .Values.backendStore.name }}:{{ .Values.backendStore.port }}/{{ .Values.backendStore.db }}\",\n              \"--artifacts-destination\", \"s3://{{ .Values.artifactStore.bucketName }}\",\n              \"--host\", \"{{ .Values.trackingServer.host }}\",\n              \"--port\", \"{{ .Values.trackingServer.port }}\",\n            ]\n          ports:\n            - containerPort: {{ .Values.trackingServer.port }}\n</code></pre> tracking-server.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: {{ .Values.trackingServer.name }}\n  namespace: {{ .Release.Namespace }}\nspec:\n  type: NodePort\n  selector:\n    app: {{ .Values.trackingServer.name }}\n  ports:\n    - port: {{ .Values.trackingServer.port }}\n      targetPort: {{ .Values.trackingServer.port }}\n      nodePort: 30500\n</code></pre>"},{"location":"side-projects/data2ml-ops/mlflow/how-it-works/","title":"How It Works?","text":"<p>__</p>"},{"location":"side-projects/data2ml-ops/mlflow/how-it-works/#core-concepts","title":"Core Concepts","text":""},{"location":"side-projects/data2ml-ops/mlflow/how-it-works/#architecture-components","title":"Architecture Components","text":""},{"location":"side-projects/data2ml-ops/mlflow/how-it-works/#key-modules-and-their-roles","title":"Key Modules and Their Roles","text":""},{"location":"side-projects/data2ml-ops/mlflow/how-it-works/#behind-the-scenes","title":"Behind the Scenes","text":""},{"location":"side-projects/data2ml-ops/mlflow/hpo/","title":"Track Hyperparameter Optimization with Optuna and MLflow","text":"In\u00a0[1]: Copied! <pre>import math\nimport logging\nfrom datetime import datetime, timedelta\n\nimport numpy as np\nimport optuna\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nimport mlflow\n</pre> import math import logging from datetime import datetime, timedelta  import numpy as np import optuna import pandas as pd import xgboost as xgb from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split  import mlflow <pre>/Users/kcl/.venvs/feast/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre>MLFLOW_TRACKING_URI = \"http://127.0.0.1:50666\"\nmlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n</pre> MLFLOW_TRACKING_URI = \"http://127.0.0.1:50666\" mlflow.set_tracking_uri(MLFLOW_TRACKING_URI) In\u00a0[3]: Copied! <pre>logger = logging.getLogger(\"mlflow\")\nlogger.setLevel(logging.WARNING)\n</pre> logger = logging.getLogger(\"mlflow\") logger.setLevel(logging.WARNING) In\u00a0[4]: Copied! <pre>def generate_apple_sales_data_with_promo_adjustment(\n    base_demand: int = 1000,\n    n_rows: int = 5000,\n    competitor_price_effect: float = -50.0,\n):\n    \"\"\"\n    Generates a synthetic dataset for predicting apple sales demand with multiple\n    influencing factors.\n\n    This function creates a pandas DataFrame with features relevant to apple sales.\n    The features include date, average_temperature, rainfall, weekend flag, holiday flag,\n    promotional flag, price_per_kg, competitor's price, marketing intensity, stock availability,\n    and the previous day's demand. The target variable, 'demand', is generated based on a\n    combination of these features with some added noise.\n\n    Args:\n        base_demand (int, optional): Base demand for apples. Defaults to 1000.\n        n_rows (int, optional): Number of rows (days) of data to generate. Defaults to 5000.\n        competitor_price_effect (float, optional): Effect of competitor's price being lower\n                                                   on our sales. Defaults to -50.\n\n    Returns:\n        pd.DataFrame: DataFrame with features and target variable for apple sales prediction.\n\n    Example:\n        &gt;&gt;&gt; df = generate_apple_sales_data_with_promo_adjustment(base_demand=1200, n_rows=6000)\n        &gt;&gt;&gt; df.head()\n    \"\"\"\n\n    # Set seed for reproducibility\n    np.random.seed(9999)\n\n    # Create date range\n    dates = [datetime.now() - timedelta(days=i) for i in range(n_rows)]\n    dates.reverse()\n\n    # Generate features\n    df = pd.DataFrame(\n        {\n            \"date\": dates,\n            \"average_temperature\": np.random.uniform(10, 35, n_rows),\n            \"rainfall\": np.random.exponential(5, n_rows),\n            \"weekend\": [(date.weekday() &gt;= 5) * 1 for date in dates],\n            \"holiday\": np.random.choice([0, 1], n_rows, p=[0.97, 0.03]),\n            \"price_per_kg\": np.random.uniform(0.5, 3, n_rows),\n            \"month\": [date.month for date in dates],\n        }\n    )\n\n    # Introduce inflation over time (years)\n    df[\"inflation_multiplier\"] = 1 + (df[\"date\"].dt.year - df[\"date\"].dt.year.min()) * 0.03\n\n    # Incorporate seasonality due to apple harvests\n    df[\"harvest_effect\"] = np.sin(2 * np.pi * (df[\"month\"] - 3) / 12) + np.sin(\n        2 * np.pi * (df[\"month\"] - 9) / 12\n    )\n\n    # Modify the price_per_kg based on harvest effect\n    df[\"price_per_kg\"] = df[\"price_per_kg\"] - df[\"harvest_effect\"] * 0.5\n\n    # Adjust promo periods to coincide with periods lagging peak harvest by 1 month\n    peak_months = [4, 10]  # months following the peak availability\n    df[\"promo\"] = np.where(\n        df[\"month\"].isin(peak_months),\n        1,\n        np.random.choice([0, 1], n_rows, p=[0.85, 0.15]),\n    )\n\n    # Generate target variable based on features\n    base_price_effect = -df[\"price_per_kg\"] * 50\n    seasonality_effect = df[\"harvest_effect\"] * 50\n    promo_effect = df[\"promo\"] * 200\n\n    df[\"demand\"] = (\n        base_demand\n        + base_price_effect\n        + seasonality_effect\n        + promo_effect\n        + df[\"weekend\"] * 300\n        + np.random.normal(0, 50, n_rows)\n    ) * df[\"inflation_multiplier\"]  # adding random noise\n\n    # Add previous day's demand\n    df[\"previous_days_demand\"] = df[\"demand\"].shift(1)\n    df[\"previous_days_demand\"].fillna(method=\"bfill\", inplace=True)  # fill the first row\n\n    # Introduce competitor pricing\n    df[\"competitor_price_per_kg\"] = np.random.uniform(0.5, 3, n_rows)\n    df[\"competitor_price_effect\"] = (\n        df[\"competitor_price_per_kg\"] &lt; df[\"price_per_kg\"]\n    ) * competitor_price_effect\n\n    # Stock availability based on past sales price (3 days lag with logarithmic decay)\n    log_decay = -np.log(df[\"price_per_kg\"].shift(3) + 1) + 2\n    df[\"stock_available\"] = np.clip(log_decay, 0.7, 1)\n\n    # Marketing intensity based on stock availability\n    # Identify where stock is above threshold\n    high_stock_indices = df[df[\"stock_available\"] &gt; 0.95].index\n\n    # For each high stock day, increase marketing intensity for the next week\n    for idx in high_stock_indices:\n        df.loc[idx : min(idx + 7, n_rows - 1), \"marketing_intensity\"] = np.random.uniform(0.7, 1)\n\n    # If the marketing_intensity column already has values, this will preserve them;\n    #  if not, it sets default values\n    fill_values = pd.Series(np.random.uniform(0, 0.5, n_rows), index=df.index)\n    df[\"marketing_intensity\"].fillna(fill_values, inplace=True)\n\n    # Adjust demand with new factors\n    df[\"demand\"] = df[\"demand\"] + df[\"competitor_price_effect\"] + df[\"marketing_intensity\"]\n\n    # Drop temporary columns\n    df.drop(\n        columns=[\n            \"inflation_multiplier\",\n            \"harvest_effect\",\n            \"month\",\n            \"competitor_price_effect\",\n            \"stock_available\",\n        ],\n        inplace=True,\n    )\n\n    return df\n</pre> def generate_apple_sales_data_with_promo_adjustment(     base_demand: int = 1000,     n_rows: int = 5000,     competitor_price_effect: float = -50.0, ):     \"\"\"     Generates a synthetic dataset for predicting apple sales demand with multiple     influencing factors.      This function creates a pandas DataFrame with features relevant to apple sales.     The features include date, average_temperature, rainfall, weekend flag, holiday flag,     promotional flag, price_per_kg, competitor's price, marketing intensity, stock availability,     and the previous day's demand. The target variable, 'demand', is generated based on a     combination of these features with some added noise.      Args:         base_demand (int, optional): Base demand for apples. Defaults to 1000.         n_rows (int, optional): Number of rows (days) of data to generate. Defaults to 5000.         competitor_price_effect (float, optional): Effect of competitor's price being lower                                                    on our sales. Defaults to -50.      Returns:         pd.DataFrame: DataFrame with features and target variable for apple sales prediction.      Example:         &gt;&gt;&gt; df = generate_apple_sales_data_with_promo_adjustment(base_demand=1200, n_rows=6000)         &gt;&gt;&gt; df.head()     \"\"\"      # Set seed for reproducibility     np.random.seed(9999)      # Create date range     dates = [datetime.now() - timedelta(days=i) for i in range(n_rows)]     dates.reverse()      # Generate features     df = pd.DataFrame(         {             \"date\": dates,             \"average_temperature\": np.random.uniform(10, 35, n_rows),             \"rainfall\": np.random.exponential(5, n_rows),             \"weekend\": [(date.weekday() &gt;= 5) * 1 for date in dates],             \"holiday\": np.random.choice([0, 1], n_rows, p=[0.97, 0.03]),             \"price_per_kg\": np.random.uniform(0.5, 3, n_rows),             \"month\": [date.month for date in dates],         }     )      # Introduce inflation over time (years)     df[\"inflation_multiplier\"] = 1 + (df[\"date\"].dt.year - df[\"date\"].dt.year.min()) * 0.03      # Incorporate seasonality due to apple harvests     df[\"harvest_effect\"] = np.sin(2 * np.pi * (df[\"month\"] - 3) / 12) + np.sin(         2 * np.pi * (df[\"month\"] - 9) / 12     )      # Modify the price_per_kg based on harvest effect     df[\"price_per_kg\"] = df[\"price_per_kg\"] - df[\"harvest_effect\"] * 0.5      # Adjust promo periods to coincide with periods lagging peak harvest by 1 month     peak_months = [4, 10]  # months following the peak availability     df[\"promo\"] = np.where(         df[\"month\"].isin(peak_months),         1,         np.random.choice([0, 1], n_rows, p=[0.85, 0.15]),     )      # Generate target variable based on features     base_price_effect = -df[\"price_per_kg\"] * 50     seasonality_effect = df[\"harvest_effect\"] * 50     promo_effect = df[\"promo\"] * 200      df[\"demand\"] = (         base_demand         + base_price_effect         + seasonality_effect         + promo_effect         + df[\"weekend\"] * 300         + np.random.normal(0, 50, n_rows)     ) * df[\"inflation_multiplier\"]  # adding random noise      # Add previous day's demand     df[\"previous_days_demand\"] = df[\"demand\"].shift(1)     df[\"previous_days_demand\"].fillna(method=\"bfill\", inplace=True)  # fill the first row      # Introduce competitor pricing     df[\"competitor_price_per_kg\"] = np.random.uniform(0.5, 3, n_rows)     df[\"competitor_price_effect\"] = (         df[\"competitor_price_per_kg\"] &lt; df[\"price_per_kg\"]     ) * competitor_price_effect      # Stock availability based on past sales price (3 days lag with logarithmic decay)     log_decay = -np.log(df[\"price_per_kg\"].shift(3) + 1) + 2     df[\"stock_available\"] = np.clip(log_decay, 0.7, 1)      # Marketing intensity based on stock availability     # Identify where stock is above threshold     high_stock_indices = df[df[\"stock_available\"] &gt; 0.95].index      # For each high stock day, increase marketing intensity for the next week     for idx in high_stock_indices:         df.loc[idx : min(idx + 7, n_rows - 1), \"marketing_intensity\"] = np.random.uniform(0.7, 1)      # If the marketing_intensity column already has values, this will preserve them;     #  if not, it sets default values     fill_values = pd.Series(np.random.uniform(0, 0.5, n_rows), index=df.index)     df[\"marketing_intensity\"].fillna(fill_values, inplace=True)      # Adjust demand with new factors     df[\"demand\"] = df[\"demand\"] + df[\"competitor_price_effect\"] + df[\"marketing_intensity\"]      # Drop temporary columns     df.drop(         columns=[             \"inflation_multiplier\",             \"harvest_effect\",             \"month\",             \"competitor_price_effect\",             \"stock_available\",         ],         inplace=True,     )      return df In\u00a0[5]: Copied! <pre>df = generate_apple_sales_data_with_promo_adjustment(base_demand=1_000, n_rows=5000)\ndf\n</pre> df = generate_apple_sales_data_with_promo_adjustment(base_demand=1_000, n_rows=5000) df <pre>/var/folders/zs/82l0dwfx1rdgz3490g0n3_qw0000gn/T/ipykernel_3699/1129670120.py:85: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df[\"previous_days_demand\"].fillna(method=\"bfill\", inplace=True)  # fill the first row\n/var/folders/zs/82l0dwfx1rdgz3490g0n3_qw0000gn/T/ipykernel_3699/1129670120.py:85: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  df[\"previous_days_demand\"].fillna(method=\"bfill\", inplace=True)  # fill the first row\n/var/folders/zs/82l0dwfx1rdgz3490g0n3_qw0000gn/T/ipykernel_3699/1129670120.py:108: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df[\"marketing_intensity\"].fillna(fill_values, inplace=True)\n</pre> Out[5]: date average_temperature rainfall weekend holiday price_per_kg promo demand previous_days_demand competitor_price_per_kg marketing_intensity 0 2011-08-24 16:51:35.064975 30.584727 1.199291 0 0 1.726258 0 851.375336 851.276659 1.935346 0.098677 1 2011-08-25 16:51:35.064973 15.465069 1.037626 0 0 0.576471 0 906.855943 851.276659 2.344720 0.019318 2 2011-08-26 16:51:35.064971 10.786525 5.656089 0 0 2.513328 0 808.304909 906.836626 0.998803 0.409485 3 2011-08-27 16:51:35.064970 23.648154 12.030937 1 0 1.839225 0 1099.833810 857.895424 0.761740 0.872803 4 2011-08-28 16:51:35.064967 13.861391 4.303812 1 0 1.531772 0 1283.949061 1148.961007 2.123436 0.820779 ... ... ... ... ... ... ... ... ... ... ... ... 4995 2025-04-27 16:51:35.054780 21.643051 3.821656 1 0 2.391010 1 1875.882437 1880.799278 1.504432 0.756489 4996 2025-04-28 16:51:35.054778 13.808813 1.080603 0 1 0.898693 1 1596.870527 1925.125948 1.343586 0.742145 4997 2025-04-29 16:51:35.054775 11.698227 1.911000 0 0 2.839860 1 1271.065524 1596.128382 2.771896 0.742145 4998 2025-04-30 16:51:35.054772 18.052081 1.000521 0 0 1.188440 1 1681.886638 1320.323379 2.564075 0.742145 4999 2025-05-01 16:51:35.054738 17.017294 0.650213 0 0 2.131694 0 1289.584771 1681.144493 0.785727 0.833140 <p>5000 rows \u00d7 11 columns</p> In\u00a0[6]: Copied! <pre># Preprocess the dataset\nX = df.drop(columns=[\"date\", \"demand\"])\ny = df[\"demand\"]\ntrain_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.25)\ndtrain = xgb.DMatrix(train_x, label=train_y)\ndvalid = xgb.DMatrix(valid_x, label=valid_y)\n</pre> # Preprocess the dataset X = df.drop(columns=[\"date\", \"demand\"]) y = df[\"demand\"] train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.25) dtrain = xgb.DMatrix(train_x, label=train_y) dvalid = xgb.DMatrix(valid_x, label=valid_y) In\u00a0[7]: Copied! <pre>import matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndef plot_correlation_with_demand(df, save_path=None):  # noqa: D417\n    \"\"\"\n    Plots the correlation of each variable in the dataframe with the 'demand' column.\n\n    Args:\n    - df (pd.DataFrame): DataFrame containing the data, including a 'demand' column.\n    - save_path (str, optional): Path to save the generated plot. If not specified, plot won't be saved.\n\n    Returns:\n    - None (Displays the plot on a Jupyter window)\n    \"\"\"\n\n    # Compute correlations between all variables and 'demand'\n    correlations = df.corr()[\"demand\"].drop(\"demand\").sort_values()\n\n    # Generate a color palette from red to green\n    colors = sns.diverging_palette(10, 130, as_cmap=True)\n    color_mapped = correlations.map(colors)\n\n    # Set Seaborn style\n    sns.set_style(\n        \"whitegrid\", {\"axes.facecolor\": \"#c2c4c2\", \"grid.linewidth\": 1.5}\n    )  # Light grey background and thicker grid lines\n\n    # Create bar plot\n    fig = plt.figure(figsize=(12, 8))\n    plt.barh(correlations.index, correlations.values, color=color_mapped)\n\n    # Set labels and title with increased font size\n    plt.title(\"Correlation with Demand\", fontsize=18)\n    plt.xlabel(\"Correlation Coefficient\", fontsize=16)\n    plt.ylabel(\"Variable\", fontsize=16)\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=14)\n    plt.grid(axis=\"x\")\n\n    plt.tight_layout()\n\n    # Save the plot if save_path is specified\n    if save_path:\n        plt.savefig(save_path, format=\"png\", dpi=600)\n\n    # prevent matplotlib from displaying the chart every time we call this function\n    plt.close(fig)\n\n    return fig\n\n\n# Test the function\ncorrelation_plot = plot_correlation_with_demand(df, save_path=\"correlation_plot.png\")\n</pre> import matplotlib.pyplot as plt import seaborn as sns   def plot_correlation_with_demand(df, save_path=None):  # noqa: D417     \"\"\"     Plots the correlation of each variable in the dataframe with the 'demand' column.      Args:     - df (pd.DataFrame): DataFrame containing the data, including a 'demand' column.     - save_path (str, optional): Path to save the generated plot. If not specified, plot won't be saved.      Returns:     - None (Displays the plot on a Jupyter window)     \"\"\"      # Compute correlations between all variables and 'demand'     correlations = df.corr()[\"demand\"].drop(\"demand\").sort_values()      # Generate a color palette from red to green     colors = sns.diverging_palette(10, 130, as_cmap=True)     color_mapped = correlations.map(colors)      # Set Seaborn style     sns.set_style(         \"whitegrid\", {\"axes.facecolor\": \"#c2c4c2\", \"grid.linewidth\": 1.5}     )  # Light grey background and thicker grid lines      # Create bar plot     fig = plt.figure(figsize=(12, 8))     plt.barh(correlations.index, correlations.values, color=color_mapped)      # Set labels and title with increased font size     plt.title(\"Correlation with Demand\", fontsize=18)     plt.xlabel(\"Correlation Coefficient\", fontsize=16)     plt.ylabel(\"Variable\", fontsize=16)     plt.xticks(fontsize=14)     plt.yticks(fontsize=14)     plt.grid(axis=\"x\")      plt.tight_layout()      # Save the plot if save_path is specified     if save_path:         plt.savefig(save_path, format=\"png\", dpi=600)      # prevent matplotlib from displaying the chart every time we call this function     plt.close(fig)      return fig   # Test the function correlation_plot = plot_correlation_with_demand(df, save_path=\"correlation_plot.png\") In\u00a0[8]: Copied! <pre>def plot_residuals(model, dvalid, valid_y, save_path=None):  # noqa: D417\n    \"\"\"\n    Plots the residuals of the model predictions against the true values.\n\n    Args:\n    - model: The trained XGBoost model.\n    - dvalid (xgb.DMatrix): The validation data in XGBoost DMatrix format.\n    - valid_y (pd.Series): The true values for the validation set.\n    - save_path (str, optional): Path to save the generated plot. If not specified, plot won't be saved.\n\n    Returns:\n    - None (Displays the residuals plot on a Jupyter window)\n    \"\"\"\n\n    # Predict using the model\n    preds = model.predict(dvalid)\n\n    # Calculate residuals\n    residuals = valid_y - preds\n\n    # Set Seaborn style\n    sns.set_style(\"whitegrid\", {\"axes.facecolor\": \"#c2c4c2\", \"grid.linewidth\": 1.5})\n\n    # Create scatter plot\n    fig = plt.figure(figsize=(12, 8))\n    plt.scatter(valid_y, residuals, color=\"blue\", alpha=0.5)\n    plt.axhline(y=0, color=\"r\", linestyle=\"-\")\n\n    # Set labels, title and other plot properties\n    plt.title(\"Residuals vs True Values\", fontsize=18)\n    plt.xlabel(\"True Values\", fontsize=16)\n    plt.ylabel(\"Residuals\", fontsize=16)\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=14)\n    plt.grid(axis=\"y\")\n\n    plt.tight_layout()\n\n    # Save the plot if save_path is specified\n    if save_path:\n        plt.savefig(save_path, format=\"png\", dpi=600)\n\n    # Show the plot\n    plt.close(fig)\n\n    return fig\n</pre> def plot_residuals(model, dvalid, valid_y, save_path=None):  # noqa: D417     \"\"\"     Plots the residuals of the model predictions against the true values.      Args:     - model: The trained XGBoost model.     - dvalid (xgb.DMatrix): The validation data in XGBoost DMatrix format.     - valid_y (pd.Series): The true values for the validation set.     - save_path (str, optional): Path to save the generated plot. If not specified, plot won't be saved.      Returns:     - None (Displays the residuals plot on a Jupyter window)     \"\"\"      # Predict using the model     preds = model.predict(dvalid)      # Calculate residuals     residuals = valid_y - preds      # Set Seaborn style     sns.set_style(\"whitegrid\", {\"axes.facecolor\": \"#c2c4c2\", \"grid.linewidth\": 1.5})      # Create scatter plot     fig = plt.figure(figsize=(12, 8))     plt.scatter(valid_y, residuals, color=\"blue\", alpha=0.5)     plt.axhline(y=0, color=\"r\", linestyle=\"-\")      # Set labels, title and other plot properties     plt.title(\"Residuals vs True Values\", fontsize=18)     plt.xlabel(\"True Values\", fontsize=16)     plt.ylabel(\"Residuals\", fontsize=16)     plt.xticks(fontsize=14)     plt.yticks(fontsize=14)     plt.grid(axis=\"y\")      plt.tight_layout()      # Save the plot if save_path is specified     if save_path:         plt.savefig(save_path, format=\"png\", dpi=600)      # Show the plot     plt.close(fig)      return fig In\u00a0[9]: Copied! <pre>def plot_feature_importance(model, booster):  # noqa: D417\n    \"\"\"\n    Plots feature importance for an XGBoost model.\n\n    Args:\n    - model: A trained XGBoost model\n\n    Returns:\n    - fig: The matplotlib figure object\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(10, 8))\n    importance_type = \"weight\" if booster == \"gblinear\" else \"gain\"\n    xgb.plot_importance(\n        model,\n        importance_type=importance_type,\n        ax=ax,\n        title=f\"Feature Importance based on {importance_type}\",\n    )\n    plt.tight_layout()\n    plt.close(fig)\n\n    return fig\n</pre> def plot_feature_importance(model, booster):  # noqa: D417     \"\"\"     Plots feature importance for an XGBoost model.      Args:     - model: A trained XGBoost model      Returns:     - fig: The matplotlib figure object     \"\"\"     fig, ax = plt.subplots(figsize=(10, 8))     importance_type = \"weight\" if booster == \"gblinear\" else \"gain\"     xgb.plot_importance(         model,         importance_type=importance_type,         ax=ax,         title=f\"Feature Importance based on {importance_type}\",     )     plt.tight_layout()     plt.close(fig)      return fig In\u00a0[10]: Copied! <pre>def get_or_create_experiment(experiment_name):\n    \"\"\"\n    Retrieve the ID of an existing MLflow experiment or create a new one if it doesn't exist.\n\n    This function checks if an experiment with the given name exists within MLflow.\n    If it does, the function returns its ID. If not, it creates a new experiment\n    with the provided name and returns its ID.\n\n    Parameters:\n    - experiment_name (str): Name of the MLflow experiment.\n\n    Returns:\n    - str: ID of the existing or newly created MLflow experiment.\n    \"\"\"\n\n    if experiment := mlflow.get_experiment_by_name(experiment_name):\n        return experiment.experiment_id\n    else:\n        return mlflow.create_experiment(experiment_name)\n</pre> def get_or_create_experiment(experiment_name):     \"\"\"     Retrieve the ID of an existing MLflow experiment or create a new one if it doesn't exist.      This function checks if an experiment with the given name exists within MLflow.     If it does, the function returns its ID. If not, it creates a new experiment     with the provided name and returns its ID.      Parameters:     - experiment_name (str): Name of the MLflow experiment.      Returns:     - str: ID of the existing or newly created MLflow experiment.     \"\"\"      if experiment := mlflow.get_experiment_by_name(experiment_name):         return experiment.experiment_id     else:         return mlflow.create_experiment(experiment_name) In\u00a0[11]: Copied! <pre>experiment_id = get_or_create_experiment(\"Apples Demand\")\nexperiment_id\n</pre> experiment_id = get_or_create_experiment(\"Apples Demand\") experiment_id Out[11]: <pre>'1'</pre> In\u00a0[12]: Copied! <pre># Set the current active MLflow experiment\nmlflow.set_experiment(experiment_id=experiment_id)\n</pre> # Set the current active MLflow experiment mlflow.set_experiment(experiment_id=experiment_id) Out[12]: <pre>&lt;Experiment: artifact_location='mlflow-artifacts:/1', creation_time=1746089509195, experiment_id='1', last_update_time=1746089509195, lifecycle_stage='active', name='Apples Demand', tags={}&gt;</pre> In\u00a0[13]: Copied! <pre># override Optuna's default logging to ERROR only\noptuna.logging.set_verbosity(optuna.logging.ERROR)\n</pre> # override Optuna's default logging to ERROR only optuna.logging.set_verbosity(optuna.logging.ERROR) In\u00a0[14]: Copied! <pre># define a logging callback that will report on only new challenger parameter configurations if a\n# trial has usurped the state of 'best conditions'\ndef champion_callback(study, frozen_trial):\n    \"\"\"\n    Logging callback that will report when a new trial iteration improves upon existing\n    best trial values.\n\n    Note: This callback is not intended for use in distributed computing systems such as Spark\n    or Ray due to the micro-batch iterative implementation for distributing trials to a cluster's\n    workers or agents.\n    The race conditions with file system state management for distributed trials will render\n    inconsistent values with this callback.\n    \"\"\"\n\n    prev_best_value = study.user_attrs.get(\"prev_best_value\", None)\n\n    if study.best_value and prev_best_value != study.best_value:\n        study.set_user_attr(\"prev_best_value\", study.best_value)\n\n        # MLflow log when a new best trial is found\n        full_params = frozen_trial.user_attrs.get(\"full_params\", {})\n        mse = frozen_trial.user_attrs.get(\"mse\", None)\n        with mlflow.start_run(run_name=f\"best-trial-{frozen_trial.number}\", nested=True):\n            mlflow.log_params(full_params)\n            mlflow.log_metric(\"mse\", mse)\n            mlflow.log_metric(\"rmse\", math.sqrt(mse))\n\n        if prev_best_value:\n            improvement_percent = (abs(prev_best_value - study.best_value) / study.best_value) * 100\n            print(\n                f\"Trial {frozen_trial.number} achieved value: {frozen_trial.value} with \"\n                f\"{improvement_percent: .4f}% improvement\"\n            )\n        else:\n            print(f\"Initial trial {frozen_trial.number} achieved value: {frozen_trial.value}\")\n</pre> # define a logging callback that will report on only new challenger parameter configurations if a # trial has usurped the state of 'best conditions' def champion_callback(study, frozen_trial):     \"\"\"     Logging callback that will report when a new trial iteration improves upon existing     best trial values.      Note: This callback is not intended for use in distributed computing systems such as Spark     or Ray due to the micro-batch iterative implementation for distributing trials to a cluster's     workers or agents.     The race conditions with file system state management for distributed trials will render     inconsistent values with this callback.     \"\"\"      prev_best_value = study.user_attrs.get(\"prev_best_value\", None)      if study.best_value and prev_best_value != study.best_value:         study.set_user_attr(\"prev_best_value\", study.best_value)          # MLflow log when a new best trial is found         full_params = frozen_trial.user_attrs.get(\"full_params\", {})         mse = frozen_trial.user_attrs.get(\"mse\", None)         with mlflow.start_run(run_name=f\"best-trial-{frozen_trial.number}\", nested=True):             mlflow.log_params(full_params)             mlflow.log_metric(\"mse\", mse)             mlflow.log_metric(\"rmse\", math.sqrt(mse))          if prev_best_value:             improvement_percent = (abs(prev_best_value - study.best_value) / study.best_value) * 100             print(                 f\"Trial {frozen_trial.number} achieved value: {frozen_trial.value} with \"                 f\"{improvement_percent: .4f}% improvement\"             )         else:             print(f\"Initial trial {frozen_trial.number} achieved value: {frozen_trial.value}\") In\u00a0[15]: Copied! <pre>def objective(trial):\n    # Define hyperparameters\n    params = {\n        \"objective\": \"reg:squarederror\",\n        \"eval_metric\": \"rmse\",\n        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n    }\n\n    if params[\"booster\"] == \"gbtree\" or params[\"booster\"] == \"dart\":\n        params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n        params[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n        params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n        params[\"grow_policy\"] = trial.suggest_categorical(\n            \"grow_policy\", [\"depthwise\", \"lossguide\"]\n        )\n    \n    # Train XGBoost model\n    bst = xgb.train(params, dtrain)\n    pred_y = bst.predict(dvalid)\n    mse = mean_squared_error(valid_y, pred_y)\n\n    trial.set_user_attr(\"full_params\", params)\n    trial.set_user_attr(\"mse\", mse)\n\n    return mse\n</pre> def objective(trial):     # Define hyperparameters     params = {         \"objective\": \"reg:squarederror\",         \"eval_metric\": \"rmse\",         \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),         \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),         \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),     }      if params[\"booster\"] == \"gbtree\" or params[\"booster\"] == \"dart\":         params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)         params[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)         params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)         params[\"grow_policy\"] = trial.suggest_categorical(             \"grow_policy\", [\"depthwise\", \"lossguide\"]         )          # Train XGBoost model     bst = xgb.train(params, dtrain)     pred_y = bst.predict(dvalid)     mse = mean_squared_error(valid_y, pred_y)      trial.set_user_attr(\"full_params\", params)     trial.set_user_attr(\"mse\", mse)      return mse In\u00a0[16]: Copied! <pre># Initiate the parent run and call the hyperparameter tuning child run logic\nrun_name = \"fourth\"\nwith mlflow.start_run(experiment_id=experiment_id, run_name=run_name, nested=True):\n    # Initialize the Optuna study\n    study = optuna.create_study(direction=\"minimize\")\n\n    # Execute the hyperparameter optimization trials.\n    # Note the addition of the `champion_callback` inclusion to control our logging\n    study.optimize(objective, n_trials=100, callbacks=[champion_callback])\n\n    mlflow.log_params(study.best_params)\n    mlflow.log_metric(\"best_mse\", study.best_value)\n    mlflow.log_metric(\"best_rmse\", math.sqrt(study.best_value))\n\n    # Log tags\n    mlflow.set_tags(\n        tags={\n            \"project\": \"Apple Demand Project\",\n            \"optimizer_engine\": \"optuna\",\n            \"model_family\": \"xgboost\",\n            \"feature_set_version\": 1,\n        }\n    )\n\n    # Log a fit model instance\n    model = xgb.train(study.best_params, dtrain)\n\n    # Log the correlation plot\n    mlflow.log_figure(figure=correlation_plot, artifact_file=\"correlation_plot.png\")\n\n    # Log the feature importances plot\n    importances = plot_feature_importance(model, booster=study.best_params.get(\"booster\"))\n    mlflow.log_figure(figure=importances, artifact_file=\"feature_importances.png\")\n\n    # Log the residuals plot\n    residuals = plot_residuals(model, dvalid, valid_y)\n    mlflow.log_figure(figure=residuals, artifact_file=\"residuals.png\")\n\n    artifact_path = \"model\"\n\n    mlflow.xgboost.log_model(\n        xgb_model=model,\n        artifact_path=artifact_path,\n        input_example=train_x.iloc[[0]],\n        model_format=\"ubj\",\n        metadata={\"model_data_version\": 1},\n    )\n\n    # Get the logged model uri so that we can load it from the artifact store\n    model_uri = mlflow.get_artifact_uri(artifact_path)\n</pre> # Initiate the parent run and call the hyperparameter tuning child run logic run_name = \"fourth\" with mlflow.start_run(experiment_id=experiment_id, run_name=run_name, nested=True):     # Initialize the Optuna study     study = optuna.create_study(direction=\"minimize\")      # Execute the hyperparameter optimization trials.     # Note the addition of the `champion_callback` inclusion to control our logging     study.optimize(objective, n_trials=100, callbacks=[champion_callback])      mlflow.log_params(study.best_params)     mlflow.log_metric(\"best_mse\", study.best_value)     mlflow.log_metric(\"best_rmse\", math.sqrt(study.best_value))      # Log tags     mlflow.set_tags(         tags={             \"project\": \"Apple Demand Project\",             \"optimizer_engine\": \"optuna\",             \"model_family\": \"xgboost\",             \"feature_set_version\": 1,         }     )      # Log a fit model instance     model = xgb.train(study.best_params, dtrain)      # Log the correlation plot     mlflow.log_figure(figure=correlation_plot, artifact_file=\"correlation_plot.png\")      # Log the feature importances plot     importances = plot_feature_importance(model, booster=study.best_params.get(\"booster\"))     mlflow.log_figure(figure=importances, artifact_file=\"feature_importances.png\")      # Log the residuals plot     residuals = plot_residuals(model, dvalid, valid_y)     mlflow.log_figure(figure=residuals, artifact_file=\"residuals.png\")      artifact_path = \"model\"      mlflow.xgboost.log_model(         xgb_model=model,         artifact_path=artifact_path,         input_example=train_x.iloc[[0]],         model_format=\"ubj\",         metadata={\"model_data_version\": 1},     )      # Get the logged model uri so that we can load it from the artifact store     model_uri = mlflow.get_artifact_uri(artifact_path) <pre>\ud83c\udfc3 View run best-trial-0 at: http://127.0.0.1:50666/#/experiments/1/runs/25de8f9716124a0e953ec50a03a85bdd\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nInitial trial 0 achieved value: 64200.9377312124\n\ud83c\udfc3 View run best-trial-2 at: http://127.0.0.1:50666/#/experiments/1/runs/f8c4f4c28f7346f39e513beaef0b2eb2\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nTrial 2 achieved value: 64072.863216692545 with  0.1999% improvement\n\ud83c\udfc3 View run best-trial-7 at: http://127.0.0.1:50666/#/experiments/1/runs/a7c0d3ff781b4722bf6a8f4a1c29cc21\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nTrial 7 achieved value: 19370.33276815125 with  230.7783% improvement\n\ud83c\udfc3 View run best-trial-9 at: http://127.0.0.1:50666/#/experiments/1/runs/b50292d1b7f54e4ab43bcd950c901853\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nTrial 9 achieved value: 19280.226444123156 with  0.4674% improvement\n\ud83c\udfc3 View run best-trial-12 at: http://127.0.0.1:50666/#/experiments/1/runs/ec7572de81d74d2e814a246deb279585\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nTrial 12 achieved value: 19249.251703373608 with  0.1609% improvement\n\ud83c\udfc3 View run best-trial-15 at: http://127.0.0.1:50666/#/experiments/1/runs/3a5f94921c5f425599caf305b7bcea63\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nTrial 15 achieved value: 19143.276660193736 with  0.5536% improvement\n\ud83c\udfc3 View run best-trial-26 at: http://127.0.0.1:50666/#/experiments/1/runs/2d64a8f2a9d046379f79c8fd2ed84da8\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nTrial 26 achieved value: 19042.25204424367 with  0.5305% improvement\n\ud83c\udfc3 View run best-trial-40 at: http://127.0.0.1:50666/#/experiments/1/runs/0ff1b4a5bba7468abf7221b0c2567857\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nTrial 40 achieved value: 18255.62810065196 with  4.3089% improvement\n\ud83c\udfc3 View run best-trial-70 at: http://127.0.0.1:50666/#/experiments/1/runs/8688d93cb6b2495eaca05a8d831be39a\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nTrial 70 achieved value: 16690.920638134605 with  9.3746% improvement\n\ud83c\udfc3 View run best-trial-71 at: http://127.0.0.1:50666/#/experiments/1/runs/08f702060d76408a92ba803e459b9d38\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nTrial 71 achieved value: 15909.402041712121 with  4.9123% improvement\n\ud83c\udfc3 View run best-trial-72 at: http://127.0.0.1:50666/#/experiments/1/runs/e89e6529f4174a8a93d7f78c5a8f2a61\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nTrial 72 achieved value: 15573.390403470594 with  2.1576% improvement\n\ud83c\udfc3 View run best-trial-76 at: http://127.0.0.1:50666/#/experiments/1/runs/07bef624d7084837a4de95636ae72508\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nTrial 76 achieved value: 15331.608560002875 with  1.5770% improvement\n\ud83c\udfc3 View run best-trial-85 at: http://127.0.0.1:50666/#/experiments/1/runs/094dec77387f47ebad1869b6ca2487a8\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nTrial 85 achieved value: 15330.17016465973 with  0.0094% improvement\n</pre> <pre>/Users/kcl/.venvs/feast/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values &lt;https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values&gt;`_ for more details.\n  warnings.warn(\n</pre> <pre>\ud83c\udfc3 View run fourth at: http://127.0.0.1:50666/#/experiments/1/runs/b1d6cefb8b9c434895e7627fe7529e4e\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\n</pre> In\u00a0[17]: Copied! <pre>model_uri\n</pre> model_uri Out[17]: <pre>'mlflow-artifacts:/1/b1d6cefb8b9c434895e7627fe7529e4e/artifacts/model'</pre> In\u00a0[35]: Copied! <pre>loaded = mlflow.xgboost.load_model(model_uri)\n</pre> loaded = mlflow.xgboost.load_model(model_uri) <pre>Downloading artifacts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:22&lt;00:00,  3.18s/it]\n</pre> In\u00a0[36]: Copied! <pre>batch_dmatrix = xgb.DMatrix(X)\n\ninference = loaded.predict(batch_dmatrix)\n\ninfer_df = df.copy()\n\ninfer_df[\"predicted_demand\"] = inference\n</pre> batch_dmatrix = xgb.DMatrix(X)  inference = loaded.predict(batch_dmatrix)  infer_df = df.copy()  infer_df[\"predicted_demand\"] = inference In\u00a0[37]: Copied! <pre>infer_df\n</pre> infer_df Out[37]: date average_temperature rainfall weekend holiday price_per_kg promo demand previous_days_demand competitor_price_per_kg marketing_intensity predicted_demand 0 2011-08-22 16:36:16.256208 30.584727 1.199291 0 0 1.726258 0 851.375336 851.276659 1.935346 0.098677 938.926270 1 2011-08-23 16:36:16.256201 15.465069 1.037626 0 0 0.576471 0 906.855943 851.276659 2.344720 0.019318 1016.131104 2 2011-08-24 16:36:16.256200 10.786525 5.656089 0 0 2.513328 0 808.304909 906.836626 0.998803 0.409485 888.394958 3 2011-08-25 16:36:16.256198 23.648154 12.030937 0 0 1.839225 0 799.833810 857.895424 0.761740 0.872803 928.042908 4 2011-08-26 16:36:16.256197 13.861391 4.303812 0 0 1.531772 0 983.949061 848.961007 2.123436 0.820779 985.474487 ... ... ... ... ... ... ... ... ... ... ... ... ... 4995 2025-04-25 16:36:16.246942 21.643051 3.821656 0 0 2.391010 1 1449.882437 1454.799278 1.504432 0.756489 1287.835571 4996 2025-04-26 16:36:16.246940 13.808813 1.080603 1 1 0.898693 1 2022.870527 1499.125948 1.343586 0.742145 1742.767700 4997 2025-04-27 16:36:16.246939 11.698227 1.911000 1 0 2.839860 1 1697.065524 2022.128382 2.771896 0.742145 1757.431885 4998 2025-04-28 16:36:16.246935 18.052081 1.000521 0 0 1.188440 1 1681.886638 1746.323379 2.564075 0.742145 1473.662109 4999 2025-04-29 16:36:16.246853 17.017294 0.650213 0 0 2.131694 1 1573.584771 1681.144493 0.785727 0.833140 1346.723755 <p>5000 rows \u00d7 12 columns</p>"},{"location":"side-projects/data2ml-ops/mlflow/hpo/#track-hyperparameter-optimization-with-optuna-and-mlflow","title":"Track Hyperparameter Optimization with Optuna and MLflow\u00b6","text":""},{"location":"side-projects/data2ml-ops/mlflow/hpo/#import-packages","title":"Import Packages\u00b6","text":""},{"location":"side-projects/data2ml-ops/mlflow/hpo/#generate-synthetic-data","title":"Generate Synthetic Data\u00b6","text":""},{"location":"side-projects/data2ml-ops/mlflow/hpo/#define-plotting-functions","title":"Define Plotting Functions\u00b6","text":"<ul> <li><code>plot_correlation_with_demand</code></li> <li><code>plot_residuals</code></li> <li><code>plot_feature_importance</code></li> </ul>"},{"location":"side-projects/data2ml-ops/mlflow/hpo/#set-up-the-experiment","title":"Set up the Experiment\u00b6","text":""},{"location":"side-projects/data2ml-ops/mlflow/hpo/#create-a-callback-function","title":"Create a Callback Function\u00b6","text":""},{"location":"side-projects/data2ml-ops/mlflow/hpo/#create-a-objective-function","title":"Create a Objective Function\u00b6","text":""},{"location":"side-projects/data2ml-ops/mlflow/hpo/#load-the-model-and-run-batch-prediction","title":"Load the Model and Run Batch Prediction\u00b6","text":""},{"location":"side-projects/data2ml-ops/mlflow/in-the-bigger-picture/","title":"In the Bigger Picture","text":""},{"location":"side-projects/data2ml-ops/mlflow/in-the-bigger-picture/#history-current-status","title":"History &amp; Current Status","text":"<p>MLflow is an open-source platform for managing the end-to-end machine learning lifecycle, initially developed by Databricks and introduced in June 2018. It provides tools for experiment tracking, model packaging, deployment, and a centralized model registry, aiming to streamline the complexities of machine learning workflows. MLflow is designed to be library-agnostic, supporting various ML libraries and frameworks. \ufffc \ufffc</p> <p>As of May 2025, MLflow has garnered over 20,000 stars on GitHub and boasts a vibrant community with more than 1,000 contributors. The platform is actively maintained, with recent releases introducing features like support for OpenAI\u2019s Responses API, Gemini embeddings, and integration with Azure Data Lake Storage. MLflow is utilized by numerous organizations worldwide, including Microsoft, Meta, Toyota, Booking.com, and Accenture, underscoring its widespread adoption in the industry.</p>"},{"location":"side-projects/data2ml-ops/mlflow/in-the-bigger-picture/#alternatives","title":"Alternatives","text":""},{"location":"side-projects/data2ml-ops/mlflow/in-the-bigger-picture/#integration-points","title":"Integration Points","text":""},{"location":"side-projects/data2ml-ops/mlflow/in-the-bigger-picture/#kubernetes","title":"Kubernetes","text":""},{"location":"side-projects/data2ml-ops/mlflow/in-the-bigger-picture/#cicd","title":"CI/CD","text":""},{"location":"side-projects/data2ml-ops/mlflow/what-why-when/","title":"What, Why, When","text":""},{"location":"side-projects/data2ml-ops/mlflow/what-why-when/#what-is-x","title":"What is X?","text":""},{"location":"side-projects/data2ml-ops/mlflow/what-why-when/#why-x","title":"Why X?","text":"<ul> <li>Data Engineer</li> <li>Data Analyst</li> <li>Data Scientist</li> <li>Machine Learning Engineer</li> </ul>"},{"location":"side-projects/data2ml-ops/mlflow/what-why-when/#when-to-use-x","title":"When to Use X?","text":""},{"location":"side-projects/data2ml-ops/others/cert-manager-introduction/","title":"Cert Manager","text":""},{"location":"side-projects/data2ml-ops/others/istio-introduction/","title":"Istio","text":""},{"location":"side-projects/data2ml-ops/others/istio-introduction/#what-is-istio","title":"What is Istio","text":"<ul> <li>Istio is an open source service mesh that layers transparently onto existing distributed applications.</li> <li>Istio extends Kubernetes to establish a programmable, application-aware network. Working with both Kubernetes and traditional workloads, Istio brings standard, universal traffic management, telemetry, and security to complex deployments.</li> <li>It gives you:<ul> <li>Secure service-to-service communication in a cluster with mutual TLS encryption, strong identity-based authentication and authorization</li> <li>Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic</li> <li>Fine-grained control of traffic behavior with rich routing rules, retries, failovers, and fault injection</li> <li>A pluggable policy layer and configuration API supporting access controls, rate limits and quotas</li> <li>Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress</li> </ul> </li> </ul>"},{"location":"side-projects/data2ml-ops/others/istio-introduction/#how-it-works","title":"How it works?","text":"<ul> <li>The control plane takes your desired configuration, and its view of the services, and dynamically programs the proxy servers, updating them as the rules or the environment changes.</li> <li>The data plane is the communication between services. Without a service mesh, the network doesn\u2019t understand the traffic being sent over, and can\u2019t make any decisions based on what type of traffic it is, or who it is from or to. It supports two data planes:<ul> <li>sidecar mode, which deploys an Envoy proxy along with each pod that you start in your cluster, or running alongside services running on VMs.</li> <li>ambient mode, which uses a per-node Layer 4 proxy, and optionally a per-namespace Envoy proxy for Layer 7 features</li> </ul> </li> </ul>"},{"location":"side-projects/data2ml-ops/others/istio-introduction/#sidecar-mode-vs-ambient-mode-in-istio","title":"Sidecar Mode vs. Ambient Mode in Istio","text":"Feature Sidecar Mode Ambient Mode Proxy location One per pod (sidecar) One per node (<code>ztunnel</code>), plus optional waypoint proxies Setup complexity Requires injection into each pod No sidecar; label namespace only Resource usage Higher Lower Feature maturity Very mature Newer, still evolving Transparency to app Yes Yes Traffic interception iptables per pod eBPF or iptables at node level"},{"location":"side-projects/data2ml-ops/others/istio-introduction/#why-istio","title":"Why Istio?","text":"<p>Simple and powerful</p> <p>Istio \u4e4b\u6240\u4ee5\u88ab\u7a31\u70ba simple and powerful\uff0c\u662f\u56e0\u70ba\uff1a</p> <ul> <li>\u5b83\u529f\u80fd\u5b8c\u6574\uff0c\u5e7e\u4e4e\u6db5\u84cb\u6240\u6709 service mesh \u6240\u9700\u7684\u6cbb\u7406\u9700\u6c42</li> <li>\u4e00\u65e6\u5efa\u69cb\u5b8c\u6210\uff0c\u5b83\u8b93\u5fae\u670d\u52d9\u7684\u904b\u7dad\u8207\u6cbb\u7406\u8b8a\u5f97\u771f\u6b63\u7c21\u55ae</li> <li>\u900f\u904e\u6a19\u6e96\u5316\u4ee3\u7406\u5c64\u548c\u7b56\u7565\u5b9a\u7fa9\uff0c\u62bd\u8c61\u4e86\u5e95\u5c64\u8907\u96dc\u6027</li> <li>\u65b0\u63a8\u51fa\u7684 Ambient \u6a21\u5f0f\u66f4\u964d\u4f4e\u4e86\u90e8\u7f72\u8207\u5b78\u7fd2\u9580\u6abb</li> </ul> <p>\u5176\u5be6\u6211\u4e00\u958b\u59cb\u4e5f\u89ba\u5f97 Istio \u6709\u9ede\u8907\u96dc\uff0c\u7562\u7adf\u88e1\u9762\u540d\u8a5e\u4e00\u5806\u3001\u53c8\u6709\u4ec0\u9ebc sidecar\u3001control plane\u3001mTLS \u4e4b\u985e\u7684\u3002\u4f46\u5f8c\u4f86\u771f\u7684\u7406\u89e3\u5b83\u4e4b\u5f8c\uff0c\u6211\u89ba\u5f97\u5b83\u6703\u88ab\u8aaa\u6210\u300csimple and powerful\u300d\u4e0d\u662f\u6c92\u9053\u7406\u7684\u3002</p> <p>Powerful \u7684\u5730\u65b9\u8d85\u660e\u986f\uff0c\u5b83\u5e7e\u4e4e\u5e6b\u4f60\u5305\u8fa6\u4e86\u6240\u6709\u5fae\u670d\u52d9\u4e4b\u9593\u7684\u7db2\u8def\u6cbb\u7406\uff1a\u50cf\u662f TLS \u52a0\u5bc6\u3001\u96f6\u4fe1\u4efb\u67b6\u69cb\u3001\u6d41\u91cf\u5206\u6d41\u3001\u932f\u8aa4\u91cd\u8a66\u3001\u7570\u5e38\u6ce8\u5165\u3001A/B \u6e2c\u8a66\u9019\u4e9b\uff0c\u904e\u53bb\u8981\u81ea\u5df1\u4e00\u500b\u4e00\u500b\u958b\u767c\u3001\u6e2c\u8a66\u3001\u5e03\u7f72\uff0c\u4f46\u7528 Istio \u7684\u8a71\uff0c\u53ea\u8981\u5b9a\u7fa9\u4e00\u500b YAML \u6a94\u5c31\u641e\u5b9a\u4e86\u3002</p> <p>\u800c\u5b83\u88ab\u8aaa simple\uff0c\u5176\u5be6\u662f\u5f9e\u6574\u500b\u7cfb\u7d71\u5c64\u7d1a\u4f86\u770b\u3002\u4f60\u7528\u5b83\u628a service mesh \u67b6\u8d77\u4f86\u4e4b\u5f8c\uff0c\u771f\u7684\u6703\u89ba\u5f97\u300c\u7ba1\u7406\u670d\u52d9\u7684\u8907\u96dc\u5ea6\u300d\u88ab\u5927\u5e45\u964d\u4f4e\u3002\u7279\u5225\u662f\u73fe\u5728\u6709\u4e86 Ambient mode\uff0c\u4e0d\u9700\u8981\u518d\u6ce8\u5165 sidecar\uff0c\u4e5f\u4e0d\u7528\u4e00\u76f4\u6539 pod spec\uff0c\u8b93\u90e8\u7f72\u66f4\u4e7e\u6de8\u3001\u64f4\u5c55\u66f4\u5bb9\u6613\u3002</p> <p>\u6240\u4ee5\u8001\u5be6\u8aaa\uff0cIstio \u672c\u8eab\u53ef\u80fd\u4e0d\u662f\u771f\u7684\u300c\u5165\u9580\u8d85\u7c21\u55ae\u300d\uff0c\u4f46\u5b83\u53ef\u4ee5\u8b93\u4f60\u5f8c\u7e8c\u7dad\u904b\u8b8a\u5f97\u66f4\u7c21\u55ae\u3001\u66f4\u5b89\u5fc3\u3002</p> <p>The Envoy proxy</p> <p>Istio inherits all the power and flexibility of Envoy, including world-class extensibility using WebAssembly</p> <p>a high performance service proxy initially built by Lyft</p> <p>Envoy would go on to become the load balancer that powers Google Cloud as well as the proxy for almost every other service mesh platform.</p> <p>\u9996\u5148\uff0c\u5728\u6d41\u91cf\u63a7\u5236\u65b9\u9762\uff0cKubernetes \u672c\u8eab\u7684 Ingress \u53ea\u80fd\u8655\u7406\u5f9e\u5916\u90e8\u9032\u5165\u96c6\u7fa4\u7684\u8acb\u6c42\uff0c\u5c0d\u65bc\u670d\u52d9\u4e4b\u9593\u7684\u5167\u90e8\u901a\u8a0a\u6beb\u7121\u63a7\u5236\u80fd\u529b\u3002\u5b83\u7121\u6cd5\u6839\u64da\u7248\u672c\u3001Header\u3001\u4f7f\u7528\u8005\u8eab\u5206\u7b49\u689d\u4ef6\u4f86\u8abf\u6574\u8def\u7531\u908f\u8f2f\u3002Istio \u5247\u900f\u904e VirtualService \u548c DestinationRule \u9019\u5169\u7a2e\u8cc7\u6e90\uff0c\u8b93\u4f60\u80fd\u7cbe\u7d30\u5730\u6307\u5b9a\u6d41\u91cf\u8a72\u5982\u4f55\u5206\u6d41\uff0c\u6bd4\u5982\u91dd\u5c0d\u7279\u5b9a\u7528\u6236\u5c0e\u5411\u65b0\u7248\u672c\u3001\u5be6\u4f5c A/B \u6e2c\u8a66\u3001\u7070\u968e\u91cb\u51fa\u7b49\uff0c\u5728\u4e0d\u6539\u8b8a\u61c9\u7528\u7a0b\u5f0f\u7684\u60c5\u6cc1\u4e0b\u505a\u5230\u5f48\u6027\u8def\u7531\u3002</p> <p>\u63a5\u8457\u662f\u5b89\u5168\u6027\u554f\u984c\u3002Kubernetes \u96d6\u7136\u652f\u63f4 Role-Based Access Control\uff08RBAC\uff09\u8207 NetworkPolicy\uff0c\u537b\u7121\u6cd5\u78ba\u4fdd\u670d\u52d9\u4e4b\u9593\u7684\u901a\u8a0a\u662f\u52a0\u5bc6\u7684\uff0c\u4e5f\u7121\u6cd5\u9a57\u8b49\u901a\u8a0a\u96d9\u65b9\u7684\u8eab\u5206\u3002Istio \u5247\u85c9\u7531\u81ea\u52d5\u5316\u7684 mutual TLS\uff08mTLS\uff09\u6a5f\u5236\uff0c\u5728\u6bcf\u500b\u670d\u52d9\u4e4b\u9593\u5efa\u7acb\u52a0\u5bc6\u901a\u9053\uff0c\u4e26\u642d\u914d\u8eab\u4efd\u9a57\u8b49\u8207\u6388\u6b0a\u7b56\u7565\uff0c\u78ba\u4fdd\u53ea\u6709\u88ab\u6388\u6b0a\u7684\u670d\u52d9\u80fd\u4e92\u76f8\u901a\u8a0a\uff0c\u5be6\u73fe\u96f6\u4fe1\u4efb\u67b6\u69cb\u7684\u57fa\u672c\u539f\u5247\u3002</p> <p>\u518d\u4f86\u662f\u53ef\u89c0\u5bdf\u6027\u3002Kubernetes \u96d6\u7136\u53ef\u4ee5\u67e5\u770b Pod logs \u548c\u4e00\u4e9b\u57fa\u672c\u7684 metrics\uff0c\u4f46\u5c0d\u65bc\u5fae\u670d\u52d9\u4e4b\u9593\u7684\u8ffd\u8e64\u3001\u5ef6\u9072\u5206\u6790\u3001\u6d41\u91cf\u74f6\u9838\u7b49\u554f\u984c\uff0c\u539f\u751f\u529f\u80fd\u660e\u986f\u4e0d\u8db3\u3002Istio \u5728\u6bcf\u500b\u670d\u52d9\u908a\u7de3\u90e8\u7f72 proxy\uff0c\u9019\u8b93\u5b83\u53ef\u4ee5\u81ea\u52d5\u6536\u96c6\u8a73\u7d30\u7684 telemetry \u8cc7\u8a0a\uff0c\u5305\u62ec request-level tracing\u3001\u6d41\u91cf\u6307\u6a19\u8207\u932f\u8aa4\u7387\uff0c\u4e26\u6574\u5408\u5230\u50cf Prometheus\u3001Grafana\u3001Jaeger \u7b49\u5de5\u5177\u4e2d\uff0c\u5e7e\u4e4e\u4e0d\u9700\u8981\u4fee\u6539\u61c9\u7528\u7a0b\u5f0f\u5c31\u80fd\u505a\u5230\u5168\u9762\u76e3\u63a7\u3002</p> <p>\u6700\u5f8c\uff0c\u5728\u9748\u6d3b\u6027\u8207\u64f4\u5c55\u6027\u4e0a\uff0cKubernetes \u7121\u6cd5\u91dd\u5c0d\u500b\u5225\u670d\u52d9\u6ce8\u5165\u81ea\u8a02\u7684\u7db2\u8def\u8655\u7406\u908f\u8f2f\u3002Istio \u900f\u904e sidecar \u6a21\u5f0f\uff08\u6216\u8f03\u65b0\u7684 ambient \u6a21\u5f0f\uff09\u8b93\u6bcf\u500b\u670d\u52d9\u90fd\u64c1\u6709\u81ea\u5df1\u7684\u7db2\u8def\u4ee3\u7406\uff0c\u4e26\u652f\u63f4 WebAssembly \u63d2\u4ef6\uff0c\u4f60\u53ef\u4ee5\u52d5\u614b\u63d2\u5165\u8a8d\u8b49\u908f\u8f2f\u3001\u8cc7\u6599\u8f49\u63db\u751a\u81f3\u7570\u5e38\u6a21\u64ec\uff0c\u5c07\u7db2\u8def\u884c\u70ba\u8abf\u6574\u70ba\u7b26\u5408\u696d\u52d9\u9700\u6c42\u7684\u6a21\u6a23\u3002</p>"},{"location":"side-projects/data2ml-ops/others/istio-introduction/#what-happened-after-installing-istio","title":"What happened after installing Istio","text":"<p>\u5728\u5b8c\u6210 Istio \u7684\u5b89\u88dd\u5f8c\uff0c\u7cfb\u7d71\u6703\u5728 Kubernetes \u4e2d\u5efa\u7acb\u4e00\u500b\u540d\u70ba istio-system \u7684\u547d\u540d\u7a7a\u9593\uff0c\u9019\u662f Istio \u63a7\u5236\u5e73\u9762\uff08Control Plane\uff09\u5143\u4ef6\u6240\u904b\u4f5c\u7684\u4e3b\u8981\u4f4d\u7f6e\u3002\u88e1\u9762\u6703\u81ea\u52d5\u90e8\u7f72\u591a\u500b\u6838\u5fc3\u5143\u4ef6\uff0c\u9019\u4e9b\u5143\u4ef6\u900f\u904e\u670d\u52d9\u8207 Pod \u7684\u5f62\u5f0f\u5448\u73fe\uff0c\u4e26\u4e14\u85c9\u7531 CRD \u8b93 Istio \u80fd\u64f4\u5c55\u51fa\u81ea\u5df1\u7684\u8cc7\u6e90\u6a21\u578b\u3002</p> <p>\u9996\u5148\uff0c\u6700\u91cd\u8981\u7684\u63a7\u5236\u5143\u4ef6\u662f istiod\uff0c\u5b83\u662f\u4e00\u500b\u6574\u5408\u578b\u7684\u63a7\u5236\u5e73\u9762\u5143\u4ef6\uff0c\u5f9e Istio 1.5 \u8d77\u958b\u59cb\u6574\u4f75\u4e86\u904e\u53bb\u7684 Pilot\u3001Mixer\u3001Citadel \u7b49\u5143\u4ef6\u3002\u9019\u500b Pod \u8ca0\u8cac\u7ba1\u7406 service discovery\u3001sidecar \u7684\u8a2d\u5b9a\u767c\u9001\uff08XDS\uff09\u3001mTLS \u6191\u8b49\u7c3d\u767c\u3001\u5b89\u5168\u6027\u7b56\u7565\u4e0b\u767c\u7b49\u5de5\u4f5c\u3002\u4f60\u6703\u5728 istio-system \u4e2d\u770b\u5230\u4e00\u500b\u53eb\u505a istiod-xxxx \u7684 Pod\uff0c\u5c0d\u61c9\u7684 Service \u5247\u901a\u5e38\u662f istiod \u6216 istio-pilot\uff0c\u53d6\u6c7a\u65bc\u7248\u672c\u8207\u5b89\u88dd\u65b9\u5f0f\u3002</p> <p>\u6b64\u5916\uff0c\u82e5\u4f60\u555f\u7528\u4e86 Ingress Gateway\uff08\u5927\u591a\u6578\u4eba\u6703\uff09\uff0c\u4f60\u9084\u6703\u770b\u5230\u4e00\u500b\u53eb\u505a istio-ingressgateway \u7684 Deployment\u3001Service \u548c Pod\u3002\u9019\u662f\u4e00\u500b\u4ee5 Envoy \u70ba\u6838\u5fc3\u7684\u8ca0\u8cac\u63a5\u6536\u5916\u90e8 HTTP/TCP \u6d41\u91cf\u7684\u4ee3\u7406\u4f3a\u670d\u5668\uff0c\u901a\u5e38\u6703\u662f NodePort \u6216 LoadBalancer \u578b\u5225\u7684 Service</p> <ul> <li>istiod\uff1a\u9019\u662f Istio \u7684\u63a7\u5236\u5e73\u9762\u6838\u5fc3\u3002\u5b83\u8ca0\u8cac\u7ba1\u7406\u670d\u52d9\u767c\u73fe\uff08service discovery\uff09\u3001\u767c\u9001\u4ee3\u7406\u8a2d\u5b9a\uff08xDS\uff09\u3001mTLS \u6191\u8b49\u7c3d\u767c\u8207\u5b89\u5168\u7b56\u7565\u4e0b\u767c\u3002\u5f9e Istio 1.5 \u958b\u59cb\uff0c\u9019\u500b\u5143\u4ef6\u6574\u5408\u4e86\u904e\u53bb\u7684 Pilot\u3001Citadel\u3001Galley \u7b49\u89d2\u8272\u3002</li> <li>istio-ingressgateway\uff1a\u9019\u662f\u4e00\u500b\u4f7f\u7528 Envoy \u5be6\u4f5c\u7684 Ingress Gateway\uff0c\u662f\u5916\u90e8\u6d41\u91cf\u9032\u5165\u670d\u52d9\u7db2\u683c\u7684\u5165\u53e3\uff0c\u901a\u5e38\u7528\u4f86\u63a5\u6536 HTTP\u3001HTTPS\u3001gRPC \u7b49\u5916\u90e8\u8acb\u6c42\uff0c\u4e26\u8f49\u767c\u81f3\u96c6\u7fa4\u5167\u90e8\u7684\u670d\u52d9\u3002</li> </ul> <p>\u9664\u4e86\u4e0a\u8ff0\u63a7\u5236\u5143\u4ef6\u4e4b\u5916\uff0cIstio \u5b89\u88dd\u4e5f\u6703\u9644\u5e36\u4e00\u4e9b\u64f4\u5145\u8cc7\u6e90\u3002\u6700\u660e\u986f\u7684\u662f\u4e00\u7cfb\u5217\u7684 CRD\uff0c\u50cf\u662f VirtualService\u3001DestinationRule\u3001Gateway\u3001PeerAuthentication\u3001AuthorizationPolicy \u7b49\u3002\u9019\u4e9b\u90fd\u662f\u4f60\u5728\u4f7f\u7528 Istio \u505a\u8def\u7531\u63a7\u5236\u3001\u6d41\u91cf\u5206\u6d41\u3001\u5b89\u5168\u7b56\u7565\u6642\u6703\u7528\u5230\u7684\u8cc7\u6e90\u3002\u9019\u4e9b CRD \u53ef\u4ee5\u900f\u904e kubectl get crd \u67e5\u770b\uff0c\u540d\u7a31\u901a\u5e38\u6703\u662f virtualservices.networking.istio.io \u9019\u6a23\u7684\u683c\u5f0f\u3002</p> <p>\u9019\u4e9b\u8cc7\u6e90\u69cb\u6210\u4e86 Istio \u7684\u6838\u5fc3\u57fa\u790e\uff0c\u8b93\u4f60\u5f97\u4ee5\u5728 Kubernetes \u4e0a\u5efa\u7acb\u4e00\u500b\u53ef\u63a7\u3001\u5b89\u5168\u4e14\u5177\u6709\u6d41\u91cf\u89c0\u5bdf\u80fd\u529b\u7684\u670d\u52d9\u7db2\u683c\u3002</p>"},{"location":"side-projects/data2ml-ops/others/istio-introduction/#references","title":"References","text":"<ul> <li>What is Istio? | Istio</li> <li>Sidecar or ambient? | Istio</li> </ul>"},{"location":"side-projects/data2ml-ops/others/knative-introduction/","title":"Knative","text":"<ul> <li>Knative is an Open-Source Enterprise-level solution to build Serverless and Event Driven Applications</li> <li>Why serverless containers?<ul> <li>Simpler Abstractions</li> <li>Autoscaling (Scale down to zero and up from zero)</li> <li>Progressive Rollouts</li> <li>Event Integrations</li> <li>Handle Events</li> <li>Plugable</li> </ul> </li> </ul> Knative Serving and Eventing <ul> <li>Knative has two main components that empower teams working with Kubernetes. Serving and Eventing work together to automate and manage tasks and applications.</li> <li>serving and eventing\u5404\u81ea\u7368\u7acb\uff0c\u53ef\u5206\u5225\u5b89\u88dd\u4f7f\u7528</li> <li>Knative Serving: Run serverless containers in Kubernetes with ease. Knative takes care of the details of networking, autoscaling (even to zero), and revision tracking. Teams can focus on core logic using any programming language.</li> <li>Knative Eventing: Universal subscription, delivery and management of events. Build modern apps by attaching compute to a data stream with declarative event connectivity and developer friendly object models.</li> </ul>"},{"location":"side-projects/data2ml-ops/others/knative-introduction/#knative-history","title":"Knative History","text":"<ul> <li>Knative \u6700\u521d\u7531 Google \u65bc 2018 \u5e74 7 \u6708\u767c\u8d77\uff0c\u4e26\u8207 IBM\u3001Red Hat\u3001VMware \u548c SAP \u7b49\u516c\u53f8\u5bc6\u5207\u5408\u4f5c\u958b\u767c\u3002</li> <li>\u767c\u5c55\u91cc\u7a0b\u7891<ul> <li>2018 \u5e74 7 \u6708\uff1aKnative \u9996\u6b21\u516c\u958b\u767c\u5e03\u3002</li> <li>2019 \u5e74 3 \u6708\uff1aBuild \u7d44\u4ef6\u6f14\u9032\u70ba Tekton\uff0c\u5c08\u6ce8\u65bc CI/CD\u3002</li> <li>2019 \u5e74 9 \u6708\uff1aServing API \u9054\u5230 v1 \u7248\u672c\u3002</li> <li>2020 \u5e74 7 \u6708\uff1aEventing API \u9054\u5230 v1 \u7248\u672c\u3002</li> <li>2021 \u5e74 11 \u6708\uff1aKnative \u767c\u5e03 1.0 \u7248\u672c\uff0c\u6a19\u8a8c\u8457\u5176\u7a69\u5b9a\u6027\u548c\u5546\u696d\u53ef\u7528\u6027\u3002</li> <li>2022 \u5e74 3 \u6708\uff1aKnative \u6210\u70ba CNCF \u7684\u5b75\u5316\u5c08\u6848\u3002</li> </ul> </li> </ul>"},{"location":"side-projects/data2ml-ops/others/knative-introduction/#knative_1","title":"Knative \u9069\u5408\u4f7f\u7528\u7684\u60c5\u5883","text":"<ul> <li>\u4e8b\u4ef6\u89f8\u767c\u578b\u61c9\u7528\uff08Event-Driven Applications\uff09\u7576\u61c9\u7528\u7a0b\u5f0f\u53ea\u9700\u8981\u5728\u7279\u5b9a\u4e8b\u4ef6\u767c\u751f\u6642\u57f7\u884c\uff0c\u4f8b\u5982\uff1a<ul> <li>GitHub webhook: \u6709 push \u4e8b\u4ef6\u6642\u89f8\u767c\u81ea\u52d5\u90e8\u7f72\u6d41\u7a0b</li> <li>Kafka message: \u6709\u8a0a\u606f\u9032\u5165\u7279\u5b9a topic \u6642\u555f\u52d5\u8655\u7406\u908f\u8f2f</li> <li>IoT \u8cc7\u6599\u4e0a\u50b3: \u8a2d\u5099\u56de\u50b3\u8cc7\u6599\u5c31\u8655\u7406\u4e00\u6b21</li> <li>\u5b9a\u6642\u5de5\u4f5c: \u6bcf10\u5206\u9418\u57f7\u884c\u4e00\u6b21\u8cc7\u6599\u6e05\u6d17\u4efb\u52d9</li> </ul> </li> <li>\u4e0d\u9700\u8981\u4e00\u76f4\u904b\u884c\u7684\u61c9\u7528 (Scale-to-zero)<ul> <li>\u958b\u767c\u74b0\u5883 API: \u4e0d\u5e38\u88ab\u8abf\u7528\uff0c\u4f46\u4e0d\u80fd\u4e0b\u7dda  </li> <li>\u81ea\u52a9\u5831\u8868\u7522\u751f: \u4f7f\u7528\u8005\u9ede\u9078\u6642\u624d\u555f\u52d5\u7522\u751f\u7a0b\u5f0f  </li> <li>\u4f7f\u7528\u8005\u89f8\u767c\u7684\u4efb\u52d9: \u4f8b\u5982\u8cc7\u6599\u5c0e\u5165\u3001\u8f49\u63db\u7b49\u81e8\u6642\u4efb\u52d9  </li> </ul> </li> <li>\u85cd\u7da0\u90e8\u7f72\u8207\u7070\u968e\u767c\u5e03 (Blue-Green / Canary Release)<ul> <li>\u767c\u5e03\u65b0\u7248\u672c: \u53ef\u5c07 10% \u6d41\u91cf\u5c0e\u5411\u65b0\u7248\u672c  </li> <li>\u9010\u6b65\u64f4\u5927\u6d41\u91cf: \u6839\u64da\u5065\u5eb7\u72c0\u6cc1\u6162\u6162\u8f49\u79fb\u6d41\u91cf  </li> <li>\u5feb\u901f\u56de\u9000: \u65b0\u7248\u6709\u554f\u984c\u6642\u7acb\u5373\u5207\u56de\u820a\u7248  </li> </ul> </li> <li>\u7121\u4f3a\u670d\u5668\u51fd\u6578 (FaaS) \u5e73\u53f0\u5efa\u8a2d<ul> <li>\u5efa\u7acb\u4f01\u696d\u5167\u90e8 FaaS: \u958b\u767c\u8005\u53ea\u9700\u63d0\u4f9b container \u6620\u50cf\u5373\u53ef  </li> <li>\u81ea\u5b9a\u7fa9\u89f8\u767c\u689d\u4ef6: \u53ef\u7d81\u5b9a Kafka\u3001Webhook\u3001Cron \u7b49  </li> <li>\u6a19\u6e96\u5316\u4e8b\u4ef6\u683c\u5f0f: \u652f\u63f4 CloudEvents \u6a19\u6e96\u683c\u5f0f  </li> </ul> </li> <li>\u7d50\u5408 DevOps / GitOps \u7684\u5feb\u901f\u4ea4\u4ed8\u5834\u666f<ul> <li>\u81ea\u52d5\u90e8\u7f72 pipeline: Git push \u2192 Tekton build \u2192 Knative deploy  </li> <li>GitOps Workflow: Git \u8a2d\u5b9a\u8b8a\u66f4 \u2192 \u81ea\u52d5\u66f4\u65b0 revision  </li> <li>\u591a\u7248\u672c\u63a7\u7ba1\u8207\u5207\u63db: \u53ef\u5feb\u901f\u5207\u63db\u7248\u672c\u3001\u6e2c\u8a66\u3001\u56de\u9000  </li> </ul> </li> <li>\u8cc7\u6e90\u654f\u611f\u578b\u7684\u5fae\u670d\u52d9\u67b6\u69cb<ul> <li>\u4f7f\u7528\u8005\u5831\u8868\u670d\u52d9: \u767d\u5929\u7528\u91cf\u9ad8\u3001\u665a\u4e0a\u5e7e\u4e4e\u6c92\u4eba\u7528  </li> <li>\u884c\u92b7\u6d3b\u52d5\u7cfb\u7d71: \u6d3b\u52d5\u671f\u9593\u9ad8\u5cf0\uff0c\u5e73\u5e38\u7121\u4eba\u4f7f\u7528  </li> <li>Edge/IoT \u7bc0\u9ede: \u9700\u7bc0\u7701 CPU/Memory \u4f7f\u7528\u91cf  </li> </ul> </li> </ul>"},{"location":"side-projects/data2ml-ops/others/knative-introduction/#knative_2","title":"\u4e0d\u9069\u5408 Knative \u7684\u5834\u666f","text":"<ul> <li>\u9ad8\u541e\u5410\u3001\u4f4e\u5ef6\u9072\u5e38\u99d0\u670d\u52d9: scale-to-zero \u6703\u9020\u6210\u51b7\u555f\u52d5\u5ef6\u9072</li> <li>WebSocket / gRPC Streaming: \u4e0d\u652f\u63f4\u9577\u9023\u7dda\u5354\u5b9a</li> <li>\u975e HTTP \u5354\u8b70: Knative Serving \u76ee\u524d\u53ea\u652f\u63f4 HTTP-based \u8acb\u6c42</li> <li>\u6709\u72c0\u614b\u670d\u52d9: Knative \u50c5\u652f\u63f4 stateless container</li> </ul>"},{"location":"side-projects/data2ml-ops/others/knative-introduction/#eventing","title":"Eventing","text":"<ul> <li>Knative Eventing is a powerful Kubernetes-based framework that enables event-driven application development. It allows developers to build loosely coupled, reactive services that respond to events from various sources. By decoupling producers and consumers of events, Knative Eventing makes it easier to scale, update, and maintain modern cloud-native applications, especially in serverless environments.</li> <li>Knative Eventing uses standard HTTP POST requests to send and receive events between event producers and sinks.</li> <li>These events conform to the CloudEvents specifications, which enables creating, parsing, sending, and receiving events in any programming language.</li> </ul>"},{"location":"side-projects/data2ml-ops/others/knative-introduction/#event-mesh","title":"Event Mesh","text":"Knative Event Mesh <ul> <li>An Event Mesh is dynamic, interconnected infrastructure which is designed to simplify distributing events from senders to recipients.</li> <li>provides asynchronous (store-and-forward) delivery of messages which allows decoupling senders and recipients in time</li> <li>Event Meshes also simplify the routing concerns of senders and recipients by decoupling them from the underlying event transport infrastructure (which may be a federated set of solutions like Kafka, RabbitMQ, or cloud provider infrastructure)</li> <li>The mesh transports events from producers to consumers via a network of interconnected event brokers across any environment, and even between clouds in a seamless and loosely coupled way.</li> <li>Event producers can publish all events to the mesh, which can route events to interested subscribers without needing the application to subdivide events to channels</li> <li>Event consumers can use mesh configuration to receive events of interest using fine-grained filter expressions rather than needing to implement multiple subscriptions and application-level event filtering to select the events of interest.</li> <li>the Broker API offers a discoverable endpoint for event ingress and the Trigger API completes the offering with its event filtering and delivery capabilities</li> </ul> <p>\u7576\u6211\u7b2c\u4e00\u6b21\u63a5\u89f8\u5230 Knative Eventing \u7684 \u201cEvent Mesh\u201d \u6982\u5ff5\u6642\uff0c\u5176\u5be6\u6709\u9ede\u7591\u60d1\u3002\u7562\u7adf\u6211\u4ee5\u524d\u7fd2\u6163\u7528 Kafka \u9019\u7a2e\u6bd4\u8f03\u76f4\u63a5\u7684\u8a0a\u606f\u7cfb\u7d71\uff0cproducer \u628a\u8cc7\u6599\u4e1f\u5230\u67d0\u500b topic\uff0cconsumer \u8a02\u95b1\u90a3\u500b topic\uff0c\u5927\u5bb6\u5404\u53f8\u5176\u8077\u3001\u6e05\u695a\u660e\u77ad\u3002\u4e45\u4e86\u4e5f\u5c31\u7406\u6240\u7576\u7136\u5730\u63a5\u53d7\u4e86\u9019\u7a2e\u300c\u5927\u5bb6\u90fd\u8981\u77e5\u9053\u8a0a\u606f\u53bb\u54ea\u3001\u5f9e\u54ea\u4f86\u300d\u7684\u6a21\u5f0f\u3002</p> <p>\u4f46\u5f8c\u4f86\u6211\u958b\u59cb\u7406\u89e3 Event Mesh \u7684\u6642\u5019\uff0c\u8166\u4e2d\u6709\u4e00\u7a2e\u300c\u554a\uff0c\u539f\u4f86\u9084\u53ef\u4ee5\u9019\u6a23\u8a2d\u8a08\u300d\u7684\u611f\u89ba\u3002Event Mesh \u4e0d\u518d\u8981\u6c42 sender \u8ddf receiver \u90fd\u77e5\u9053\u8a0a\u606f\u901a\u904e\u4e86\u54ea\u689d\u901a\u9053\uff0c\u4e5f\u4e0d\u9700\u8981\u5927\u5bb6\u786c\u7d81\u5728\u540c\u4e00\u500b Kafka topic \u4e0a\u3002\u76f8\u53cd\u5730\uff0c\u5b83\u5f37\u8abf\u7684\u662f\u300c\u4e8b\u4ef6\u672c\u8eab\u300d\uff0c\u6bd4\u5982\u9019\u662f\u4e00\u500b\u4f86\u81ea\u67d0\u500b\u4f86\u6e90\u3001\u67d0\u7a2e\u985e\u578b\u3001\u767c\u751f\u5728\u67d0\u500b\u6642\u9593\u9ede\u7684\u4e8b\u4ef6\u2014\u2014\u9019\u4e9b\u5c6c\u6027\u624d\u662f\u5b83\u80fd\u4e0d\u80fd\u88ab\u8655\u7406\u7684\u95dc\u9375\u3002\u7cfb\u7d71\u6703\u6839\u64da\u9019\u4e9b\u5c6c\u6027\uff0c\u81ea\u52d5\u628a\u4e8b\u4ef6\u9001\u5230\u771f\u6b63\u9700\u8981\u5b83\u7684\u5730\u65b9\u3002\u9019\u4e2d\u9593\u7684\u8def\u600e\u9ebc\u8d70\uff0c\u4e0d\u518d\u662f\u61c9\u7528\u7a0b\u5f0f\u7684\u8cac\u4efb\uff0c\u800c\u662f Event Mesh \u5e6b\u4f60\u8655\u7406\u597d\u3002</p> <p>\u6700\u8b93\u6211\u9a5a\u8a1d\u7684\u662f\uff0c\u5b83\u751a\u81f3\u53ef\u4ee5\u5728\u80cc\u5f8c\u540c\u6642\u7528 Kafka\u3001RabbitMQ\uff0c\u751a\u81f3 cloud provider \u7684 Pub/Sub\uff0c\u4f5c\u70ba\u4e8b\u4ef6\u50b3\u8f38\u7684\u5e95\u5c64\u3002\u63db\u53e5\u8a71\u8aaa\uff0c\u4f60\u4e0d\u7528\u9078\u908a\u7ad9\uff0c\u4e5f\u4e0d\u9700\u8981\u5728\u8a2d\u8a08\u521d\u671f\u5c31\u7d81\u6b7b\u5728\u67d0\u500b\u6280\u8853\u4e0a\u3002\u53ea\u8981\u4e8b\u4ef6\u9001\u5f97\u51fa\u4f86\uff0c\u6709\u8208\u8da3\u7684\u670d\u52d9\u5c31\u6703\u6536\u5230\uff0c\u4e0d\u9700\u8981\u4e8b\u524d\u7d04\u597d topic\u3001\u4e5f\u4e0d\u9700\u8981\u7dad\u8b77\u4e00\u5806 subscriptions\u3002\u9019\u7a2e\u9b06\u8026\u5408\u7684\u8a2d\u8a08\u8b93\u7cfb\u7d71\u64f4\u5c55\u8d77\u4f86\u8f15\u9b06\u5f88\u591a\uff0c\u958b\u767c\u8d77\u4f86\u4e5f\u6bd4\u8f03\u81ea\u7531\uff0c\u7279\u5225\u9069\u5408\u5fae\u670d\u52d9\u6216 serverless \u67b6\u69cb\u3002</p> <p>\u5982\u679c\u4f60\u4e5f\u66fe\u7d93\u89ba\u5f97 Kafka \u7684 topic \u8a2d\u8a08\u5f88\u9748\u6d3b\u4f46\u8d8a\u4f86\u8d8a\u96e3\u7ba1\u7406\uff0c\u90a3\u6211\u771f\u7684\u5f88\u63a8\u85a6\u4f60\u770b\u770b Event Mesh \u7684\u505a\u6cd5\u3002\u5b83\u8b93\u4e8b\u4ef6\u6210\u70ba\u67b6\u69cb\u7684\u4e3b\u89d2\uff0c\u800c\u4e0d\u662f\u67d0\u500b\u5de5\u5177\u6216\u5e73\u53f0\u3002\u9019\u7a2e\u8f49\u8b8a\uff0c\u5c0d\u6211\u4f86\u8aaa\u4e0d\u53ea\u662f\u6280\u8853\u4e0a\u7684\u6f14\u9032\uff0c\u4e5f\u662f\u4e00\u7a2e\u601d\u7dad\u4e0a\u7684\u91cb\u653e\u3002</p> Event Sources <ul> <li>An event source is a Kubernetes custom resource (CR), created by a developer or cluster administrator, that acts as a link between an event producer and an event sink. A sink can be a k8s service, including Knative Services, a Channel, or a Broker that receives events from an event source.</li> <li>Apache Kafka, RabbitMQ, Amazon S3, Amazon SQS etc.</li> </ul> Brokers <p> Brokers are Kubernetes custom resources that define an event mesh for collecting a pool of events. Brokers provide a discoverable endpoint for event ingress, and use Triggers for event delivery. Event producers can send events to a broker by POSTing the event.</p>"},{"location":"side-projects/data2ml-ops/others/knative-introduction/#triggers","title":"Triggers","text":"<p>A trigger represents a desire to subscribe to events from a specific broker.</p>"},{"location":"side-projects/data2ml-ops/others/knative-introduction/#event-sinks","title":"Event Sinks","text":"<ul> <li>When you create an event source, you can specify a sink where events are sent to from the source. A sink is an Addressable or a Callable resource that can receive incoming events from other resources.</li> <li>Knative Services, Channels, and Brokers are all examples of sinks.</li> <li>Amazon S3, SNS, SQS, Kafka, Logger, Redis Sink,</li> </ul> <p>The core components of Knative Eventing include Event Sources, Brokers, Triggers and Sink. Event sources can originate from systems like GitHub (webhooks), Apache Kafka, CronJobs, Kubernetes API server events, or even custom containers that emit CloudEvents. A Broker acts as a central event mesh that receives and buffers incoming events. Triggers are routing rules that filter events from the Broker and forward them to services based on specific criteria. Currently, the most common event delivery mechanism is HTTP using the CloudEvents specification, typically in a push-based manner to HTTP endpoints such as Knative Services.</p> <p>For example, imagine you\u2019ve deployed Knative Eventing with a PingSource that emits an event every minute. This event is sent to a Broker, which acts as an event hub. A Trigger listens on that Broker and filters events based on attributes like the event type. When a matching event arrives, the Trigger forwards it to a Knative Service (a containerized HTTP handler). Behind the scenes, Knative handles service discovery, traffic routing, autoscaling (even from zero), and ensures that the container is activated just-in-time to handle the event. This creates a seamless, scalable, and efficient event-driven pipeline without needing to manage infrastructure manually.</p>"},{"location":"side-projects/data2ml-ops/others/knative-introduction/#serving","title":"Serving","text":""},{"location":"side-projects/data2ml-ops/others/knative-introduction/#resources","title":"Resources","text":"Knative Serving Resources: Services, Routes, Configurations, Revisions <ul> <li>Service: The main entry point that manages the full lifecycle of your app.</li> <li>Route: Sends traffic to specific revisions, with support for splitting and naming.</li> <li>Configuration: Stores deployment settings; changes create new revisions.</li> <li>Revision: A read-only snapshot of code and config that auto-scales with traffic.</li> </ul>"},{"location":"side-projects/data2ml-ops/others/knative-introduction/#componenets","title":"Componenets","text":"Knative Serving Architecture <ul> <li>Activator:<ul> <li>It is responsible to queue incoming requests (if a Knative Service is scaled-to-zero)</li> <li>It communicates with the autoscaler to bring scaled-to-zero Services back up and forward the queued requests.</li> <li>Activator can also act as a request buffer to handle traffic bursts.</li> </ul> </li> <li>Autoscaler: scale the Knative Services based on configuration, metrics and incoming requests.</li> <li>Controller: manages the state of Knative resources within the cluster</li> <li>Queue-proxy:<ul> <li>The Queue-Proxy is a sidecar container in the Knative Service's Pod.</li> <li>collect metrics and enforcing the desired concurrency when forwarding requests to the user's container</li> <li>a queue if necessary, similar to the Activator.</li> </ul> </li> <li>Webhooks: validate and mutate Knative Resources.</li> </ul>"},{"location":"side-projects/data2ml-ops/others/knative-introduction/#networking-layer","title":"Networking Layer","text":"<ul> <li>Knative Serving depends on a Networking Layer that fulfils the Knative Networking Specification.</li> <li>Knative Serving defines an internal <code>KIngress</code> resource, which acts as an abstraction for different multiple pluggable networking layers</li> <li>Currently, three networking layers are available and supported by the community:<ul> <li>net-istio</li> <li>net-kourier</li> <li>net-contour</li> </ul> </li> </ul> <p>How does the network traffic flow?</p> Knative Serving Network Traffic Flow <ul> <li>The <code>Ingress Gateway</code> is used to route requests to the activator (proxy mode) or directly to a Knative Service Pod (serve mode), depending on the mode (proxy/serve, see here for more details).</li> <li>Each networking layer has a controller that is responsible to watch the KIngress resources and configure the Ingress Gateway accordingly.</li> <li>For the Ingress Gateway to be reachable outside the cluster, it must be exposed using a Kubernetes Service of <code>type: LoadBalancer</code> or <code>type: NodePort</code></li> </ul>"},{"location":"side-projects/data2ml-ops/others/knative-introduction/#references","title":"References","text":"<ul> <li>Knative Serving | Knative</li> <li>Knative Serving Architecture | Knative</li> <li>Knative Eventing | Knative</li> <li>Event Mesh | Knative</li> <li>Event Sources | Knative</li> <li>About Sinks | Knative</li> <li>About Brokers | Knative</li> <li>Using Triggers | Knative</li> </ul>"},{"location":"side-projects/data2ml-ops/others/mlserver-introduction/","title":"MLServer","text":"<ul> <li>MLServer is an open source inference server for your machine learning models.</li> <li>MLServer aims to provide an easy way to start serving your machine learning models through a REST and gRPC interface</li> </ul>"},{"location":"side-projects/data2ml-ops/others/mlserver-introduction/#inference-runtimes","title":"Inference Runtimes","text":"<ul> <li>Inference runtimes allow you to define how your model should be used within MLServer. You can think of them as the backend glue between MLServer and your machine learning framework of choice.</li> <li>Out of the box, MLServer comes with a set of pre-packaged runtimes which let you interact with a subset of common ML frameworks. This allows you to start serving models saved in these frameworks straight away.</li> </ul> MLServer Inference Runtime MLServer Supported Inference Runtime"},{"location":"side-projects/data2ml-ops/others/mlserver-introduction/#openapi-support","title":"OpenAPI Support","text":"<ul> <li>OpenAPI spec: dataplane.json</li> <li>MLServer follows the Open Inference Protocol (previously known as the \u201cV2 Protocol\u201d). fully compliant with KServing\u2019s V2 Dataplane spec.</li> <li>Support Swagger UI: <ul> <li>The autogenerated Swagger UI can be accessed under the <code>/v2/docs</code> endpoint.</li> <li>MLServer will also autogenerate a Swagger UI tailored to individual models, showing the endpoints available for each one. under the following endpoints:<ul> <li><code>/v2/models/{model_name}/docs</code></li> <li><code>/v2/models/{model_name}/versions/{model_version}/docs</code></li> </ul> </li> </ul> </li> </ul> MLServer Swagger UI MLServer Model Swagger UI"},{"location":"side-projects/data2ml-ops/others/mlserver-introduction/#parallel-inference","title":"Parallel Inference","text":"MLServer Parallel Inference <ul> <li>Python has some native issues</li> <li>The Global Interpreter Lock (GIL) is a mutex lock that exists in most Python interpreters (e.g. CPython). Its main purpose is to lock Python\u2019s execution so that it only runs on a single processor at the same time. This simplifies certain things to the interpreter. However, it also adds the limitation that a single Python process will never be able to leverage multiple cores.</li> <li>Out of the box, MLServer overcome the python native issue, to support to offload inference workloads to a pool of workers running in separate processes.</li> <li><code>parallel_workers</code> on the <code>settings.json</code> file</li> </ul>"},{"location":"side-projects/data2ml-ops/others/mlserver-introduction/#multi-model-servingmms","title":"Multi-Model Serving(MMs)","text":"<ul> <li>within a single instance of MLServer, you can serve multiple models under different paths. This also includes multiple versions of the same model.</li> </ul>"},{"location":"side-projects/data2ml-ops/others/mlserver-introduction/#adaptive-batching","title":"Adaptive Batching","text":"<ul> <li>MLServer includes support to batch requests together transparently on-the-fly. We refer to this as \u201cadaptive batching\u201d, although it can also be known as \u201cpredictive batching\u201d.</li> <li>Why? <ul> <li>Maximise resource usage</li> <li>Minimise any inference overhead</li> </ul> </li> <li>\u9700\u8981\u4ed4\u7d30\u8abf\u6574\uff0c\u56e0\u6b64MLServer won\u2019t enable by default adaptive batching on newly loaded models.</li> <li>Usage: <code>max_batch_size</code>, <code>max_batch_time</code> on the <code>model-settings.json</code> file</li> </ul>"},{"location":"side-projects/data2ml-ops/others/mlserver-introduction/#metrics","title":"Metrics","text":"<ul> <li>Out-of-the-box, MLServer exposes a set of metrics that help you monitor your machine learning workloads in production.</li> <li>On top of these, you can also register and track your own custom metrics as part of your custom inference runtimes.</li> <li>Default Metrics<ul> <li><code>model_infer_request_success</code>: Number of successful inference requests.</li> <li><code>model_infer_request_failure</code>: Number of failed inference requests.</li> <li><code>batch_request_queue</code>: Queue size for the adaptive batching queue.</li> <li><code>parallel_request_queue</code>: Queue size for the inference workers queue.</li> </ul> </li> <li>REST Metrics<ul> <li><code>[rest_server]_requests</code>: Number of REST requests, labelled by endpoint and status code.</li> <li><code>[rest_server]_requests_duration_seconds</code>: Latency of REST requests.</li> <li><code>[rest_server]_requests_in_progress</code>: Number of in-flight REST requests.</li> </ul> </li> <li>gRPC Metrics<ul> <li><code>grpc_server_handled</code>: Number of gRPC requests, labelled by gRPC code and method.</li> <li><code>grpc_server_started</code>: Number of in-flight gRPC requests.</li> </ul> </li> </ul>"},{"location":"side-projects/data2ml-ops/others/mlserver-introduction/#reference","title":"Reference","text":"<ul> <li>Inference Runtimes | MLServer</li> <li>Multi-Model Serving | MLServer</li> </ul>"},{"location":"side-projects/data2ml-ops/ray/deployment/","title":"Deploy Ray Cluster on Kubernetes Using KubeRay","text":"<p>KubeRay simplifies managing Ray clusters on Kubernetes by introducing three key Custom Resource Definitions (CRDs): RayCluster, RayJob, and RayService. These CRDs make it easy to tailor Ray clusters for different use cases.<sup>1</sup></p> <p>The KubeRay operator offers a Kubernetes-native approach to managing Ray clusters. A typical Ray cluster includes a head node pod and multiple worker node pods. With optional autoscaling, the operator can dynamically adjust the cluster size based on workload demands, adding or removing pods as needed.<sup>1</sup></p> What is KubeRay?<sup>1</sup> Architecture (Click to Enlarge) <p>Setting up KubeRay is straightforward. This guide will walk you through installing the KubeRay operator and deploying your first Ray cluster using Helm. By the end, you'll have a fully functional Ray environment running on your Kubernetes cluster.<sup>2</sup><sup>3</sup></p>"},{"location":"side-projects/data2ml-ops/ray/deployment/#install-kuberay-operator","title":"Install KubeRay Operator","text":"<p>Start by adding the KubeRay Helm repository to access the required charts:</p> <pre><code>helm repo add kuberay https://ray-project.github.io/kuberay-helm/\n</code></pre> <pre><code>\"kuberay\" has been added to your repositories\n</code></pre> <p>Update your local Helm chart list to ensure you're using the latest version:</p> <pre><code>helm repo update\n</code></pre> <pre><code>Hang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"kuberay\" chart repository\nUpdate Complete. \u2388Happy Helming!\u2388\n</code></pre> <p>Next, create a namespace to manage KubeRay resources:</p> <pre><code>kubectl create ns kuberay\n</code></pre> <pre><code>namespace/kuberay created\n</code></pre> <p>Now, install the KubeRay operator in the namespace. This sets up the controller to manage Ray clusters:</p> <pre><code>helm install kuberay-operator kuberay/kuberay-operator \\\n  --version 1.3.0 \\\n  -n kuberay\n</code></pre> <pre><code>NAME: kuberay-operator\nLAST DEPLOYED: Wed May 14 20:29:44 2025\nNAMESPACE: kuberay\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <p>Verify that the KubeRay operator pod is running:</p> <pre><code>kubectl get pods -n kuberay\n</code></pre> <pre><code>NAME                                READY   STATUS    RESTARTS   AGE\nkuberay-operator-66d848f5cd-5npp6   1/1     Running   0          23s\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/deployment/#deploy-a-ray-cluster","title":"Deploy a Ray Cluster","text":"<p>Export the default <code>values.yaml</code> file to customize memory settings. If you've encountered OOM issues, it's a good idea to increase memory allocation upfront.<sup>4</sup></p> <pre><code>helm show values kuberay/ray-cluster &gt; values.yaml\nnano values.yaml\n</code></pre> values.yaml values.yaml<pre><code># Default values for ray-cluster.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\n# The KubeRay community welcomes PRs to expose additional configuration\n# in this Helm chart.\n\nimage:\n  repository: rayproject/ray\n  tag: 2.41.0\n  pullPolicy: IfNotPresent\n\nnameOverride: \"kuberay\"\nfullnameOverride: \"\"\n\nimagePullSecrets: []\n  # - name: an-existing-secret\n\n# common defined values shared between the head and worker\ncommon:\n  # containerEnv specifies environment variables for the Ray head and worker containers.\n  # Follows standard K8s container env schema.\n  containerEnv: []\n  #  - name: BLAH\n  #    value: VAL\nhead:\n  # rayVersion determines the autoscaler's image version.\n  # It should match the Ray version in the image of the containers.\n  # rayVersion: 2.41.0\n  # If enableInTreeAutoscaling is true, the autoscaler sidecar will be added to the Ray head pod.\n  # Ray autoscaler integration is supported only for Ray versions &gt;= 1.11.0\n  # Ray autoscaler integration is Beta with KubeRay &gt;= 0.3.0 and Ray &gt;= 2.0.0.\n  # enableInTreeAutoscaling: true\n  # autoscalerOptions is an OPTIONAL field specifying configuration overrides for the Ray autoscaler.\n  # The example configuration shown below represents the DEFAULT values.\n  # autoscalerOptions:\n    # upscalingMode: Default\n    # idleTimeoutSeconds is the number of seconds to wait before scaling down a worker pod which is not using Ray resources.\n    # idleTimeoutSeconds: 60\n    # imagePullPolicy optionally overrides the autoscaler container's default image pull policy (IfNotPresent).\n    # imagePullPolicy: IfNotPresent\n    # Optionally specify the autoscaler container's securityContext.\n    # securityContext: {}\n    # env: []\n    # envFrom: []\n    # resources specifies optional resource request and limit overrides for the autoscaler container.\n    # For large Ray clusters, we recommend monitoring container resource usage to determine if overriding the defaults is required.\n    # resources:\n    #   limits:\n    #     cpu: \"500m\"\n    #     memory: \"512Mi\"\n    #   requests:\n    #     cpu: \"500m\"\n    #     memory: \"512Mi\"\n  labels: {}\n  # Note: From KubeRay v0.6.0, users need to create the ServiceAccount by themselves if they specify the `serviceAccountName`\n  # in the headGroupSpec. See https://github.com/ray-project/kuberay/pull/1128 for more details.\n  serviceAccountName: \"\"\n  restartPolicy: \"\"\n  rayStartParams: {}\n  # containerEnv specifies environment variables for the Ray container,\n  # Follows standard K8s container env schema.\n  containerEnv: []\n  # - name: EXAMPLE_ENV\n  #   value: \"1\"\n  envFrom: []\n    # - secretRef:\n    #     name: my-env-secret\n  # ports optionally allows specifying ports for the Ray container.\n  # ports: []\n  # resource requests and limits for the Ray head container.\n  # Modify as needed for your application.\n  # Note that the resources in this example are much too small for production;\n  # we don't recommend allocating less than 8G memory for a Ray pod in production.\n  # Ray pods should be sized to take up entire K8s nodes when possible.\n  # Always set CPU and memory limits for Ray pods.\n  # It is usually best to set requests equal to limits.\n  # See https://docs.ray.io/en/latest/cluster/kubernetes/user-guides/config.html#resources\n  # for further guidance.\n  resources:\n    limits:\n      cpu: \"1\"\n      # To avoid out-of-memory issues, never allocate less than 2G memory for the Ray head.\n      memory: \"4G\"\n    requests:\n      cpu: \"1\"\n      memory: \"4G\"\n  annotations: {}\n  nodeSelector: {}\n  tolerations: []\n  affinity: {}\n  # Pod security context.\n  podSecurityContext: {}\n  # Ray container security context.\n  securityContext: {}\n  # Optional: The following volumes/volumeMounts configurations are optional but recommended because\n  # Ray writes logs to /tmp/ray/session_latests/logs instead of stdout/stderr.\n  volumes:\n    - name: log-volume\n      emptyDir: {}\n  volumeMounts:\n    - mountPath: /tmp/ray\n      name: log-volume\n  # sidecarContainers specifies additional containers to attach to the Ray pod.\n  # Follows standard K8s container spec.\n  sidecarContainers: []\n  # See docs/guidance/pod-command.md for more details about how to specify\n  # container command for head Pod.\n  command: []\n  args: []\n  # Optional, for the user to provide any additional fields to the service.\n  # See https://pkg.go.dev/k8s.io/Kubernetes/pkg/api/v1#Service\n  headService: {}\n    # metadata:\n    #   annotations:\n    #     prometheus.io/scrape: \"true\"\n\n  # Custom pod DNS configuration\n  # See https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-dns-config\n  # dnsConfig:\n  #   nameservers:\n  #     - 8.8.8.8\n  #   searches:\n  #     - example.local\n  #   options:\n  #     - name: ndots\n  #       value: \"2\"\n  #     - name: edns0\n  topologySpreadConstraints: {}\n\n\nworker:\n  # If you want to disable the default workergroup\n  # uncomment the line below\n  # disabled: true\n  groupName: workergroup\n  replicas: 1\n  minReplicas: 1\n  maxReplicas: 3\n  labels: {}\n  serviceAccountName: \"\"\n  restartPolicy: \"\"\n  rayStartParams: {}\n  # containerEnv specifies environment variables for the Ray container,\n  # Follows standard K8s container env schema.\n  containerEnv: []\n  # - name: EXAMPLE_ENV\n  #   value: \"1\"\n  envFrom: []\n    # - secretRef:\n    #     name: my-env-secret\n  # ports optionally allows specifying ports for the Ray container.\n  # ports: []\n  # resource requests and limits for the Ray head container.\n  # Modify as needed for your application.\n  # Note that the resources in this example are much too small for production;\n  # we don't recommend allocating less than 8G memory for a Ray pod in production.\n  # Ray pods should be sized to take up entire K8s nodes when possible.\n  # Always set CPU and memory limits for Ray pods.\n  # It is usually best to set requests equal to limits.\n  # See https://docs.ray.io/en/latest/cluster/kubernetes/user-guides/config.html#resources\n  # for further guidance.\n  resources:\n    limits:\n      cpu: \"1\"\n      memory: \"3G\"\n    requests:\n      cpu: \"1\"\n      memory: \"3G\"\n  annotations: {}\n  nodeSelector: {}\n  tolerations: []\n  affinity: {}\n  # Pod security context.\n  podSecurityContext: {}\n  # Ray container security context.\n  securityContext: {}\n  # Optional: The following volumes/volumeMounts configurations are optional but recommended because\n  # Ray writes logs to /tmp/ray/session_latests/logs instead of stdout/stderr.\n  volumes:\n    - name: log-volume\n      emptyDir: {}\n  volumeMounts:\n    - mountPath: /tmp/ray\n      name: log-volume\n  # sidecarContainers specifies additional containers to attach to the Ray pod.\n  # Follows standard K8s container spec.\n  sidecarContainers: []\n  # See docs/guidance/pod-command.md for more details about how to specify\n  # container command for worker Pod.\n  command: []\n  args: []\n  topologySpreadConstraints: {}\n\n\n  # Custom pod DNS configuration\n  # See https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-dns-config\n  # dnsConfig:\n  #   nameservers:\n  #     - 8.8.8.8\n  #   searches:\n  #     - example.local\n  #   options:\n  #     - name: ndots\n  #       value: \"2\"\n  #     - name: edns0\n\n# The map's key is used as the groupName.\n# For example, key:small-group in the map below\n# will be used as the groupName\nadditionalWorkerGroups:\n  smallGroup:\n    # Disabled by default\n    disabled: true\n    replicas: 0\n    minReplicas: 0\n    maxReplicas: 3\n    labels: {}\n    serviceAccountName: \"\"\n    restartPolicy: \"\"\n    rayStartParams: {}\n    # containerEnv specifies environment variables for the Ray container,\n    # Follows standard K8s container env schema.\n    containerEnv: []\n      # - name: EXAMPLE_ENV\n      #   value: \"1\"\n    envFrom: []\n        # - secretRef:\n        #     name: my-env-secret\n    # ports optionally allows specifying ports for the Ray container.\n    # ports: []\n    # resource requests and limits for the Ray head container.\n    # Modify as needed for your application.\n    # Note that the resources in this example are much too small for production;\n    # we don't recommend allocating less than 8G memory for a Ray pod in production.\n    # Ray pods should be sized to take up entire K8s nodes when possible.\n    # Always set CPU and memory limits for Ray pods.\n    # It is usually best to set requests equal to limits.\n    # See https://docs.ray.io/en/latest/cluster/kubernetes/user-guides/config.html#resources\n    # for further guidance.\n    resources:\n      limits:\n        cpu: 1\n        memory: \"3G\"\n      requests:\n        cpu: 1\n        memory: \"3G\"\n    annotations: {}\n    nodeSelector: {}\n    tolerations: []\n    affinity: {}\n    # Pod security context.\n    podSecurityContext: {}\n    # Ray container security context.\n    securityContext: {}\n    # Optional: The following volumes/volumeMounts configurations are optional but recommended because\n    # Ray writes logs to /tmp/ray/session_latests/logs instead of stdout/stderr.\n    volumes:\n      - name: log-volume\n        emptyDir: {}\n    volumeMounts:\n      - mountPath: /tmp/ray\n        name: log-volume\n    sidecarContainers: []\n    # See docs/guidance/pod-command.md for more details about how to specify\n    # container command for worker Pod.\n    command: []\n    args: []\n\n    # Topology Spread Constraints for worker pods\n    # See: https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/\n    topologySpreadConstraints: {}\n\n    # Custom pod DNS configuration\n    # See https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-dns-config\n    # dnsConfig:\n    #   nameservers:\n    #     - 8.8.8.8\n    #   searches:\n    #     - example.local\n    #   options:\n    #     - name: ndots\n    #       value: \"2\"\n    #     - name: edns0\n\n# Configuration for Head's Kubernetes Service\nservice:\n  # This is optional, and the default is ClusterIP.\n  type: ClusterIP\n</code></pre> <p>Install the Ray cluster using the customized <code>values.yaml</code>. Here, we're using the image tag <code>2.46.0-py310-aarch64</code> for Python 3.10, Ray 2.46.0, and MacOS ARM architecture. You can find all supported Ray images on Docker Hub.<sup>5</sup></p> <pre><code>helm install raycluster kuberay/ray-cluster \\\n  --version 1.3.0 \\\n  --set 'image.tag=2.46.0-py310-aarch64' \\\n  -n kuberay \\\n  -f values.yaml\n</code></pre> <pre><code>NAME: raycluster\nLAST DEPLOYED: Wed May 14 20:31:53 2025\nNAMESPACE: kuberay\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <p>Once the RayCluster CR is created, you can check its status:</p> <pre><code>kubectl get rayclusters -n kuberay\n</code></pre> <pre><code>NAME                 DESIRED WORKERS   AVAILABLE WORKERS   CPUS   MEMORY   GPUS   STATUS   AGE\nraycluster-kuberay   1                                     2      4G       0               62s\n</code></pre> <p>To view the running pods in your Ray cluster, use:</p> <pre><code>kubectl get pods --selector=ray.io/cluster=raycluster-kuberay -n kuberay\n</code></pre> <pre><code>NAME                                          READY   STATUS    RESTARTS   AGE\nraycluster-kuberay-head-k6ktp                 1/1     Running   0          5m49s\nraycluster-kuberay-workergroup-worker-zrxbj   1/1     Running   0          5m49s\n</code></pre> <ol> <li> <p>Ray on Kubernetes | Ray Docs \u21a9\u21a9\u21a9</p> </li> <li> <p>KubeRay Operator Installation | Ray Docs \u21a9</p> </li> <li> <p>RayCluster Quickstart | Ray Docs \u21a9</p> </li> <li> <p>Out-Of-Memory Prevention | Ray Docs \u21a9</p> </li> <li> <p>rayproject/ray | Docker Hub \u21a9</p> </li> </ol>"},{"location":"side-projects/data2ml-ops/ray/how-it-works/","title":"How It Works?","text":""},{"location":"side-projects/data2ml-ops/ray/how-it-works/#key-modules-and-their-roles","title":"Key Modules and Their Roles","text":"<p>Ray Core is a powerful distributed computing framework that provides a small set of essential primitives (tasks, actors, and objects) for building and scaling distributed applications.<sup>1</sup></p> <p>On top of Ray Core, Ray provides different AI libraries for different ML workloads.</p> Key Modules<sup>2</sup> Module Description Details Ray Data Scalable datasets for ML Ray Data provides distributed data processing optimized for machine learning and AI workloads. It efficiently streams data through data pipelines.<sup>2</sup> Ray Train Distributed model training Ray Train makes distributed model training simple. It abstracts away the complexity of setting up distributed training across popular frameworks like PyTorch and TensorFlow.<sup>2</sup> Ray Tune Hyperparameter tuning at scale Ray Tune is a library for hyperparameter tuning at any scale. It automatically finds the best hyperparameters for your models with efficient distributed search algorithms.<sup>2</sup> Ray Serve Scalable model serving Ray Serve provides scalable and programmable serving for ML models and business logic. Deploy models from any framework with production-ready performance.<sup>2</sup> Ray RLlib Industry-grade reinforcement learning RLlib is a reinforcement learning (RL) library that offers high performance implementations of popular RL algorithms and supports various training environments. RLlib offers high scalability and unified APIs for a variety of industry- and research applications.<sup>2</sup>"},{"location":"side-projects/data2ml-ops/ray/how-it-works/#architecture-components","title":"Architecture Components","text":"Ray Cluster<sup>3</sup> Ray Cluster <p>A Ray cluster consists of a single head node and any number of connected worker nodes. Ray nodes are implemented as pods when running on Kubernetes.<sup>3</sup></p> Head Node <p>Every Ray cluster has one node which is designated as the head node of the cluster. The head node is identical to other worker nodes, except that it also runs singleton processes responsible for cluster management such as the autoscaler, GCS and the Ray driver processes which run Ray jobs.<sup>3</sup></p> Worker Node <p>Worker nodes do not run any head node management processes, and serve only to run user code in Ray tasks and actors.<sup>3</sup></p> Autoscaling <p>When the resource demands of the Ray workload exceed the current capacity of the cluster, the autoscaler will try to increase the number of worker nodes. When worker nodes sit idle, the autoscaler will remove worker nodes from the cluster.<sup>3</sup></p> Ray Jobs <p>A Ray job is a single application: it is the collection of Ray tasks, objects, and actors that originate from the same script. There are two ways to run a Ray job on a Ray cluster: (1) Ray Jobs API and (2) Run the driver script directly on the Ray cluster.<sup>3</sup></p> 2 Ways of running Ray Jobs<sup>3</sup>"},{"location":"side-projects/data2ml-ops/ray/how-it-works/#core-concepts","title":"Core Concepts","text":"Ray CoreRay DataRay TrainRay TuneRay Serve Tasks <p>Ray enables arbitrary functions to execute asynchronously on separate Python workers. These asynchronous Ray functions are called tasks. Ray enables tasks to specify their resource requirements in terms of CPUs, GPUs, and custom resources.<sup>4</sup></p> Actors <p>Actors extend the Ray API from functions (tasks) to classes. An actor is essentially a stateful worker (or a service).<sup>4</sup></p> Objects <p>Tasks and actors create objects and compute on objects. You can refer to these objects as remote objects. Ray caches remote objects in its distributed shared-memory object store.<sup>4</sup></p> Datasets <p><code>Dataset</code> is the main user-facing Python API. It represents a distributed data collection and define data loading and processing operations. The Dataset API is lazy. Each Dataset consists of blocks.<sup>5</sup></p> Blocks <p>Each Dataset consists of blocks. A block is a contiguous subset of rows from a dataset, which are distributed across the cluster and processed independently in parallel.<sup>5</sup></p> <p> Datasets and Blocks<sup>5</sup> </p> Training Function <p>The training function is a user-defined Python function that contains the end-to-end model training loop logic. When launching a distributed training job, each worker executes this training function.<sup>6</sup></p> Workers <p>Ray Train distributes model training compute to individual worker processes across the cluster. Each worker is a process that executes the training funciton.<sup>6</sup></p> Scaling Configuration <p>The <code>ScalingConfig</code> is the mechanism for defining the scale of the training job. Two common parameters are <code>num_workers</code> and <code>use_gpu</code>.<sup>6</sup></p> Trainer <p>The Trainer ties the previous three concepts together to launch distributed training jobs.<sup>6</sup></p> <p> Ray Tune Configuration<sup>7</sup> </p> Search Space <p>A search space defines valid values for your hyperparameters and can specify how these values are sampled. Tune offers various functions to define search spaces and sampling methods.<sup>7</sup><sup>8</sup></p> Search Algorithms <p>To optimize the hyperparameters of your training process, you use a Search Algorithm which suggests hyperparameter configurations. Tune has Search Algorithms that integrate with many popular optimization libraries, such as HyperOpt or Optuna.<sup>7</sup> Tune automatically converts the provided search space into the search spaces the search algorithms and underlying libraries expect.</p> Schedulers <p>In short, schedulers can stop, pause, or tweak the hyperparameters of running trials, potentially making your hyperparameter tuning process much faster. Tune includes distributed implementations of early stopping algorithms such as Median Stopping Rule, HyperBand, and ASHA. Tune also includes a distributed implementation of Population Based Training (PBT) and Population Based Bandits (PB2). When using schedulers, you may face compatibility issues<sup>7</sup></p> Trainables <p>In short, a Trainable is an object that you can pass into a Tune run. Ray Tune has two ways of defining a trainable, namely the Function API and the Class API. The Function API is generally recommended.<sup>7</sup></p> Trials <p>You use <code>Tuner.fit()</code> to execute and manage hyperparameter tuning and generate your trials. The Tuner.fit() function also provides many features such as logging, checkpointing, and early stopping.<sup>7</sup></p> Analyses <p><code>Tuner.fit()</code> returns an <code>ResultGrid</code> object which has methods you can use for analyzing your training.<sup>7</sup></p> Deployment <p>A deployment contains business logic or an ML model to handle incoming requests and can be scaled up to run across a Ray cluster. At runtime, a deployment consists of a number of replicas, which are individual copies of the class or function that are started in separate Ray Actors (processes).<sup>9</sup></p> Application <p>An application is the unit of upgrade in a Ray Serve cluster. An application consists of one or more deployments. One of these deployments is considered the \u201cingress\u201d deployment, which handles all inbound traffic.<sup>9</sup></p> DeploymentHandle (composing deployments) <p>Ray Serve enables flexible model composition and scaling by allowing multiple independent deployments to call into each other.<sup>9</sup></p> Ingress deployment (HTTP handling) <p>The ingress deployment defines the HTTP handling logic for the application.<sup>9</sup></p>"},{"location":"side-projects/data2ml-ops/ray/how-it-works/#behind-the-scenes","title":"Behind the Scenes","text":"Ray DataRay TrainRay Tune <p>Ray Data uses a two-phase planning process to execute operations efficiently:<sup>5</sup></p> <ul> <li>Logical plans consist of logical operators that describe what operation to perform.</li> <li>Physical plans consist of physical operators that describe how to execute the operation.</li> </ul> <p>The building blocks of these plans are operators:<sup>5</sup></p> <ul> <li>Logical plans consist of logical operators that describe what operation to perform.</li> <li>Physical plans consist of physical operators that describe how to execute the operation.</li> </ul> <p>Ray Data uses a streaming execution model to efficiently process large datasets. It can process data in a streaming fashion through a pipeline of operations.<sup>5</sup></p> <p> Streaming Topology<sup>5</sup> </p> <p>In the streaming execution model, operators are connected in a pipeline, with each operator\u2019s output queue feeding directly into the input queue of the next downstream operator. This creates an efficient flow of data through the execution plan.<sup>5</sup></p> <p>Calling the <code>Trainer.fit()</code> method executes the training job by<sup>6</sup>:</p> <ol> <li>Launching workers as defined by the <code>scaling_config</code>.</li> <li>Setting up the framework's distributed environment on all workers.</li> <li>Running the training function on all workers.</li> </ol> <p>Calling the <code>Trainer.fit()</code> method executes the tuning job by:</p> <ol> <li>The driver process launches and schedules trials across the Ray cluster based on the search space and resources defined.</li> <li>Each trial runs as a Ray task or actor on worker nodes, executing training functions in parallel.</li> <li>Results are collected, and once all trials finish (or meet stop criteria), tuner.fit() returns the best configs and metrics.</li> </ol> <ol> <li> <p>Ray Core | Ray Docs \u21a9</p> </li> <li> <p>Getting Started | Ray Docs \u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Key Concepts | Ray Cluster | Ray Docs \u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Key Concepts | Ray Core | Ray Docs \u21a9\u21a9\u21a9</p> </li> <li> <p>Key Concepts | Ray Data | Ray Docs \u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Overview | Ray Train | Ray Docs \u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Key Concepts | Ray Tune | Ray Docs \u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Tune Search Space API | Ray Docs \u21a9</p> </li> <li> <p>Key Concepts | Ray Tune | Ray Docs \u21a9\u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"side-projects/data2ml-ops/ray/in-the-bigger-picture/","title":"In the Bigger Picture","text":""},{"location":"side-projects/data2ml-ops/ray/in-the-bigger-picture/#history-current-status","title":"History &amp; Current Status","text":"<p>Ray was originally developed by UC Berkeley's RISELab (formerly AMPLab) and was first open-sourced in September 2017.<sup>1</sup> The project was designed to simplify distributed computing for machine learning and AI workloads. Later, the team spun off a company, Anyscale, Inc.<sup>2</sup>, to maintain and commercialize Ray. As of May 2025, Ray has over 37,000 stars on GitHub.<sup>3</sup> In April 2024, Thoughtworks included Ray in the Trial phase of its Technology Radar, indicating it is a promising and maturing tool worth evaluating in real-world projects.<sup>4</sup></p> <p>The community is active and growing, with more than 10,000 members on Slack<sup>5</sup> and over 1,100 contributors on GitHub. Ray is used by more than 23,000 developers and organizations worldwide, including OpenAI, Shopify, and Uber. An annual Ray Summit<sup>6</sup> brings the community together to share use cases, roadmap updates, and technical deep dives.</p> Ray Use Cases<sup>1</sup>"},{"location":"side-projects/data2ml-ops/ray/in-the-bigger-picture/#alternatives","title":"Alternatives","text":""},{"location":"side-projects/data2ml-ops/ray/in-the-bigger-picture/#distributed-data-processing","title":"Distributed Data Processing","text":"<p>(Generated by ChatGPT 4o Deep Research on 2025/05/20)</p> Ray DataApache SparkDaskDaft <p>An open-source unified framework designed to scale Python and AI workloads (such as machine learning) across distributed environments.</p> <p>Pros</p> <ul> <li>Python-native distributed computing framework with a flexible and simple interface, avoiding the overhead of the JVM. Especially suitable for data science and ML workloads.</li> <li>Comprehensive ecosystem including built-in libraries like Ray Tune, Ray Train, Ray Serve, and RLlib\u2014helpful for building large-scale ML applications.</li> <li>Abstracts distributed complexity by offering parallel computing primitives, enabling non-experts in distributed systems to easily get started.</li> </ul> <p>Cons</p> <ul> <li>Lacks high-level data processing interfaces: Ray Datasets only supports basic parallel processing; it lacks full ETL capabilities like rich querying, visualization, or aggregations. It's not positioned as a full data science ETL solution.</li> <li>Scheduling overhead for many small tasks: Ray may incur non-trivial overhead when managing a large number of tiny tasks (batching is often needed). Compared to mature big data engines, its stability and tuning experience are still evolving.</li> </ul> <p>When to Use</p> <ul> <li>Distributed Python computation / ML: Ideal when you need to scale arbitrary Python or ML pipelines (e.g., hyperparameter tuning, distributed training, reinforcement learning).</li> <li>Heterogeneous resource scheduling: Suited for workloads requiring GPUs, TPUs, or managing multiple concurrent tasks like data preprocessing and training.</li> </ul> <p>A unified analytics engine for large-scale data processing. Offers APIs in Scala, Java, Python, and supports batch, streaming, and ML workloads.</p> <p>Pros</p> <ul> <li>Strong big data processing capabilities: Stable and battle-tested, Spark is the de facto standard for distributed computing on large datasets.</li> <li>Robust ecosystem: A unified platform supporting SQL (Spark SQL), DataFrame APIs (pandas API on Spark), machine learning (MLlib), graph computation (GraphX), and streaming (Structured Streaming).</li> <li>Widespread adoption and support: Large community and broad industry usage, with extensive third-party integrations and enterprise support.</li> </ul> <p>Cons</p> <ul> <li>Indirect Python support: Spark runs on the JVM, and PySpark uses serialization to move data between Python and the JVM. Heavy use of Python UDFs may introduce performance overhead. Also lacks the interactive, dynamic experience native Python users are used to.</li> <li>High resource overhead: Cluster startup and scheduling can be expensive, especially for small-to-medium workloads. Lightweight alternatives may outperform Spark in such scenarios.</li> </ul> <p>When to Use</p> <ul> <li>Massive-scale batch data processing: Best suited for terabyte-to-petabyte-scale jobs, especially if Hadoop/Spark infrastructure is already in place. Ideal for complex ETL, warehousing, and batch analytics.</li> <li>All-in-one platform requirements: Excellent when you need a unified stack offering SQL, ML, and streaming in one platform.</li> </ul> <p>A flexible Python parallel computing library that extends the PyData ecosystem (like Pandas/NumPy) to handle distributed or out-of-core computation.</p> <p>Pros</p> <ul> <li>Familiar interface: Easy to adopt for Python users, with APIs that closely resemble Pandas. You can scale up existing Pandas/NumPy code with minimal changes.</li> <li>Lightweight and flexible: Can run on a single machine and scale to a cluster. More suitable than Spark for medium-sized data workloads with lower deployment overhead.</li> <li>General-purpose parallel computing: Supports not just DataFrames but also Dask Array, Dask Delayed, and Dask Futures\u2014enabling parallelization of arbitrary Python code and numerical operations.</li> </ul> <p>Cons</p> <ul> <li>Limited scalability at extreme data volumes: For jobs over 1TB, memory management and scheduling can become difficult. Spark may outperform Dask in such scenarios. Also lacks a mature query optimizer for complex joins and aggregations.</li> <li>Partial API limitations: Dask's data structures are immutable and do not support some in-place Pandas operations (like <code>.loc</code> assignment). You may need to use <code>map_partitions</code> and other patterns. Some Pandas edge behaviors may also differ, requiring a learning curve.</li> </ul> <p>When to Use</p> <ul> <li>Data science in Python with medium-scale data: Ideal for datasets from a few GBs to several TBs, especially if you're already using Pandas/Numpy and want to speed up existing code with parallelism.</li> <li>Flexible and custom computation: Useful for local development that later scales to clusters. Suitable when the workload involves custom Python functions and numerical computations, rather than pure SQL. Also great for scientific computing tasks that need fault tolerance and visualized scheduling.</li> </ul> <p>A new-generation unified data engine offering both Python DataFrame and SQL interfaces. Built in Rust for high performance, with seamless scaling from local to petabyte-scale distributed execution.</p> <p>Pros</p> <ul> <li>High performance and unified interface: Supports both DataFrame-style operations and SQL queries. Built in Rust and based on Apache Arrow memory format for fast local processing. Scales to distributed clusters via integration with Ray. Benchmarks show significant performance gains over Spark and Dask.</li> <li>Supports complex data types: Designed for multimodal data like images, videos, and embeddings, with built-in functions and type support to simplify processing of unstructured data.</li> <li>Query optimization and cloud integration: Includes a built-in query optimizer (e.g., predicate pushdown, cost estimation). Integrates with data catalogs like Iceberg/Delta and cloud storage like S3, achieving fast I/O in cloud environments.</li> </ul> <p>Cons</p> <ul> <li>Immature ecosystem: Still in early (0.x) development stages. Community and ecosystem are limited compared to Spark or Dask. Lacks extensive tooling and production-grade stability.</li> <li>Requires external tools for distributed execution: Daft focuses on data processing itself. Distributed resource management is delegated to Ray, which adds extra setup and operational complexity.</li> <li>Incomplete feature set: Some advanced analytics features are still under development. API stability and error messaging could be improved compared to mature engines.</li> </ul> <p>When to Use</p> <ul> <li>High-performance Python workloads with complex data: Great for handling unstructured data like images, videos, and embeddings at scale, especially when you want to stay in the Python ecosystem.</li> <li>Seamless local-to-distributed scaling: Ideal for teams looking to prototype locally using DataFrames and easily scale to clusters/cloud later. A good alternative to Spark/Dask for scenarios needing both SQL and Python transformations in a performant pipeline.</li> </ul>"},{"location":"side-projects/data2ml-ops/ray/in-the-bigger-picture/#distributed-hpo","title":"Distributed HPO","text":"<p>(Generated by ChatGPT 4o Deep Research on 2025/05/20)</p> Ray TuneKatib <p>A Python library for experiment execution and hyperparameter tuning at any scale.</p> <p>Pros</p> <ul> <li>Wide algorithm support:Supports a wide range of state-of-the-art search algorithms (e.g. PBT, HyperBand/ASHA) and integrates with many optimization libraries like Optuna, HyperOpt, Ax, Nevergrad, etc. This allows users to leverage cutting-edge tuning methods easily.</li> <li>Scalable and distributed:Can parallelize and distribute trials across multiple CPUs/GPUs and even multiple nodes using Ray's cluster architecture, often without any code changes. This makes it simple to scale up experiments from a laptop to a distributed cluster.</li> <li>Easy integration:Provides a simple Python API that works seamlessly with popular ML frameworks (TensorFlow, PyTorch, scikit-learn, etc.). It also offers convenient tools (like TensorBoard logging) for monitoring experiments, lowering the learning curve for new users.</li> </ul> <p>Cons</p> <ul> <li>Kubernetes integration not native:Ray Tune is not built into Kubernetes; running it on K8s requires deploying a Ray cluster (e.g. via the KubeRay operator), which is extra overhead compared to a K8s-native solution.</li> <li>Python-centric:It's primarily a Python library, so each trial runs in a Python process. This makes it less language-agnostic than a tool like Katib \u2013 non-Python workloads are not as straightforward to tune with Ray Tune.</li> <li>Resource overhead: Using Ray introduces additional background processes and resource usage (for the Ray runtime). For very small-scale or simple hyperparameter searches, this added complexity can be overkill when a lightweight tool (like Optuna alone) might suffice.</li> </ul> <p>When to Use</p> <ul> <li>Use Ray Tune when you want to perform hyperparameter search inside a Python workflow (e.g. in a Jupyter notebook or script) and easily scale it from local execution to distributed runs without changing your code.</li> <li>It is a good choice if you don't have a Kubernetes infrastructure or prefer not to rely on one. Ray Tune can manage distributed training on its own (on VMs or a Ray cluster) and thus suits scenarios where setting up Kubeflow/Katib would be too heavyweight.</li> </ul> <p>An open-source, Kubernetes-native hyperparameter tuning system that is framework-agnostic and supports AutoML features like early stopping and NAS.</p> <p>Pros</p> <ul> <li>Kubernetes-native: Designed to run as part of a K8s cluster, Katib treats hyperparameter tuning jobs as Kubernetes workloads. This tight integration makes it a natural fit for cloud-native environments and enables multi-tenant usage in production (multiple users can share the service). It seamlessly works with Kubeflow, allowing tuning to plug into pipelines and Kubernetes resource management.</li> <li>Framework/language agnostic: Katib can tune any model training code in any language or framework, as long as it's containerized. It's not tied to a specific ML library \u2013 whether you use TensorFlow, PyTorch, R, or even a custom script, Katib can run it because it launches jobs in containers.</li> <li>Rich algorithm library: Offers diverse hyperparameter search strategies out-of-the-box: random search, grid search, Bayesian optimization (via Skopt), Hyperband/ASHA, CMA-ES, Tree of Parzen Estimators (TPE), Population Based Training, and more. It's integrated with libraries like Hyperopt/Optuna to provide state-of-the-art optimization methods. Katib even supports advanced AutoML techniques such as neural architecture search (NAS) and early stopping criteria, which few other open-source tools provide.</li> </ul> <p>Cons</p> <ul> <li>Kubernetes dependency: Katib requires a Kubernetes environment (typically as part of Kubeflow) to run. This means it's not usable in a simple local setup and has a higher operational overhead if you don't already have a K8s cluster.</li> <li>Higher learning curve: Configuring Katib experiments involves writing Kubernetes CRD (YAML) configurations for experiments, parameters, and metrics. Users must understand Kubernetes concepts and Katib's CRD schema, which can be less straightforward than using a Python library API. This complexity can slow down initial experimentation, especially for those new to K8s.</li> <li>Beta-stage project: As of now, Katib is in beta status. While it is used in production by some, certain features or integrations may not be as mature or well-documented as more established libraries. It largely relies on the Kubeflow ecosystem, so using it outside of Kubeflow/Kubernetes contexts might not be practical.</li> </ul> <p>When to Use</p> <ul> <li>Use Katib when you are working in a Kubernetes-based ML platform (especially with Kubeflow) and want a hyperparameter tuning service that integrates naturally with your cluster jobs and pipelines.</li> <li>It is the go-to choice if you need to tune heterogeneous workloads or non-Python applications. For example, if your training code is in R, Java, or any framework outside of Python, Katib's container-based approach allows you to optimize those just as easily. Similarly, if you have an existing training pipeline (e.g., a Kubeflow TFJob/PyTorchJob), Katib can hook into it to adjust hyperparameters.</li> <li>Choose Katib for large-scale or collaborative scenarios: it supports multi-user (multi-tenant) usage and distributed training jobs natively. If you require advanced AutoML features like NAS or want a centrally managed HPO service for your team or organization, Katib provides these capabilities in a cloud-native way.</li> </ul>"},{"location":"side-projects/data2ml-ops/ray/in-the-bigger-picture/#model-serving","title":"Model Serving","text":"<p>(Generated by ChatGPT 4o Deep Research on 2025/05/20)</p> Ray ServeBentoMLKServeSeldon Core <p>Ray Serve is an open-source, scalable model serving library built on Ray, allowing you to serve machine learning models (and arbitrary Python business logic) as high-performance, resilient web services. It's framework-agnostic and can run anywhere Ray can (from a laptop to a Kubernetes cluster) with minimal code changes.</p> <p>Pros</p> <ul> <li>Flexible model composition and Python-first workflow: You can deploy multiple models and custom business logic in a single pipeline, all in Python code, which makes it easy to develop and test locally before scaling up. Ray Serve's integration with FastAPI also simplifies turning your code into RESTful endpoints.</li> <li>Scalable and resource-efficient: Built on the Ray distributed computing framework, it natively scales across CPU/GPU nodes and even multiple machines. It supports fine-grained resource control (e.g. fractional GPUs and auto-batching), enabling high throughput serving and efficient utilization of hardware. Ray's autoscaler can dynamically add or remove replicas based on load.</li> <li>Kubernetes integration optional: Unlike purely Kubernetes-focused tools, Ray Serve doesn't require K8s \u2013 you can start on a local machine or VM and later use KubeRay (Ray's K8s operator) to deploy to a cluster without major code changes. This lowers the barrier to entry while still allowing cloud-native deployment when needed.</li> </ul> <p>Cons</p> <ul> <li>Not a full ML platform: Ray Serve focuses on serving and lacks built-in model lifecycle management features (no native model registry/versioning UI or advanced monitoring dashboards). You may need to implement or integrate external tools for model tracking, canary testing, or extensive metrics.</li> <li>Operational overhead of Ray: Using Ray Serve means running a Ray cluster, which introduces additional complexity and resource overhead. Deploying on Kubernetes, for example, requires operating the Ray runtime (via an operator or manual setup), which can be overkill if you only need simple single-model serving. </li> <li>Limited traffic-splitting out-of-the-box: There's no built-in canary or A/B traffic management API (unlike KServe or Seldon's declarative canary support). Achieving A/B tests would require custom logic in your application (e.g. routing requests between Ray Serve deployments manually).</li> </ul> <p>When to Use</p> <ul> <li>Dynamic or complex inference pipelines: Use Ray Serve when your application involves multiple models or steps that need to be composed and scaled together (for example, an ensemble or a workflow of preprocessing \u2192 model A \u2192 model B). Its ability to handle distributed workflows and call one deployment from another shines in these scenarios.</li> <li>Scaling Python services from prototyping to production: If you want to prototype a model service locally (using pure Python) and later seamlessly scale it to a cluster or cloud environment without rewriting for a new serving stack, Ray Serve is an excellent choice. It's ideal for teams already using Ray for other tasks (training, tuning) who want to reuse that infrastructure for serving.</li> <li>Resource-intensive models with custom logic: When you need fine-grained control over resource allocation (e.g. serving large models with fractional GPU sharing) or have custom Python logic alongside predictions, Ray Serve provides the flexibility and performance to meet those needs. It's well-suited if your use case doesn't fit the one-model-per-container paradigm of other frameworks.</li> </ul> <p>BentoML is an open-source platform that simplifies packaging and deploying ML models at scale. It provides a unified, framework-agnostic way to containerize models (into a self-contained \u201cBento\u201d bundle) and deploy them to various environments with minimal coding, bridging the gap between model development and production deployment.</p> <p>Pros</p> <ul> <li>Easy and developer-friendly: BentoML offers a simple Python API to package models and serve them, which means you can deploy a model with just a few lines of code. It requires minimal setup and no deep Kubernetes knowledge to get started \u2013 great for small teams or startups to quickly ship models as microservices. The development workflow is very \u201cpythonic\u201d and supports local testing (e.g., you can run a Bento service locally like any Flask app).</li> <li>Framework-agnostic &amp; portable: It supports a wide range of ML frameworks (TensorFlow, PyTorch, Scikit-learn, Hugging Face transformers, etc.) and can deploy to multiple targets \u2013 from Docker containers on your infrastructure to serverless platforms or Kubernetes via Bento's operator/Yatai service. This flexibility lets you use the same packaged model artifact across different environments, avoiding lock-in to a specific serving backend.</li> <li>Built-in tooling for deployments: BentoML automatically generates REST/gRPC API servers for your model and includes basic observability features. For example, it has integrated logging and monitoring hooks \u2013 you can get inference metrics and logs, and even integrate with Prometheus/Grafana for live monitoring. It also supports versioning your models and services, which makes it possible to do A/B tests or canary releases by deploying multiple versions of a model and routing traffic accordingly.</li> </ul> <p>Cons</p> <ul> <li>Not ideal for extreme scale: While BentoML can handle moderate production workloads, it isn't as optimized for massive scale or ultra-low latency scenarios as some specialized serving systems. Horizontal scaling (e.g., running many replicas) isn't managed by BentoML itself \u2013 you'd rely on Kubernetes or an external orchestrator to scale out, which adds extra work. For very high-throughput or large numbers of models, the overhead of each Bento service and the absence of built-in autoscaling means it might not be the most resource-efficient choice.</li> <li>Lacks native Kubernetes integration: Unlike KServe or Seldon, BentoML doesn't deploy via a Kubernetes CRD by default. You either use BentoML's CLI/Yatai to build a container and deploy it, or manually handle the Kubernetes deployment (e.g., create your own Kubernetes Deployment/Service for the Bento container). This means features like canary routing or auto-scaling must be set up through Kubernetes or other tools, not toggled via BentoML configs.</li> <li>Limited built-in MLOps features: BentoML focuses on the serving container and basic logging/metrics. It doesn't natively provide advanced monitoring dashboards, data drift detection, or experiment tracking \u2013 you'd integrate external tools for those needs. Similarly, governance features (like role-based access, model approval workflows, etc.) are not part of the open-source BentoML, though some are offered in BentoCloud. In short, it's a lightweight serving tool rather than a full-fledged enterprise ML platform.</li> </ul> <p>When to Use</p> <ul> <li>Rapid prototyping and small-to-medium deployments: Use BentoML when you want to go from a trained model to a deployed service quickly with minimal overhead \u2013 for instance, a small team that needs to frequently deploy models for different projects. It shines in early-stage production scenarios where simplicity and speed are favored over complex infrastructure.</li> <li>Polyglot model environments: If your use case involves various model types (different frameworks or libraries) and you need a consistent way to package and deploy all of them, BentoML is a good fit. It provides one standardized workflow to containerize models from any framework and deploy to your environment of choice.</li> <li>Customization and control in Python: When you require custom pre- or post-processing logic, or want to integrate business logic into your prediction service, BentoML allows you to write that logic in Python alongside model inference. This is useful in scenarios where alternative tools (which often auto-launch models in generic servers) don't easily support custom code. In BentoML, you have full control to define the request handling, which can be handy for experimental features or complex input/output handling.</li> </ul> <p>KServe (formerly KFServing) is a Kubernetes-native model serving platform that provides a Custom Resource Definition (CRD) called <code>InferenceService</code> to deploy ML models on Kubernetes in a standardized way. It focuses on \u201cserverless\u201d inference \u2013 automatically scaling model servers up and down (even to zero) based on traffic \u2013 and supports many popular ML frameworks out-of-the-box.</p> <p>Pros</p> <ul> <li>Deep Kubernetes integration: KServe defines an <code>InferenceService</code> CRD for model deployments, which makes it feel like a natural extension of Kubernetes. You can deploy models by simply specifying the model artifact (e.g., a URI to a saved model on cloud storage) and selecting an appropriate runtime (TensorFlow Serving, TorchServe, scikit-learn, XGBoost, etc.), without needing to write custom serving code. This declarative approach is convenient for teams already comfortable with <code>kubectl</code> and YAML.</li> <li>Serverless autoscaling (Knative): KServe leverages Knative Serving under the hood to handle scaling. It can automatically scale up replicas based on request load and scale down to zero when no traffic is present. This on-demand scaling is efficient for cost and resource usage, and it works for CPU and GPU workloads alike. Built-in autoscaling metrics and concurrency controls help handle spikes in traffic.</li> <li>Advanced deployment strategies: It supports canary deployments and traffic splitting natively. You can deploy a new version of a model and direct a percentage of traffic to it (for A/B testing or gradual rollouts) using simple CRD fields. KServe also introduced inference graphs, allowing you to chain models or have ensembles within a single <code>InferenceService</code> (though this is somewhat basic compared to Seldon's graphs). Additionally, KServe supports standard ML endpoint protocols (like KFServing V2 and even the OpenAI API schema for generative models) for interoperability.</li> </ul> <p>Cons</p> <ul> <li>Setup complexity: Installing and managing KServe can be complex. It typically requires deploying Knative and Istio (for networking) on your Kubernetes cluster, which means extra moving parts. This overhead implies you need solid Kubernetes/cloud-native expertise to operate it. Teams without existing K8s infrastructure might find it heavyweight.</li> <li>Limited flexibility for custom code: KServe excels at serving pretrained models with standard predictors, but if you need custom pre-processing, post-processing, or arbitrary Python logic, it's less straightforward. You often have to build a custom docker image implementing KServe's SDK interfaces for transformers or predictors. In comparison, tools like Seldon or BentoML (or writing your own service) may offer more flexibility for custom inference logic.</li> <li>Fewer built-in MLOps features: While KServe covers the basics of scaling and canarying, it doesn't inherently provide some advanced features like data drift detection, out-of-the-box monitoring dashboards, or automatic logging of requests/responses \u2013 those would rely on integrating with other tools (e.g., Prometheus for metrics, Kubeflow or custom pipelines for data logging). In areas like explainability or outlier detection, KServe is more bare-bones than Seldon Core's extensive feature set.</li> </ul> <p>When to Use</p> <ul> <li>Kubernetes-first organizations: Choose KServe if your team is already invested in Kubernetes (possibly also using Kubeflow) and you want a model serving solution that fits natively into that ecosystem. It's ideal when you prefer deploying models via Kubernetes manifests and want the control of K8s primitives (like using HPA, etc.) with the convenience of not writing a serving application from scratch.</li> <li>Multiple models, multiple frameworks: KServe is well-suited when you have models in different frameworks and want a unified way to serve them. Because it provides built-in support for many framework servers, you can hand off a TensorFlow model or a PyTorch model to KServe in a similar fashion. This makes it easier to standardize deployment in a heterogeneous ML environment.</li> <li>Auto-scaling API endpoints: If your use case demands elastic scaling (including scale-to-zero) to handle sporadic or bursty traffic patterns, and you want to pay for/use resources only on-demand, KServe's Knative-based design is a strong advantage. It also allows gradual rollouts of new model versions easily. In summary, use KServe for production scenarios where you need robust scaling and traffic management for your model APIs, and you're okay with the operational overhead of maintaining the KServe/Knative stack.</li> </ul> <p>Seldon Core is an open-source MLOps framework for deploying, managing, and monitoring machine learning models at scale on Kubernetes. It converts your models into production-ready microservices and offers an advanced set of deployment features (from experiments and ensembles to explainers and outlier detectors) to support complex production ML use cases.</p> <p>Pros</p> <ul> <li>Rich feature set for production ML: Seldon Core provides many advanced capabilities out-of-the-box: you can do A/B testing and canary rollouts of models, incorporate explainability (e.g., SHAP values) and outlier detection in your deployments, and even define inference graphs to chain multiple models or preprocessing steps. These features make it possible to implement complex deployment strategies (multi-armed bandits, shadow deployments, etc.) with the framework's support rather than building those from scratch.</li> <li>Scales to enterprise needs: Designed to handle deployments of potentially thousands of models, Seldon is built with scalability in mind . It uses a CRD (<code>SeldonDeployment</code>) to deploy models on Kubernetes, and can manage scaling via standard Kubernetes mechanisms (e.g., Horizontal Pod Autoscalers). It also supports numerous frameworks and languages (you can deploy models from TensorFlow, PyTorch, Scikit-learn, or even custom predictors in languages like Java) giving it versatility in complex organizations.</li> <li>Integrated with monitoring and governance tools: Seldon has native integrations for metrics and logging \u2013 it works well with Prometheus/Grafana for monitoring and can emit detailed metrics about predictions. It also leverages Istio for traffic routing and security policies in Kubernetes. Moreover, it can integrate with workflow orchestrators (Airflow, Argo, Kubeflow Pipelines) and includes audit and logging components, which is useful for governance and compliance in enterprise settings.</li> </ul> <p>Cons</p> <ul> <li>High complexity and setup overhead: Seldon Core's powerful features come at the cost of complexity. Setting up Seldon Core involves deploying its operators and (often) integrating with Istio for ingress, plus configuring all the CRD specs properly \u2013 this requires significant Kubernetes expertise. The learning curve is steep, and for small projects the resource overhead (multiple pods for controllers, sidecars for logging/metrics, etc.) can be heavy.</li> <li>Documentation and usability challenges: Some of Seldon's more advanced features (like complex inference graphs or custom routers) are not trivial to implement and the documentation/examples can be lacking. This means that while those features exist, leveraging them may require considerable experimentation and community support. In contrast, simpler frameworks might get you from zero to serving faster if your needs are basic.</li> <li>Potential overkill for simple use cases: If you just need to deploy one or two models with straightforward inference, Seldon can be over-engineered. The overhead of its components and the requirement to containerize your model to fit Seldon's runtime might slow you down unnecessarily. In such cases, a lighter-weight solution (like BentoML or a simple Flask app) could be more appropriate. Seldon really shines in more demanding scenarios, not one-off model deploys.</li> </ul> <p>When to Use</p> <ul> <li>Enterprise-scale deployments with advanced requirements: Seldon Core is an excellent choice when you are dealing with large-scale ML systems in production \u2013 for example, a situation where you have many models or microservices and you need sophisticated routing (A/B tests, canaries) and monitoring on all of them. If reliability, traceability, and robust governance are top priorities (as in regulated industries or mission-critical ML), Seldon's comprehensive features are very valuable.</li> <li>Need for out-of-the-box MLOps features: If your use cases demand things like real-time explainability of model decisions, automatic outlier or drift detection, or complex inference pipelines (ensembles, cascades of models) and you prefer having these capabilities ready-made, Seldon Core provides them built-in. It's a suitable framework when you want to avoid implementing these features from scratch and are willing to invest time in mastering Seldon's framework.</li> <li>Kubernetes-heavy environments with expert DevOps: Use Seldon when you have a strong DevOps/MLOps team familiar with Kubernetes who can maintain the infrastructure. In scenarios where the ML platform is a first-class part of the engineering organization, Seldon Core gives a lot of control and can be tuned to very specific needs. It's often used in tandem with Kubeflow or other K8s tools in companies building out a full ML platform. If your team has the bandwidth to handle a more complex system for the sake of advanced functionality, Seldon is a top contender.</li> </ul> <ol> <li> <p>Ray \u21a9\u21a9</p> </li> <li> <p>Anyscale, Inc. \u21a9</p> </li> <li> <p>Ray | GitHub \u21a9</p> </li> <li> <p>Ray | Technology Radar | Thoughtworks \u21a9</p> </li> <li> <p>Join Ray Slack | Ray \u21a9</p> </li> <li> <p>Ray Summit 2024 \u21a9</p> </li> <li> <p>Comparing Ray Data to other systems | Ray Docs \u21a9</p> </li> <li> <p>Spark, Dask, and Ray: choosing the right framework \u21a9</p> </li> <li> <p>Offline Batch Inference: Comparing Ray, Apache Spark, and SageMaker \u21a9</p> </li> </ol>"},{"location":"side-projects/data2ml-ops/ray/ray-job/","title":"Submit a Ray Tune Job to Your Ray Cluster","text":"<p>In this guide, we'll walk through how to submit the Ray Tune script we created in the previous article to a Ray cluster. There are three main methods:</p> <ol> <li>Using the Ray Jobs API<sup>1</sup>, either<ul> <li>Ray Jobs CLI<sup>2</sup><sup>3</sup></li> <li>Ray Jobs Python SDK<sup>4</sup><sup>5</sup></li> </ul> </li> <li>Defining a <code>RayJob</code> custom resource and submitting it through the Kubernetes Operator.<sup>6</sup></li> <li>Executing the job interactively from the head node (not recommended).</li> </ol> <p>Each method has its ideal use case:</p> <p>When to Use Ray Jobs API</p> <ul> <li>Local Development and Testing: Perfect for quick iterations or debugging directly on a local or remote Ray cluster without dealing with Kubernetes complexity.</li> <li>Ad-Hoc or Short-Lived Jobs: Ideal for submitting one-off tasks via the API to an existing Ray cluster.</li> </ul> <p>When to Use <code>RayJob</code> on Kubernetes</p> <ul> <li>Automated Cluster Lifecycle Management: Ideal when you want Kubernetes to automatically spin up and tear down Ray clusters for each job.</li> <li>Leverage Kubernetes-Native Features: Useful when integrating with scheduling policies, resource quotas, monitoring tools, or other native Kubernetes features.</li> </ul> <p>In this guide, we'll use Ray Jobs CLI for submitting our Ray Tune job to the cluster.</p>"},{"location":"side-projects/data2ml-ops/ray/ray-job/#port-forward","title":"Port Forward","text":"<p>Since our Ray Cluster wasn't exposed via a LoadBalancer or NodePort, we'll use port forwarding to access the Ray Dashboard (which runs on port <code>8265</code> by default):</p> <pre><code>kubectl get service raycluster-kuberay-head-svc -n kuberay\n</code></pre> <pre><code>NAME                          TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                                         AGE\nraycluster-kuberay-head-svc   ClusterIP   None         &lt;none&gt;        10001/TCP,8265/TCP,6379/TCP,8080/TCP,8000/TCP   13m\n</code></pre> <pre><code>kubectl port-forward service/raycluster-kuberay-head-svc 8265:8265 -n kuberay &gt; /dev/null &amp;\n</code></pre> <pre><code>[1] 56915\n</code></pre> <p>We can now access the Ray Cluster dashboard at <code>http://127.0.0.1:8265</code>.</p> <p></p>"},{"location":"side-projects/data2ml-ops/ray/ray-job/#prepare-runtime-environment","title":"Prepare Runtime Environment","text":"<p>Before launching the Ray Tune job, we need to make sure that each trial's worker process has the correct runtime environment\u2014such as the working directory and required dependencies. We'll define this using a <code>runtime-env.yaml</code> and a <code>requirements.txt</code>, and pass them along during job submission.</p> runtime-env.yaml<pre><code>working_dir: .\npip: requirements.txt\n</code></pre> requirements.txt<pre><code>ray[tune,data,client]==2.46.0\nmlflow==2.22.0\noptuna==4.3.0\nxgboost==3.0.0\nfeast[gcp,redis]==0.48.0\npsycopg[binary]==3.2.7\nimbalanced-learn==0.13.0\nprotobuf==5.29.4\n</code></pre> <p>The runtime environment can be specififed via<sup>7</sup></p> <ul> <li><code>ray job submit --runtime-env=...</code>: the runtime environments are applied to both the driver process (entrypoint script) and the worker processes (all the tasks and actors created from the drive process)</li> <li><code>ray.init(rutime_env=...)</code> : the runtime environments are applied to the workers processes (all the tasks and actors), but not the driver process (entrypoint script).</li> </ul> <p>The runtime environment can include one or more fields below:<sup>8</sup><sup>9</sup></p> <ul> <li><code>working_dir</code></li> <li><code>py_modules</code></li> <li><code>py_executable</code></li> <li><code>excludes</code></li> <li><code>pip</code></li> <li><code>uv</code></li> <li><code>conda</code></li> <li><code>env_vars</code></li> <li><code>image_uri</code></li> <li><code>config</code></li> </ul> <p>Runtime environments and Docker can work hand in hand or independently, depending on your needs. For example, you might rely on a container image in the Cluster Launcher to manage large or static dependencies, while runtime environments are better suited for dynamic, job-specific configurations. When combined, the runtime environment extends the container image, inheriting its packages, files, and environment variables to provide a seamless and flexible setup.<sup>10</sup></p>"},{"location":"side-projects/data2ml-ops/ray/ray-job/#submit-a-ray-job-to-the-ray-cluster","title":"Submit a Ray Job to the Ray Cluster","text":"<p>With the dashboard accessible and runtime environment prepared, you can now submit the Ray Tune job to the cluster:</p> ray-job-submit-command.txt<pre><code>ray job submit \\\n  --address http://localhost:8265 \\\n  --runtime-env runtime-env.yaml \\\n  -- python training.py\n</code></pre> <pre><code>Job submission server address: http://localhost:8265\n2025-05-16 02:19:52,245 INFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_927761c1b60fb91e.zip.\n2025-05-16 02:19:52,245 INFO packaging.py:576 -- Creating a file package for local module '.'.\n\n-------------------------------------------------------\nJob 'raysubmit_8uVJaNE7m2cvM9uZ' submitted successfully\n-------------------------------------------------------\n\nNext steps\n  Query the logs of the job:\n    ray job logs raysubmit_8uVJaNE7m2cvM9uZ\n  Query the status of the job:\n    ray job status raysubmit_8uVJaNE7m2cvM9uZ\n  Request the job to be stopped:\n    ray job stop raysubmit_8uVJaNE7m2cvM9uZ\n\nTailing logs until the job exits (disable with --no-wait):\n2025-05-15 11:19:52,389 INFO job_manager.py:531 -- Runtime env is setting up.\n[I 2025-05-15 11:21:54,810] A new study created in memory with name: optuna\n2025-05-15 11:21:56,460 INFO worker.py:1554 -- Using address 10.244.0.99:6379 set in the environment variable RAY_ADDRESS\n2025-05-15 11:21:56,465 INFO worker.py:1694 -- Connecting to existing Ray cluster at address: 10.244.0.99:6379...\n2025-05-15 11:21:56,485 INFO worker.py:1888 -- Connected to Ray cluster.\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Configuration for experiment     fraud_detection   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Search algorithm                 SearchGenerator   \u2502\n\u2502 Scheduler                        FIFOScheduler     \u2502\n\u2502 Number of trials                 100               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\nView detailed results here: ray/fraud_detection\nTo visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2025-05-15_11-18-04_381196_1/artifacts/2025-05-15_11-22-03/fraud_detection/driver_artifacts`\n\n...\n...\n...\n...\n...\n\n------------------------------------------\nJob 'raysubmit_8uVJaNE7m2cvM9uZ' succeeded\n------------------------------------------\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/ray-job/#see-results","title":"See Results","text":"<p>Once the job finishes, you can verify the results in the Ray Dashboard:</p> <pre><code>http://127.0.0.1:8265\n</code></pre> <p></p> <p>You can also check the experiment in the MLflow UI. The parent-child run setup should be correctly reflected:</p> <p></p> <p>The hyperparameter tuning job completed successfully and produced a versioned MLflow model:</p> <p></p> <p>Since our MLflow instance is integrated with MinIO, the trained model is also available there. You can now use this model for deployment:</p> <p></p> <ol> <li> <p>Ray Jobs API \u21a9</p> </li> <li> <p>Quickstart using the Ray Jobs CLI \u21a9</p> </li> <li> <p>Ray Jobs CLI API Reference \u21a9</p> </li> <li> <p>Ray Jobs Python SDK \u21a9</p> </li> <li> <p>Ray Jobs Python SDK API Reference \u21a9</p> </li> <li> <p>RayJob Quickstart \u21a9</p> </li> <li> <p>Runtime Environment Specified by Both Job and Driver \u21a9</p> </li> <li> <p>Runtime Environment API Reference \u21a9</p> </li> <li> <p><code>ray.runtime_env.RuntimeEnv</code> \u21a9</p> </li> <li> <p>What is the relationship between runtime environments and Docker? \u21a9</p> </li> </ol>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/","title":"Integrate Ray Tune with Optuna, Imblearn, MLflow and MinIO","text":"<p>This guide walks you through how to use Ray Tune for hyperparameter tuning in a fraud detection model. The workflow includes:</p> <ol> <li>Loading training data from MinIO</li> <li>Defining a search space with Optuna, using over-sampling and down-sampling techniques like <code>AllKNN</code> and <code>SMOTE</code> specifically in <code>imblearn</code> packages to handle class imbalance.</li> <li>Training an XGBoost binary classifier with boosters like <code>gbtree</code>, <code>gblinear</code>, and <code>dart</code>, and tuning hyperparameters such as lambda, alpha, and eta.</li> <li>Logging metrics including accuracy, precision, recall, F1, and ROC AUC to both Ray Tune and MLflow.</li> <li>Manually configuring MLflow to support parent-child runs, instead of using the default <code>MLflowLoggerCallback</code> and <code>setup_mlflow</code></li> <li>Retraining and saving the best model with the optimal hyperparameters after tuning.</li> </ol> <p>Here is a full training transcipt.</p> Full Training Script training.py<pre><code># Import packages\nimport time\nfrom datetime import datetime, timedelta\nfrom pprint import pprint\nfrom typing import Any, Dict, Optional\n\nimport pandas as pd\nimport pyarrow.fs\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.under_sampling import AllKNN\nfrom sklearn.metrics import (accuracy_score, average_precision_score, f1_score,\n                             log_loss, precision_score, recall_score,\n                             roc_auc_score)\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\n\nimport mlflow\nimport ray\nfrom feast import FeatureStore\nfrom ray import tune\nfrom ray.tune.search import ConcurrencyLimiter\nfrom ray.tune.search.optuna import OptunaSearch\n\n\nfs = pyarrow.fs.S3FileSystem(\n    access_key=\"minio_user\",\n    secret_key=\"minio_password\",\n    scheme=\"http\",\n    endpoint_override=\"minio-api.minio.svc.cluster.local:9000\"\n)\n\n# Get Training Data\nwith fs.open_input_file(\"ray/training_data.csv\") as f:\n    data = pd.read_csv(f)\n\n# Alternative 1: Ray Data\n# ds = ray.data.read_csv(\n#     \"s3://ray/training_data.csv\",\n#     filesystem=fs\n# )\n\n# Alternative 2: Feast\n# now = datetime.now()\n# two_days_ago = datetime.now() - timedelta(days=2)\n# store = FeatureStore('.')\n# fs_fraud_detection_v1 = store.get_feature_service('fraud_detection_v1')\n# data = store.get_historical_features(\n#     entity_df=f\"\"\"\n#     select \n#         src_account as entity_id,\n#         timestamp as event_timestamp,\n#         is_fraud\n#     from\n#         feast-oss.fraud_tutorial.transactions\n#     where\n#         timestamp between timestamp('{two_days_ago.isoformat()}') \n#         and timestamp('{now.isoformat()}')\"\"\",\n#     features=fs_fraud_detection_v1,\n#     full_feature_names=False\n# ).to_df()\n\n# Configure Ray Tune\ndef space(trial) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Define-by-run function to construct a conditional search space.\n    Ensure no actual computation takes place here.\n\n    Args:\n        trial: Optuna Trial object\n\n    Returns:\n        Dict containing constant parameters or None\n    \"\"\"\n    # Resampler\n    resampler = trial.suggest_categorical(\"resampler\", [\"allknn\", \"smote\", \"passthrough\"])\n\n    # Booster\n    booster = trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"])\n    lmbd = trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True)\n    alpha = trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True)\n    if booster in [\"gbtree\", \"dart\"]:\n        max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n        eta = trial.suggest_float(\"eta\", 1e-3, 0.3, log=True)\n        gamma = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n        grow_policy = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n    # Constants\n    return {\n        \"objective\": \"binary:logistic\",\n        \"random_state\": 1025\n    }\n\ndef training_function(\n    config, data,\n    run_id, mlflow_tracking_uri, experiment_name\n):\n    # Set up mlflow \n    mlflow.set_tracking_uri(mlflow_tracking_uri)\n    mlflow.set_experiment(experiment_name=experiment_name)\n\n    # Split data\n    X = data[[\n        'has_fraud_7d',\n        'num_transactions_7d',\n        'credit_score',\n        'account_age_days',\n        'has_2fa_installed'\n    ]]\n    y = data['is_fraud']\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, stratify=y,\n        random_state=config[\"random_state\"]\n    )\n\n    # Define the resampler\n    if config[\"resampler\"] == \"allknn\":\n        resampler = AllKNN()\n    elif config[\"resampler\"] == \"smote\":\n        resampler = SMOTE()\n    else:\n        resampler = \"passthrough\"\n\n    # Define the classifier\n    new_config = {k: v for k, v in config.items() if k != \"resampler\"}\n    classifier = XGBClassifier(**new_config)\n\n    # Combine the resampler and classifier together\n    model = Pipeline(steps=[\n        (\"resampler\", resampler),\n        (\"classifier\", classifier)\n    ])\n\n    # Train the model\n    with mlflow.start_run(run_id=run_id):\n        with mlflow.start_run(\n            run_name=f\"{config['resampler']}-{config['booster']}-{config['lambda']:.2f}-{config['alpha']:.2f}\",\n            nested=True\n        ):\n            model.fit(X_train, y_train)\n            # Evaluate the model\n            y_prob = model.predict_proba(X_test)\n            y_prob = y_prob[:, 1]\n            y_pred = (y_prob &gt; 0.5).astype(int)\n            metrics = {\n                \"accuracy\": accuracy_score(y_test, y_pred),\n                \"precision\": precision_score(y_test, y_pred),\n                \"recall\": recall_score(y_test, y_pred),\n                \"f1\": f1_score(y_test, y_pred),\n                \"roc_auc\": roc_auc_score(y_test, y_prob),\n                \"log_loss\": log_loss(y_test, y_prob),\n                \"average_precision\": average_precision_score(y_test, y_prob)\n            }\n            # Log the metrics and hyperparameters\n            mlflow.log_params(config)\n            mlflow.log_metrics(metrics)\n            tune.report(metrics)\n\nsearch_alg = OptunaSearch(space=space, metric=\"f1\", mode=\"max\")\nsearch_alg = ConcurrencyLimiter(search_alg, max_concurrent=4)\ntune_config = tune.TuneConfig(\n    search_alg=search_alg,\n    num_samples=100,\n)\n\nEXPERIMENT_NAME = 'fraud_detection'\nRUN_NAME = 'first'\nTRACKING_URI = \"http://tracking-server.mlflow.svc.cluster.local:5000\"\nmlflow.set_tracking_uri(TRACKING_URI)\nif mlflow.get_experiment_by_name(EXPERIMENT_NAME) == None:\n    mlflow.create_experiment(EXPERIMENT_NAME)\nmlflow.set_experiment(EXPERIMENT_NAME)\nrun_config = tune.RunConfig(\n    name=EXPERIMENT_NAME,\n    storage_path=\"ray/\",\n    storage_filesystem=fs\n)\n\n# Run Ray Tune\nray.init()\nwith mlflow.start_run(run_name=RUN_NAME, nested=True) as run:\n    tuner = tune.Tuner(\n        tune.with_parameters(\n            training_function,\n            data=data,\n            run_id=run.info.run_id,\n            mlflow_tracking_uri=TRACKING_URI,\n            experiment_name=EXPERIMENT_NAME\n        ),\n        tune_config=tune_config,\n        run_config=run_config\n    )\n    results = tuner.fit()\n\n    # Retrain the model with the hyperparameters with best result\n    config = results.get_best_result(metric='f1', mode='max').config\n\n    # 1. Split data\n    X = data[[\n        'has_fraud_7d',\n        'num_transactions_7d',\n        'credit_score',\n        'account_age_days',\n        'has_2fa_installed'\n    ]]\n    y = data['is_fraud']\n\n    # 2. Define resampler\n    if config[\"resampler\"] == \"allknn\":\n        resampler = AllKNN()\n    elif config[\"resampler\"] == \"smote\":\n        resampler = SMOTE()\n    else:\n        resampler = \"passthrough\"\n\n    # 3. Define classifier\n    new_config = {k: v for k, v in config.items() if k != \"resampler\"}\n    classifier = XGBClassifier(**new_config)\n\n    # 4. Combine the resampler and classifier together\n    model = Pipeline(steps=[\n        (\"resampler\", resampler),\n        (\"classifier\", classifier)\n    ])\n\n    # 5. Train and evaluate the model\n    model.fit(X, y)\n    y_prob = model.predict_proba(X)\n    y_prob = y_prob[:, 1]\n    y_pred = (y_prob &gt; 0.5).astype(int)\n    metrics = {\n        \"accuracy\": accuracy_score(y, y_pred),\n        \"precision\": precision_score(y, y_pred),\n        \"recall\": recall_score(y, y_pred),\n        \"f1\": f1_score(y, y_pred),\n        \"roc_auc\": roc_auc_score(y, y_prob),\n        \"log_loss\": log_loss(y, y_prob),\n        \"average_precision\": average_precision_score(y, y_prob)\n    }\n\n    # 6. Log the hyperparameters, metrics and the model\n    mlflow.log_params(config)\n    mlflow.sklearn.log_model(\n        sk_model=model,\n        artifact_path=\"model\",\n        input_example=X.iloc[[0]],\n        metadata={\"version\": f\"{EXPERIMENT_NAME}_v1\"}\n    )\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#import-packages","title":"Import Packages","text":"<p>First, let\u2019s import the required packages.</p> training.py<pre><code># Import packages\nimport time\nfrom datetime import datetime, timedelta\nfrom pprint import pprint\nfrom typing import Any, Dict, Optional\n\nimport pandas as pd\nimport pyarrow.fs\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.under_sampling import AllKNN\nfrom sklearn.metrics import (accuracy_score, average_precision_score, f1_score,\n                             log_loss, precision_score, recall_score,\n                             roc_auc_score)\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\n\nimport mlflow\nimport ray\nfrom feast import FeatureStore\nfrom ray import tune\nfrom ray.tune.search import ConcurrencyLimiter\nfrom ray.tune.search.optuna import OptunaSearch\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#minio-integration","title":"MinIO Integration","text":"<p>We\u2019ll use <code>pyarrow.fs.S3FileSystem</code> to interact with MinIO deployed on Kubernetes. There are two main tasks here.</p> <ol> <li>Load training data stored in MinIO.</li> <li>Save Ray Tune metadata (like checkpoints and logs) back to MinIO during the tuning process.</li> </ol> <p>Here's how we configure the connection to MinIO using <code>S3FileSystem</code>, including the access key, secret key, and endpoint.</p> training.py<pre><code>fs = pyarrow.fs.S3FileSystem(\n    access_key=\"minio_user\",\n    secret_key=\"minio_password\",\n    scheme=\"http\",\n    endpoint_override=\"minio-api.minio.svc.cluster.local:9000\"\n)\n</code></pre> <p>These values should match what you specified when deploying MinIO on Kubernetes. For more details, refer to the configuration section below or revisit this article.</p> Info minio.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio\n  namespace: minio\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n        - name: minio\n          image: minio/minio\n          args:\n            - server\n            - /data\n            - --console-address\n            - :9001\n          env:\n            - name: MINIO_ROOT_USER\n              value: minio_user\n            - name: MINIO_ROOT_PASSWORD\n              value: minio_password\n          ports:\n            - containerPort: 9000\n              protocol: TCP\n            - containerPort: 9001\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /minio/health/live\n              port: 9000\n            initialDelaySeconds: 30\n            periodSeconds: 20\n            timeoutSeconds: 15\n            failureThreshold: 6\n          readinessProbe:\n            httpGet:\n              path: /minio/health/ready\n              port: 9000\n            initialDelaySeconds: 15\n            periodSeconds: 10\n            timeoutSeconds: 10\n            failureThreshold: 3\n          volumeMounts:\n            - name: storage\n              mountPath: /data\n      volumes:\n        - name: storage\n          hostPath:\n            path: /home/docker/data/minio\n            type: DirectoryOrCreate\n      restartPolicy: Always\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: minio-console\n  namespace: minio\nspec:\n  selector:\n    app: minio\n  type: NodePort\n  ports:\n    - name: console\n      port: 9001\n      targetPort: 9001\n      nodePort: 30901\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: minio-api\n  namespace: minio\nspec:\n  selector:\n    app: minio\n  type: ClusterIP\n  ports:\n    - name: api\n      port: 9000\n      targetPort: 9000\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: minio-create-bucket\n  namespace: minio\nspec:\n  backoffLimit: 6\n  completions: 1\n  template:\n    metadata:\n      labels:\n        job: minio-create-bucket\n    spec:\n      initContainers:\n        - name: wait-for-minio\n          image: busybox\n          command:\n            - sh\n            - -c\n            - |\n              until nc -z minio-api.minio.svc.cluster.local 9000; do\n                echo \"Waiting for MinIO...\"\n                sleep 2\n              done\n              echo \"MinIO is ready!\"\n      containers:\n        - name: minio-create-buckets\n          image: minio/mc\n          command:\n            - sh\n            - -c\n            - |\n              mc alias set minio http://minio-api.minio.svc.cluster.local:9000 minio_user minio_password &amp;&amp;\n              for bucket in mlflow dbt sqlmesh ray; do\n                if ! mc ls minio/$bucket &gt;/dev/null 2&gt;&amp;1; then\n                  echo \"Creating bucket: $bucket\"\n                  mc mb minio/$bucket\n                  echo \"Bucket created: $bucket\"\n                else\n                  echo \"Bucket already exists: $bucket\"\n                fi\n              done\n      restartPolicy: OnFailure\n      terminationGracePeriodSeconds: 30\n</code></pre> <p>For other custom storage configuration, see here<sup>1</sup> for more.</p>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#get-the-training-data","title":"Get the Training Data","text":"<p>Since this is a demo project with a small dataset that fits into memory, we\u2019ll use Pandas to read the CSV file directly through the configured filesystem.</p> <p>If the dataset were larger or didn't fit in memory, we would use Ray Data instead. In the future, this could also integrate with Feast Offline Feature Server<sup>2</sup> for more advanced feature management.</p> training.py<pre><code># Get Training Data\nwith fs.open_input_file(\"ray/training_data.csv\") as f:\n    data = pd.read_csv(f)\n\n# Alternative 1: Ray Data\n# ds = ray.data.read_csv(\n#     \"s3://ray/training_data.csv\",\n#     filesystem=fs\n# )\n\n# Alternative 2: Feast\n# now = datetime.now()\n# two_days_ago = datetime.now() - timedelta(days=2)\n# store = FeatureStore('.')\n# fs_fraud_detection_v1 = store.get_feature_service('fraud_detection_v1')\n# data = store.get_historical_features(\n#     entity_df=f\"\"\"\n#     select \n#         src_account as entity_id,\n#         timestamp as event_timestamp,\n#         is_fraud\n#     from\n#         feast-oss.fraud_tutorial.transactions\n#     where\n#         timestamp between timestamp('{two_days_ago.isoformat()}') \n#         and timestamp('{now.isoformat()}')\"\"\",\n#     features=fs_fraud_detection_v1,\n#     full_feature_names=False\n# ).to_df()\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#define-the-runconfig","title":"Define the RunConfig","text":"<p>Next, we configure where Ray Tune stores its metadata by setting the <code>storage_path</code> and <code>storage_filesystem</code> fields in <code>tune.RunConfig()</code>.</p> training.py<pre><code>run_config = tune.RunConfig(\n    name=EXPERIMENT_NAME,\n    storage_path=\"ray/\",\n    storage_filesystem=fs\n)\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#optuna-integration-tuneconfig","title":"Optuna Integration (TuneConfig)","text":"<p>Ray Tune supports <code>OptunaSearch</code><sup>3</sup>, which we\u2019ll use to define the hyperparameter search strategy. A common way to define the search space is by passing a dictionary directly via the <code>param_space</code> argument in <code>tune.Tuner()</code>.</p> <pre><code>tuner = tune.Tuner(\n    objective,\n    tune_config=tune.TuneConfig(\n        metric=\"mean_loss\",\n        mode=\"min\",\n        search_alg=OptunaSearch(),\n        num_samples=1000,\n    ),\n    param_space={\n        \"steps\": 100,\n        \"width\": tune.uniform(0, 20),\n        \"height\": tune.uniform(-100, 100),\n        \"activation\": tune.choice([\"relu\", \"tanh\"]),        \n    },\n)\nresults = tuner.fit()\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#define-the-search-space","title":"Define the Search Space","text":"<p>Sometimes, we want a more flexible search space\u2014especially one with conditional logic. In such cases, we can pass a define-by-run function to <code>OptunaSearch()</code>, which dynamically defines the search space at runtime.<sup>3</sup></p> <p>This function, typically called <code>space</code>, takes a <code>trial</code> object as input. We use <code>trial.suggest_*()</code> methods from Optuna, along with conditionals and loops, to construct the space.<sup>4</sup></p> <p>This setup is helpful for handling more complex scenarios\u2014such as including or excluding hyperparameters based on earlier choices.</p> training.py<pre><code>def space(trial) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Define-by-run function to construct a conditional search space.\n    Ensure no actual computation takes place here.\n\n    Args:\n        trial: Optuna Trial object\n\n    Returns:\n        Dict containing constant parameters or None\n    \"\"\"\n    # Resampler\n    resampler = trial.suggest_categorical(\"resampler\", [\"allknn\", \"smote\", \"passthrough\"])\n\n    # Booster\n    booster = trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"])\n    lmbd = trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True)\n    alpha = trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True)\n    if booster in [\"gbtree\", \"dart\"]:\n        max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n        eta = trial.suggest_float(\"eta\", 1e-3, 0.3, log=True)\n        gamma = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n        grow_policy = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n    # Constants\n    return {\n        \"objective\": \"binary:logistic\",\n        \"random_state\": 1025\n    }\n</code></pre> <p>This <code>space()</code> function defines a conditional hyperparameter search space using Optuna's define-by-run API. Instead of declaring all parameters upfront, the search space is built dynamically as the <code>trial</code> runs. The function suggests different values for categorical and numerical hyperparameters, such as the <code>resampler</code> method (<code>allknn</code><sup>5</sup>, <code>smote</code><sup>6</sup>, or <code>passthrough</code>)<sup>5</sup> and the <code>booster</code> type (<code>gbtree</code>, <code>gblinear</code>, or <code>dart</code>). Based on the chosen booster, additional parameters like <code>max_depth</code>, <code>eta</code>, and <code>grow_policy</code> are conditionally added.</p> <p>Importantly, no actual model training or heavy computation is done inside this function\u2014it only defines the search space structure.<sup>3</sup> The function returns a dictionary of constant parameters (like the learning <code>objective</code> and <code>random_state</code>) to be merged later with the sampled hyperparameters. This design keeps the search logic modular and clean, separating the definition of search space from the training logic.</p>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#define-the-search-algorithm","title":"Define the Search Algorithm","text":"<p>Now we configure the search algorithm using Optuna. We pass our <code>space()</code> function into <code>OptunaSearch</code>, specifying that we want to maximize the F1 score. To avoid exhausting system resources, we wrap it in a <code>ConcurrencyLimiter</code> that restricts parallel trials to 4. Finally, the <code>TuneConfig</code> object ties everything together, specifying the search algorithm and the total number of trials (<code>num_samples=100</code>) to explore.</p> training.py<pre><code>search_alg = OptunaSearch(space=space, metric=\"f1\", mode=\"max\")\nsearch_alg = ConcurrencyLimiter(search_alg, max_concurrent=4)\ntune_config = tune.TuneConfig(\n    search_alg=search_alg,\n    num_samples=100,\n)\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#mlflow-integration","title":"MLflow Integration","text":"<p>Ray offers built-in integration with MLflow through <code>MLflowLoggerCallback</code><sup>7</sup> and <code>setup_mlflow</code><sup>7</sup>. These are convenient options, but they don't support parent-child runs<sup>8</sup>, which are essential for organizing experiments hierarchically. I've tried Databricks approach<sup>9</sup> for setting up parent-child runs but it didn't work.</p> <p>Thankfully, it's not difficult to manually integrate MLflow. So instead of using the built-in methods, we manually set up MLflow tracking inside the script. This integration spans multiple parts of the pipeline:</p> <ol> <li>Set up the tracking URI and experiment in the driver process.</li> <li>Start a parent run in the driver.</li> <li>Set up and log to MLflow from within each trial (i.e., in the worker process).</li> <li>After all trials finish, retrain the best model and log it under the parent run.</li> </ol>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#set-up-the-tracking-uri-and-the-experiment-in-the-driver-process","title":"Set up the Tracking URI and the Experiment in the Driver Process","text":"<p>We begin by setting the experiment name, the run name for this tuning session, and the address of the MLflow tracking server running inside the Kubernetes cluster.</p> training.py<pre><code>EXPERIMENT_NAME = 'fraud_detection'\nRUN_NAME = 'first'\nTRACKING_URI = \"http://tracking-server.mlflow.svc.cluster.local:5000\"\n</code></pre> training.py<pre><code>mlflow.set_tracking_uri(TRACKING_URI)\nif mlflow.get_experiment_by_name(EXPERIMENT_NAME) == None:\n    mlflow.create_experiment(EXPERIMENT_NAME)\nmlflow.set_experiment(EXPERIMENT_NAME)\n</code></pre> <p>These values should match what you specified when deploying MLflow on Kubernetes. For more details, refer to the configuration section below or revisit this article.</p> Info tracking-server.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: {{ .Values.trackingServer.name }}\n  namespace: {{ .Release.Namespace }}\nspec:\n  type: NodePort\n  selector:\n    app: {{ .Values.trackingServer.name }}\n  ports:\n    - port: {{ .Values.trackingServer.port }}\n      targetPort: {{ .Values.trackingServer.port }}\n      nodePort: 30500\n</code></pre> values.yaml<pre><code>trackingServer:\n  name: tracking-server\n  host: 0.0.0.0\n  port: 5000\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#start-the-parent-run-in-the-driver-process","title":"Start the Parent Run in the Driver Process","text":"<p>When we launch <code>Tuner.fit()</code>, we also start an MLflow parent run inside the Ray driver process. Since each trial runs in a worker process, it won\u2019t automatically inherit the MLflow context. So we need to explicitly pass the MLflow tracking URI, experiment name, and parent run ID into each worker so they can log their results correctly under the parent run.</p> training.py<pre><code># Run Ray Tune\nray.init()\nwith mlflow.start_run(run_name=RUN_NAME, nested=True) as run:\n    tuner = tune.Tuner(\n        tune.with_parameters(\n            training_function,\n            data=data,\n            run_id=run.info.run_id,\n            mlflow_tracking_uri=TRACKING_URI,\n            experiment_name=EXPERIMENT_NAME\n        ),\n        tune_config=tune_config,\n        run_config=run_config\n    )\n    results = tuner.fit()\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#integrate-with-mlflow-in-the-worker-process","title":"Integrate with MLflow in the Worker Process","text":"<p>Each trial starts by configuring MLflow to point to the correct tracking server and parent run. Inside the trial, we begin a nested (child) run under the parent run. After training, we log hyperparameters and evaluation metrics, which will be associated with this specific trial.</p> training.py<pre><code>def training_function(\n    config, data,\n    run_id, mlflow_tracking_uri, experiment_name\n):\n    # Set up mlflow \n    mlflow.set_tracking_uri(mlflow_tracking_uri)\n    mlflow.set_experiment(experiment_name=experiment_name)\n\n    # Split data\n    X = data[[\n        'has_fraud_7d',\n        'num_transactions_7d',\n        'credit_score',\n        'account_age_days',\n        'has_2fa_installed'\n    ]]\n    y = data['is_fraud']\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, stratify=y,\n        random_state=config[\"random_state\"]\n    )\n\n    # Define the resampler\n    if config[\"resampler\"] == \"allknn\":\n        resampler = AllKNN()\n    elif config[\"resampler\"] == \"smote\":\n        resampler = SMOTE()\n    else:\n        resampler = \"passthrough\"\n\n    # Define the classifier\n    new_config = {k: v for k, v in config.items() if k != \"resampler\"}\n    classifier = XGBClassifier(**new_config)\n\n    # Combine the resampler and classifier together\n    model = Pipeline(steps=[\n        (\"resampler\", resampler),\n        (\"classifier\", classifier)\n    ])\n\n    # Train the model\n    with mlflow.start_run(run_id=run_id):\n        with mlflow.start_run(\n            run_name=f\"{config['resampler']}-{config['booster']}-{config['lambda']:.2f}-{config['alpha']:.2f}\",\n            nested=True\n        ):\n            model.fit(X_train, y_train)\n            # Evaluate the model\n            y_prob = model.predict_proba(X_test)\n            y_prob = y_prob[:, 1]\n            y_pred = (y_prob &gt; 0.5).astype(int)\n            metrics = {\n                \"accuracy\": accuracy_score(y_test, y_pred),\n                \"precision\": precision_score(y_test, y_pred),\n                \"recall\": recall_score(y_test, y_pred),\n                \"f1\": f1_score(y_test, y_pred),\n                \"roc_auc\": roc_auc_score(y_test, y_prob),\n                \"log_loss\": log_loss(y_test, y_prob),\n                \"average_precision\": average_precision_score(y_test, y_prob)\n            }\n            # Log the metrics and hyperparameters\n            mlflow.log_params(config)\n            mlflow.log_metrics(metrics)\n            tune.report(metrics)\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#retrain-and-save-the-model-with-best-params-in-the-driver-process","title":"Retrain and Save the Model with Best Params in the Driver Process","text":"<p>Once all trials finish, we return to the driver process, where we access the <code>ResultGrid</code>. This object contains all trial results. We then select the best set of hyperparameters (e.g., the one with the highest F1 score), retrain the model with those parameters, and log the final model to MLflow under the original parent run.</p> training.py<pre><code>    # Retrain the model with the hyperparameters with best result\n    config = results.get_best_result(metric='f1', mode='max').config\n\n    # 1. Split data\n    X = data[[\n        'has_fraud_7d',\n        'num_transactions_7d',\n        'credit_score',\n        'account_age_days',\n        'has_2fa_installed'\n    ]]\n    y = data['is_fraud']\n\n    # 2. Define resampler\n    if config[\"resampler\"] == \"allknn\":\n        resampler = AllKNN()\n    elif config[\"resampler\"] == \"smote\":\n        resampler = SMOTE()\n    else:\n        resampler = \"passthrough\"\n\n    # 3. Define classifier\n    new_config = {k: v for k, v in config.items() if k != \"resampler\"}\n    classifier = XGBClassifier(**new_config)\n\n    # 4. Combine the resampler and classifier together\n    model = Pipeline(steps=[\n        (\"resampler\", resampler),\n        (\"classifier\", classifier)\n    ])\n\n    # 5. Train and evaluate the model\n    model.fit(X, y)\n    y_prob = model.predict_proba(X)\n    y_prob = y_prob[:, 1]\n    y_pred = (y_prob &gt; 0.5).astype(int)\n    metrics = {\n        \"accuracy\": accuracy_score(y, y_pred),\n        \"precision\": precision_score(y, y_pred),\n        \"recall\": recall_score(y, y_pred),\n        \"f1\": f1_score(y, y_pred),\n        \"roc_auc\": roc_auc_score(y, y_prob),\n        \"log_loss\": log_loss(y, y_prob),\n        \"average_precision\": average_precision_score(y, y_prob)\n    }\n\n    # 6. Log the hyperparameters, metrics and the model\n    mlflow.log_params(config)\n    mlflow.sklearn.log_model(\n        sk_model=model,\n        artifact_path=\"model\",\n        input_example=X.iloc[[0]],\n        metadata={\"version\": f\"{EXPERIMENT_NAME}_v1\"}\n    )\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#training-function-trainable","title":"Training Function (Trainable)","text":"<p>This is the training logic executed inside each worker process. Here's the typical workflow:</p> training.py<pre><code>def training_function(\n    config, data,\n    run_id, mlflow_tracking_uri, experiment_name\n):\n    # Set up mlflow \n    mlflow.set_tracking_uri(mlflow_tracking_uri)\n    mlflow.set_experiment(experiment_name=experiment_name)\n\n    # Split data\n    X = data[[\n        'has_fraud_7d',\n        'num_transactions_7d',\n        'credit_score',\n        'account_age_days',\n        'has_2fa_installed'\n    ]]\n    y = data['is_fraud']\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, stratify=y,\n        random_state=config[\"random_state\"]\n    )\n\n    # Define the resampler\n    if config[\"resampler\"] == \"allknn\":\n        resampler = AllKNN()\n    elif config[\"resampler\"] == \"smote\":\n        resampler = SMOTE()\n    else:\n        resampler = \"passthrough\"\n\n    # Define the classifier\n    new_config = {k: v for k, v in config.items() if k != \"resampler\"}\n    classifier = XGBClassifier(**new_config)\n\n    # Combine the resampler and classifier together\n    model = Pipeline(steps=[\n        (\"resampler\", resampler),\n        (\"classifier\", classifier)\n    ])\n\n    # Train the model\n    with mlflow.start_run(run_id=run_id):\n        with mlflow.start_run(\n            run_name=f\"{config['resampler']}-{config['booster']}-{config['lambda']:.2f}-{config['alpha']:.2f}\",\n            nested=True\n        ):\n            model.fit(X_train, y_train)\n            # Evaluate the model\n            y_prob = model.predict_proba(X_test)\n            y_prob = y_prob[:, 1]\n            y_pred = (y_prob &gt; 0.5).astype(int)\n            metrics = {\n                \"accuracy\": accuracy_score(y_test, y_pred),\n                \"precision\": precision_score(y_test, y_pred),\n                \"recall\": recall_score(y_test, y_pred),\n                \"f1\": f1_score(y_test, y_pred),\n                \"roc_auc\": roc_auc_score(y_test, y_prob),\n                \"log_loss\": log_loss(y_test, y_prob),\n                \"average_precision\": average_precision_score(y_test, y_prob)\n            }\n            # Log the metrics and hyperparameters\n            mlflow.log_params(config)\n            mlflow.log_metrics(metrics)\n            tune.report(metrics)\n</code></pre> <ol> <li>Retrieve hyperparameters and the dataset (from <code>config</code> and <code>data</code>).</li> <li>Split the dataset into training and validation sets.</li> <li>Set up a pipeline with the selected resampling method and booster.</li> <li>Train the model and log evaluation metrics and hyperparameters.</li> </ol>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#run-ray-tune","title":"Run Ray Tune","text":""},{"location":"side-projects/data2ml-ops/ray/ray-tune/#run-the-hyperparameter-optimization","title":"Run the Hyperparameter Optimization","text":"<p>With the data ready, search space and Optuna strategy defined, and MLflow properly configured, we\u2019re all set to launch Ray Tune via <code>tune.Tuner()</code>.</p> training.py<pre><code># Run Ray Tune\nray.init()\nwith mlflow.start_run(run_name=RUN_NAME, nested=True) as run:\n    tuner = tune.Tuner(\n        tune.with_parameters(\n            training_function,\n            data=data,\n            run_id=run.info.run_id,\n            mlflow_tracking_uri=TRACKING_URI,\n            experiment_name=EXPERIMENT_NAME\n        ),\n        tune_config=tune_config,\n        run_config=run_config\n    )\n    results = tuner.fit()\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#retrain-and-save-the-model-with-the-best-hyperparameters","title":"Retrain and Save the Model with the Best Hyperparameters","text":"<p>After <code>tune.fit()</code> completes and all trials are evaluated, we move on to retraining the best model and logging it\u2014just as explained in the previous section.</p> <p>Once everything is in place, the next step is to submit the script to a Ray cluster. There are several ways to do that, and we\u2019ll cover them in the next article.</p> <ol> <li> <p>Configuring Persistent Storage | Ray Docs \u21a9</p> </li> <li> <p>Feast Offline Feature Server | Feast Docs \u21a9</p> </li> <li> <p>Running Tune experiments with Optuna | Ray Docs \u21a9\u21a9\u21a9</p> </li> <li> <p>Pythonic Search Space | Optuna Docs \u21a9</p> </li> <li> <p>AllKNN | imbalanced-learn Docs \u21a9\u21a9</p> </li> <li> <p>SMOTE | imbalanced-learn Docs \u21a9</p> </li> <li> <p>Using MLflow with Tune | Ray Docs \u21a9\u21a9</p> </li> <li> <p>Understanding Parent and Child Runs in MLflow | MLflow Docs \u21a9</p> </li> <li> <p>Integrate MLflow and Ray | Databricks Docs \u21a9</p> </li> </ol>"},{"location":"side-projects/data2ml-ops/ray/what-why-when/","title":"What, Why, When","text":""},{"location":"side-projects/data2ml-ops/ray/what-why-when/#what-is-ray","title":"What is Ray?","text":"<p>Ray is an open-source framework designed to scale Python and AI workloads efficiently. It provides a unified API that allows developers to run Python applications seamlessly\u2014from a laptop to a large distributed cluster.</p> <p>Built with flexibility in mind, Ray includes a suite of libraries tailored for common machine learning tasks such as data loading and transformation (Ray Data), distributed training (Ray Train), hyperparameter tuning (Ray Tune), and scalable model serving (Ray Serve). These libraries are modular and interoperable, making it easier to build, scale, and manage end-to-end ML pipelines.</p> <p>A good introduction to Ray:</p>"},{"location":"side-projects/data2ml-ops/ray/what-why-when/#why-ray","title":"Why Ray?","text":"<p>For data engineers, Ray offers Python-native distributed processing capabilities that make it easy to scale ETL and data workflows. With Ray Data, large-scale transformations and ingestion tasks can be executed efficiently without relying on heavier frameworks, improving overall pipeline performance.</p> <p>For data analysts, Ray enables parallel computation on large datasets using familiar tools like Pandas. This accelerates data preparation and reduces wait times, making the analysis process faster and more productive\u2014even when working with messy or high-volume data.</p> <p>For data scientists, Ray's Train and Tune modules support scalable model training and hyperparameter optimization. It integrates seamlessly with popular ML frameworks, enabling rapid experimentation and efficient model tuning across different environments.</p> <p>For machine learning engineers, Ray provides an end-to-end solution from training to deployment. With Ray Serve, models can be deployed and scaled with minimal overhead, while supporting flexible resource allocation and production-grade performance across varied use cases.</p>"},{"location":"side-projects/data2ml-ops/ray/what-why-when/#when-to-use-ray","title":"When to Use Ray?","text":""},{"location":"side-projects/data2ml-ops/ray/what-why-when/#scaling-python-workloads-without-rewriting-code","title":"Scaling Python Workloads Without Rewriting Code","text":"<p>When you need to scale Python applications\u2014such as data preprocessing, simulations, or backtesting\u2014Ray enables parallel execution across multiple cores or nodes with minimal code changes. Its Python-native API and dynamic task scheduling make it ideal for workloads that require fine-grained parallelism or involve dynamic task graphs<sup>2</sup>. This allows developers to scale their applications efficiently without the complexity of traditional distributed systems.</p>"},{"location":"side-projects/data2ml-ops/ray/what-why-when/#accelerating-machine-learning-workflows","title":"Accelerating Machine Learning Workflows","text":"<p>For machine learning tasks like distributed training and hyperparameter tuning, Ray's libraries\u2014such as Ray Train and Ray Tune\u2014provide scalable solutions that integrate seamlessly with popular ML frameworks like PyTorch and TensorFlow. This enables faster experimentation and model optimization by leveraging distributed computing resources, reducing the time from development to deployment.</p>"},{"location":"side-projects/data2ml-ops/ray/what-why-when/#deploying-scalable-and-responsive-ml-services","title":"Deploying Scalable and Responsive ML Services","text":"<p>When deploying machine learning models that require low-latency inference at scale, Ray Serve offers a flexible and production-ready serving layer. It supports dynamic model loading, autoscaling, and request batching, making it suitable for online prediction services and real-time ML applications. This ensures that ML services can handle varying loads efficiently while maintaining high performance.</p> <ol> <li> <p>Getting Started | Ray Docs \u21a9</p> </li> <li> <p>Ray: Your Gateway to Scalable AI and Machine Learning Applications \u21a9</p> </li> </ol>"},{"location":"side-projects/dcard-hw/","title":"2020 Dcard Data Engineering Intern","text":"<p>Dcard is a popular social media platform in Taiwan, especially among college students and young adults. It was launched in 2011 as a university-only online forum, similar in spirit to how Facebook started within universities.</p> <p>This project is a pre-interview assignment for the 2020 Dcard Data Engineer Internship Program.</p> <p>On Dcard's app and website, there is an important section called \"Trending Posts,\" where users can find the hottest discussion topics on the platform. As data enthusiasts, we are also curious about which posts have the potential to become trending. If we consider this factor in our recommendations, we might help users discover great posts faster. Therefore, in this assignment, we aim to predict whether a post has the potential to appear in the \"Trending Posts\" section based on some data.</p> <p>To simplify the problem, we define a trending post as one that receives at least 1000 likes within 36 hours of being posted. During testing, we will calculate whether a post's like count exceeds 1000 within 36 hours to determine the ground truth or prediction benchmark.</p> <p>Abstract</p> <pre><code>$ tree\n.\n\u251c\u2500\u2500 requirements.txt: A list of required Python packages and their versions.\n\u251c\u2500\u2500 preprocessing.py: A shared utility script for database connections, preprocessing, and other common functions.\n\u251c\u2500\u2500 training.py: A utility script for training the model.\n\u251c\u2500\u2500 predict.py: A utility script for making predictions.\n\u251c\u2500\u2500 outputs\n\u2502   \u251c\u2500\u2500 best_model.h5: The best model obtained after training.\n\u2502   \u251c\u2500\u2500 cv_results.csv: Cross-validation results.\n\u2502   \u2514\u2500\u2500 output.csv: Prediction results for the public testing dataset.\n\u2514\u2500\u2500 eda_evaluation.ipynb: A Jupyter notebook used for generating visualizations.\n</code></pre> <p>The training dataset includes articles spanning from April 1, 2019, to the end of October 2019, covering approximately seven months. The dataset contains around 793,000 articles, of which about 2.32% (approximately 18,000 articles) are classified as popular. Through exploratory data analysis, we observed high correlations among variables. Additionally, the timing of article publication significantly influences the proportion of popular articles and the total number of likes within the first 36 hours of posting.</p> <p>We decided to use a \"binary classification model without considering sequential information\" as our primary approach, focusing on handling imbalanced datasets, tree-based ensemble models, and subsequent discussions. The training process was divided into three main stages:</p> <ol> <li>Resampling</li> <li>Feature Transformation</li> <li>Classification</li> </ol> <p>After experimentation, we opted to omit the \"Feature Transformation\" stage. In total, 108 combinations were tested using <code>GridSearchCV</code> with <code>cv=3</code> to find the optimal configuration.</p> <p>Using the f1-score as the evaluation metric, the best-performing model was an <code>AdaBoostClassifier</code> without any resampling. This model consisted of 100 decision trees, each limited to a depth of 2. The average f1-score from cross-validation was 0.56, while the f1-score on the public test set was 0.53. Key findings from the experiments include:</p> <ul> <li>Different resampling strategies significantly impact the f1-score.</li> <li>Resampling strategies can effectively identify genuinely popular articles. However, this comes at the cost of reduced trust in the model's predictions of popular articles.</li> <li>Under both \"SMOTE resampling\" and \"no resampling\" scenarios, the choice of classifier did not lead to substantial changes in the f1-score.</li> <li>The choice of classifier had a relatively minor impact on the f1-score.</li> </ul> <p>Finally, we discussed several potential future directions, including exploring other resampling techniques, alternative evaluation metrics, and incorporating sequential information.</p>"},{"location":"side-projects/dcard-hw/#training-dataset","title":"Training Dataset","text":"<p>The training dataset covers posts from April 1, 2019, to the end of October 2019, approximately 7 months. It contains around 794,000 posts, of which about 2.32% (approximately 18,000 posts) are trending.</p> <pre><code>posts_train                Contains 793,751 records and 3 columns\npost_shared_train          Contains 304,260 records and 3 columns\npost_comment_created_train Contains 2,372,228 records and 3 columns\npost_liked_train           Contains 3,395,903 records and 3 columns\npost_collected_train       Contains 1,235,126 records and 3 columns\n</code></pre> <p>Table: <code>posts_train</code></p> column_name data_type description <code>post_key</code> string Unique identifier of the post <code>created_at_hour</code> datetime The hour when the post was created <code>like_count_36_hour</code> integer Number of likes the post received within 36 hours (only in train table) <p>Table: <code>post_shared_train</code></p> column_name data_type description <code>post_key</code> string Unique identifier of the post <code>created_at_hour</code> datetime The hour of the sharing activity <code>count</code> integer Number of shares the post received in that hour <p>Table: <code>post_comment_created_train</code></p> column_name data_type description <code>post_key</code> string Unique identifier of the post <code>created_at_hour</code> datetime The hour of the comment activity <code>count</code> integer Number of comments the post received in that hour <p>Table: <code>post_liked_train</code></p> column_name data_type description <code>post_key</code> string Unique identifier of the post <code>created_at_hour</code> datetime The hour of the like activity <code>count</code> integer Number of likes the post received in that hour <p>Table: <code>post_collected_train</code></p> column_name data_type description <code>post_key</code> string Unique identifier of the post <code>created_at_hour</code> datetime The hour of the collection activity <code>count</code> integer Number of times the post was bookmarked in that hour"},{"location":"side-projects/dcard-hw/#testing-dataset","title":"Testing Dataset","text":"<pre><code>posts_test                 Contains 225,986 records and 3 columns\npost_shared_test           Contains 83,376 records and 3 columns\npost_comment_created_test  Contains 607,251 records and 3 columns\npost_liked_test            Contains 908,910 records and 3 columns\npost_collected_test        Contains 275,073 records and 3 columns\n</code></pre>"},{"location":"side-projects/dcard-hw/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>For offline evaluation, only the first 10 hours of data for each post will be used as input for prediction. The primary evaluation metric is the F1-score.</p>"},{"location":"side-projects/dcard-hw/#submission-requirements","title":"Submission Requirements","text":"<p>Upon completing the assignment, you must submit at least the following four files. Failure to include any of these will be considered incomplete.</p> <ol> <li><code>Report.pdf</code><ul> <li>Instructions on how to use your code</li> <li>Methods and rationale</li> <li>Evaluation results on the provided testing data</li> <li>Experimental observations</li> </ul> </li> <li><code>train.py</code></li> <li><code>predict.py</code></li> <li><code>requirements.txt</code> or Pipfile</li> <li>(Optional) If your prediction requires a model file, please include it (we will not train it for you) and explain how to use it in Report.pdf.</li> </ol> <p>We have some requirements for the program structure to facilitate testing:</p> <ul> <li> <p>Training</p> <ul> <li>The outermost layer should be wrapped in train.py.</li> <li>The program should be executable as <code>python train.py {database_host} {model_filepath}</code>.</li> <li>Example: <code>python train.py localhost:8080 ./model.h5</code></li> </ul> </li> <li> <p>Prediction</p> <ul> <li>The program should be executable as <code>python predict.py {database_host} {model_filepath} {output_filepath}</code>.</li> <li>Specify where your model_filepath is located.</li> <li>Example: <code>python predict.py localhost:8080 ./model.h5 ./sample_output.csv</code></li> <li>Your program must achieve the following during prediction:<ul> <li>Read data from the database. The data format will match the tables described in the next section. For evaluation, we will use our own test data.</li> <li>Use another database's xxx_test tables as the test set during actual testing. Your predict.py should use these tables as input.</li> <li>Output a CSV file with two columns as shown below, including a header (refer to the provided sample_output.csv):<ul> <li>post_key: string type</li> <li>is_trending: bool type</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"side-projects/dcard-hw/#usage-instructions","title":"Usage Instructions","text":"<p>Environment:</p> <ul> <li>Operating System: Ubuntu 18.04 LTS Desktop</li> <li>Python version: Python 3.6.8</li> <li>Required Python packages and their versions are listed in <code>requirements.txt</code>.</li> </ul>"},{"location":"side-projects/dcard-hw/#trainingpy","title":"<code>training.py</code>","text":"<p>The usage of <code>training.py</code> is as follows:</p> <pre><code>usage: training.py [-h] -u USERNAME -p PASSWORD --host HOSTNAME\n                   [--port PORTNUMBER] [--protocol PROTOCOL]\n                   DATABASE OUTPUT_PATH\n</code></pre> <p>At a minimum, you must provide five parameters: \"username,\" \"password,\" \"host IP address,\" \"database name,\" and \"output path.\" To train on the training set, use the following command:</p> <pre><code>python training.py -u \"USERNAME\"\\\n                   -p \"PASSWORD\"\\\n                   --host \"HOSTNAME\"\\\n                   \"DATABASE\"\\\n                   \"OUTPUT_PATH\"\n</code></pre> <p>By default, the program connects to a PostgreSQL database on port 5432. If needed, you can use the <code>--protocol</code> and <code>--port</code> options to connect to other databases, such as MySQL:</p> Note <pre><code>python training.py -u \"USERNAME\"\\\n                -p \"PASSWORD\"\\\n                --host \"HOSTNAME\"\\\n                --port \"3306\"\\\n                --protocol \"mysql\"\\\n                \"DATABASE\"\\\n                \"OUTPUT_PATH\"\n</code></pre> <p>Danger</p> <p>After training, the program generates two files: \"best model\" and \"cross-validation results.\" The default filenames are <code>best_model.h5</code> and <code>cv_results.csv</code> (these cannot be changed). Therefore, when specifying <code>OUTPUT_PATH</code>, only the folder name is required.</p> <p>For more details, use the <code>-h</code> or <code>--help</code> options:</p> Note <pre><code>$ python training.py -h\nusage: training.py [-h] -u USERNAME -p PASSWORD --host HOSTNAME\n                [--port PORTNUMBER] [--protocol PROTOCOL]\n                DATABASE OUTPUT_PATH\n\npositional arguments:\nDATABASE             (Required) Database to use when connecting to server.\nOUTPUT_PATH          (Required) Best prediction model and cross validation\n                    results outputs file path.\n\noptional arguments:\n-h, --help           show this help message and exit\n-u USERNAME          (Required) User for login if not current user.\n-p PASSWORD          (Required) Password to use when connecting to server.\n--host HOSTNAME      (Required) Host address to connect.\n--port PORTNUMBER    Port number to use for connection (default: 5432)\n--protocol PROTOCOL  Protocol to connect. (default: postgres)\n</code></pre>"},{"location":"side-projects/dcard-hw/#predictpy","title":"<code>predict.py</code>","text":"<p>The usage of <code>predict.py</code> is as follows:</p> <pre><code>usage: predict.py [-h] -u USERNAME -p PASSWORD --host HOSTNAME\n                  [--port PORTNUMBER] [--protocol PROTOCOL] [-n]\n                  DATABASE MODEL_NAME OUTPUT_PATH\n</code></pre> <p>Similar to <code>training.py</code>, you must provide five parameters, with an additional parameter for the \"model path\" used to predict trending posts. To predict on the public test set, use the following command:</p> <pre><code>python predict.py -u \"USERNAME\"\\\n                  -p \"PASSWORD\"\\\n                  --host \"HOSTNAME\"\\\n                  \"DATABASE\"\\\n                  \"MODELNAME\"\\\n                  \"OUTPUT_PATH\"\n</code></pre> <p>After execution, the program outputs <code>output.csv</code> (filename cannot be changed) to the specified folder. Note that the <code>MODEL_NAME</code> option must include the model file name, not the folder path.</p> <p>As mentioned in the \"Assignment Supplementary Notes and Corrections\" email, the <code>posts_test</code> table in the private test set does not include the <code>like_count_36_hour</code> column. Therefore, you must use the <code>-n</code> option to indicate that this column is absent. To predict on the private test set, use the following command:</p> <pre><code>python predict.py -u \"USERNAME\"\\\n                  -p \"PASSWORD\"\\\n                  --host \"HOSTNAME\"\\\n                  -n\\\n                  \"DATABASE\"\\\n                  \"MODELNAME\"\\\n                  \"OUTPUT_PATH\"\n</code></pre> <p>If needed, you can also use the <code>--port</code> and <code>--protocol</code> options to connect to other databases.</p> <p>For more details, use the <code>-h</code> or <code>--help</code> options:</p> Note <pre><code>$ python predict.py -h\nusage: predict.py [-h] -u USERNAME -p PASSWORD --host HOSTNAME\n                [--port PORTNUMBER] [--protocol PROTOCOL] [-n]\n                DATABASE MODEL_NAME OUTPUT_PATH\n\npositional arguments:\nDATABASE             (Required) Database to use when connecting to server.\nMODEL_NAME           (Required) Prediction model name. If it is not in the\n                    current directory, please specify where it is.\nOUTPUT_PATH          (Required) File path of predicted results.\n\noptional arguments:\n-h, --help           show this help message and exit\n-u USERNAME          (Required) User for login if not current user.\n-p PASSWORD          (Required) Password to use when connecting to server.\n--host HOSTNAME      (Required) Host address to connect.\n--port PORTNUMBER    Port number to use for connection (default: 5432)\n--protocol PROTOCOL  Protocol to connect. (default: postgres)\n-n                   No like_count_36_hour column when the option is given.\n</code></pre>"},{"location":"side-projects/dcard-hw/eda_evaluation/","title":"\u532f\u5165\u76f8\u95dc\u5957\u4ef6","text":"Table of Contents <ul><li>1\u00a0\u00a0\u532f\u5165\u76f8\u95dc\u5957\u4ef6</li><li>2\u00a0\u00a0\u4e8b\u524d\u6e96\u5099</li><li>3\u00a0\u00a0EDA</li><li>4\u00a0\u00a0Evaluation<ul><li>4.1\u00a0\u00a0Resampler</li><li>4.2\u00a0\u00a0Resampler + Classifier</li><li>4.3\u00a0\u00a0Classifier</li><li>4.4\u00a0\u00a0Classifier + n_estimator</li><li>4.5\u00a0\u00a0<code>AdaBoostClassifier</code> + <code>max_depth</code></li><li>4.6\u00a0\u00a0<code>GradientBoostingClassifier</code>, <code>XGBClassifier</code> + <code>learning_rate</code></li><li>4.7\u00a0\u00a0Best Model<ul><li>4.7.1\u00a0\u00a0f1-score</li><li>4.7.2\u00a0\u00a0balanced accuracy</li></ul></li></ul></li></ul> In\u00a0[1]: Copied! <pre># Import built-in packages\nfrom math import isnan\nfrom functools import reduce\n\n# Import 3-rd party packages\nimport sqlalchemy\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom plotnine import *\n</pre> # Import built-in packages from math import isnan from functools import reduce  # Import 3-rd party packages import sqlalchemy import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from plotnine import * In\u00a0[2]: Copied! <pre>def print_info(info, width=61, fillchar='='):\n    \"\"\"\n    \u5370\u51fa\u683c\u5f0f\u5316\u7684\u8cc7\u8a0a\n    \"\"\"\n    temp_width = width - (width-len(info))//2\n    print(info.rjust(temp_width, fillchar).ljust(width, fillchar))\n</pre> def print_info(info, width=61, fillchar='='):     \"\"\"     \u5370\u51fa\u683c\u5f0f\u5316\u7684\u8cc7\u8a0a     \"\"\"     temp_width = width - (width-len(info))//2     print(info.rjust(temp_width, fillchar).ljust(width, fillchar)) In\u00a0[3]: Copied! <pre>def get_connector(user, host, database, password=None, port='5432', protocol='postgres'):\n    \"\"\"\n    \u53d6\u5f97\u9023\u7dda\u5f15\u64ce\uff0c\u9810\u8a2d\u70ba\u9023\u7dda\u81f3 PostgreSQL\uff0c\u57e0\u865f\u9810\u8a2d\u70ba 5432\u3002\n    \"\"\"\n    print_info(\"GETTING CONNECTOR START!\")\n    user_info = f'{user}:{password}' if password else user\n    url = f'{protocol}://{user_info}@{host}:{port}/{database}'\n    engine = sqlalchemy.create_engine(url, client_encoding='utf-8')\n    print_info(\"DONE!\")\n    return engine\n</pre> def get_connector(user, host, database, password=None, port='5432', protocol='postgres'):     \"\"\"     \u53d6\u5f97\u9023\u7dda\u5f15\u64ce\uff0c\u9810\u8a2d\u70ba\u9023\u7dda\u81f3 PostgreSQL\uff0c\u57e0\u865f\u9810\u8a2d\u70ba 5432\u3002     \"\"\"     print_info(\"GETTING CONNECTOR START!\")     user_info = f'{user}:{password}' if password else user     url = f'{protocol}://{user_info}@{host}:{port}/{database}'     engine = sqlalchemy.create_engine(url, client_encoding='utf-8')     print_info(\"DONE!\")     return engine In\u00a0[4]: Copied! <pre>def get_tables(engine, table_names):\n    \"\"\"\n    \u4f9d\u7167 `tables_names` \u7684\u9806\u5e8f\uff0c\u53d6\u5f97 tables\uff0c\u4e26\u4f9d\u5e8f\u5132\u5b58\u65bc `list` \u7576\u4e2d\uff0c\u56de\u50b3\u578b\u614b\u70ba `list`\uff0c\u6bcf\u500b element \u70ba `DataFrame`\u3002\n    \"\"\"\n    print_info(\"GETTING TABLES START!\")\n    rslt = []\n    for tn in table_names:\n        query = f'SELECT * FROM {tn}'\n        exec(f'{tn} = pd.read_sql(query, engine)')\n        # exec(f\"{tn} = pd.read_csv('{tn}.csv', encoding='utf8')\") # from current working directory\n        print(\n            f'{format(tn, \"26s\")} \u7e3d\u5171\u6709 {eval(f\"{tn}.shape[0]\"):9,} \u7b46\u8cc7\u6599\u548c {eval(f\"{tn}.shape[1]\")} \u500b\u6b04\u4f4d')\n        exec(f'rslt.append({tn})')\n    print_info(\"DONE!\")\n    return rslt\n</pre> def get_tables(engine, table_names):     \"\"\"     \u4f9d\u7167 `tables_names` \u7684\u9806\u5e8f\uff0c\u53d6\u5f97 tables\uff0c\u4e26\u4f9d\u5e8f\u5132\u5b58\u65bc `list` \u7576\u4e2d\uff0c\u56de\u50b3\u578b\u614b\u70ba `list`\uff0c\u6bcf\u500b element \u70ba `DataFrame`\u3002     \"\"\"     print_info(\"GETTING TABLES START!\")     rslt = []     for tn in table_names:         query = f'SELECT * FROM {tn}'         exec(f'{tn} = pd.read_sql(query, engine)')         # exec(f\"{tn} = pd.read_csv('{tn}.csv', encoding='utf8')\") # from current working directory         print(             f'{format(tn, \"26s\")} \u7e3d\u5171\u6709 {eval(f\"{tn}.shape[0]\"):9,} \u7b46\u8cc7\u6599\u548c {eval(f\"{tn}.shape[1]\")} \u500b\u6b04\u4f4d')         exec(f'rslt.append({tn})')     print_info(\"DONE!\")     return rslt In\u00a0[5]: Copied! <pre>def merge_tables(tables, table_names, how):\n    \"\"\"\n    \u5408\u4f75\u6240\u6709 tables\uff0c\u56de\u50b3\u578b\u614b\u70ba `DataFrame`\u3002\n    \"\"\"\n    print_info(\"MERGING TABLES START!\")\n    # \u5206\u5225\u8655\u7406 post_{shared, comment_created, liked, collected}_{train, test} \u56db\u500b tables\n    # groupby \u6bcf\u7bc7\u6587\u7ae0\uff0c\u5c07\u524d\u5341\u5c0f\u6642\u7684\u5206\u4eab\u6578\u3001\u8a55\u8ad6\u6578\u3001\u611b\u5fc3\u6578\u3001\u6536\u85cf\u6578\u52a0\u7e3d\u8d77\u4f86\n    for idx, (table, tn) in enumerate(zip(tables, table_names)):\n        if len(tn.split('_'))==2: continue                  # for handling posts_{train, test} table\n        col_name = f\"{tn.split('_')[1]}_count\"              # tn.split('_')[1] is either {shared, comment, liked, collected}\n        mapper = {'count': col_name}\n        exec(f\"tables[{idx}] = table.groupby(['post_key'], as_index=False).sum().rename(columns=mapper)\")\n    # \u5c07 tables \u5408\u4f75\u8d77\u4f86\u4e26\u56de\u50b3\u3002\n    total_df = reduce(lambda left, right: pd.merge(left, right, on=['post_key'], how=how), tables)\n    print_info(\"DONE!\")\n    return total_df\n</pre> def merge_tables(tables, table_names, how):     \"\"\"     \u5408\u4f75\u6240\u6709 tables\uff0c\u56de\u50b3\u578b\u614b\u70ba `DataFrame`\u3002     \"\"\"     print_info(\"MERGING TABLES START!\")     # \u5206\u5225\u8655\u7406 post_{shared, comment_created, liked, collected}_{train, test} \u56db\u500b tables     # groupby \u6bcf\u7bc7\u6587\u7ae0\uff0c\u5c07\u524d\u5341\u5c0f\u6642\u7684\u5206\u4eab\u6578\u3001\u8a55\u8ad6\u6578\u3001\u611b\u5fc3\u6578\u3001\u6536\u85cf\u6578\u52a0\u7e3d\u8d77\u4f86     for idx, (table, tn) in enumerate(zip(tables, table_names)):         if len(tn.split('_'))==2: continue                  # for handling posts_{train, test} table         col_name = f\"{tn.split('_')[1]}_count\"              # tn.split('_')[1] is either {shared, comment, liked, collected}         mapper = {'count': col_name}         exec(f\"tables[{idx}] = table.groupby(['post_key'], as_index=False).sum().rename(columns=mapper)\")     # \u5c07 tables \u5408\u4f75\u8d77\u4f86\u4e26\u56de\u50b3\u3002     total_df = reduce(lambda left, right: pd.merge(left, right, on=['post_key'], how=how), tables)     print_info(\"DONE!\")     return total_df In\u00a0[6]: Copied! <pre>def preprocess_total_df(total_df):\n    \"\"\"\n    \u9810\u8655\u7406\u525b\u5408\u4f75\u597d\u7684 total_df \u4ee5\u7b26\u5408\u5f8c\u7e8c\u5efa\u6a21\u9700\u6c42\uff0c\u56de\u50b3\u578b\u614b\u70ba `DataFrame`\u3002\n    \"\"\"\n    print_info(\"PREPROCESSING TOTAL_DF START!\")\n    total_df.set_index('post_key', inplace=True)                                    # post_key \u6b04\u4f4d\u8a2d\u70ba\u7d22\u5f15\n    total_df['created_at_hour'] = pd.to_datetime(total_df['created_at_hour'])       # \u5c07 created_at_hour \u6b04\u4f4d\u8f49\u63db\u6210 datetime \u578b\u614b\n    total_df['weekday'] = total_df['created_at_hour'].dt.dayofweek                  # \u64f7\u53d6\u51fa\u767c\u6587\u7684 weekday\n    total_df['hour'] = total_df['created_at_hour'].dt.hour                          # \u64f7\u53d6\u51fa\u767c\u6587\u7684 hour\n    total_df.fillna(0, inplace=True)                                                # NaN \u503c\u88dc 0\n    total_df['is_trending'] = 0+(total_df['like_count_36_hour']&gt;=1000)              # \u8f49\u63db\u6210 is_trending \u985e\u5225\u6b04\u4f4d\n    total_df = total_df.drop(['created_at_hour'], axis=1)                           # drop \u6389\u4e0d\u5fc5\u8981\u7684\u6b04\u4f4d\n    # \u5c07\u8a08\u6b21\u6b04\u4f4d\u8f49\u63db\u6210 int \u578b\u614b\n    col_names = ['shared_count', 'comment_count', 'liked_count', 'collected_count']\n    for cn in col_names:\n        total_df[cn] = total_df[cn].astype(dtype='int')\n    print_info(\"DONE!\")\n    return total_df\n</pre> def preprocess_total_df(total_df):     \"\"\"     \u9810\u8655\u7406\u525b\u5408\u4f75\u597d\u7684 total_df \u4ee5\u7b26\u5408\u5f8c\u7e8c\u5efa\u6a21\u9700\u6c42\uff0c\u56de\u50b3\u578b\u614b\u70ba `DataFrame`\u3002     \"\"\"     print_info(\"PREPROCESSING TOTAL_DF START!\")     total_df.set_index('post_key', inplace=True)                                    # post_key \u6b04\u4f4d\u8a2d\u70ba\u7d22\u5f15     total_df['created_at_hour'] = pd.to_datetime(total_df['created_at_hour'])       # \u5c07 created_at_hour \u6b04\u4f4d\u8f49\u63db\u6210 datetime \u578b\u614b     total_df['weekday'] = total_df['created_at_hour'].dt.dayofweek                  # \u64f7\u53d6\u51fa\u767c\u6587\u7684 weekday     total_df['hour'] = total_df['created_at_hour'].dt.hour                          # \u64f7\u53d6\u51fa\u767c\u6587\u7684 hour     total_df.fillna(0, inplace=True)                                                # NaN \u503c\u88dc 0     total_df['is_trending'] = 0+(total_df['like_count_36_hour']&gt;=1000)              # \u8f49\u63db\u6210 is_trending \u985e\u5225\u6b04\u4f4d     total_df = total_df.drop(['created_at_hour'], axis=1)                           # drop \u6389\u4e0d\u5fc5\u8981\u7684\u6b04\u4f4d     # \u5c07\u8a08\u6b21\u6b04\u4f4d\u8f49\u63db\u6210 int \u578b\u614b     col_names = ['shared_count', 'comment_count', 'liked_count', 'collected_count']     for cn in col_names:         total_df[cn] = total_df[cn].astype(dtype='int')     print_info(\"DONE!\")     return total_df In\u00a0[7]: Copied! <pre># Get engine\nengine = get_connector(\n    user=\"candidate\",\n    password=\"dcard-data-intern-2020\",\n    host=\"35.187.144.113\",\n    database=\"intern_task\"\n)\n# Get tables from db\ntable_names_train = ['posts_train', 'post_shared_train', \n                     'post_comment_created_train', 'post_liked_train', 'post_collected_train']\ntables_train = get_tables(engine, table_names_train)\n# Merge tables\ntotal_df_train = merge_tables(tables_train, table_names_train, how='left')\n# Preprocess total_df\ntotal_df_train = preprocess_total_df(total_df_train)\n\nengine.dispose()\n</pre> # Get engine engine = get_connector(     user=\"candidate\",     password=\"dcard-data-intern-2020\",     host=\"35.187.144.113\",     database=\"intern_task\" ) # Get tables from db table_names_train = ['posts_train', 'post_shared_train',                       'post_comment_created_train', 'post_liked_train', 'post_collected_train'] tables_train = get_tables(engine, table_names_train) # Merge tables total_df_train = merge_tables(tables_train, table_names_train, how='left') # Preprocess total_df total_df_train = preprocess_total_df(total_df_train)  engine.dispose() <pre>===================GETTING CONNECTOR START!==================\n============================DONE!============================\n====================GETTING TABLES START!====================\nposts_train                \u7e3d\u5171\u6709   793,751 \u7b46\u8cc7\u6599\u548c 3 \u500b\u6b04\u4f4d\npost_shared_train          \u7e3d\u5171\u6709   304,260 \u7b46\u8cc7\u6599\u548c 3 \u500b\u6b04\u4f4d\npost_comment_created_train \u7e3d\u5171\u6709 2,372,228 \u7b46\u8cc7\u6599\u548c 3 \u500b\u6b04\u4f4d\npost_liked_train           \u7e3d\u5171\u6709 3,395,903 \u7b46\u8cc7\u6599\u548c 3 \u500b\u6b04\u4f4d\npost_collected_train       \u7e3d\u5171\u6709 1,235,126 \u7b46\u8cc7\u6599\u548c 3 \u500b\u6b04\u4f4d\n============================DONE!============================\n====================MERGING TABLES START!====================\n============================DONE!============================\n================PREPROCESSING TOTAL_DF START!================\n============================DONE!============================\n</pre> In\u00a0[8]: Copied! <pre>cv_results = pd.read_csv('./outputs/cv_results.csv')\n</pre> cv_results = pd.read_csv('./outputs/cv_results.csv') In\u00a0[9]: Copied! <pre>temp = total_df_train.drop(columns=['weekday', 'hour', 'is_trending'])\nsns.heatmap(temp.corr(), cmap='YlGnBu')\n</pre> temp = total_df_train.drop(columns=['weekday', 'hour', 'is_trending']) sns.heatmap(temp.corr(), cmap='YlGnBu') Out[9]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x23755717ba8&gt;</pre> In\u00a0[10]: Copied! <pre>mapper = dict(zip([0, 1, 2, 3, 4, 5, 6], ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']))\n</pre> mapper = dict(zip([0, 1, 2, 3, 4, 5, 6], ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])) In\u00a0[11]: Copied! <pre># \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\u7684\u767c\u6587\u6578\nnum_articles_heatmap_df = total_df_train.groupby(['weekday', 'hour']).size().reset_index().rename(columns={0:'count'})\nnum_articles_heatmap_df = num_articles_heatmap_df.pivot(index='weekday', columns='hour', values='count')\nnum_articles_heatmap_df = num_articles_heatmap_df.rename(mapper=mapper, axis=0)\nnum_articles_heatmap_df = num_articles_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday', \n                                                           'Thursday', 'Friday', 'Saturday', 'Sunday'])\nplt.figure(figsize=(20, 5))\nplt.title(f'Number of Articles by Day of Week / Hour of Day')\nsns.heatmap(num_articles_heatmap_df, cmap='OrRd', cbar=False)\n</pre> # \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\u7684\u767c\u6587\u6578 num_articles_heatmap_df = total_df_train.groupby(['weekday', 'hour']).size().reset_index().rename(columns={0:'count'}) num_articles_heatmap_df = num_articles_heatmap_df.pivot(index='weekday', columns='hour', values='count') num_articles_heatmap_df = num_articles_heatmap_df.rename(mapper=mapper, axis=0) num_articles_heatmap_df = num_articles_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday',                                                             'Thursday', 'Friday', 'Saturday', 'Sunday']) plt.figure(figsize=(20, 5)) plt.title(f'Number of Articles by Day of Week / Hour of Day') sns.heatmap(num_articles_heatmap_df, cmap='OrRd', cbar=False) Out[11]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x237490fb518&gt;</pre> In\u00a0[12]: Copied! <pre># \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\u7684\u71b1\u9580\u6587\u7ae0\u6bd4\u4f8b\nnum_pops_heatmap_df = total_df_train.groupby(['weekday', 'hour'])['is_trending'].sum().reset_index()\nnum_pops_heatmap_df = num_pops_heatmap_df.pivot(index='weekday', columns='hour', values='is_trending')\nnum_pops_heatmap_df = num_pops_heatmap_df.rename(mapper=mapper, axis=0)\nnum_pops_heatmap_df = num_pops_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday', \n                                                   'Thursday', 'Friday', 'Saturday', 'Sunday'])\npct_pops_heatmap_df = num_pops_heatmap_df/num_articles_heatmap_df\nplt.figure(figsize=(20, 5))\nplt.title(f'Percentage of Popular Articles by Day of Week / Hour of Day ')\nsns.heatmap(pct_pops_heatmap_df, cmap='Blues', cbar=False)\n</pre> # \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\u7684\u71b1\u9580\u6587\u7ae0\u6bd4\u4f8b num_pops_heatmap_df = total_df_train.groupby(['weekday', 'hour'])['is_trending'].sum().reset_index() num_pops_heatmap_df = num_pops_heatmap_df.pivot(index='weekday', columns='hour', values='is_trending') num_pops_heatmap_df = num_pops_heatmap_df.rename(mapper=mapper, axis=0) num_pops_heatmap_df = num_pops_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday',                                                     'Thursday', 'Friday', 'Saturday', 'Sunday']) pct_pops_heatmap_df = num_pops_heatmap_df/num_articles_heatmap_df plt.figure(figsize=(20, 5)) plt.title(f'Percentage of Popular Articles by Day of Week / Hour of Day ') sns.heatmap(pct_pops_heatmap_df, cmap='Blues', cbar=False) Out[12]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2375e9e5cc0&gt;</pre> In\u00a0[13]: Copied! <pre># \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\uff0c\u524d 10 \u5c0f\u6642\u611b\u5fc3\u5e73\u5747\u6578\nnum_likes_10_heatmap_df = total_df_train.groupby(['weekday', 'hour'])['liked_count'].mean().reset_index()\nnum_likes_10_heatmap_df = num_likes_10_heatmap_df.pivot(index='weekday', columns='hour', values='liked_count')\nnum_likes_10_heatmap_df = num_likes_10_heatmap_df.rename(mapper=mapper, axis=0)\nnum_likes_10_heatmap_df = num_likes_10_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday', \n                                                           'Thursday', 'Friday', 'Saturday', 'Sunday'])\nplt.figure(figsize=(20, 5))\nplt.title(f'Average Likes within 10 hours by Day of Week / Hour of Day')\nsns.heatmap(num_likes_10_heatmap_df, cmap='Purples', cbar=False)\n</pre> # \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\uff0c\u524d 10 \u5c0f\u6642\u611b\u5fc3\u5e73\u5747\u6578 num_likes_10_heatmap_df = total_df_train.groupby(['weekday', 'hour'])['liked_count'].mean().reset_index() num_likes_10_heatmap_df = num_likes_10_heatmap_df.pivot(index='weekday', columns='hour', values='liked_count') num_likes_10_heatmap_df = num_likes_10_heatmap_df.rename(mapper=mapper, axis=0) num_likes_10_heatmap_df = num_likes_10_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday',                                                             'Thursday', 'Friday', 'Saturday', 'Sunday']) plt.figure(figsize=(20, 5)) plt.title(f'Average Likes within 10 hours by Day of Week / Hour of Day') sns.heatmap(num_likes_10_heatmap_df, cmap='Purples', cbar=False) Out[13]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x237625a2ef0&gt;</pre> In\u00a0[14]: Copied! <pre># \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\uff0c\u524d 36 \u5c0f\u6642\u611b\u5fc3\u5e73\u5747\u6578\nnum_likes_36_heatmap_df = total_df_train.groupby(['weekday', 'hour'])['like_count_36_hour'].mean().reset_index()\nnum_likes_36_heatmap_df = num_likes_36_heatmap_df.pivot(index='weekday', columns='hour', values='like_count_36_hour')\nnum_likes_36_heatmap_df = num_likes_36_heatmap_df.rename(mapper=mapper, axis=0)\nnum_likes_36_heatmap_df = num_likes_36_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday', \n                                                           'Thursday', 'Friday', 'Saturday', 'Sunday'])\nplt.figure(figsize=(20, 5))\nplt.title(f'Average Likes within 36 hours by Day of Week / Hour of Day ')\nsns.heatmap(num_likes_36_heatmap_df, cmap='YlGn', cbar=False)\n</pre> # \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\uff0c\u524d 36 \u5c0f\u6642\u611b\u5fc3\u5e73\u5747\u6578 num_likes_36_heatmap_df = total_df_train.groupby(['weekday', 'hour'])['like_count_36_hour'].mean().reset_index() num_likes_36_heatmap_df = num_likes_36_heatmap_df.pivot(index='weekday', columns='hour', values='like_count_36_hour') num_likes_36_heatmap_df = num_likes_36_heatmap_df.rename(mapper=mapper, axis=0) num_likes_36_heatmap_df = num_likes_36_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday',                                                             'Thursday', 'Friday', 'Saturday', 'Sunday']) plt.figure(figsize=(20, 5)) plt.title(f'Average Likes within 36 hours by Day of Week / Hour of Day ') sns.heatmap(num_likes_36_heatmap_df, cmap='YlGn', cbar=False) Out[14]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2376129c400&gt;</pre> In\u00a0[15]: Copied! <pre># \u53bb\u9664\u7528\u4e0d\u5230\u7684\u6b04\u4f4d\u8cc7\u8a0a\ncv_results = cv_results.drop(columns=[col for col in cv_results.columns if 'split' in col])\n# \u5c07\u6b04\u4f4d\u5167\u5bb9\u5316\u7c21\ndef transform(elem, mapper):\n    if type(elem)==float and isnan(elem):\n        return elem\n    for sub_str in mapper:\n        if sub_str in elem:\n            return mapper[sub_str]\n    return elem\n# resampler\nmapper = {\n    'SMOTE': 'SMOTE',\n    'NearMiss': 'NearMiss'\n}\ncv_results['param_resampler'] = cv_results['param_resampler'].apply(transform, args=(mapper,))\n# classifier\nmapper = {\n    'AdaBoostClassifier': 'AdaBoostClassifier',\n    'XGBClassifier': 'XGBClassifier',\n    'GradientBoostingClassifier': 'GradientBoostingClassifier'\n}\ncv_results['param_classifier'] = cv_results['param_classifier'].apply(transform, args=(mapper,))\n# classifier__base_estimator\nmapper = {\n    'max_depth=1': 'DecisionTreeClassifier(max_depth=1)',\n    'max_depth=2': 'DecisionTreeClassifier(max_depth=2)',\n    'max_depth=3': 'DecisionTreeClassifier(max_depth=3)'\n}\ncv_results['param_classifier__base_estimator'] = cv_results['param_classifier__base_estimator'].apply(transform, args=(mapper,))\n</pre> # \u53bb\u9664\u7528\u4e0d\u5230\u7684\u6b04\u4f4d\u8cc7\u8a0a cv_results = cv_results.drop(columns=[col for col in cv_results.columns if 'split' in col]) # \u5c07\u6b04\u4f4d\u5167\u5bb9\u5316\u7c21 def transform(elem, mapper):     if type(elem)==float and isnan(elem):         return elem     for sub_str in mapper:         if sub_str in elem:             return mapper[sub_str]     return elem # resampler mapper = {     'SMOTE': 'SMOTE',     'NearMiss': 'NearMiss' } cv_results['param_resampler'] = cv_results['param_resampler'].apply(transform, args=(mapper,)) # classifier mapper = {     'AdaBoostClassifier': 'AdaBoostClassifier',     'XGBClassifier': 'XGBClassifier',     'GradientBoostingClassifier': 'GradientBoostingClassifier' } cv_results['param_classifier'] = cv_results['param_classifier'].apply(transform, args=(mapper,)) # classifier__base_estimator mapper = {     'max_depth=1': 'DecisionTreeClassifier(max_depth=1)',     'max_depth=2': 'DecisionTreeClassifier(max_depth=2)',     'max_depth=3': 'DecisionTreeClassifier(max_depth=3)' } cv_results['param_classifier__base_estimator'] = cv_results['param_classifier__base_estimator'].apply(transform, args=(mapper,)) In\u00a0[16]: Copied! <pre>temp = cv_results.groupby(['param_resampler'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean()\ntemp.reset_index(inplace=True)\n\n(ggplot(temp, aes(x='param_resampler', y='mean_test_f1_score', fill='param_resampler'))\n + geom_bar(stat=\"identity\")\n + ylim(0,1)\n + scale_fill_brewer('qualitative', 2)\n + geom_text(aes(label='mean_test_f1_score'), format_string='{:.2f}')\n + ggtitle(f'Average F1 Score by Resampler')\n + labs(fill=f'Resampler')\n + xlab('Resampler')\n + ylab(f'Average F1 score'))\n</pre> temp = cv_results.groupby(['param_resampler'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean() temp.reset_index(inplace=True)  (ggplot(temp, aes(x='param_resampler', y='mean_test_f1_score', fill='param_resampler'))  + geom_bar(stat=\"identity\")  + ylim(0,1)  + scale_fill_brewer('qualitative', 2)  + geom_text(aes(label='mean_test_f1_score'), format_string='{:.2f}')  + ggtitle(f'Average F1 Score by Resampler')  + labs(fill=f'Resampler')  + xlab('Resampler')  + ylab(f'Average F1 score')) Out[16]: <pre>&lt;ggplot: (-9223371884558871573)&gt;</pre> In\u00a0[17]: Copied! <pre>(ggplot(temp, aes(x='param_resampler', y='mean_test_recall', fill='param_resampler'))\n + geom_bar(stat=\"identity\")\n + ylim(0,1)\n + scale_fill_brewer('qualitative', 2)\n + geom_text(aes(label='mean_test_recall'), format_string='{:.2f}')\n + ggtitle(f'Average Recall by Resampler')\n + labs(fill=f'Resampler')\n + xlab('Resampler')\n + ylab(f'Average Recall'))\n</pre> (ggplot(temp, aes(x='param_resampler', y='mean_test_recall', fill='param_resampler'))  + geom_bar(stat=\"identity\")  + ylim(0,1)  + scale_fill_brewer('qualitative', 2)  + geom_text(aes(label='mean_test_recall'), format_string='{:.2f}')  + ggtitle(f'Average Recall by Resampler')  + labs(fill=f'Resampler')  + xlab('Resampler')  + ylab(f'Average Recall')) Out[17]: <pre>&lt;ggplot: (-9223371884558718762)&gt;</pre> In\u00a0[18]: Copied! <pre>(ggplot(temp, aes(x='param_resampler', y='mean_test_precision', fill='param_resampler'))\n + geom_bar(stat=\"identity\")\n + ylim(0,1)\n + scale_fill_brewer('qualitative', 2)\n + geom_text(aes(label='mean_test_precision'), format_string='{:.2f}')\n + ggtitle(f'Average Precision by Resampler')\n + labs(fill=f'Resampler')\n + xlab('Resampler')\n + ylab(f'Average Precision'))\n</pre> (ggplot(temp, aes(x='param_resampler', y='mean_test_precision', fill='param_resampler'))  + geom_bar(stat=\"identity\")  + ylim(0,1)  + scale_fill_brewer('qualitative', 2)  + geom_text(aes(label='mean_test_precision'), format_string='{:.2f}')  + ggtitle(f'Average Precision by Resampler')  + labs(fill=f'Resampler')  + xlab('Resampler')  + ylab(f'Average Precision')) Out[18]: <pre>&lt;ggplot: (152294750826)&gt;</pre> In\u00a0[19]: Copied! <pre>(ggplot(temp, aes(x='param_resampler', y='mean_test_balanced_accuracy', fill='param_resampler'))\n + geom_bar(stat=\"identity\")\n + ylim(0,1)\n + scale_fill_brewer('qualitative', 2)\n + geom_text(aes(label='mean_test_balanced_accuracy'), format_string='{:.2f}')\n + ggtitle(f'Average Balanced Accuracy by Resampler')\n + labs(fill=f'Resampler')\n + xlab('Resampler')\n + ylab(f'Average Balanced Accuracy'))\n</pre> (ggplot(temp, aes(x='param_resampler', y='mean_test_balanced_accuracy', fill='param_resampler'))  + geom_bar(stat=\"identity\")  + ylim(0,1)  + scale_fill_brewer('qualitative', 2)  + geom_text(aes(label='mean_test_balanced_accuracy'), format_string='{:.2f}')  + ggtitle(f'Average Balanced Accuracy by Resampler')  + labs(fill=f'Resampler')  + xlab('Resampler')  + ylab(f'Average Balanced Accuracy')) Out[19]: <pre>&lt;ggplot: (-9223371884547166951)&gt;</pre> In\u00a0[20]: Copied! <pre>temp = cv_results.groupby(['param_resampler', 'param_classifier'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean()\ntemp.reset_index(inplace=True)\n\n(ggplot(temp, aes(x='param_resampler', y='mean_test_f1_score', fill='param_classifier'))\n + geom_bar(position='dodge', stat=\"identity\")\n + ylim(0,1)\n + geom_text(aes(label='mean_test_f1_score'), position=position_dodge(width=0.9), format_string='{:.2f}')\n + ggtitle(f'Average F1 Score by Resampler and Classifier')\n + labs(fill=f'Classifier')\n + xlab('Resampler')\n + ylab(f'Average F1 score'))\n</pre> temp = cv_results.groupby(['param_resampler', 'param_classifier'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean() temp.reset_index(inplace=True)  (ggplot(temp, aes(x='param_resampler', y='mean_test_f1_score', fill='param_classifier'))  + geom_bar(position='dodge', stat=\"identity\")  + ylim(0,1)  + geom_text(aes(label='mean_test_f1_score'), position=position_dodge(width=0.9), format_string='{:.2f}')  + ggtitle(f'Average F1 Score by Resampler and Classifier')  + labs(fill=f'Classifier')  + xlab('Resampler')  + ylab(f'Average F1 score')) Out[20]: <pre>&lt;ggplot: (-9223371884550063342)&gt;</pre> In\u00a0[21]: Copied! <pre>temp = cv_results.groupby(['param_classifier'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean()\ntemp.reset_index(inplace=True)\n\n(ggplot(temp, aes(x='param_classifier', y='mean_test_f1_score', fill='param_classifier'))\n + geom_bar(stat=\"identity\")\n + ylim(0,1)\n + geom_text(aes(label='mean_test_f1_score'), format_string='{:.2f}')\n + ggtitle('Average F1 Score by Classifier')\n + labs(fill='Classifier')\n + xlab('Classifier')\n + ylab(f'Average F1 score'))\n</pre> temp = cv_results.groupby(['param_classifier'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean() temp.reset_index(inplace=True)  (ggplot(temp, aes(x='param_classifier', y='mean_test_f1_score', fill='param_classifier'))  + geom_bar(stat=\"identity\")  + ylim(0,1)  + geom_text(aes(label='mean_test_f1_score'), format_string='{:.2f}')  + ggtitle('Average F1 Score by Classifier')  + labs(fill='Classifier')  + xlab('Classifier')  + ylab(f'Average F1 score')) Out[21]: <pre>&lt;ggplot: (-9223371884545924818)&gt;</pre> In\u00a0[22]: Copied! <pre>temp = cv_results.groupby(['param_classifier', 'param_classifier__n_estimators'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean()\ntemp.reset_index(inplace=True)\n\n(ggplot(temp, aes(x='param_classifier__n_estimators', y='mean_test_f1_score', color='param_classifier'))\n + geom_line()\n + geom_point()\n + ylim(0,1)\n + ggtitle('Average F1 Score by Classifier and Number of Estimators')\n + labs(color='Classifier')\n + xlab('Number of Estimators')\n + ylab('Average F1 score'))\n</pre> temp = cv_results.groupby(['param_classifier', 'param_classifier__n_estimators'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean() temp.reset_index(inplace=True)  (ggplot(temp, aes(x='param_classifier__n_estimators', y='mean_test_f1_score', color='param_classifier'))  + geom_line()  + geom_point()  + ylim(0,1)  + ggtitle('Average F1 Score by Classifier and Number of Estimators')  + labs(color='Classifier')  + xlab('Number of Estimators')  + ylab('Average F1 score')) Out[22]: <pre>&lt;ggplot: (-9223371884546480871)&gt;</pre> In\u00a0[23]: Copied! <pre>cv_results[cv_results['param_resampler']=='passthrough'].groupby(['param_classifier', 'param_classifier__base_estimator'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean()\n</pre> cv_results[cv_results['param_resampler']=='passthrough'].groupby(['param_classifier', 'param_classifier__base_estimator'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean() Out[23]: mean_test_precision mean_test_recall mean_test_specificity mean_test_f1_score mean_test_balanced_accuracy param_classifier param_classifier__base_estimator AdaBoostClassifier DecisionTreeClassifier(max_depth=1) 0.738579 0.436288 0.996339 0.548524 0.716314 DecisionTreeClassifier(max_depth=2) 0.759336 0.443006 0.996670 0.559510 0.719838 DecisionTreeClassifier(max_depth=3) 0.755862 0.441223 0.996619 0.557159 0.718921 In\u00a0[24]: Copied! <pre>cv_results[cv_results['param_resampler']=='passthrough'].groupby(['param_classifier', 'param_classifier__learning_rate'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean()\n</pre> cv_results[cv_results['param_resampler']=='passthrough'].groupby(['param_classifier', 'param_classifier__learning_rate'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean() Out[24]: mean_test_precision mean_test_recall mean_test_specificity mean_test_f1_score mean_test_balanced_accuracy param_classifier param_classifier__learning_rate GradientBoostingClassifier 0.025 0.790585 0.395179 0.997518 0.526909 0.696348 0.050 0.780465 0.423388 0.997177 0.548966 0.710282 0.100 0.778204 0.434859 0.997062 0.557939 0.715961 XGBClassifier 0.025 0.754734 0.404196 0.996884 0.526422 0.700540 0.050 0.776283 0.406060 0.997226 0.533204 0.701643 0.100 0.783911 0.419787 0.997256 0.546763 0.708522 In\u00a0[25]: Copied! <pre>print(cv_results[cv_results['rank_test_f1_score']==1]['params'].values[0])\n</pre> print(cv_results[cv_results['rank_test_f1_score']==1]['params'].values[0]) <pre>{'classifier': AdaBoostClassifier(algorithm='SAMME.R',\n                   base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n                                                         class_weight=None,\n                                                         criterion='gini',\n                                                         max_depth=2,\n                                                         max_features=None,\n                                                         max_leaf_nodes=None,\n                                                         min_impurity_decrease=0.0,\n                                                         min_impurity_split=None,\n                                                         min_samples_leaf=1,\n                                                         min_samples_split=2,\n                                                         min_weight_fraction_leaf=0.0,\n                                                         presort='deprecated',\n                                                         random_state=None,\n                                                         splitter='best'),\n                   learning_rate=1.0, n_estimators=100, random_state=None), 'classifier__base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n                       max_depth=2, max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, presort='deprecated',\n                       random_state=None, splitter='best'), 'classifier__n_estimators': 100, 'resampler': 'passthrough'}\n</pre> In\u00a0[26]: Copied! <pre>temp = cv_results[cv_results['rank_test_f1_score']==1]\ntemp[['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy']]\n</pre> temp = cv_results[cv_results['rank_test_f1_score']==1] temp[['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy']] Out[26]: mean_test_precision mean_test_recall mean_test_specificity mean_test_f1_score mean_test_balanced_accuracy 15 0.759668 0.44419 0.996667 0.560527 0.720429 In\u00a0[27]: Copied! <pre>print(cv_results[cv_results['rank_test_balanced_accuracy']==1]['params'].values[0])\n</pre> print(cv_results[cv_results['rank_test_balanced_accuracy']==1]['params'].values[0]) <pre>{'classifier': GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n                           learning_rate=0.1, loss='deviance', max_depth=3,\n                           max_features=None, max_leaf_nodes=None,\n                           min_impurity_decrease=0.0, min_impurity_split=None,\n                           min_samples_leaf=1, min_samples_split=2,\n                           min_weight_fraction_leaf=0.0, n_estimators=100,\n                           n_iter_no_change=None, presort='deprecated',\n                           random_state=None, subsample=1.0, tol=0.0001,\n                           validation_fraction=0.1, verbose=0,\n                           warm_start=False), 'classifier__learning_rate': 0.025, 'classifier__n_estimators': 120, 'resampler': SMOTE(k_neighbors=5, n_jobs=None, random_state=None, sampling_strategy='auto')}\n</pre> In\u00a0[28]: Copied! <pre>temp = cv_results[cv_results['rank_test_balanced_accuracy']==1]\ntemp[['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy']]\n</pre> temp = cv_results[cv_results['rank_test_balanced_accuracy']==1] temp[['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy']] Out[28]: mean_test_precision mean_test_recall mean_test_specificity mean_test_f1_score mean_test_balanced_accuracy 46 0.199325 0.958312 0.908746 0.330003 0.933529"},{"location":"side-projects/dcard-hw/eda_evaluation/","title":"\u532f\u5165\u76f8\u95dc\u5957\u4ef6\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/","title":"\u4e8b\u524d\u6e96\u5099\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#eda","title":"EDA\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#evaluation","title":"Evaluation\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#resampler","title":"Resampler\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#resampler-classifier","title":"Resampler + Classifier\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#classifier","title":"Classifier\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#classifier-n_estimator","title":"Classifier + n_estimator\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#adaboostclassifier-max_depth","title":"<code>AdaBoostClassifier</code> + <code>max_depth</code>\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#gradientboostingclassifier-xgbclassifier-learning_rate","title":"<code>GradientBoostingClassifier</code>, <code>XGBClassifier</code> + <code>learning_rate</code>\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#best-model","title":"Best Model\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#f1-score","title":"f1-score\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#balanced-accuracy","title":"balanced accuracy\u00b6","text":""},{"location":"side-projects/dcard-hw/docs/1-eda/","title":"Exploratory Data Analysis (EDA)","text":"<p>The dataset is divided into training and testing sets. To avoid data leakage, only the training set is analyzed during EDA, leaving the testing set aside.</p> <p>When we first receive a dataset, the initial step is to examine its details, including the number of records and columns in each table. Below is the dataset information as of the update on 2020/04/13:</p> <pre><code>posts_train                Contains 793,751 records and 3 columns\npost_shared_train          Contains 304,260 records and 3 columns\npost_comment_created_train Contains 2,372,228 records and 3 columns\npost_liked_train           Contains 3,395,903 records and 3 columns\npost_collected_train       Contains 1,235,126 records and 3 columns\n</code></pre> <p>The training set covers posts from April 1, 2019, to the end of October 2019, spanning approximately seven months with around 793,000 posts. The goal is to build a predictive model that uses 10-hour post metrics (e.g., shares, comments, likes, and saves) to predict whether a post will receive 1,000 likes within 36 hours, classifying it as a \"popular post.\"</p> <p>Approximately 2.32% of the training posts are popular, equating to about 18,000 posts. This imbalance in the dataset necessitates techniques like over/undersampling during preprocessing and alternative evaluation metrics during model assessment.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#problem-definition","title":"Problem Definition","text":"<p>The task can be approached in four ways, based on \"whether sequence information is considered\" and \"whether the problem is framed as regression or binary classification\":</p> Regression Binary Classification With Sequence Info RNNs (e.g., GRU), traditional time series models (e.g., ARMA, ARIMA) Same as left Without Sequence Info Poisson regression, SVM, tree-based models, etc. Logistic regression, SVM, tree-based models, etc. <p>For simplicity and time constraints, we focus on \"without sequence info\" and \"binary classification,\" aggregating 10-hour metrics and building a binary classification model to predict popular posts. The focus will be on handling imbalanced data, tree-based models, and subsequent discussions.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#relationships-between-variables","title":"Relationships Between Variables","text":"<p>We simplify the dataset to include total shares, comments, likes, and saves within 10 hours and use a heatmap to observe their relationships with the total likes within 36 hours:</p> <p></p> <p>Info</p> <p>Key observations from the heatmap:</p> <ul> <li>Total likes within 36 hours moderately correlate with total likes within 10 hours (.58), shares (.36), and saves (.36), but weakly with comments (.17).</li> <li>Total likes within 10 hours moderately correlate with shares (.63) and saves (.61).</li> <li>Shares and saves within 10 hours moderately correlate (.48).</li> </ul> <p>In simple terms, posts with more likes within 10 hours tend to have more shares and saves. However, the strongest predictor of total likes within 36 hours is the likes within 10 hours. Comments show little correlation with total likes.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#heatmaps-of-key-metrics","title":"Heatmaps of Key Metrics","text":"<p>Danger</p> <p>To protect Dcard's proprietary information, color bars (<code>cbar=False</code>) are omitted, showing only relative relationships.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#total-posts-by-time","title":"Total Posts by Time","text":"<p>We examine whether the number of posts varies across different time periods:</p> <p></p> <p>The x-axis represents 24 hours, and the y-axis represents days of the week.</p> <p>Info</p> <p>Observations:</p> <ul> <li>Posts are concentrated during midday, afternoon, and evening (12:00\u201318:00), with weekdays slightly higher than weekends.</li> <li>The second-highest posting period is weekday mornings (05:00\u201312:00).</li> <li>Posts are relatively fewer during evenings (18:00\u201301:00) on both weekdays and weekends.</li> </ul> <p>These trends are reasonable, as students primarily post during the day. The relatively high number of early morning posts might be due to companies posting content before students wake up.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#popular-post-proportion-by-time","title":"Popular Post Proportion by Time","text":"<p>Next, we analyze whether certain time periods have a higher proportion of popular posts:</p> <p></p> <p>Info</p> <p>Observations:</p> <ul> <li>Posts during late-night and early-morning hours on weekends have a higher likelihood of being popular, likely due to increased user activity during these times.</li> <li>The heatmap confirms that the proportion of popular posts varies by time.</li> </ul>"},{"location":"side-projects/dcard-hw/docs/1-eda/#average-likes-within-10-hours-by-time","title":"Average Likes Within 10 Hours by Time","text":"<p>We then examine the average likes within 10 hours for posts made at different times:</p> <p></p> <p>Info</p> <p>Observations:</p> <ul> <li>Posts made between 21:00\u201311:00 generally receive more likes within 10 hours.</li> <li>Posts made between 11:00\u201321:00, especially during late afternoon and dinner hours, receive fewer likes on average.</li> </ul> <p>This difference might be because students are less active during late afternoon and dinner hours but more active during the evening. Early morning posts are also visible to students the next day.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#average-likes-within-36-hours-by-time","title":"Average Likes Within 36 Hours by Time","text":"<p>Finally, we analyze the average likes within 36 hours for posts made at different times:</p> <p></p> <p>The trends are consistent with the 10-hour analysis and are not elaborated further.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#summary","title":"Summary","text":"<p>Info</p> <p>Key takeaways:</p> <ul> <li>Variables are generally highly correlated. Polynomial transformations (<code>PolynomialFeatures</code>) may not yield significant improvements during feature engineering.</li> <li>Posting time significantly impacts the proportion of popular posts and the number of likes, and this information should be incorporated into the model.</li> </ul>"},{"location":"side-projects/dcard-hw/docs/2-training/","title":"Feature Engineering","text":"<p>After conducting exploratory data analysis (EDA), we gained a deeper understanding of the training dataset. Before diving into feature engineering, we first organize the training dataset into the following format:</p> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 793751 entries, 0 to 793750\nData columns (total 8 columns):\npost_key           793751 non-null object\nshared_count       793751 non-null int64\ncomment_count      793751 non-null int64\nlike_count         793751 non-null int64\ncollected_count    793751 non-null int64\nweekday            793751 non-null int64\nhour               793751 non-null int64\nis_trending        793751 non-null int64\ndtypes: bool(1), int64(6), object(1)\nmemory usage: 43.1+ MB\n</code></pre> <p>In this section, we will discuss the techniques and models used throughout the data pipeline, including over/undersampling, polynomial transformations, one-hot encoding, and tree-based models.</p> <p>Info</p> <p>The training process can be divided into three main stages:</p> <ol> <li>Resampling</li> <li>Column Transformation</li> <li>Classification</li> </ol> <p>These stages can be represented as the following <code>Pipeline</code> object:</p> <pre><code>cachedir = mkdtemp()\npipe = Pipeline(steps=[('resampler', 'passthrough'),\n                       # ('columntransformer', 'passthrough'),\n                       ('classifier', 'passthrough')],\n                memory=cachedir)\n</code></pre> <p>For each stage, we experiment with two to three different approaches and several hyperparameter settings to identify the optimal combination.</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#handling-imbalanced-datasets-stage-1","title":"Handling Imbalanced Datasets (STAGE 1)","text":"<p>In a binary classification problem, an imbalanced dataset refers to a scenario where the target variable (\\(y\\)) is predominantly of one class (majority) with only a small proportion belonging to the other class (minority). </p> <p>Training a model on such a dataset without addressing the imbalance often results in a biased model that predicts most samples as the majority class, ignoring valuable information from the minority class.</p> <p>A potential solution is resampling, which can be categorized into oversampling and undersampling:</p> <ul> <li>Oversampling: Increases the proportion of minority samples in the dataset.</li> <li>Undersampling: Reduces the proportion of majority samples in the dataset.</li> </ul> <p>Both methods help the model pay more attention to minority samples during training. The simplest approach is random sampling, where majority samples are removed, or minority samples are duplicated.</p> <p>The <code>imblearn</code> library provides implementations for various resampling techniques, including <code>RandomOverSampler</code> and <code>RandomUnderSampler</code>. Additionally, we utilize <code>SMOTE</code> and <code>NearMiss</code>. Below is a brief overview:</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#smote","title":"SMOTE","text":"<p>SMOTE (Synthetic Minority Oversampling Technique) is an oversampling method that synthesizes new minority samples between existing ones, increasing the proportion of the minority class. The following diagram illustrates this:</p> <p> Image Source</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#nearmiss","title":"NearMiss","text":"<p>NearMiss is an undersampling method with three versions. We focus on NearMiss-1, which calculates the average distance of all majority samples to their \\(k\\) nearest minority neighbors and removes the majority samples closest to the minority samples until the class ratio is 1:1. The following diagram illustrates this:</p> <p> Image Source</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#polynomial-transformation-and-one-hot-encoding-stage-2","title":"Polynomial Transformation and One-hot Encoding (STAGE 2)","text":"<p>Next, we use <code>sklearn</code>'s <code>PolynomialFeatures</code> and <code>OneHotEncoder</code> to transform specific features:</p> <ul> <li>For <code>shared_count</code>, <code>comment_count</code>, <code>liked_count</code>, and <code>collected_count</code>, we apply second-degree polynomial transformations to capture non-linear relationships and interactions between features.</li> <li>For the <code>weekday</code> feature, we convert integer values (<code>0</code> - <code>6</code>, representing Monday to Sunday) into one-hot encoded vectors, e.g., <code>[1, 0, 0, 0, 0, 0, 0]</code> for Monday.</li> </ul>"},{"location":"side-projects/dcard-hw/docs/2-training/#tree-based-ensemble-models-stage-3","title":"Tree-based Ensemble Models (STAGE 3)","text":"<p>For classification, we primarily use tree-based ensemble models, including <code>AdaBoostClassifier</code>, <code>GradientBoostingClassifier</code>, and <code>XGBClassifier</code>. These models are chosen for several reasons:</p> <ul> <li>They are invariant to monotonic transformations of features, reducing the need for extensive feature engineering.</li> <li>They offer high interpretability, making it easier to understand feature importance.</li> <li>They perform well on large and complex datasets and are often top performers in Kaggle competitions (e.g., <code>XGBoost</code>, <code>LightGBM</code>, <code>CatBoost</code>).</li> </ul> <p>Ensemble learning can be categorized into Bagging (bootstrap aggregating) and Boosting.</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#bagging","title":"Bagging","text":"<p>The most well-known Bagging application is Random Forest, which builds multiple decision trees using bootstrap sampling and random feature selection. Each tree learns a subset of features, and their predictions are aggregated for the final result.</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#boosting","title":"Boosting","text":""},{"location":"side-projects/dcard-hw/docs/2-training/#adaptive-boosting","title":"Adaptive Boosting","text":"<p>Adaptive Boosting (AdaBoost) sequentially builds \\(T\\) weak learners \\(h_t(x)\\), with each model focusing on samples misclassified by the previous one. Each model is assigned a weight \\(\\alpha_t\\) based on its performance:</p> <ul> <li>Higher weights indicate better performance.</li> <li>Lower weights indicate worse performance.</li> </ul> <p>The final model \\(H(x)\\) aggregates the predictions of all \\(T\\) weak learners. For more details, refer to this note.</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#gradient-boosting","title":"Gradient Boosting","text":"<p>Gradient Boosting builds \\(T\\) models \\(h_t(x)\\) sequentially, where each model predicts the gradient (pseudo-residuals) of the previous model's errors. The final model \\(H(x)\\) is the sum of all previous models. For mathematical derivations, refer to this note.</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#extreme-gradient-boosting","title":"Extreme Gradient Boosting","text":"<p>XGBoost is an optimized implementation of Gradient Boosting with enhancements like weighted quantile sketch, parallel learning, and cache-aware access. For more details, refer to this paper.</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#hyperparameter-optimization","title":"Hyperparameter Optimization","text":"<p>After understanding the techniques used in each stage, we define the possible methods and hyperparameter combinations for each stage as follows:</p> <p>Resampler - <code>passthrough</code>: No resampling. - <code>NearMiss</code>: Default parameters. - <code>SMOTE</code>: Default parameters.</p> <p>Column Transformer - <code>passthrough</code>: No feature transformation. - <code>col_trans</code>: Apply polynomial transformations and one-hot encoding.</p> <p>Classifier - <code>AdaBoostClassifier</code>: Default parameters with tree depth limited to <code>[1, 2, 3]</code>. - <code>GradientBoostingClassifier</code>, <code>XGBClassifier</code>: Default parameters with learning rates <code>[0.025, 0.05, 0.1]</code>.</p> <p>For all classifiers, we set the number of decision trees to <code>[90, 100, 110, 120]</code> and tune additional hyperparameters.</p> <p>Initially, there are 216 combinations to test, which is too many given time constraints. Experiments show that models with feature transformations generally perform worse, likely due to high feature correlations identified during EDA. As a result, we omit the \"Feature Transformation\" stage, reducing the combinations to 108. We use <code>GridSearchCV</code> with <code>cv=3</code> to find the best combination.</p> <p>The parameter grid is defined as follows:</p> <pre><code># poly_cols = ['shared_count', 'comment_count', 'liked_count', 'collected_count']\n# col_trans = make_column_transformer((OneHotEncoder(dtype='int'), ['weekday']),\n#                                     (PolynomialFeatures(include_bias=False), poly_cols),\n#                                     remainder='passthrough')\nparam_grid_ada = {\n    'resampler': ['passthrough', SMOTE(), NearMiss()],\n    # 'columntransformer': ['passthrough', col_trans],\n    'classifier': [AdaBoostClassifier()],\n    'classifier__n_estimators': [90, 100, 110, 120],\n    'classifier__base_estimator': [DecisionTreeClassifier(max_depth=1), \n                                   DecisionTreeClassifier(max_depth=2),\n                                   DecisionTreeClassifier(max_depth=3)]\n}\nparam_grid_gb = {\n    'resampler': ['passthrough', SMOTE(), NearMiss()],\n    # 'columntransformer': ['passthrough', col_trans],\n    'classifier': [GradientBoostingClassifier(), XGBClassifier()],\n    'classifier__n_estimators': [90, 100, 110, 120],\n    'classifier__learning_rate': [0.025, 0.05, 0.1]\n}\nparam_grid = [param_grid_ada, param_grid_gb]\n</code></pre>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/","title":"Results and Discussion","text":""},{"location":"side-projects/dcard-hw/docs/3-evaluation/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>Before discussing the results, let us revisit some commonly used metrics for binary classification, explained using this assignment as an example:</p> Actual\uff3cPredicted Negative Positive Negative \\(\\color{red}{\\text{TN}}\\) \\(\\color{blue}{\\text{FP}}\\) Positive \\(\\color{green}{\\text{FN}}\\) \\(\\color{orange}{\\text{TP}}\\) <p>\\(\\text{Precision}\\): Measures the proportion of articles predicted as popular that are actually popular. Higher values indicate greater trust in the model's predictions for popular articles. Formula: $$ \\text{Precision} = \\frac{\\color{orange}{\\text{TP}}}{\\color{blue}{\\text{FP}} + \\color{orange}{\\text{TP}}} $$</p> <p>\\(\\text{Recall}\\): Measures the proportion of actual popular articles that are correctly predicted by the model. Also known as True Positive Rate (TPR) or Sensitivity. Higher values indicate the model's ability to capture actual popular articles. Formula: $$ \\text{Recall} = \\dfrac{\\color{orange}{\\text{TP}}}{\\color{green}{\\text{FN}} + \\color{orange}{\\text{TP}}} $$ </p> <p>\\(\\text{Specificity}\\): Measures the proportion of actual non-popular articles that are correctly predicted by the model. Also known as True Negative Rate (TNR). Higher values indicate the model's ability to capture actual non-popular articles. Formula: $$ \\text{Specificity} = \\dfrac{\\color{red}{\\text{TN}}}{\\color{red}{\\text{TN}}+\\color{blue}{\\text{FP}}} $$</p> <p>\\(\\text{F1-score}\\): A harmonic mean of \\(\\text{Precision}\\) and \\(\\text{Recall}\\), ranging from \\(0\\) to \\(1\\), with higher values being better. Formula: $$ \\text{F1-score} = \\dfrac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$</p> <p>\\(\\text{Balanced Acc.}\\): A combined metric of \\(\\text{TPR}\\) and \\(\\text{TNR}\\), ranging from \\(0\\) to \\(1\\), with higher values being better. Formula: $$ \\text{Balanced Acc.} = \\dfrac{\\text{TNR} + \\text{TPR}}{2} $$</p> <p>When using <code>GridSearchCV</code> to find the best parameter combination, we record these five metrics and select the best combination based on the f1-score. Example code: <pre><code>scoring = {\n    'precision': 'precision',\n    'recall': 'recall',\n    'specificity': make_scorer(specificity_score),\n    'balanced_accuracy': 'balanced_accuracy',\n    'f1_score': 'f1',\n}\ngrid_search = GridSearchCV(pipe, param_grid=param_grid, scoring=scoring, refit='f1_score', \n                           n_jobs=-1, cv=3, return_train_score=True)\n</code></pre></p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#experimental-results","title":"Experimental Results","text":"<p>Info</p> <p>The best model is <code>AdaBoostClassifier</code> without any resampling, consisting of 100 decision trees with a maximum depth of 2. The average f1-score during cross-validation is 0.56, while the f1-score on the public test set is 0.53. Detailed prediction information is as follows:</p> <p></p> Note <pre><code>===================GETTING CONNECTOR START!==================\n============================DONE!============================\n====================GETTING TABLES START!====================\nposts_test                 Total:   225,986 rows, 3 columns\npost_shared_test           Total:    83,376 rows, 3 columns\npost_comment_created_test  Total:   607,251 rows, 3 columns\npost_liked_test            Total:   908,910 rows, 3 columns\npost_collected_test        Total:   275,073 rows, 3 columns\n============================DONE!============================\n====================MERGING TABLES START!====================\n============================DONE!============================\n================PREPROCESSING TOTAL_DF START!================\n============================DONE!============================\n==================PREDICTING TESTSET START!==================\nf1-score     = 0.53\nbalanced acc = 0.70\n\n            precision    recall  f1-score   support\n\n        0       0.99      1.00      0.99    221479\n        1       0.75      0.40      0.53      4507\n\n    accuracy                           0.99    225986\nmacro avg       0.87      0.70      0.76    225986\nweighted avg       0.98      0.99      0.98    225986\n\n============================DONE!============================\n</code></pre> <p>Now, let us analyze the experimental results. (All figures below are based on cross-validation results, not the entire training set or public test set.)</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#resampler","title":"Resampler","text":"<p>First, let us examine how different resampling strategies affect the f1-score: </p> <p>Info</p> <p>Different resampling strategies indeed affect the f1-score:</p> <ul> <li>NearMiss (undersampling) has the lowest f1-score, likely due to excessive removal of non-popular articles, losing too much majority class information.</li> <li>SMOTE (oversampling) achieves a moderate f1-score.</li> <li>No resampling achieves the highest f1-score.</li> </ul> <p>Next, we investigate how these resampling strategies impact precision and recall:</p> <p></p> <p>Info</p> <ul> <li>NearMiss and SMOTE significantly increase the model's focus on the minority class, resulting in excellent recall scores of 0.91 and 0.95, respectively. However, this comes at the cost of precision, which drops to 0.07 and 0.20, respectively.</li> <li>In other words, resampling strategies can capture actual popular articles but reduce the trustworthiness of the predicted popular articles.</li> </ul> <p>We further explore whether resampling strategies interact with different classifiers to influence the f1-score:</p> <p></p> <p>Info</p> <ul> <li>Under \"SMOTE\" and \"No Resampling\" strategies, different classifiers do not significantly affect the f1-score.</li> <li>However, under the NearMiss strategy, <code>XGBClassifier</code> achieves the highest f1-score (0.18), while <code>AdaBoostClassifier</code> has the lowest (0.07).<ul> <li><code>AdaBoostClassifier</code> performs poorly because it relies on weak classifiers, which struggle with limited majority class information.</li> <li><code>XGBClassifier</code> outperforms <code>GradientBoostingClassifier</code> due to its optimized GBDT implementation.</li> </ul> </li> </ul>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#classifier","title":"Classifier","text":"<p>Next, let us examine how different classifiers affect the f1-score:</p> <p></p> <p>Info</p> <ul> <li>Different classifiers have minimal impact on the f1-score. On average, <code>XGBClassifier</code> achieves the highest score (0.35), primarily due to its performance under the NearMiss strategy.</li> </ul> <p>Finally, we analyze whether the number of internal classifiers in ensemble models affects the f1-score:</p> <p></p> <p>Clearly, the number of classifiers has little impact. Similarly, the tree depth for <code>AdaBoostClassifier</code> and the learning rate for the other two models also have minimal impact on the f1-score (figures omitted).</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#future-directions","title":"Future Directions","text":"<p>The experimental results are summarized above. Due to time constraints, additional attempts were not included. Potential future directions are outlined below:</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#explore-other-resampling-techniques","title":"Explore Other Resampling Techniques","text":"<p>Resampling techniques can increase the model's focus on the minority class. Although the experimental results were not ideal, we can continue fine-tuning hyperparameters or exploring other resampling techniques. Refer to the \"Over-sampling\" and \"Under-sampling\" sections of the <code>imblearn</code> User Guide for potential directions.</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#consider-other-evaluation-metrics","title":"Consider Other Evaluation Metrics","text":"<p>The assignment requires using f1-score as the evaluation metric. However, if we use balanced accuracy instead, the best model would be a <code>GradientBoostingClassifier</code> trained with SMOTE, consisting of 120 classifiers and a learning rate of 0.1, achieving a balanced accuracy of 0.93.</p> <p>The impact of different resampling strategies on balanced accuracy is shown below:</p> <p></p> <p>Info</p> <p>SMOTE achieves the highest balanced accuracy. If the goal is to preliminarily identify potentially popular articles for subsequent workflows, balanced accuracy might be a better evaluation metric.</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#explore-other-feature-transformations-and-classifiers","title":"Explore Other Feature Transformations and Classifiers","text":"<p>The experiment only considered tree-based ensemble models, which require minimal feature transformation. However, we could explore logistic regression, support vector machines, Poisson regression, etc., combined with effective feature transformations. For example, converting <code>weekday</code> and <code>hour</code> into circular coordinates (refer to this post) could improve model performance.</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#incorporate-sequential-information","title":"Incorporate Sequential Information","text":"<p>The experiment ignored the \"time trends\" of shares, comments, likes, and collections within 10 hours of posting. One potential direction is to use recurrent neural networks (RNN, LSTM, GRU, etc.) to capture these trends and nonlinear relationships between variables.</p> <p>A simple approach is to combine the four count variables into a 4-dimensional vector (e.g., <code>[4, 23, 17, 0]</code> for 4 shares, 23 comments, etc.), with a sequence length of 10. Each article's sequential information would then be a <code>(10, 4)</code> matrix, which can be fed into the model for training.</p> <p>For details on LSTM models, refer to my notes.</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#explore-other-hyperparameter-optimization-methods","title":"Explore Other Hyperparameter Optimization Methods","text":"<p>The experiment used <code>GridSearchCV</code> for hyperparameter optimization. However, <code>RandomizedSearchCV</code> might be a better choice for optimizing a large number of hyperparameter combinations. Refer to this 2012 JMLR paper for details.</p> <p>Additionally, consider Bayesian optimization implementations provided by <code>optuna</code> or <code>hyperopt</code>. Watch this video for details, and compare the two libraries in this article.</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#incorporate-text-data-and-user-behavior","title":"Incorporate Text Data and User Behavior","text":"<p>The assignment does not include text data or user behavior. Since the ultimate goal is to \"recommend articles more accurately to users,\" consider incorporating Latent Dirichlet Allocation (LDA) topic modeling to enrich article topic information. Refer to my presentation for details on LDA.</p> <p>Additionally, combining user behavior data could enable more refined personalized text recommendations. Refer to this video and this paper for details.</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#tags-dcard","title":"tags: <code>dcard</code>","text":""},{"location":"side-projects/dcard-hw/docs/4-resampling/","title":"Resampling","text":""},{"location":"side-projects/dcard-hw/docs/4-resampling/#over-sampling","title":"Over Sampling","text":""},{"location":"side-projects/dcard-hw/docs/4-resampling/#under-sampling","title":"Under Sampling","text":""},{"location":"side-projects/leetcode/","title":"LeetCode Problem Lists","text":""},{"location":"side-projects/leetcode/#leetcode-15-patterns","title":"LeetCode 15 Patterns","text":"<p>Two Pointers</p> <ul> <li> 1. Two Sum</li> <li> 167. Two Sum II - Input Array Is Sorted</li> <li> 15. 3Sum</li> <li> 11. Container With Most Water</li> </ul> <p>Overlapping Intervals</p> <ul> <li> 56. Merge Intervals</li> <li> 57. Insert Interval</li> <li> 435. Non-overlapping Intervals</li> </ul> <p>Sliding Window</p> <ul> <li> 643. Maximum Average Subarray I</li> <li> 3. Longest Substring Without Repeating Characters</li> <li> 76. Minimum Window Substring</li> </ul> <p>Prefix Sum</p> <ul> <li> 303. Range Sum Query - Immutable</li> <li> 525. Contiguous Array</li> <li> 560. Subarray Sum Equals K</li> </ul> <p>Fast and Slow Pointers</p> <ul> <li> 141. Linked List Cycle</li> <li> 202. Happy Number</li> <li> 287. Find the Duplicate Number</li> </ul> <p>Linked List In-place Reversal</p> <ul> <li> 206. Reverse Linked List</li> <li> 92. Reverse Linked List II</li> <li> 24. Swap Nodes in Pairs</li> </ul> <p>Monotonic Stack</p> <ul> <li> 496. Next Greater Element I</li> <li> 739. Daily Temperatures</li> <li> 84. Largest Rectangle in Histogram</li> </ul> <p>Top K Elements</p> <ul> <li> 215. Kth Largest Element in an Array</li> <li> 347. Top K Frequent Elements</li> <li> 373. Find K Pairs with Smallest Sums</li> </ul> <p>Dynamic Programming</p> <ul> <li> 70. Climbing Stairs</li> <li> 322. Coin Change</li> <li> 300. Longest Increasing Subsequence</li> <li> 312. Burst Balloons</li> <li> 416. Partition Equal Subset Sum</li> <li> 1143. Longest Common Subsequence</li> </ul> <p>Backtracking</p> <ul> <li> 46. Permutations</li> <li> 78. Subsets</li> <li> 51. N-Queens</li> </ul> <p>Binary Search</p> <ul> <li> 374. Guess Number Higher or Lower</li> <li> 2300. Successful Pairs of Spells and Potions</li> <li> 162. Find Peak Element</li> <li> 875. Koko Eating Bananas</li> </ul> <p>Modified Binary Search</p> <ul> <li> 33. Search in Rotated Sorted Array</li> <li> 153. Find Minimum in Rotated Sorted Array</li> <li> 240. Search a 2D Matrix II</li> </ul> <p>Binary Tree Traversal</p> <ul> <li> 257. Binary Tree Paths</li> <li> 230. Kth Smallest Element in a BST</li> <li> 124. Binary Tree Maximum Path Sum</li> <li> 107. Binary Tree Level Order Traversal II</li> </ul> <p>DFS</p> <ul> <li> 133. Clone Graph</li> <li> 113. Path Sum II</li> <li> 210. Course Schedule II</li> </ul> <p>BFS</p> <ul> <li> 102. Binary Tree Level Order Traversal</li> <li> 994. Rotting Oranges</li> <li> 127. Word Ladder</li> </ul> <p>Matrix Traversal</p> <ul> <li> 733. Flood Fill</li> <li> 200. Number of Islands</li> <li> 130. Surrounded Regions</li> </ul>"},{"location":"side-projects/leetcode/#advanced-sql-50","title":"Advanced SQL 50","text":""},{"location":"side-projects/leetcode/#references","title":"References","text":"<ul> <li>Grind 75</li> <li>SQL 50</li> </ul>"},{"location":"side-projects/leetcode/1-two-sum/","title":"1. Two Sum","text":""},{"location":"side-projects/leetcode/1-two-sum/#summary","title":"Summary","text":"Solution Approach Explanation Time Complexity Space Complexity Brute Force (Nested Loop) Check every possible pair of numbers to see if they add up to the target. This approach is straightforward but slow. \\(O(n^2)\\): We need to check all pairs, which requires a nested loop. \\(O(1)\\): Constant space as we don't need extra storage. Two-pointer technique (Sorted Array) Sort the array and use two pointers (one at the start, one at the end). Move pointers based on the sum comparison to target. \\(O(n\\log n)\\): Sorting takes \\(O(n \\log n)\\), and a linear scan is \\(O(n)\\). \\(O(1)\\): Only pointers, no extra data structures needed. Hash Map (One-pass) Traverse the array while storing each number in a hash map. For each number, check if the complement (target - number) is already in the map. \\(O(n)\\): Single pass through the array to check and store elements. \\(O(n)\\): Extra space for storing up to <code>n</code> elements in the hash map."},{"location":"side-projects/leetcode/1-two-sum/#implementation","title":"Implementation","text":"Python <pre><code># Reminders\n# 1. exactly one solution\n# 2. may not use the same element twice\n# 3. return indices\n# 4. input: may be some duplicate values\n# 5. return the answer in any order.\n\n# Naive approach\n# Nested for-loop\n# - outer loop: go through the array from the first element to the second last.\n# - inner loop: go through the array from the element that is next to the element that the outer loop focus on to the last\n# time complexity: O(n^2)\n# space complexity: O(1)\n\n\n# Hashmap approach\n# time complexity: O(n)\n# space complexity: O(n)\n# Explanation:\n# 1. go through the array, store the complement and the index of it for each number in dict\n# 2. if the number is in the dict, then we found the answer\n\n\n# d = {2:0}\n# target = 9\n# complement = 2\n#           &gt;\n# nums = [2,7,11,15]\n\nclass Solution:\n    def twoSum(self, nums: List[int], target: int) -&gt; List[int]:\n        num_to_idx = {}\n        for i, num in enumerate(nums):\n            complement = target - num\n            if complement in num_to_idx:\n                return [num_to_idx[complement], i]\n            num_to_idx[num] = i\n        return []\n</code></pre>"},{"location":"side-projects/leetcode/1004-max-consecutive-ones-iii/","title":"1004. Max Consecutive Ones Iii","text":""},{"location":"side-projects/leetcode/102-binary-tree-level-order-traversal/","title":"102. Binary Tree Level Order Traversal","text":"<p>NeetCode</p>"},{"location":"side-projects/leetcode/104-maximum-depth-of-binary-tree/","title":"104. Maximum Depth Of Binary Tree","text":""},{"location":"side-projects/leetcode/107-binary-tree-level-order-traversal-ii/","title":"107. Binary Tree Level Order Traversal II","text":"<p>Video</p>"},{"location":"side-projects/leetcode/1071-greatest-common-divisor-of-strings/","title":"1071. Greatest Common Divisor Of Strings","text":""},{"location":"side-projects/leetcode/11-container-with-most-water/","title":"11. Container With Most Water","text":""},{"location":"side-projects/leetcode/113-path-sum-ii/","title":"113. Path Sum II","text":""},{"location":"side-projects/leetcode/1137-n-th-tribonacci-number/","title":"1137. N Th Tribonacci Number","text":""},{"location":"side-projects/leetcode/1143-longest-common-subsequence/","title":"1143. Longest Common Subsequence","text":""},{"location":"side-projects/leetcode/1161-maximum-level-sum-of-a-binary-tree/","title":"1161. Maximum Level Sum Of A Binary Tree","text":""},{"location":"side-projects/leetcode/1207-unique-number-of-occurrences/","title":"1207. Unique Number Of Occurrences","text":""},{"location":"side-projects/leetcode/124-binary-tree-maximum-path-sum/","title":"124. Binary Tree Maximum Path Sum","text":"<p>NeetCode</p>"},{"location":"side-projects/leetcode/1268-search-suggestions-system/","title":"1268. Search Suggestions System","text":""},{"location":"side-projects/leetcode/127-word-ladder/","title":"127. Word Ladder","text":"<p>NeetCode</p>"},{"location":"side-projects/leetcode/13-path-sum-ii/","title":"13. Path Sum II","text":"<p>Video</p>"},{"location":"side-projects/leetcode/130-surrounded-regions/","title":"130. Surrounded Regions","text":"<p>NeetCode</p>"},{"location":"side-projects/leetcode/1318-minimum-flips-to-make-a-or-b-equal-to-c/","title":"1318. Minimum Flips To Make A Or B Equal To C","text":""},{"location":"side-projects/leetcode/133-clone-graph/","title":"133. Clone Graph","text":"<p>NeetCode</p>"},{"location":"side-projects/leetcode/136-single-number/","title":"136. Single Number","text":""},{"location":"side-projects/leetcode/1372-longest-zigzag-path-in-a-binary-tree/","title":"1372. Longest Zigzag Path In A Binary Tree","text":""},{"location":"side-projects/leetcode/141-linked-list-cycle/","title":"141. Linked List Cycle","text":""},{"location":"side-projects/leetcode/1431-kids-with-the-greatest-number-of-candies/","title":"1431. Kids With The Greatest Number Of Candies","text":""},{"location":"side-projects/leetcode/1448-count-good-nodes-in-binary-tree/","title":"1448. Count Good Nodes In Binary Tree","text":""},{"location":"side-projects/leetcode/1456-maximum-vowels-in-a-substring-of-given-length/","title":"1456. Maximum Vowels In A Substring Of Given Length","text":""},{"location":"side-projects/leetcode/1466-reorder-routes-to-make-all-paths-lead-to-the-city-zero/","title":"1466. Reorder Routes To Make All Paths Lead To The City Zero","text":""},{"location":"side-projects/leetcode/1493-longest-subarray-of-1s-after-deleting-one-element/","title":"1493. Longest Subarray Of 1s After Deleting One Element","text":""},{"location":"side-projects/leetcode/15-3sum/","title":"15. 3Sum","text":""},{"location":"side-projects/leetcode/15-3sum/#summary","title":"Summary","text":"Solution Approach Explanation Time Complexity Space Complexity Brute Force Iterate through all triplets and check if their sum is zero. This approach is straightforward but inefficient for large arrays. \\(O(n^3)\\) - three nested loops iterate through the array \\(O(1)\\) - no extra space used beyond input storage Two Pointers Sort the array first, then for each element, use two pointers from both ends to find pairs that sum to the target. Skip duplicates to avoid redundant triplets. \\(O(n^2)\\) - outer loop iterates \\(n\\) times, inner two-pointer search takes \\(O(n)\\) \\(O(1)\\) - only uses constant extra space for pointers and variables HashSet without Sorting Convert 3Sum to 2Sum for each element. Use sets for deduplication and avoid processing duplicate first elements. No array modification required. \\(O(n^2)\\) - outer loop iterates \\(n\\) times, inner 2Sum takes \\(O(n)\\) \\(O(n)\\) - uses sets for storing seen elements and results"},{"location":"side-projects/leetcode/15-3sum/#implementation","title":"Implementation","text":"Python"},{"location":"side-projects/leetcode/151-reverse-words-in-a-string/","title":"151. Reverse Words In A String","text":""},{"location":"side-projects/leetcode/153-find-minimum-in-rotated-sorted-array/","title":"153. Find Minimum in Rotated Sorted Array","text":"<p>NeetCode</p>"},{"location":"side-projects/leetcode/162-find-peak-element/","title":"162. Find Peak Element","text":"<p>NeetCode</p>"},{"location":"side-projects/leetcode/1657-determine-if-two-strings-are-close/","title":"1657. Determine If Two Strings Are Close","text":""},{"location":"side-projects/leetcode/167-two-sum-ii-input-array-is-sorted/","title":"167. Two Sum II - Input Array Is Sorted","text":""},{"location":"side-projects/leetcode/167-two-sum-ii-input-array-is-sorted/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Naive Approach (Nested Loop) Use two nested loops to try every possible pair of numbers. For each pair, check if their sum matches the target. Return the indices if a match is found. O(n\u00b2): The outer loop iterates over each element, and the inner loop checks all elements after it, resulting in quadratic time. O(1): Constant space since no extra data structures are used. Two-pointer technique Use two pointers on the sorted array, one starting at the beginning and the other at the end. Check the sum and adjust the pointers accordingly to find the solution in one pass. O(n): Linear scan as the two pointers converge towards the solution. O(1): Only pointers are used, with no extra space."},{"location":"side-projects/leetcode/167-two-sum-ii-input-array-is-sorted/#implementation","title":"Implementation","text":"Python <pre><code>class Solution:\n    def twoSum(self, numbers: List[int], target: int) -&gt; List[int]:\n        l = 0\n        r = len(numbers) - 1\n        while l &lt; r:\n            curr_sum = numbers[l] + numbers[r]\n            if curr_sum &gt; target:\n                r -= 1\n            elif curr_sum &lt; target:\n                l += 1\n            else:\n                return [l+1, r+1]\n        return []\n</code></pre>"},{"location":"side-projects/leetcode/1679-maximum-number-of-k-sum-pairs/","title":"1679. Maximum Number Of K Sum Pairs","text":""},{"location":"side-projects/leetcode/17-letter-combinations-of-a-phone-number/","title":"17. Letter Combinations Of A Phone Number","text":""},{"location":"side-projects/leetcode/1732-find-the-highest-altitude/","title":"1732. Find The Highest Altitude","text":""},{"location":"side-projects/leetcode/1768-merge-strings-alternately/","title":"1768. Merge Strings Alternately","text":""},{"location":"side-projects/leetcode/1926-nearest-exit-from-entrance-in-maze/","title":"1926. Nearest Exit From Entrance In Maze","text":""},{"location":"side-projects/leetcode/198-house-robber/","title":"198. House Robber","text":""},{"location":"side-projects/leetcode/199-binary-tree-right-side-view/","title":"199. Binary Tree Right Side View","text":""},{"location":"side-projects/leetcode/200-number-of-islands/","title":"200. Number of Islands","text":"<p>NeetCode</p>"},{"location":"side-projects/leetcode/202-happy-number/","title":"202. Happy Number","text":""},{"location":"side-projects/leetcode/206-reverse-linked-list/","title":"206. Reverse Linked List","text":""},{"location":"side-projects/leetcode/208-implement-trie-prefix-tree/","title":"208. Implement Trie Prefix Tree","text":""},{"location":"side-projects/leetcode/2095-delete-the-middle-node-of-a-linked-list/","title":"2095. Delete The Middle Node Of A Linked List","text":""},{"location":"side-projects/leetcode/210-course-schedule-ii/","title":"210. Course Schedule II","text":"<p>NeetCode</p>"},{"location":"side-projects/leetcode/2130-maximum-twin-sum-of-a-linked-list/","title":"2130. Maximum Twin Sum Of A Linked List","text":""},{"location":"side-projects/leetcode/215-kth-largest-element-in-an-array/","title":"215. Kth Largest Element In An Array","text":""},{"location":"side-projects/leetcode/216-combination-sum-iii/","title":"216. Combination Sum Iii","text":""},{"location":"side-projects/leetcode/2215-find-the-difference-of-two-arrays/","title":"2215. Find The Difference Of Two Arrays","text":""},{"location":"side-projects/leetcode/230-kth-smallest-element-in-a-bst/","title":"230. Kth Smallest Element in a BST","text":"<p>NeetCode</p>"},{"location":"side-projects/leetcode/2300-successful-pairs-of-spells-and-potions/","title":"2300. Successful Pairs Of Spells And Potions","text":"<p>NeetCode</p>"},{"location":"side-projects/leetcode/2336-smallest-number-in-infinite-set/","title":"2336. Smallest Number In Infinite Set","text":""},{"location":"side-projects/leetcode/2352-equal-row-and-column-pairs/","title":"2352. Equal Row And Column Pairs","text":""},{"location":"side-projects/leetcode/236-lowest-common-ancestor-of-a-binary-tree/","title":"236. Lowest Common Ancestor Of A Binary Tree","text":""},{"location":"side-projects/leetcode/238-product-of-array-except-self/","title":"238. Product Of Array Except Self","text":""},{"location":"side-projects/leetcode/2390-removing-stars-from-a-string/","title":"2390. Removing Stars From A String","text":""},{"location":"side-projects/leetcode/24-swap-nodes-in-pairs/","title":"24. Swap Nodes in Pairs","text":""},{"location":"side-projects/leetcode/240-search-a-2d-matrix-ii/","title":"240. Search a 2D Matrix II","text":"<p>Video</p>"},{"location":"side-projects/leetcode/2462-total-cost-to-hire-k-workers/","title":"2462. Total Cost To Hire K Workers","text":""},{"location":"side-projects/leetcode/2542-maximum-subsequence-score/","title":"2542. Maximum Subsequence Score","text":""},{"location":"side-projects/leetcode/257-binary-tree-paths/","title":"257. Binary Tree Paths","text":"<p>Video</p>"},{"location":"side-projects/leetcode/283-move-zeroes/","title":"283. Move Zeroes","text":""},{"location":"side-projects/leetcode/287-find-the-duplicate-number/","title":"287. Find the Duplicate Number","text":""},{"location":"side-projects/leetcode/3-longest-substring-without-repeating-characters/","title":"3. Longest Substring Without Repeating Characters","text":""},{"location":"side-projects/leetcode/300-longest-increasing-subsequence/","title":"300. Longest Increasing Subsequence","text":""},{"location":"side-projects/leetcode/303-range-sum-query-immutable/","title":"303. Range Sum Query - Immutable","text":""},{"location":"side-projects/leetcode/312-burst-balloons/","title":"312. Burst Balloons","text":""},{"location":"side-projects/leetcode/322-coin-change/","title":"Index","text":"<ol> <li>Coin Change</li> </ol>"},{"location":"side-projects/leetcode/328-odd-even-linked-list/","title":"328. Odd Even Linked List","text":""},{"location":"side-projects/leetcode/33-search-in-rotated-sorted-array/","title":"33. Search In Rotated Sorted Array","text":"<p>NeetCode</p>"},{"location":"side-projects/leetcode/334-increasing-triplet-subsequence/","title":"334. Increasing Triplet Subsequence","text":""},{"location":"side-projects/leetcode/338-counting-bits/","title":"338. Counting Bits","text":""},{"location":"side-projects/leetcode/345-reverse-vowels-of-a-string/","title":"345. Reverse Vowels Of A String","text":""},{"location":"side-projects/leetcode/347-top-k-frequent-elements/","title":"347. Top K Frequent Elements","text":""},{"location":"side-projects/leetcode/373-find-k-pairs-with-smallest-sums/","title":"373. Find K Pairs with Smallest Sums","text":""},{"location":"side-projects/leetcode/374-guess-number-higher-or-lower/","title":"374. Guess Number Higher Or Lower","text":"<p>NeetCode</p>"},{"location":"side-projects/leetcode/392-is-subsequence/","title":"392. Is Subsequence","text":""},{"location":"side-projects/leetcode/394-decode-string/","title":"394. Decode String","text":""},{"location":"side-projects/leetcode/399-evaluate-division/","title":"399. Evaluate Division","text":""},{"location":"side-projects/leetcode/416-partition-equal-subset-sum/","title":"416. Partition Equal Subset Sum","text":""},{"location":"side-projects/leetcode/435-non-overlapping-intervals/","title":"435. Non Overlapping Intervals","text":""},{"location":"side-projects/leetcode/437-path-sum-iii/","title":"437. Path Sum Iii","text":""},{"location":"side-projects/leetcode/443-string-compression/","title":"443. String Compression","text":""},{"location":"side-projects/leetcode/450-delete-node-in-a-bst/","title":"450. Delete Node In A Bst","text":""},{"location":"side-projects/leetcode/452-minimum-number-of-arrows-to-burst-balloons/","title":"452. Minimum Number Of Arrows To Burst Balloons","text":""},{"location":"side-projects/leetcode/46-permutations/","title":"46. Permutations","text":""},{"location":"side-projects/leetcode/496-next-greater-element-i/","title":"496. Next Greater Element I","text":""},{"location":"side-projects/leetcode/51-n-queens/","title":"51. N-Queens","text":""},{"location":"side-projects/leetcode/525-contiguous-array/","title":"525. Contiguous Array","text":""},{"location":"side-projects/leetcode/547-number-of-provinces/","title":"547. Number Of Provinces","text":""},{"location":"side-projects/leetcode/56-merge-intervals/","title":"56. Merge Intervals","text":""},{"location":"side-projects/leetcode/560-subarray-sum-equals-k/","title":"560. Subarray Sum Equals K","text":""},{"location":"side-projects/leetcode/57-insert-interval/","title":"57. Insert Interval","text":""},{"location":"side-projects/leetcode/605-can-place-flowers/","title":"605. Can Place Flowers","text":""},{"location":"side-projects/leetcode/62-unique-paths/","title":"62. Unique Paths","text":""},{"location":"side-projects/leetcode/643-maximum-average-subarray-i/","title":"643. Maximum Average Subarray I","text":""},{"location":"side-projects/leetcode/649-dota2-senate/","title":"649. Dota2 Senate","text":""},{"location":"side-projects/leetcode/70-climbing-stairs/","title":"70. Climbing Stairs","text":"<p>NeetCode</p>"},{"location":"side-projects/leetcode/700-search-in-a-binary-search-tree/","title":"700. Search In A Binary Search Tree","text":""},{"location":"side-projects/leetcode/714-best-time-to-buy-and-sell-stock-with-transaction-fee/","title":"714. Best Time To Buy And Sell Stock With Transaction Fee","text":""},{"location":"side-projects/leetcode/72-edit-distance/","title":"72. Edit Distance","text":""},{"location":"side-projects/leetcode/724-find-pivot-index/","title":"724. Find Pivot Index","text":""},{"location":"side-projects/leetcode/733-flood-fill/","title":"733. Flood Fill","text":""},{"location":"side-projects/leetcode/735-asteroid-collision/","title":"735. Asteroid Collision","text":""},{"location":"side-projects/leetcode/739-daily-temperatures/","title":"739. Daily Temperatures","text":""},{"location":"side-projects/leetcode/746-min-cost-climbing-stairs/","title":"746. Min Cost Climbing Stairs","text":""},{"location":"side-projects/leetcode/76-minimum-window-substring/","title":"76. Minimum Window Substring","text":""},{"location":"side-projects/leetcode/78-subsets/","title":"78. Subsets","text":""},{"location":"side-projects/leetcode/790-domino-and-tromino-tiling/","title":"790. Domino And Tromino Tiling","text":""},{"location":"side-projects/leetcode/84-largest-rectangle-in-histogram/","title":"84. Largest Rectangle in Histogram","text":""},{"location":"side-projects/leetcode/841-keys-and-rooms/","title":"841. Keys And Rooms","text":""},{"location":"side-projects/leetcode/872-leaf-similar-trees/","title":"872. Leaf Similar Trees","text":""},{"location":"side-projects/leetcode/875-koko-eating-bananas/","title":"875. Koko Eating Bananas","text":""},{"location":"side-projects/leetcode/901-online-stock-span/","title":"901. Online Stock Span","text":""},{"location":"side-projects/leetcode/92-reverse-linked-list-ii/","title":"92. Reverse Linked List II","text":""},{"location":"side-projects/leetcode/933-number-of-recent-calls/","title":"933. Number Of Recent Calls","text":""},{"location":"side-projects/leetcode/994-rotting-oranges/","title":"994. Rotting Oranges","text":"<p>NeetCode</p>"},{"location":"side-projects/restful-apis-with-flask/","title":"RESTful APIs with Flask","text":"<p>This repository documents my journey of learning how to build RESTful APIs using Flask. It includes step-by-step implementations of various concepts, from basic API design principles to advanced features like authentication, database integration, deployment, and third-party integrations. </p> <p>The content is based on two Udemy courses: \"REST APIs with Flask and Python\" and \"Advanced REST APIs with Flask and Python\". Each section highlights key topics, tools, and techniques, making it a comprehensive resource for anyone looking to learn Flask for API development.</p> <p></p> <p>Certificate</p> <p></p> <p>Certificate</p>"},{"location":"side-projects/restful-apis-with-flask/#about-this-project","title":"About This Project","text":"<p>The goal of this project is to: - Learn and experiment with Flask for building RESTful APIs. - Understand best practices for API design and implementation. - Explore integrations with databases, authentication, and other web technologies.</p>"},{"location":"side-projects/restful-apis-with-flask/#features","title":"Features","text":"<ul> <li>RESTful API Design: Follows REST principles for clean and scalable APIs.</li> <li>Flask Framework: Built using Flask for simplicity and flexibility.</li> <li>Database Integration: Includes examples of working with databases like SQLite or SQLAlchemy.</li> <li>Authentication: Demonstrates how to secure APIs with authentication mechanisms.</li> <li>Error Handling: Implements robust error handling for better user experience.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/#how-to-use-this-repository","title":"How to Use This Repository","text":"<p>Feel free to browse the code, read the documentation, and run the examples. If you're new to Flask or REST APIs, this project can serve as a learning resource.</p>"},{"location":"side-projects/restful-apis-with-flask/#getting-started","title":"Getting Started","text":"<ol> <li> <p>Clone the repository:     <pre><code>gh repo clone your-username/rest-apis-with-flask\ncd rest-apis-with-flask\n</code></pre></p> </li> <li> <p>Install dependencies:     <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Run the application:     <pre><code>python app.py\n</code></pre></p> </li> </ol> <p>Thank you for visiting, and I hope you find this project helpful!</p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/","title":"\u5c07 Flask \u61c9\u7528\u7a0b\u5f0f\u90e8\u7f72\u5728 Ubuntu 16.04 Server","text":"<p>\u9996\u5148\uff0c\u6211\u5011\u5fc5\u9808\u8a3b\u518a DigitalOcean \u5e33\u865f\u4e26\u5728\u4e0a\u9762\u79df\u7528\u4e00\u500b\u865b\u64ec\u4e3b\u6a5f\uff0c\u7248\u672c\u70ba Ubuntu 16.04 Server\u3002\u5982\u4f55\u4ee5 SSH \u9023\u7dda\u4e0d\u662f\u8ab2\u7a0b\u91cd\u9ede\uff0c\u56e0\u6b64\u6211\u5011\u5728\u6b64\u5148\u7565\u904e\u3002\u9023\u7dda\u5f8c\u7dca\u63a5\u8457\u9032\u884c\u5e7e\u9805\u4e8b\u524d\u6e96\u5099\uff1a</p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#_1","title":"\u66f4\u65b0\u5009\u5eab\u6e05\u55ae","text":"<pre><code># apt-get update\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#os","title":"\u5728 OS \u4e0a\u5275\u5efa\u65b0\u4f7f\u7528\u8005","text":"<pre><code># adduser &lt;username&gt;\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#super-user","title":"\u7d66\u4e88 super user \u6b0a\u9650","text":"<p>\u9032\u5165 <code>/etc/sudoers</code> \u6a94\u6848\uff1a <pre><code># visudo\n</code></pre></p> <p>\u5728 \"User privilege specification\" \u4e0b\u65b9\u66ff\u65b0\u4f7f\u7528\u8005\u52a0\u5165 super user \u7684\u6b0a\u9650\uff1a <pre><code>&lt;username&gt; ALL=(ALL:ALL) ALL\n</code></pre></p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#1-postresql","title":"1. \u5b89\u88dd\u4e26\u8a2d\u5b9a PostreSQL \u8cc7\u6599\u5eab","text":""},{"location":"side-projects/restful-apis-with-flask/basics/section9/#postgresql","title":"\u5b89\u88dd PostgreSQL","text":"<pre><code># apt-get install postgresql postgresql-contrib\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#postgres","title":"\u5207\u63db\u6210 <code>postgres</code> \u4f7f\u7528\u8005","text":"<pre><code># sudo -i -u postgres\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#postgresql_1","title":"\u66ff\u65b0\u4f7f\u7528\u8005\uff0c\u5275\u5efa PostgreSQL \u7576\u4e2d\u7684\u5e33\u865f\u548c\u8cc7\u6599\u5eab","text":"<pre><code>$ createuser &lt;username&gt; -P\n$ createdb &lt;username&gt;\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#postgresql_2","title":"\u5f37\u5236\u4ee5\u5bc6\u78bc\u767b\u5165 PostgreSQL","text":"<p>\u9032\u5165 <code>pg_hba.conf</code> \u914d\u7f6e\u6a94\uff1a <pre><code>$ nano /etc/postgresql/10/main/pg_hba.conf\n</code></pre></p> <p>\u5c07 <pre><code>local all all peer\n</code></pre> \u6539\u70ba <pre><code>local all all md5\n</code></pre></p> <p>\u5b8c\u6210\u5f8c\u8df3\u51fa\u914d\u7f6e\u6a94\uff0c\u63a5\u8457\u6539\u70ba\u65b0\u4f7f\u7528\u8005\u4f86\u64cd\u4f5c\u3002</p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#2-nginx","title":"2. \u5b89\u88dd\u4e26\u8a2d\u5b9a Nginx \u4f3a\u670d\u5668","text":""},{"location":"side-projects/restful-apis-with-flask/basics/section9/#nginx","title":"\u5b89\u88dd Nginx","text":"<pre><code>$ sudo apt-get install nginx\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#nginx-ssh","title":"\u958b\u555f\u9632\u706b\u7246\u4e26\u5141\u8a31 <code>nginx</code> \u548c <code>ssh</code>","text":"<pre><code>$ sudo ufw enable\n$ sudo ufw allow 'Nginx HTTP'\n$ sudo ufw allow ssh\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#flask-nginx","title":"\u66ff\u6211\u5011\u7684 Flask \u61c9\u7528\u7a0b\u5f0f\u52a0\u5165 Nginx \u914d\u7f6e\u6a94","text":"<p>\u5275\u5efa\u4e26\u9032\u5165 <code>items-rest.conf</code> \u914d\u7f6e\u6a94\uff1a <pre><code>$ sudo nano /etc/nginx/sites-available/items-rest.conf\n</code></pre></p> <p>\u8f38\u5165\u4ee5\u4e0b\u5167\u5bb9\uff1a <pre><code>server {\n    listen 80;\n    real_ip_header X-Forwarded-For;\n    set_real_ip_from 127.0.0.1;\n    server_name localhost;\n\n    location / {\n        include uwsgi_params;\n        uwsgi_pass unix:/var/www/html/items-rest/socket.sock;\n        uwsgi_modifier1 30;\n    }\n\n    error_page 404 /404.html;\n    location = 404.html {\n        root /usr/share/nginx/html;\n    }\n\n    error_page 500 502  503 504 50x.html;\n    location = /50x.html {\n        root /usr/share/nginx/html;\n    }\n}\n</code></pre></p> <p>\u5b8c\u6210\u5f8c\u8df3\u51fa\u914d\u7f6e\u6a94\uff0c\u63a5\u8457\u5efa\u7acb soft link\uff0c\u555f\u7528\u914d\u7f6e\uff1a <pre><code>$ sudo ln -s /etc/nginx/sites-available/items-rest.conf /etc/nginx/sites-enabled/\n</code></pre></p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#3-flask","title":"3. \u8a2d\u5b9a Flask \u61c9\u7528\u7a0b\u5f0f\u6240\u9700\u57f7\u884c\u74b0\u5883","text":""},{"location":"side-projects/restful-apis-with-flask/basics/section9/#_2","title":"\u5275\u5efa\u5c08\u6848\u76ee\u9304\u4e26\u7d66\u4e88\u9069\u7576\u6b0a\u9650","text":"<pre><code>$ sudo mkdir /var/www/html/items-rest\n$ sudo chown &lt;username&gt;:&lt;username&gt; /var/www/html/items-rest\n</code></pre> <p>\u5b8c\u6210\u5f8c\u9032\u5165\u8a72\u76ee\u9304\uff1a <pre><code>$ cd /var/www/html/items-rest\n</code></pre></p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#_3","title":"\u8a2d\u5b9a\u5c08\u6848\u6240\u9700\u74b0\u5883","text":"<p>\u4e0b\u8f09\u5c08\u6848\u5167\u5bb9\u4e26\u5275\u5efa\u65e5\u8a8c\u6a94\u76ee\u9304\uff1a <pre><code>$ git clone https://github.com/schoolofcode-me/stores-rest-api.git .\n$ mkdir log\n</code></pre></p> <p>\u5efa\u7acb\u865b\u64ec\u74b0\u5883\u4e26\u5b89\u88dd\u6240\u9700\u5957\u4ef6\uff1a <pre><code>$ sudo apt-get install python-pip python3-dev libpq-dev\n$ pip install virtualenv\n$ virtualenv venv --python=python3.6\n$ source venv/bin/activate\n(venv)$ pip install -r requirements.txt\n</code></pre></p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#4-uwsgi","title":"4. \u8a2d\u5b9a uWSGI \u4f3a\u670d\u5668","text":""},{"location":"side-projects/restful-apis-with-flask/basics/section9/#uwsgi_items_restservice-ubuntu","title":"\u5275\u5efa <code>uwsgi_items_rest.service</code> Ubuntu \u670d\u52d9","text":"<p>\u5275\u5efa\u4e26\u9032\u5165 <code>uwsgi_items_rest.service</code> \u6a94\uff1a <pre><code>$ sudo nano /etc/systemd/system/uwsgi_items_rest.service\n</code></pre></p> <p>\u8f38\u5165\u4ee5\u4e0b\u5167\u5bb9\uff1a <pre><code>[Unit]\nDescription=uWSGI items rest\n\n[Service]\nEnvironment=DATABASE_URL=postgres://jose:1234@localhost:5432/jose\nExecStart=/var/www/html/items-rest/venv/bin/uwsgi --master --emperor /var/www/html/items-rest/uwsgi.ini --die-on-term --uid jose --gid jose --logto /var/www/html/items-rest/log/emperor.log\nRestart=always\nKillSignal=SIGQUIT\nType=notify\nNotifyAccess=all\n\n[Install]\nWantedBy=multi-user.target\n</code></pre></p> <p>\u5b8c\u6210\u5f8c\u8df3\u51fa\u6a94\u6848\u3002</p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#uwsgiini","title":"\u4fee\u6539 <code>uwsgi.ini</code> \u914d\u7f6e\u6a94","text":"<p>\u9032\u5165\u5c08\u6848\u5167\u7684 <code>uwsgi.ini</code> \u914d\u7f6e\u6a94\uff1a <pre><code>$ nano uwsgi.ini\n</code></pre></p> <p>\u8f38\u5165\u4ee5\u4e0b\u5167\u5bb9\uff1a <pre><code>[uwsgi]\nbase = /var/www/html/items-rest\napp = run\nmodule = %(app)\n\nhome = %(base)/venv\npythonpath = %(base)\n\nsocket = %(base)/socket.sock\n\nchmod-socket = 777\n\nprocesses = 8\n\nthreads = 8\n\nharakiri = 15\n\ncallable = app\n\nlogto = /var/www/html/items-rest/log/%n.log\n</code></pre></p> <p>\u5b8c\u6210\u5f8c\u8df3\u51fa\u914d\u7f6e\u6a94\u3002</p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#5-flask","title":"5. \u555f\u52d5 Flask \u61c9\u7528\u7a0b\u5f0f","text":"<p>\u522a\u9664 Nginx \u9810\u8a2d\u914d\u7f6e\u6a94\uff0c\u907f\u514d\u8b80\u53d6\u932f\u8aa4\u7684\u914d\u7f6e\u6a94\uff0c\u63a5\u8457 reload \u4e26 restart\uff1a <pre><code>$ sudo rm /etc/nginx/sites-enabled/default\n$ sudo systemctl reload nginx \n$ sudo systemctl restart nginx\n</code></pre></p> <p>\u555f\u52d5 <code>uwsgi_items_rest</code> \u670d\u52d9\uff1a <pre><code>$ sudo systemctl start uwsgi_items_rest\n</code></pre></p> <p>\u5b8c\u6210\uff01</p>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/","title":"Advanced","text":""},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-1-course-preparations","title":"Section 1: Course Preparations","text":"<p>Preparations for the course:</p> <ul> <li>Simplified authentication mechanism.</li> <li>Added type hinting.</li> <li>Unified code style.</li> <li>Changed all <code>Resource</code> methods to class methods (using <code>@classmethod</code>).</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-2-marshmallow-integration","title":"Section 2: Marshmallow Integration","text":"<p>Introducing <code>marshmallow</code>, <code>flask-marshmallow</code>, and <code>marshmallow-sqlalchemy</code>:</p> <ul> <li>Simplified request parsing, <code>Model</code> object creation, and JSON responses by defining <code>Schema</code> for each <code>Resource</code>.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-3-email-verification","title":"Section 3: Email Verification","text":"<ul> <li>Implemented user email verification process (using Mailgun).</li> <li>Used <code>.env</code> files to store sensitive data.</li> <li>Returned <code>.html</code> files in <code>Flask-RESTful</code> using <code>make_response()</code> and <code>render_template()</code>.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-4-optimized-email-verification","title":"Section 4: Optimized Email Verification","text":"<p>Optimized the email verification process:</p> <ul> <li>Added expiration for verification and resend functionality.</li> <li>Refactored project structure by treating <code>confirmation</code> as a resource.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-6-secure-configuration-and-file-uploads","title":"Section 6: Secure Configuration and File Uploads","text":"<ul> <li>Configured the application more securely (using <code>from_object()</code> and <code>from_envvar()</code>).</li> <li>Learned the relationships between <code>WSGI</code>, <code>uwsgi</code>, <code>uWSGI</code>, and <code>Werkzeug</code>.</li> <li>Introduced <code>Flask-Uploads</code> for handling file uploads, downloads, and deletions (using <code>UploadSet</code>, <code>FileStorage</code>).</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-7-database-version-control","title":"Section 7: Database Version Control","text":"<ul> <li>Introduced <code>Flask-Migrate</code> for database version control, including adding, deleting, and modifying details.</li> <li>Common commands include <code>flask db init</code>, <code>flask db upgrade</code>, <code>flask db downgrade</code>.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-8-oauth-integration","title":"Section 8: OAuth Integration","text":"<ul> <li>Learned OAuth third-party login flow (e.g., GitHub), including authentication, authorization, and obtaining <code>access_token</code>.</li> <li>Introduced <code>Flask-OAuthlib</code>.</li> <li>Used Flask's <code>g</code> to store <code>access_token</code>.</li> <li>Allowed third-party login users to set passwords.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-9-payment-integration","title":"Section 9: Payment Integration","text":"<ul> <li>Integrated <code>Stripe</code> for third-party payment processing.</li> <li>Added an \"Order\" resource and implemented many-to-many relationships using <code>Flask-SQLAlchemy</code>.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/","title":"Basics","text":""},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-3-introduction-to-flask","title":"Section 3: Introduction to Flask","text":"<ul> <li>Introduction to the Flask web framework, using decorators to set up application routes.</li> <li>Understanding common HTTP request methods: GET, POST, PUT, DELETE.</li> <li>Understanding common HTTP status codes: 200, 201, 202, 401, 404.</li> <li>Understanding RESTful API design principles focusing on \"resources\" and statelessness.</li> <li>Implementing a RESTful API server application.</li> <li>Testing APIs using the Postman application.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-4-flask-restful-and-jwt","title":"Section 4: Flask-RESTful and JWT","text":"<ul> <li>Implementing RESTful API server applications using <code>Flask-RESTful</code>.</li> <li>Implementing JSON Web Token (JWT) authentication using <code>Flask-JWT</code>.</li> <li>Parsing user input JSON data using <code>RequestParser</code>.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-5-database-integration-with-sqlite","title":"Section 5: Database Integration with SQLite","text":"<ul> <li>Introducing <code>sqlite3</code> to store user and item information in a database.</li> <li>Implementing user registration functionality.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-6-database-integration-with-sqlalchemy","title":"Section 6: Database Integration with SQLAlchemy","text":"<ul> <li>Introducing <code>Flask-SQLAlchemy</code> to interact with the database using ORM.</li> <li>Adding store information with a one-to-many relationship to items.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-7-deploying-to-heroku","title":"Section 7: Deploying to Heroku","text":"<p>Deploying the Flask application to Heroku and using Heroku's PostgreSQL. Steps:</p> <ol> <li>Modify the project locally (e.g., add <code>Procfile</code>, <code>runtime.txt</code>, <code>uwsgi.ini</code>), then <code>commit</code> and <code>push</code> to the specified GitHub repo.</li> <li>Register on Heroku, create an application, connect it to the GitHub repo, and add the <code>heroku/python</code> buildpack and <code>Heroku Postgres</code> add-on.</li> <li>Install the Heroku CLI locally (see here) and log in using <code>heroku login</code>.</li> <li>Add a Heroku remote using <code>heroku git:remote -a &lt;app-name&gt;</code>.</li> <li>Deploy the project by pushing the <code>basics/section8</code> subdirectory to Heroku using <code>git subtree push --prefix basics/section8 heroku master</code>.</li> </ol> <p>Testing: Access here to retrieve all stores and their items in the database, returned in JSON format.</p>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-8-deploying-to-digitalocean","title":"Section 8: Deploying to DigitalOcean","text":"<p>Deploying the Flask application to a DigitalOcean Droplet. Steps:</p> <ol> <li>Register on DigitalOcean, create a Droplet with Ubuntu 16.04, set up SSH, and connect using PuTTY.</li> <li>Create a new user on the operating system.</li> <li>Install and configure PostgreSQL, including creating a new user and database with appropriate permissions.</li> <li>Install and configure the Nginx server, including firewall settings, error pages, and uwsgi parameters.</li> <li>Set up a Python virtual environment, install required packages, and clone the project from GitHub.</li> <li>Configure an Ubuntu service to run the uwsgi server, including log directories, processes, and threads.</li> </ol> <p>Testing: Access here (created on 2020/05/30) to retrieve all stores and their items in the database, returned in JSON format.</p>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-9-domain-and-https","title":"Section 9: Domain and HTTPS","text":"<p>Book</p> <ul> <li>Registering a domain and configuring DNS servers.</li> <li>Obtaining an SSL certificate for HTTPS communication and configuring Nginx.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-11-advanced-jwt-features","title":"Section 11: Advanced JWT Features","text":"<p>Introducing <code>Flask-JWT-Extended</code>:</p> <ul> <li>Implementing token-refreshing to improve user experience by avoiding frequent logins while requiring re-login for critical actions for security (using <code>@jwt_refresh_token_required</code>, <code>create_refresh_token()</code>, <code>create_access_token()</code>).</li> <li>Responding with appropriate data based on user roles (visitor, user, admin) using <code>@jwt.user_claims_loader</code>, <code>@jwt_optional</code>, <code>get_jwt_claims()</code>.</li> <li>Returning specific error messages for token-related issues using <code>@jwt.expired_token_loader</code>, <code>@jwt.invalid_token_loader</code>, <code>@jwt.needs_fresh_token_loader</code>.</li> <li>Implementing a logout mechanism using a blacklist (with <code>@jwt.token_in_blacklist_loader</code>, <code>get_raw_jwt()</code>).</li> </ul> <p>Book</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/category/data/","title":"Data","text":""},{"location":"blog/category/mkdocs/","title":"MkDocs","text":""},{"location":"blog/category/cicd/","title":"CI/CD","text":""}]}