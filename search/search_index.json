{"config":{"lang":["en","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to KC's Data &amp; Life Notes","text":"<p>Welcome! I'm KC (Kuan-Chou), a software engineer with a passion for data, AI, and continuous learning. This website is a space where I share my projects, learning journey, and experiences. Feel free to explore and get to know me better!</p> <ul> <li> <p> About Me</p> <p>Learn more about my background, including education, work experience, public speaking, and other activities.</p> <p>\u2192 About Me</p> </li> <li> <p> Side Projects</p> <p>Explore my personal projects in data, AI, and programming.</p> <p>\u2192 Side Projects</p> </li> <li> <p> Blog</p> <p>Read my thoughts and insights on data, AI, books, and more.</p> <p>\u2192 Blog</p> </li> <li> <p> Monthly Recaps</p> <p>What I'm studying, building, and aiming For</p> <p>\u2192 Monthly Recaps</p> </li> </ul>"},{"location":"disclaimer/","title":"Disclaimer","text":"<p>Disclaimer: The content on this page is created purely from personal study and self-learning. It is intended as personal notes and reflections, not as professional advice. There is no commercial purpose, financial gain, or endorsement intended. While I strive for accuracy, the information may contain errors or outdated details. Please use the content at your own discretion and verify with authoritative sources when needed.</p>"},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#tag:apache-airflow","title":"Apache Airflow","text":"<ul> <li>            How Airflow Works?          </li> <li>            What's New in Apache Airflow 3          </li> </ul>"},{"location":"tags/#tag:apache-flink","title":"Apache Flink","text":"<ul> <li>            Flink          </li> <li>            How Flink Works          </li> <li>            Streaming Processing Window Types          </li> </ul>"},{"location":"tags/#tag:apache-hive","title":"Apache Hive","text":"<ul> <li>            Migrate from Hive to Iceberg          </li> </ul>"},{"location":"tags/#tag:apache-iceberg","title":"Apache Iceberg","text":"<ul> <li>            Apache Iceberg in Production: Insights from Netflix in 2023          </li> <li>            Best Practices for Optimizing Apache Iceberg Workloads in AWS          </li> <li>            Ch4 Optimizing the Performance of Apache Iceberg          </li> <li>            Compaction - AWS EMR vs. AWS S3 Tables          </li> <li>            Deep Dive into Kafka Connect Icerberg Sink Connector          </li> <li>            Migrate from Hive to Iceberg          </li> <li>            Retail Lakehouse with Debezium, Kafka, Iceberg, and Trino          </li> <li>            The Lakehouse Series: Apache Iceberg Overview          </li> </ul>"},{"location":"tags/#tag:apache-kafka","title":"Apache Kafka","text":"<ul> <li>            Deep Dive into Kafka Connect Icerberg Sink Connector          </li> <li>            Exactly Once Semantics in Kafka          </li> <li>            Retail Lakehouse with Debezium, Kafka, Iceberg, and Trino          </li> </ul>"},{"location":"tags/#tag:apache-spark","title":"Apache Spark","text":"<ul> <li>            5 Practical Ways to Speed Up Your Apache Spark Queries          </li> <li>            Apache Spark          </li> <li>            Ch1 &amp; 2 Introduction to Spark          </li> <li>            Ch3 Structured APIs          </li> <li>            Ch7 Optimizing and Tuning Spark Applications          </li> <li>            How Spark Works          </li> <li>            Retail Lakehouse with Debezium, Kafka, Iceberg, and Trino          </li> <li>            Spark Performance Tuning          </li> <li>            Streaming Processing Window Types          </li> </ul>"},{"location":"tags/#tag:claude-code","title":"Claude Code","text":"<ul> <li>            Claude Code          </li> </ul>"},{"location":"tags/#tag:ddia","title":"DDIA","text":"<ul> <li>            Ch8 transactions          </li> </ul>"},{"location":"tags/#tag:debizium","title":"Debizium","text":"<ul> <li>            Retail Lakehouse with Debezium, Kafka, Iceberg, and Trino          </li> </ul>"},{"location":"tags/#tag:dynamodb","title":"DynamoDB","text":"<ul> <li>            DynamoDB          </li> </ul>"},{"location":"tags/#tag:elasticsearch","title":"Elasticsearch","text":"<ul> <li>            Elasticsearch          </li> </ul>"},{"location":"tags/#tag:feast","title":"Feast","text":"<ul> <li>            Fraud Detection: from DataOps to MLOps          </li> </ul>"},{"location":"tags/#tag:istio","title":"Istio","text":"<ul> <li>            What Is Istio?          </li> </ul>"},{"location":"tags/#tag:kserve","title":"KServe","text":"<ul> <li>            Fraud Detection: from DataOps to MLOps          </li> </ul>"},{"location":"tags/#tag:knative","title":"Knative","text":"<ul> <li>            What Is Knative?          </li> </ul>"},{"location":"tags/#tag:kubernetes","title":"Kubernetes","text":"<ul> <li>            Fraud Detection: from DataOps to MLOps          </li> </ul>"},{"location":"tags/#tag:mlops","title":"MLOps","text":"<ul> <li>            Fraud Detection: from DataOps to MLOps          </li> </ul>"},{"location":"tags/#tag:mlflow","title":"MLflow","text":"<ul> <li>            Fraud Detection: from DataOps to MLOps          </li> </ul>"},{"location":"tags/#tag:netflix","title":"Netflix","text":"<ul> <li>            Apache Iceberg in Production: Insights from Netflix in 2023          </li> </ul>"},{"location":"tags/#tag:opentelemetry","title":"OpenTelemetry","text":"<ul> <li>            How OpenTelemetry Works?          </li> <li>            What is OpenTelemetry?          </li> </ul>"},{"location":"tags/#tag:prometheus","title":"Prometheus","text":"<ul> <li>            How Prometheus Works?          </li> <li>            How Thanos Works?          </li> </ul>"},{"location":"tags/#tag:ray","title":"Ray","text":"<ul> <li>            Fraud Detection: from DataOps to MLOps          </li> </ul>"},{"location":"tags/#tag:redis","title":"Redis","text":"<ul> <li>            Redis          </li> </ul>"},{"location":"tags/#tag:risingwave","title":"RisingWave","text":"<ul> <li>            RisingWave          </li> <li>            Risingwave integration          </li> </ul>"},{"location":"tags/#tag:sqlmesh","title":"SQLMesh","text":"<ul> <li>            Comparisons: SQLMesh vs dbt Core          </li> <li>            Risingwave integration          </li> </ul>"},{"location":"tags/#tag:sre","title":"SRE","text":"<ul> <li>            How OpenTelemetry Works?          </li> <li>            How Prometheus Works?          </li> <li>            How Thanos Works?          </li> <li>            What is OpenTelemetry?          </li> </ul>"},{"location":"tags/#tag:streaming-processing","title":"Streaming Processing","text":"<ul> <li>            Streaming Processing Window Types          </li> </ul>"},{"location":"tags/#tag:system-design","title":"System Design","text":"<ul> <li>            API Performance Optimization          </li> <li>            CAP          </li> <li>            Consensus Algorithms          </li> <li>            Consistent Hashing          </li> <li>            Database Performance Optimization          </li> <li>            Design Dropbox          </li> <li>            Design FB Post Search          </li> <li>            Design a Distributed Message Queue          </li> <li>            Design a Key-Value Store          </li> <li>            Design a Messaging Service Like WhatsApp          </li> <li>            Design a Notification Service          </li> <li>            Design a Rate Limiter          </li> <li>            Design a Ride-Sharing Service Like Uber          </li> <li>            Design a Ticket Booking Site Like Ticketmaster          </li> <li>            Design a Top K Heavy Hitters Service          </li> <li>            Indexing          </li> <li>            Locking          </li> </ul>"},{"location":"tags/#tag:thanos","title":"Thanos","text":"<ul> <li>            How Thanos Works?          </li> </ul>"},{"location":"tags/#tag:the-lakehouse-series","title":"The Lakehouse Series","text":"<ul> <li>            Apache Iceberg in Production: Insights from Netflix in 2023          </li> <li>            Best Practices for Optimizing Apache Iceberg Workloads in AWS          </li> <li>            Migrate from Hive to Iceberg          </li> <li>            The Lakehouse Series: Apache Hudi Overview          </li> <li>            The Lakehouse Series: Apache Iceberg Overview          </li> <li>            The Lakehouse Series: From Data Lakes to Data Lakehouses          </li> <li>            The Lakehouse Series: OLTP vs. OLAP (A Parquet Primer)          </li> </ul>"},{"location":"tags/#tag:trino","title":"Trino","text":"<ul> <li>            Configure OAuth 2.0 Authentication          </li> <li>            Fault-tolerant Execution in Trino          </li> <li>            How It Works?          </li> <li>            Join Optimization          </li> <li>            Monitoring with Prometheus and Grafana          </li> <li>            Retail Lakehouse with Debezium, Kafka, Iceberg, and Trino          </li> </ul>"},{"location":"tags/#tag:dbt","title":"dbt","text":"<ul> <li>            Comparisons: SQLMesh vs dbt Core          </li> <li>            Fraud Detection: from DataOps to MLOps          </li> </ul>"},{"location":"tags/#tag:git-submodules","title":"git-submodules","text":"<ul> <li>            How to Use MkDocs to Integrate GitHub Actions and Git Submodule for Cross-repo Documentation          </li> </ul>"},{"location":"tags/#tag:github-actions","title":"github-actions","text":"<ul> <li>            How to Use MkDocs to Integrate GitHub Actions and Git Submodule for Cross-repo Documentation          </li> </ul>"},{"location":"tags/#tag:mkdocs","title":"mkdocs","text":"<ul> <li>            How to Use MkDocs to Integrate GitHub Actions and Git Submodule for Cross-repo Documentation          </li> </ul>"},{"location":"about-me/","title":"\u2728 About Me","text":"<p>Hi, I'm KC. I'm a Data Engineer focused on building large-scale, distributed, data-intensive platforms that are scalable, fault-tolerant, and evolvable on Kubernetes. If you're going to work with me, this little guide will help you get a sense of how I communicate, collaborate, and show up at work.</p>"},{"location":"about-me/#what-i-value","title":"\ud83c\udf31 What I Value","text":"<ul> <li>Ownership: I take responsibility for business outcomes, not just technical tasks. For example, I volunteered to own a data domain end-to-end\u2014from gathering requirements and building pipelines to delivering dashboards and running A/B tests. This proactive approach helped teams make data-driven product decisions faster.</li> <li>Collaboration: I believe diverse perspectives lead to stronger solutions. At a media company, I led a small cross-functional growth team that designed low-cost, high-impact experiments and built a clear metric system around the North Star goal. This alignment enabled editors and engineers to work toward shared targets with measurable impact.</li> <li>Transparency: I value clear communication and knowledge sharing. With a background in psychology and business, I translate technical concepts into language non-technical teams like marketing and editorial can understand. I also give internal tech talks on tools such as dbt, Terraform, and MLOps, which improved cross-team alignment and adoption.</li> <li>Growth: I seek environments that encourage continuous learning and improvement. I've pursued self-learning in machine learning and algorithms since grad school, and later introduced dbt and Terraform into production in the company. These initiatives not only advanced my own skills but also modernized the team's workflow, reducing manual effort and improving reliability.</li> </ul>"},{"location":"about-me/#my-strengths","title":"\ud83d\ude80 My Strengths","text":"<p>Here are three areas where I think I bring the most value:</p> <ol> <li>Data Infrastructure &amp; Lakehouse Expertise: I have solid experience in building scalable, highly available data platforms and lakehouse architectures, and in choosing the right technology to support both team and organizational needs.</li> <li>Cross-functional Collaboration: With a background in psychology and business management, I work well with colleagues from diverse disciplines.</li> <li>Learning Fast and Navigating Ambiguity: I'm comfortable figuring out what matters most when requirements are unclear, and driving progress toward long-term goals.</li> </ol>"},{"location":"about-me/#communication-style","title":"\ud83d\udcac Communication Style","text":"<ul> <li>When dealing with complex problems, I prefer to think things through first, gather enough context, and then move into discussion. That way the conversation is clearer and everyone has the same foundation to build on.</li> <li>I prefer to start with written communication, since it gives both of us space to think, document ideas, and avoid missing details. When things get more complex, I appreciate direct conversations, whether online or in person.</li> <li>I also like to use diagrams and drawings to clarify ideas visually.</li> <li>I love sharing articles or resources that can help us get on the same page, so please don't hesitate to ask me to share what I have or share anything useful you find.</li> </ul> <p>If you want to reach me:</p> <ul> <li>Work messages: I usually reply within 2 hours.</li> <li>Email: I'm slower.</li> <li>Urgent matters: You can find me on LinkedIn.</li> </ul> <p>Also, I enjoy a bit of small talk at the beginning of a conversation. As someone from Taiwan, small talk isn't part of our culture, so I'm still practicing. If I seem a little awkward at first, please know it's not that I don't care. I'm learning, and I truly value genuine human connection.</p>"},{"location":"about-me/#my-rhythm","title":"\ud83d\udd52 My Rhythm","text":"<ul> <li>I currently based in Taiwan and UST (UTC+8) is my timezone.</li> <li>My peak focus time is usually 10:30 AM to 4:30 PM on weekdays.</li> <li>On weekends, you'll often find me tinkering with personal projects.</li> </ul>"},{"location":"about-me/#feedback","title":"\ud83d\ude4c Feedback","text":"<p>I welcome feedback, and I see it as a gift.</p> <p>The style I appreciate most is gentle but honest. Tell me directly what could be improved, with no need to sugar-coat it. Constructive feedback helps me grow and I genuinely value it.</p>"},{"location":"about-me/#outside-of-work","title":"\ud83c\udfb6 Outside of Work","text":"<p>Outside of work, I'm passionate about dance and music. I used to do popping (street dance), and later shifted to Hustle partner dance. Dancing gives me pure joy.</p> <p>I also listen to a lot of music: Jazz, Soul, Funk, Disco, Hip-hop, R\\&amp;B, and Neo Soul are my go-tos. If you're into music or dancing, let's hang out sometime!</p>"},{"location":"about-me/public-speakings/","title":"Public Speakings","text":"<p>Sharing knowledge and ideas with others is something I value deeply. Here, you can find my past speaking engagements, talks, and presentations, along with topics I'm passionate about. I'm always open to new opportunities to connect and speak.</p>"},{"location":"about-me/public-speakings/#taipei-dbt-meetup-27","title":"Taipei dbt Meetup #27","text":"<p>In this talk, presented to an audience of around 100 attendees, I explored three key aspects of implementing dbt: people, process, and pipelines. From the people perspective, I discussed the challenges within data teams, such as the division of responsibilities between data engineers and analysts, which often creates bottlenecks. I also highlighted organizational challenges, like the lack of accountability for data quality in cross-functional teams, and how dbt helps address these issues by fostering collaboration and domain-oriented modeling.</p> <p>From the pipeline perspective, I shared how dbt simplifies complex workflows by leveraging tools like Jinja SQL and Google Cloud's BigQuery remote functions, enabling a unified and maintainable pipeline. Lastly, I touched on infrastructure challenges, particularly the need to manage separate repositories for Terraform and dbt, and how integrating these repositories can streamline workflows despite some trade-offs. Overall, the talk emphasized how dbt can transform data workflows and improve collaboration across teams.</p> <p>I'm thrilled to have had the opportunity to share and exchange ideas with like-minded individuals during this event. Engaging in discussions and learning from others is always an enriching experience.</p>"},{"location":"about-me/public-speakings/#devopsdays-taipei-2024","title":"DevOpsDays Taipei 2024","text":"<p>In this talk, presented to an audience of over 300 attendees, I shared how TVBS approaches DataOps and the foundational principles behind its architecture design: data democracy, bridging the gap between data engineers and analysts, and bringing code to data. </p> <p>I also discussed the four competitive advantages of our ELT approach: ease of maintenance, a focus on high-value activities, reducing technical gaps, and leveraging a large, mature community. </p> <p>For those interested in a deeper dive, I recommend exploring the following publications:</p> <ul> <li>TVBS\u6578\u64da\u67b6\u69cb\u5927\u89e3\u5bc6 (1) \u2014 \u524d\u4e16\u4eca\u751f</li> <li>TVBS\u6578\u64da\u67b6\u69cb\u5927\u89e3\u5bc6 (2) \u2014 \u73fe\u4ee3\u6578\u64da\u68e7(Modern Data Stack)</li> <li>TVBS\u6578\u64da\u67b6\u69cb\u5927\u89e3\u5bc6 (3) \u2014 Next Steps</li> </ul> <p>It was an incredible experience to share these insights and engage with such a large and enthusiastic audience. \u807d\u773e\u7684\u56de\u994b\u548c\u4e92\u52d5\u8b93\u9019\u6b21\u6f14\u8b1b\u66f4\u52a0\u5145\u5be6\u548c\u6709\u610f\u7fa9\u3002</p> Feedback <p>Work Experience </p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/cross-repo-docs-mkdocs-workflow/","title":"How to Use MkDocs to Integrate GitHub Actions and Git Submodule for Cross-repo Documentation","text":"<p>TLDR</p> <p>After reading this article, you will learn how to:</p> <ul> <li>Use Git Submodule to centrally manage documentation sources across multiple projects  </li> <li>Configure GitHub Actions for cross-project automation and integration workflows  </li> <li>Utilize Reusable Workflows to reuse CI/CD scripts and reduce maintenance costs  </li> <li>Leverage MkDocs Monorepo Plugin to merge documentation from multiple projects into a single website</li> </ul> <p>Ever tried hunting for that one crucial piece of documentation across five different repositories? It's like playing an exhausting game of hide-and-seek where the information you need is always in the other repo. Sound familiar?</p> <p>In many organizations, teams scatter their documentation across separate Git repositories like breadcrumbs in a forest. Frontend docs live here, backend knowledge sits there, and that critical ML pipeline explanation? Somewhere in yet another repository. While this approach keeps ownership clear and projects manageable, it creates a documentation nightmare that would make even the most patient developer pull their hair out.</p> <p>Picture this: a new team member joins your organization. Instead of a smooth onboarding experience, they embark on a treasure hunt across repositories, asking colleagues endless questions and trying to piece together how everything connects. It's like trying to assemble IKEA furniture with instructions scattered across different boxes \u2013 technically possible, but unnecessarily frustrating.</p> <p>I've been there too. While switching to a monorepo seemed tempting (like moving all your furniture to one room), we decided to keep our multi-repo structure for good reasons \u2013 authorization boundaries, deployment flexibility, and version independence. But I wasn't about to let scattered documentation continue being the thorn in our side.</p> <p>That's when I discovered the perfect recipe: combining Git Submodules (think of them as neat organizational folders), MkDocs Monorepo Plugin (the master chef that brings everything together), and GitHub Actions (your tireless automation assistant). The result? A centralized documentation platform that updates itself automatically. It's like having a personal librarian who keeps all your books organized and up-to-date!</p> <p></p> <p>Specifically, this setup involves three repositories:</p> <ul> <li><code>monorepo</code> (Main Repo) is the repository I use to build my personal website, utilizing MkDocs and deployed on GitHub Pages. It already has a predefined <code>publish-docs.yml</code> GitHub Actions Workflow.</li> <li><code>data2ml-ops</code> (Sub Repo) is my personal project for practicing DataOps and MLOps, which also uses MkDocs for documentation building.</li> <li><code>reusable-workflows</code> is used to store reusable workflows.</li> </ul> <p>My mission? Merge the learning notes from <code>data2ml-ops</code> into <code>monorepo</code> so I can showcase insights from different projects on one beautiful website. No more jumping between sites or wondering if documentation is outdated!</p> <p>The magic happens like this: whenever <code>data2ml-ops</code> gets updated documentation and pushes to GitHub, it automatically triggers the documentation build and deployment in <code>monorepo</code>. It's like having a loyal assistant who immediately updates your main presentation whenever you make changes to your notes. No more worrying about documentation drift!</p> <p>This article will explain step by step how I completed this integration, including implementation methods and considerations.</p> <ol> <li> <p>Add Submodule    Add <code>data2ml-ops</code> as a Git Submodule in <code>monorepo</code>.</p> </li> <li> <p>Create Documentation Deployment Workflow    Add a GitHub Actions Workflow in <code>monorepo</code> responsible for building and deploying documentation to GitHub Pages.</p> </li> <li> <p>Create Reusable Workflow    Create a Reusable Workflow in <code>reusable-workflows</code> responsible for triggering the documentation deployment process from step 2.</p> </li> <li> <p>Configure Sub-repo Workflow    Create a Workflow in <code>data2ml-ops</code> that uses the Reusable Workflow from step 3 to trigger the documentation deployment process in <code>monorepo</code>.</p> </li> <li> <p>Integrate MkDocs Monorepo Plugin    Add the MkDocs Monorepo Plugin to <code>monorepo</code> to integrate multiple documentation sources.</p> </li> <li> <p>Testing and Verification    Submit new changes, test whether the overall process works properly, and check if documentation is successfully deployed.</p> </li> </ol>","tags":["github-actions","git-submodules","mkdocs"]},{"location":"blog/cross-repo-docs-mkdocs-workflow/#1-add-submodule","title":"1. Add Submodule","text":"<p>kuanchoulai10/monorepo</p> <p>Think of Git Submodules as your project's way of having roommates \u2013 you can live together in the same house (repository) while keeping your personal belongings (version history) completely separate<sup>1</sup>. It's perfect for these scenarios:</p> <ol> <li>Modular management: Like having separate apartments for your frontend and backend teams, but with a shared lobby where they can meet and collaborate.</li> <li>Version independence: Your main repository acts like a strict landlord, locking onto specific versions of submodules. No surprise changes allowed!</li> <li>Reusability: That awesome utility library? Instead of copying it everywhere like a hoarder, just reference it as a submodule. Clean and efficient!</li> </ol> <p>Let's invite <code>data2ml-ops</code> to move into our <code>monorepo</code> using the digital equivalent of a lease agreement:</p> <pre><code>git submodule add https://github.com/kuanchoulai10/data2ml-ops.git data2ml-ops\n</code></pre> <pre><code>Cloning into '/Users/kcl/projects/monorepo/data2ml-ops'...\nremote: Enumerating objects: 197, done.\nremote: Counting objects: 100% (197/197), done.\nremote: Compressing objects: 100% (110/110), done.\nremote: Total 197 (delta 78), reused 183 (delta 66), pack-reused 0 (from 0)\nReceiving objects: 100% (197/197), 1.04 MiB | 1.94 MiB/s, done.\nResolving deltas: 100% (78/78), done.\n\n<pre><code>git status\n</code></pre>\n<pre><code>On branch main\nYour branch is up to date with 'origin/main'.\n\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n    modified:   .gitmodules\n    new file:   data2ml-ops\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n</code></pre>\n<p>Perfect! Git has thoughtfully created two essential files:</p>\n<ol>\n<li><code>.gitmodules</code> \u2013 Think of this as your address book, telling Git where to find each submodule:\n    <pre><code>[submodule \"data2ml-ops\"]\n  path = data2ml-ops\n  url = https://github.com/kuanchoulai10/data2ml-ops.git\n</code></pre></li>\n<li><code>data2ml-ops</code> \u2013 This is like a bookmark that points to exactly which version (commit) of the submodule we're using.</li>\n</ol>\n<p>Time to share our changes with the world! Commit and push these files to the remote repository:</p>\n<pre><code>git commit -m \"Add data2ml-ops as submodule\"\n</code></pre>\n<pre><code>git push\n</code></pre>\n<pre><code>Enumerating objects: 5, done.\nCounting objects: 100% (5/5), done.\nDelta compression using up to 8 threads\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 404 bytes | 404.00 KiB/s, done.\nTotal 3 (delta 2), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (2/2), completed with 2 local objects.\nTo https://github.com/kuanchoulai10/monorepo.git\n   6c36b76..c1e29ff  main -&gt; main\n</code></pre>\n<p>Now when you visit GitHub, you'll see something magical: <code>data2ml-ops @ 7369c16</code>. Click on it, and voil\u00e0! It teleports you to the actual repository. It's like having a portal in your house that leads directly to your friend's place \u2013 the reference is there, but you're not actually storing their stuff in your garage.</p>\n<p></p>","tags":["github-actions","git-submodules","mkdocs"]},{"location":"blog/cross-repo-docs-mkdocs-workflow/#2-create-documentation-deployment-workflow","title":"2. Create Documentation Deployment Workflow","text":"<p>kuanchoulai10/monorepo</p>\n\n<p>Time to set up our documentation deployment pipeline! Think of GitHub Actions as your personal automation butler \u2013 and the best part? It works for free on public repositories<sup>2</sup>. </p>\n<p>In the GitHub Actions universe, events are like doorbells that wake up your butler. Whether someone rings the <code>Push</code> doorbell, the <code>Pull Request</code> chime, or the <code>Merge</code> bell, each one triggers a workflow \u2013 basically a to-do list of automated tasks your butler follows religiously<sup>3</sup>.</p>\n<p>I won't bore you with every detail of writing deployment workflows (that's a whole other adventure), but let's focus on the Git submodules magic tricks.</p>\n<p>Our workflow has two ears, always listening for:</p>\n<ol>\n<li>New pushes to <code>monorepo</code> \u2013 When the main house gets updates, rebuild everything!</li>\n<li>Special <code>repository_dispatch</code> webhook events \u2013 Think of these as secret knock patterns that external repositories can use to wake up our workflow<sup>8</sup><sup>4</sup>. When other submodule repos update their docs, they'll send this special signal saying \"Hey, time to rebuild!\"</li>\n</ol>\n.github/workflows/publish-docs.yml<pre><code>on:\n  push:\n    branches:\n      - main\n  repository_dispatch:\n    types: [update-submodules]\n</code></pre>\n<p>Our workflow is like a well-organized recipe with one main job. The first step? Always get all the ingredients (checkout the codebase). But here's the crucial part \u2013 when using <code>actions/checkout@v4</code>, you absolutely must add <code>submodules: recursive</code>. Without this magical incantation, your submodule folders would be as empty as a diet soda's promise of satisfaction!</p>\n.github/workflows/publish-docs.yml<pre><code>      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n          submodules: recursive\n</code></pre>\n<p>Since our workflow might be awakened by updates from submodule repositories (like a helpful neighbor calling to say they've cleaned their yard), we need to be polite guests and:</p>\n<ol>\n<li>Update all submodules \u2013 Fetch the latest changes from our submodule neighbors</li>\n<li>Commit and push changes \u2013 If we found updates, we'll neatly file them away in our main repository</li>\n</ol>\n.github/workflows/publish-docs.yml<pre><code>      - name: Update Submodules\n        run: |\n          git submodule update --remote --merge\n      - name: Commit and Push Submodule Updates\n        run: |\n          git add .\n          if ! git diff --cached --quiet; then\n              git commit -m \"Update submodules to latest commit\"\n              git push origin main\n          else\n              echo \"No changes to commit\"\n          fi\n</code></pre>\n<p>This creates a beautiful synchronization dance \u2013 whenever a submodule updates, our documentation stays fresh and current. Don't forget to commit and push these changes when you're done!</p>","tags":["github-actions","git-submodules","mkdocs"]},{"location":"blog/cross-repo-docs-mkdocs-workflow/#3-create-reusable-workflow","title":"3. Create Reusable Workflow","text":"<p>kuanchoulai10/reusable-workflows</p>\n\n<p>Time to create our universal remote control! This reusable workflow is like that friend who's really good at getting everyone organized for group projects \u2013 simple, reliable, and saves everyone time.</p>\n<p>The logic is beautifully straightforward: use <code>curl</code> (the Swiss Army knife of web requests) to send a gentle tap on <code>monorepo</code>'s shoulder via the <code>repository_dispatch</code> webhook<sup>4</sup>. It's like sending a text message that says \"Hey, I updated my docs, could you refresh the website please?\" The message includes <code>event_type: update-submodules</code> so our main repo knows exactly what kind of help we need.</p>\n<p>Since this is a reusable workflow (the automation equivalent of a recipe you can share with friends), it listens for <code>workflow_call</code> events<sup>5</sup>:</p>\n.github/workflows/trigger-monorepo-to-build-doc.yml<pre><code>name: Trigger Monorepo to Build Docs\n\non:\n  workflow_call:\n    secrets:\n      PAT:\n        required: true\n        description: 'Personal Access Token with repo scope'\n\njobs:\n  trigger_monorepo:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Trigger update-submodule workflow\n        run: |\n          curl -L \\\n            -X POST \\\n            -H \"Accept: application/vnd.github+json\" \\\n            -H \"Authorization: Bearer ${{ secrets.PAT }}\" \\\n            -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n            https://api.github.com/repos/kuanchoulai10/monorepo/dispatches \\\n            -d '{\"event_type\":\"update-submodules\"}'\n</code></pre>\n<p>Why go through the trouble of creating reusable workflows? Imagine updating your automation process and having to visit each repository individually to make changes \u2013 it's like manually updating your address on every account when you move. With reusable workflows, you update once and everyone benefits. Smart and efficient!</p>\n<p>Remember to commit and push when you're done setting up this automation masterpiece.</p>","tags":["github-actions","git-submodules","mkdocs"]},{"location":"blog/cross-repo-docs-mkdocs-workflow/#4-configure-sub-repo-workflow","title":"4. Configure Sub-repo Workflow","text":"<p>kuanchoulai10/data2ml-ops</p>\n\n<p>Now comes the exciting part \u2013 teaching our sub-repository how to use that universal remote we just created! But first, we need to create a special key (Personal Access Token) that gives our automation the right permissions.</p>\n<p>Head over to GitHub's top-right corner: click your profile picture &gt; Settings &gt; Developer Settings &gt; Personal access tokens &gt; Fine-grained tokens. Think of this token as a VIP pass that allows reading metadata from the monorepo and pushing code with new submodule commits. Make sure to grant \"Contents\" repository permissions (write)<sup>4</sup> \u2013 it's like giving your automation assistant the keys to the filing cabinet.</p>\n<p></p>\n<p>Once you've created your token (and copied it \u2013 this is important!), head back to <code>data2ml-ops</code> and create a repository secret called <code>PAT</code>. Paste your token there like you're hiding a spare key under a digital doormat.</p>\n<p></p>\n<p>Now for the grand finale \u2013 creating the workflow that calls our reusable workflow<sup>6</sup>. It's surprisingly simple, like speed-dialing a friend. Just use <code>uses</code> in your job and point it to where your reusable workflow lives:</p>\n.github/workflows/trigger-monorepo-to-build-doc.yml<pre><code>name: Trigger Monorepo to Build Docs\n\non:\n  push:\n    paths:\n      - 'mkdocs.yml'\n      - 'README.md'\n      - 'docs/**'\n\njobs:\n  trigger_monorepo:\n    uses: kuanchoulai10/reusable-workflows/.github/workflows/trigger-monorepo-to-build-doc.yml@main\n    secrets:\n      PAT: ${{ secrets.PAT }}\n</code></pre>\n<p>Commit and push these changes, and you've just created your first cross-repository communication channel!</p>","tags":["github-actions","git-submodules","mkdocs"]},{"location":"blog/cross-repo-docs-mkdocs-workflow/#5-integrate-mkdocs-monorepo-plugin","title":"5. Integrate MkDocs Monorepo Plugin","text":"<p>kuanchoulai10/monorepo</p>\n\n<p>Almost there! Time to add the secret sauce that makes multiple documentation sources sing in harmony. Enter <code>mkdocs-monorepo-plugin</code> \u2013 the conductor of our documentation orchestra, crafted by the brilliant minds at Backstage.</p>\n<p>Quick backstory: Backstage is Spotify's gift to the developer world (it joined CNCF in 2020). Think of it as the ultimate dashboard for your engineering organization \u2013 like a mission control center where you can see all your microservices, documentation, CI/CD pipelines, and APIs in one beautiful, organized view<sup>7</sup>.</p>\n<p>First, let's invite this plugin to the party:</p>\n<pre><code>pip install mkdocs-monorepo-plugin\n</code></pre>\n<p>Once installed, the magic happens with just a few lines. Add <code>monorepo</code> to your plugins, and suddenly you have access to a powerful new syntax: <code>!include</code>. It's like having a magic wand that can summon documentation from other repositories:</p>\n<pre><code>...\n\nnav:\n  - Data2ML Ops: \"!include ./data2ml-ops/mkdocs.yml\"\n\n...\n\nplugins:\n  - monorepo\n</code></pre>\n<p>This elegant little configuration tells MkDocs: \"Hey, go grab the navigation structure from that submodule and weave it seamlessly into our main site.\" It's documentation magic at its finest!</p>\n<p>Don't forget to commit and push these changes \u2013 we're almost ready for the big reveal!</p>","tags":["github-actions","git-submodules","mkdocs"]},{"location":"blog/cross-repo-docs-mkdocs-workflow/#6-testing-and-verification","title":"6. Testing and Verification","text":"<p>kuanchoulai10/data2ml-ops</p>\n\n<p>The moment of truth has arrived! Time to put our automation masterpiece to the test. Let's make some updates to the documentation in <code>data2ml-ops</code> and push them to GitHub, then sit back and watch our creation come to life.</p>\n<p>Check out the GitHub Actions workflow doing its thing:</p>\n<p></p>\n<p><code>data2ml-ops</code> run history</p>\n<p>Behind the scenes, our workflow is using that reusable workflow like a well-oiled machine. It sends a polite <code>curl</code> request to the GitHub API, creating a <code>repository_dispatch</code> event in <code>monorepo</code>. This digital tap on the shoulder then wakes up the <code>publish-docs.yml</code> workflow \u2013 it's like watching a perfectly choreographed dance!</p>\n<p>Switch over to the <code>monorepo</code> page, and you'll see our documentation deployment process has indeed sprung into action:</p>\n<p></p>\n<p><code>monorepo</code> run history</p>\n<p>Look at that beautiful update! Our submodule reference has smoothly transitioned from <code>data2ml-ops @ 7369c16</code> to <code>data2ml-ops @ 887a9a0</code>. It's like watching your bookmark automatically update to point to the latest chapter of your favorite book.</p>\n<p></p>\n<p>The final proof? Visit the actual website, and there it is \u2013 the documentation from my <code>data2ml-ops</code> project, freshly deployed and beautifully integrated! It's like watching all the pieces of a puzzle click into place.</p>\n<p></p>\n<p>And just like that, we've created a self-updating documentation ecosystem that keeps everything in sync without manual intervention. No more documentation archaeology expeditions or wondering if what you're reading is current. Your cross-repository documentation now flows as smoothly as a well-conducted symphony!</p>","tags":["github-actions","git-submodules","mkdocs"]},{"location":"blog/cross-repo-docs-mkdocs-workflow/#references","title":"References","text":"<ol>\n<li>\n<p>Git Submodules Basic Explanation | gitaarik GitHub\u00a0\u21a9</p>\n</li>\n<li>\n<p>About billing for GitHub Actions | GitHub Docs\u00a0\u21a9</p>\n</li>\n<li>\n<p>The components of GitHub Actions | GitHub Docs\u00a0\u21a9</p>\n</li>\n<li>\n<p>Create a repository dispatch event | GitHub Docs\u00a0\u21a9\u21a9\u21a9</p>\n</li>\n<li>\n<p>Creating a reusable workflow | GitHub Docs\u00a0\u21a9</p>\n</li>\n<li>\n<p>Calling a reusable workflow | GitHub Docs\u00a0\u21a9</p>\n</li>\n<li>\n<p>backstage/mkdocs-monorepo-plugin | GitHub\u00a0\u21a9</p>\n</li>\n<li>\n<p><code>repository_dispatch</code> Event | GitHub Docs\u00a0\u21a9</p>\n</li>\n<li>\n<p>Creating a fine-grained personal access token | GitHub Docs\u00a0\u21a9</p>\n</li>\n</ol>","tags":["github-actions","git-submodules","mkdocs"]},{"location":"blog/oltp-vs-olap-a-parquet-primer/","title":"The Lakehouse Series: OLTP vs. OLAP (A Parquet Primer)","text":"<p>TLDR</p> <p>After reading this article, you will learn:</p> <ul> <li>The key differences between OLTP and OLAP workloads, and why storage format matters</li> <li>How Parquet organizes data internally and optimizes data storage using compression techniques like dictionary encoding and RLE</li> <li>Where Parquet falls short in today's data landscape</li> </ul>","tags":["The Lakehouse Series"]},{"location":"blog/oltp-vs-olap-a-parquet-primer/#oltp-vs-olap-the-data-processing-showdown","title":"OLTP vs. OLAP: The Data Processing Showdown","text":"<p>Picture this: you're at a bustling coffee shop. The barista (OLTP) takes your order, processes payment, and updates inventory \u2014 all in seconds. Meanwhile, the shop owner (OLAP) sits in the back office, analyzing months of sales data to figure out which pastries sell best on rainy Tuesdays. Same data, completely different games!</p>","tags":["The Lakehouse Series"]},{"location":"blog/oltp-vs-olap-a-parquet-primer/#what-is-oltp-the-speed-demon","title":"What is OLTP? (The Speed Demon)","text":"<p>OLTP stands for Online Transaction Processing \u2014 think of it as your data's personal speed trainer. It's obsessed with handling individual transactions faster than you can say \"latte with oat milk.\"</p> <p>OLTP is like that friend who finishes your sentences before you're halfway through. It specializes in real-time data updates and retrievals with the patience of a caffeinated hummingbird. Here's where it shines:</p> <ul> <li>Banking Scenario: When you withdraw cash, OLTP updates your balance faster than you can pocket the money. It's also the invisible hero when you update your address for that credit card application \u2014 boom, done, next!</li> <li>E-commerce Scenario: Adding items to your cart, completing purchases, or deleting that embarrassing profile from your teenage years \u2014 OLTP handles it all with sub-second precision.</li> </ul> <p>The magic? OLTP focuses on processing individual rows of data, making it the perfect sprinter for workloads that demand instant gratification and rock-solid reliability.</p>","tags":["The Lakehouse Series"]},{"location":"blog/oltp-vs-olap-a-parquet-primer/#what-is-olap-the-deep-thinker","title":"What is OLAP? (The Deep Thinker)","text":"<p>OLAP stands for Online Analytical Processing \u2014 imagine a brilliant detective who loves crunching numbers more than solving crimes. While OLTP races around handling transactions, OLAP sits in a cozy chair, analyzing patterns like Sherlock Holmes with a spreadsheet addiction.</p> <p>OLAP is your go-to when you need to analyze massive datasets and extract insights. It's less interested in individual records and more fascinated by the big picture across specific columns. Check out its superpowers:</p> <ul> <li>Banking Scenario: Need to know the average age of customers across different branches? OLAP dives into thousands of records, groups them by location, and serves up insights that would make a marketing manager weep with joy.</li> <li>E-commerce Scenario: Those beautiful dashboards showing sales trends, customer demographics, and year-over-year growth? That's OLAP flexing its analytical muscles.</li> </ul> <p>The secret sauce? OLAP excels at processing subsets of columns rather than individual rows, making it the marathon champion of data aggregation and business intelligence.</p>","tags":["The Lakehouse Series"]},{"location":"blog/oltp-vs-olap-a-parquet-primer/#why-storage-formats-matter","title":"Why Storage Formats Matter","text":"<p>Choosing the right data format is like picking the perfect tool for a job \u2014 use a hammer when you need a screwdriver, and you'll have a very bad time!</p> <p>Before we dive into the storage format showdown, let's map out the data landscape. Imagine data formats on a spectrum:</p> <p>At one end, we have unstructured data \u2014 the wild west of information. Think images, videos, and that chaotic folder of memes on your desktop. No rules, no schema, just pure digital chaos.</p> <p>At the other end sits structured data \u2014 the neat freak of the data world. CSV files fall here, with their obsessive love for rows and columns, following rules stricter than a Swiss train schedule.</p> <p>Hanging out in the middle is semi-structured data like JSON and XML \u2014 the cool kids who have some structure but refuse to be completely boxed in. They're like organized chaos, with key-value pairs that make sense but don't follow traditional table manners.</p> <p>Now, let's get specific about structured data layouts. Imagine we have a <code>users</code> table with 5 rows and 4 columns (age, gender, country, and average order value) \u2014 our guinea pig for this storage format experiment:</p> <p>We can classify this logical structured data into two categories based on how the data is physically stored on disk.</p> <p>Row-Based Storage: The Transaction Speedster</p> <p>Row-based formats store data like reading a book \u2014 line by line, left to right. It's how traditional databases like PostgreSQL and MySQL organize their data, and for good reason!</p> <p>Since data is stored row by row, it's incredibly efficient for typical OLTP operations. Want to insert a new user? Easy \u2014 just append a new row. Need to update someone's profile? Find the row offset and boom, mission accomplished. Deleting a user? Same deal.</p> <p>See how elegant it is? To delete the second row, we just need its offset \u2014 like having the exact address of a house. But here's the catch: if you want to analyze the average age of all users, you'd have to knock on every door in the neighborhood. Not exactly efficient!</p> <p>Column-Based Storage: The Analytics Powerhouse</p> <p>Column-based formats flip the script entirely \u2014 instead of reading like a book, they read like scanning a newspaper column. Each column lives together, creating some serious analytical superpowers.</p> <p>This layout is compression heaven. That gender column with mostly \"male\" and \"female\" values? Column storage can compress it down to almost nothing, like vacuum-packing your winter clothes.</p> <p>Need the average age? Just grab the age column and you're done \u2014 no need to wade through irrelevant data. But deleting a specific user? Now you're playing hide-and-seek across multiple columns. Not exactly OLTP's cup of tea!</p>","tags":["The Lakehouse Series"]},{"location":"blog/oltp-vs-olap-a-parquet-primer/#enter-parquet-the-best-of-both-worlds","title":"Enter Parquet: The Best of Both Worlds","text":"<p>What if I told you there's a storage format that's like having a sports car that's also an SUV? Meet Parquet \u2014 the overachiever of the data storage world!</p> <p>Parquet emerged from the brilliant minds at Twitter and Cloudera back in 2012. By 2013, it had graduated to become a top-level Apache project, and the data world hasn't been the same since.</p> <p>Here's the genius: Parquet organizes data into row groups (like small neighborhoods) that are then divided into columns (like sorting each neighborhood by house type). In our example, we have 2 row groups \u2014 the first contains rows 1-3, the second contains rows 4-5. Within each row group, columns live separately, enabling both row-wise convenience and column-wise efficiency.</p> <p>It's like having the best of both worlds \u2014 you can quickly find a specific user (row-wise operations) or analyze age patterns across thousands of users (column-wise operations) without breaking a sweat!</p> <p>Parquet's Anatomy: A Peek Under the Hood</p> <p>A Parquet file is like a well-organized filing cabinet with some seriously smart features:</p> <ol> <li>Magic Number: Every Parquet file starts with \"PAR1\" \u2014 its digital signature that screams \"I'm special!\"</li> <li>Row Groups: The main organizing principle, like having separate drawers for different categories of data</li> <li>Column Chunks: Within each row group, columns get their own dedicated space \u2014 think individual folders within each drawer</li> <li>Pages: The smallest storage unit, like individual documents within each folder</li> <li>Footer: The master index at the end that contains all the metadata \u2014 like having a detailed table of contents that tells you exactly where everything is without opening every drawer</li> </ol> <p>This structure enables lightning-fast metadata reading and incredibly efficient data retrieval. It's like having a librarian who knows exactly where every book is without checking the shelves!</p> <p>Efficient Compression in Parquet</p> <p>Efficient compression is a cornerstone of Parquet's design, enabling it to store massive datasets while minimizing disk usage and maximizing query performance. By leveraging techniques like dictionary encoding, run-length encoding (RLE), and bit-packing, Parquet achieves remarkable compression ratios without sacrificing speed. Let's explore how these methods work together to optimize columnar data storage.</p> <p>The image illustrates how Parquet efficiently compresses columnar data using dictionary encoding, followed by run-length encoding (RLE) and bit-packing. In the original column <code>[\"US\", \"TW\", \"FR\", \"JP\", \"JP\", \"KR\", \"CA\", \"CA\", \"CA\"]</code>, dictionary encoding first replaces each unique string with a unique integer ID. For example, <code>\"US\"</code> becomes <code>0</code>, <code>\"TW\"</code> \u2192 <code>1</code>, ..., <code>\"CA\"</code> \u2192 <code>5</code>, resulting in the encoded sequence: <code>[0, 1, 2, 3, 3, 4, 5, 5, 5]</code>.</p> <p>To further reduce storage, Parquet applies RLE and bit-packing to the integer sequence. Run-length encoding compresses consecutive repeated values, such as <code>\"JP\"</code> <code>(3, 3)</code> and <code>\"CA\"</code> <code>(5, 5, 5)</code>, into pairs like <code>3,2</code> (meaning value 3 repeated 2 times) and <code>5,3</code>. Bit-packing then ensures each integer is stored using the minimum number of bits necessary (in this case, 3 bits for up to 6 dictionary values). This layered approach dramatically reduces data size and speeds up scan performance by enabling efficient decoding and skipping of unneeded values.</p>","tags":["The Lakehouse Series"]},{"location":"blog/oltp-vs-olap-a-parquet-primer/#beyond-parquet-the-next-evolution","title":"Beyond Parquet: The Next Evolution","text":"<p>Even superheroes have kryptonite, and Parquet is no exception. Despite being the Swiss Army knife of data storage, it does have some Achilles' heels that can make you pull your hair out:</p> <ul> <li>Subsecond latency: Parquet struggles with workloads requiring microsecond updates, such as high-frequency trading.</li> <li>High-concurrency read-write: Maintaining consistency during concurrent operations is challenging.</li> </ul> <p>Parquet shines in OLAP systems where batch processing and read-heavy workloads dominate. However, as modern data applications grow more interactive and collaborative, expectations have shifted. Today's data platforms increasingly require features like:</p> <ul> <li>ACID transactions: Ensuring data reliability during concurrent operations.</li> <li>Time travel queries: Accessing historical data snapshots.</li> <li>Concurrent reads and writes: Supporting high-performance, multi-user environments.</li> </ul> <p>To address these needs, open formats like Apache Hudi, Iceberg, and Delta Lake have emerged. Built on top of Parquet, these formats extend its functionality to support advanced features, making them ideal for modern lakehouse architectures.</p> <p>Stay tuned for the next chapter in our Lakehouse series, where we'll explore how these formats are revolutionizing data processing!</p>","tags":["The Lakehouse Series"]},{"location":"blog/oltp-vs-olap-a-parquet-primer/#references","title":"References","text":"<p>I particularly enjoyed this talk by Boudewijn Braams, a senior software engineer at Databricks. He provided an insightful and engaging explanation of Parquet's internal structure and compression techniques, using vivid analogies that left a lasting impression. This presentation is not only suitable for beginners but also offers valuable insights and optimization tips for experienced data engineers.</p> <p>For official documentation, you can refer to the Parquet File Format documentation, which provides a comprehensive overview of its structure and features.</p>","tags":["The Lakehouse Series"]},{"location":"blog/apache-hudi-overview/","title":"The Lakehouse Series: Apache Hudi Overview","text":"<p>TLDR</p> <p>After reading this article, you will learn:</p> <ul> <li>How Apache Hudi's timeline-based architecture tracks all table changes and enables time travel queries</li> <li>The difference between Copy-on-Write (COW) and Merge-on-Read (MOR) storage types for different workload patterns</li> <li>How Hudi organizes their data in a structured way with table services</li> <li>The various query types Hudi supports, including snapshot, incremental, and read-optimized queries</li> </ul> <p>Apache Hudi was created by Vinoth Chandar at Uber in 2016 to solve real-world data ingestion challenges in Uber's massive data platform. Originally developed to handle frequent updates and deletes in their data lake while maintaining query performance, Hudi was open-sourced in 2017 and became an Apache Software Foundation top-level project in 2020. As of June, 2025, the project has garnered over 5,800 GitHub stars with contributions from more than 500 developers worldwide. The project is currently maintained by the Apache Software Foundation, with key contributors from Uber, Amazon, Onehouse, and other organizations driving its development roadmap.</p> <p>Hudi's core design philosophy centers around providing incremental processing primitives for data lakes, treating datasets as continuously evolving streams rather than static snapshots. The framework is built around the concept of upserts (update + insert operations) and incremental consumption, enabling near real-time data pipelines with strong consistency guarantees. Hudi distinguishes itself by offering two storage types - Copy-on-Write (COW) for read-heavy workloads and Merge-on-Read (MOR) for write-heavy scenarios - allowing users to optimize for their specific use case.</p>","tags":["The Lakehouse Series"]},{"location":"blog/apache-hudi-overview/#timeline","title":"Timeline","text":"<p>The first step to know Apache Hudi is to understand its timeline, which is a key component of Hudi's architecture. You could imagine in Uber, data is constantly being ingested, updated, and queried in every second. Hudi's timeline captures all these changes in a structured way, allowing users to track the evolution of their datasets immediately and accurately. The timeline is a versioned log that records all actions performed on a Hudi table, enabling features like time travel queries, incremental processing, and data lineage tracking, and acting as a source of truth for the state of the table.</p> Actions in the Timeline <p>Apache Hudi maintains a timeline for each table, which is a sequence of different types of actions that have been performed on the table and is stored in the <code>.hoodie</code> directory under the base path of the table. Each action can be thought of as a commit that captures the state of the table at a specific point in time, and has the following attribute associated to it:</p> <ul> <li>Requested instant: Instant time representing when the action was requested on the timeline and acts as the transaction id. An immutable plan for the action should be generated before the action is requested.</li> <li>Completed instant: Instant time representing when the action was completed on the timeline. All relevant changes to table data/metadata should be made before the action is completed.</li> <li>State: state of the action. valid states are <code>REQUESTED</code>, <code>INFLIGHT</code> and <code>COMPLETED</code> during an action's lifecycle.</li> <li>Type: the kind of action performed. For example, <code>COMMIT</code>, <code>DELTA_COMMIT</code>, <code>REPLACE_COMMIT</code>, <code>COMPACTION</code>, <code>INDEXING</code>, <code>ROLLBACK</code>, etc.</li> </ul> <p>For valid action types, refer to the Hudi documentation.</p>","tags":["The Lakehouse Series"]},{"location":"blog/apache-hudi-overview/#storage-layout","title":"Storage Layout","text":"<pre><code>orders_hudi/\n\u251c\u2500\u2500 .hoodie/\n\u2502   \u251c\u2500\u2500 hoodie.properties\n\u2502   \u251c\u2500\u2500 20250703020000000.commit\n\u2502   \u251c\u2500\u2500 20250703030000000.deltacommit\n\u2502   \u2514\u2500\u2500 metadata/\n\u2502       \u251c\u2500\u2500 file_id_0_0.parquet\n\u2502       \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 country_code=US/\n\u2502   \u2514\u2500\u2500 order_year=2025/\n\u2502       \u2514\u2500\u2500 order_month=07/\n\u2502           \u2514\u2500\u2500 order_day=21/\n\u2502               \u251c\u2500\u2500 4b8c9d27_0_20250703020000000.parquet\n\u2502               \u251c\u2500\u2500 4b8c9d27_0_20250703030000000.log.avro\n\u2502               \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 country_code=CA/...\n</code></pre> <p>Hudi organizes data tables into a directory structure under a base path on a storage. Tables are optionally broken up into partitions, based on partition columns defined in the table schema. One thing to note is that Hudi does not support partition evolution, meaning once a partition is created, it cannot be removed or modified. In contrast, Apache Iceberg does support partition evolution.</p> <p>Within each partition, files are organized into file groups, uniquely identified by a file ID (uuid) (<code>4b8c9d27</code> in this case). Each file group contains several file slices (<code>20250703020000000</code> and <code>20250703030000000</code> in this case). Each slice contains a base file (parquet/orc/hfile) written by a commit that completed at a certain instant, along with set of log files (<code>.log.</code>) written by commits that completed before the next base file's requested instant.</p> <p>a file group is essentially a chunk of row data, and a file slice captures the state of that chunk at a specific commit time.</p> <p>Here is a simplified example of how Hudi reads and writes data:</p> Apache Hudi's Table format <ol> <li>In the first step, new records are written to the latest file slice of the appropriate file group. If a suitable file group doesn\u2019t exist, a new one is created. The files are stored under the partition path of the table.</li> <li>Next, the write action is committed to the timeline. This commit is added to the event log, recording the exact point in time when the write occurred.</li> <li>Then, when a read is triggered, Hudi consults the timeline and metadata table to find the relevant file slices. This determines which version of the data should be read.</li> <li>Finally, the read operation loads the file slices. If needed, Hudi merges base files with log files to reconstruct the correct version of the data before returning the results.</li> </ol> <p>Hudi employs Multiversion Concurrency Control (MVCC), where compaction action merges logs and base files to produce new file slices and cleaning action gets rid of unused/older file slices to reclaim space on the file system. All metadata including timeline, metadata table are stored in a special <code>.hoodie</code> directory under the base path.</p> <p>Here is another example of how Hudi manages data over time, illustrating the evolution of file slices and the actions taken at different instants:</p> File Format Structure in Hudi <p>At t1, Apache Hudi creates an initial base file (b) for a file group, capturing a snapshot of the data at that time\u2014this forms the starting point for File Slice 1. As new data arrives, delta commits at t2 and t3 append data blocks to a log file, allowing efficient updates without rewriting the base file. At t4, another delta commit occurs, but the operation is rolled back, resulting in a rollback block being written to a second log file. This rollback preserves ACID semantics by marking the data from t4 as invalid. </p> <p>At t5, Hudi performs a compaction, merging all accumulated log files into a new base file (b'), effectively starting File Slice 2. This compaction improves read performance by reducing the need for on-the-fly merging. Following this, at t6, a delete block is appended to a new log file associated with b', indicating a record deletion.</p> <p>Later on, at t10, a clean operation is executed to remove obsolete or unreferenced files, such as outdated log files or old base files, maintaining storage hygiene. Finally, at t15, a clustering operation reorganizes the files (e.g., by sorting) to further optimize query performance. Throughout this timeline, Hudi maintains a detailed history of actions to support versioned reads, time travel, and transactional guarantees.</p>","tags":["The Lakehouse Series"]},{"location":"blog/apache-hudi-overview/#table-services","title":"Table Services","text":"Table services in Hudi <p>Beyond compaction and cleaning, Apache Hudi provides several other table services to enhance the performance and manageability of data tables. Here are some of the key services:</p> <ul> <li>Clustering: The clustering service, akin to features in cloud data warehouses, allows users to group frequently queried records using sort keys or merge smaller Base Files into larger ones for optimal file size management.</li> <li>Compaction: Hudi's compaction service, featuring strategies like date partitioning and I/O bounding, merges Base Files with delta logs to create updated Base Files.</li> <li>Cleaning: Cleaner service works off the timeline incrementally, removing File Slices that are past the configured retention period for incremental queries, while also allowing sufficient time for long running batch jobs (e.g Hive ETLs) to finish running.</li> <li>Indexing: Hudi's scalable metadata table contains auxiliary data about the table.</li> </ul>","tags":["The Lakehouse Series"]},{"location":"blog/apache-hudi-overview/#table-types","title":"Table Types","text":"<p>Hudi table types define how data is stored and how write operations are implemented on top of the table (i.e how data is written). In turn, query types define how the underlying data is exposed to the queries (i.e. how data is read).</p>","tags":["The Lakehouse Series"]},{"location":"blog/apache-hudi-overview/#copy-on-write-cow","title":"Copy-on-Write (COW)","text":"Copy-on-Write (COW) Table <p>The Copy-on-Write (CoW) table type is optimized for read-heavy workloads. In this mode, record updates or deletes trigger the creation of new base files in a file group and there are no log files written. This ensures that each query reads only the base files, offering high read performance with no need to merge log files dynamically. While CoW tables are ideal for OLAP scans/queries, their write operations can be slower due to the overhead of rewriting base files during updates or deletes, even if small percentage of records are modified in each file.</p> <p>Good use-cases for CoW tables:</p> <ul> <li>Batch ETLs/Data Pipelines</li> <li>Data Warehousing on Data Lakes</li> <li>Static or Slowly Changing Data</li> </ul>","tags":["The Lakehouse Series"]},{"location":"blog/apache-hudi-overview/#merge-on-read-mor","title":"Merge-on-Read (MOR)","text":"Merge-on-Read (MOR) Table <p>The Merge-on-Read (MoR) table type balances the write and read performance by combining lightweight log files with the base file using periodic compaction. Data updates and deletes are written to log files (in row based formats like Avro or columnar/base file formats) and these changes in log files are then merged dynamically with base files during query execution. This approach reduces write latency and supports near real-time data availability. However, query performance may vary depending on whether the log files are compacted.</p> <p>Great fit for the following use-cases:</p> <ul> <li>Change Data Capture Pipelines</li> <li>Streaming Data Ingestion</li> <li>Hybrid Batch + Streaming workloads</li> <li>Frequent Updates and Deletes</li> </ul>","tags":["The Lakehouse Series"]},{"location":"blog/apache-hudi-overview/#comparison","title":"Comparison","text":"Comparison Between COW and MOR Tables","tags":["The Lakehouse Series"]},{"location":"blog/apache-hudi-overview/#query-types","title":"Query Types","text":"<ul> <li>Snapshot Queries: Queries see the latest snapshot of the table as of the latest completed action. These are the regular SQL queries everyone is used to running on a table. </li> <li>Time Travel Queries: Queries a snapshot of a table as of a given instant in the past.</li> <li>Read Optimized Queries(Only MoR tables) : Read-optimized queries provides excellent snapshot query performance via purely columnar files (e.g. Parquet base files).</li> <li>Incremental Queries (Latest State): Incremental queries only return new data written to the table since an instant on the timeline.</li> <li>Incremental Queries (CDC): These are another type of incremental queries, that provides database like change data capture streams out of Hudi tables.</li> </ul> <p>About Incremental Queries:</p> Incremental Queries <p>In streaming workflows, Hudi combines the timeline (ingestion time) with partitions (event time), enabling both accurate tracking of data updates and precise querying by event time. This makes incremental ETL pipelines more efficient and reliable.</p>","tags":["The Lakehouse Series"]},{"location":"blog/apache-hudi-overview/#references","title":"References","text":"<ul> <li>Apache Hudi - The Data Lake Platform</li> <li>Timeline | Hudi Documentation</li> <li>Apache Hudi Stack | Hudi Documentation</li> <li>Table &amp; Query Types | Hudi Documentation</li> </ul>","tags":["The Lakehouse Series"]},{"location":"blog/apache-iceberg-overview/","title":"The Lakehouse Series: Apache Iceberg Overview","text":"<p>TLDR</p> <p>After reading this article, you will learn:</p> <ul> <li>Apache Iceberg's 3-tier metadata architecture (metadata files, manifest lists, and manifest files)</li> <li>How Iceberg catalogs work, including the REST catalog standard for multi-engine compatibility</li> <li>Query capabilities including time travel, incremental reads, and metadata queries</li> <li>Spark procedures for snapshot management, metadata maintenance, and table migration</li> </ul> Iceberg Storage Layout <p>Apache Iceberg was born at Netflix in 2017 from the brilliant minds of Ryan Blue and Daniel Weeks, who were wrestling with the limitations of Hive tables at Netflix's mind-boggling scale. When you're dealing with petabytes of data and schema evolution nightmares, you need something better than what existed. Open-sourced in 2018 and graduating to an Apache Software Foundation top-level project in 2020, Iceberg has become a heavyweight in the lakehouse arena. As of June 2025, the project boasts over 7,500 GitHub stars with contributions from more than 600 developers worldwide, backed by industry giants like Netflix, Apple, Tabular, Dremio, and AWS.</p> <p>What sets Iceberg apart is its architectural elegance. Built on a sophisticated 3-tier metadata hierarchy, Iceberg orchestrates table-level metadata files (<code>metadata.json</code>), snapshot-level manifest lists (<code>*.avro</code>), and data-level manifest files (<code>*.avro</code>) into a symphony of efficiency.</p> <p>Iceberg's design philosophy centers around hidden partitioning and schema evolution without the headaches. Forget about manually managing partitions or rewriting entire tables when your schema changes \u2014 Iceberg handles this behind the scenes like a data concierge. The format's storage-agnostic approach means true multi-engine compatibility through standardized APIs and REST-based catalogs, making it the diplomat of the lakehouse world.</p> <p>At its core, Iceberg treats every table state as an immutable, versioned snapshot with complete lineage tracking. This enables bulletproof concurrent reads and writes, automatic schema evolution, and surgical partition pruning. Built with both backward and forward compatibility in mind, Iceberg ensures your data infrastructure won't break when you upgrade.</p>","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"blog/apache-iceberg-overview/#storage-layout","title":"Storage Layout","text":"Iceberg Storage Layout","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"blog/apache-iceberg-overview/#catalog","title":"Catalog","text":"<p>The Iceberg catalog serves as the centralized metadata repository that manages table locations, schema information, and other table-level metadata. It integrates with existing metadata stores such as Hive Metastore, AWS Glue, or custom-built solutions, providing a unified interface for query engines like Spark, Trino, Flink, and others to discover and access table metadata efficiently. Before performing any operations with Apache Iceberg, the first step typically involves setting up and configuring a catalog \u2014 think of it as establishing the command center for your data infrastructure.</p> <p>A particularly exciting advancement in this area is the introduction of the REST catalog in Iceberg version 0.14.0, released in July 2022. This catalog acts as a standardized communication layer \u2014 essentially a universal translator \u2014 allowing query engines to interact with metadata through a common RESTful API, rather than relying on engine-specific logic embedded in catalog clients. In this model, the logic that handles metadata operations resides on the catalog server, and can be implemented in any language or backed by any technology, as long as it adheres to the Iceberg REST Open API specification.</p> <p>One of the most significant benefits of the REST catalog is that it enables a single, lightweight client to communicate with a wide range of catalog backends. This decouples clients from the specific implementations of the catalog, reducing the need to manage multiple dependencies or worry about compatibility issues. Whether you're using Starburst, Athena, or any other engine that supports the Iceberg REST catalog, the interaction becomes seamless \u2014 much like using a single remote control that works across all your devices.</p> <p>The REST catalog itself is not a product, but an API standard. The actual server implementation must be built by developers or the community following the specification. Several open-source projects and products are working to implement or support REST-based catalogs for Iceberg, including Project Nessie, Apache Polaris, Apache Gravitino, and LakeKeeper, etc.</p> <p></p> <p>For more details on Apache Iceberg catalogs, see the official documentation.</p>","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"blog/apache-iceberg-overview/#metadata-files","title":"Metadata Files","text":"<p>Metadata files in Iceberg are JSON files stored at the table level, and they play a critical role in managing the table's structure, state, and data references. These files enable query engines to accurately interpret and interact with the table.</p> <p>Key components include:</p> <ul> <li>Schemas: Defines the structure of the table, including field names, types, and nullability. Iceberg retains all historical schemas - to support schema evolution without disrupting downstream systems.</li> <li>Snapshots: Represent the state of the table at specific points in time. Each snapshot includes a timestamp, a list of data file manifests, and metadata about the change, enabling time travel and strong consistency guarantees.</li> <li>Current Snapshot ID: Identifies the latest active snapshot that query engines should read from.</li> <li>Partition Specs: Specify how data is partitioned, including transformations like bucketing and truncation. Multiple versions can coexist to support evolving partition strategies.</li> <li>Location: Indicates the root path in storage where the table's metadata, manifest files, and data files are stored.</li> </ul>","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"blog/apache-iceberg-overview/#snapshots","title":"Snapshots","text":"<p>A snapshot captures the complete state of a table at a specific point in time, recording which data files were part of the table during that version.</p> <p>To efficiently manage large datasets, A snapshot's data files are organized into multiple manifest files, which are all referenced together in one manifest list file.</p>","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"blog/apache-iceberg-overview/#manifest-list-files","title":"Manifest List Files","text":"<p>At the snapshot level, the manifest list file is an Avro-formatted file that records all the manifest files associated with a given snapshot.</p> <p>Each entry in the manifest list includes:</p> <ul> <li><code>manifest_path</code>: The file path of the manifest</li> <li><code>manifest_length</code>: The size of the manifest file</li> <li><code>partition_spec_id</code>: The ID of the partition spec used</li> <li><code>added_snapshot_id</code>: The snapshot ID in which the manifest was added</li> <li>A count of data files: added, existing, and deleted</li> <li>Partition-level statistics used for query pruning</li> </ul> <p>This structure improves performance by allowing query engines to filter relevant manifests based on metadata without reading every manifest file in full.</p>","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"blog/apache-iceberg-overview/#manifest-files","title":"Manifest Files","text":"<p>At the data level, manifest files are Avro-formatted files that store metadata for each data file included in a snapshot.</p> <p>For each data file, a manifest records:</p> <ul> <li><code>file_path</code>: The physical path to the file</li> <li><code>file_format</code>: The file format (e.g., Parquet, ORC, Avro)</li> <li><code>partition_data</code>: The partition values of the file</li> <li><code>record_count</code>: The number of records in the file</li> <li><code>file_size_in_bytes</code>: The size of the file</li> <li>Column-level statistics: including value counts, null value counts, and min/max values</li> </ul> <p>These metadata entries enable query engines to perform partition pruning and skip irrelevant files during query planning, improving performance.</p>","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"blog/apache-iceberg-overview/#putting-all-together","title":"Putting All Together","text":"<p>What is the relationship between a snapshot and a manifest list file?</p> <p>In Apache Iceberg, each snapshot has a corresponding manifest list file. This file lists all the manifest files that belong to that snapshot.</p> <p>Can one manifest list file include multiple manifest files?</p> <p>Yes. A single manifest list file can include multiple manifest files, each of which tracks a set of data files (e.g., Parquet, ORC, Avro) that belong to the snapshot.</p> <p>This design supports query optimization because query engines can use summary information from the manifest list to filter out irrelevant manifest files, reducing unnecessary scans and improving performance.</p> <p>Do data files listed in manifest files within a single snapshot ever overlap?</p> <p>Under normal conditions, no. One of Iceberg's core design principles is that each data file is tracked only once within a single snapshot.</p> <p>The manifest files referenced in a snapshot's manifest list cover non-overlapping sets of data files. This ensures:</p> <ul> <li>Efficient queries: Avoid redundant scans of the same data file.</li> <li>Data consistency: Ensure no duplicate records are counted or queried.</li> <li>Simplified metadata management: Easier updates and lower risk of errors.</li> </ul> <p>Can a single metadata file (<code>metadata.json</code>) reference multiple manifest list files?</p> <p>Yes.</p> <p>Each <code>metadata.json</code> file represents one version of the table and tracks all snapshots belonging to that version.</p> <p>Since each snapshot has its own manifest list file, a single metadata file can reference multiple manifest list files, one for each snapshot.</p> <p>For a deeper dive into Apache Iceberg's Storage Layout, I recommend checking out Confluent's article on What is Apache Iceberg\u2122?, which provides detailed explanations, or the video below:</p>","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"blog/apache-iceberg-overview/#query-types","title":"Query Types","text":"<p>Apache Iceberg offers a range of query capabilities that, while not identical to Apache Hudi's, provide robust data management and analysis features:</p> <ul> <li>Time Travel: Query historical snapshots of your data using SQL clauses like <code>TIMESTAMP AS OF</code> or <code>VERSION AS OF</code>. See Spark Time travel Queries and Trino Time Travel Queries for more details.     <pre><code>-- time travel to October 26, 1986 at 01:21:00\nSELECT * FROM prod.db.table TIMESTAMP AS OF '1986-10-26 01:21:00';\n\n-- time travel to snapshot with id 10963874102873L\nSELECT * FROM prod.db.table VERSION AS OF 10963874102873;\n\n-- time travel to the head snapshot of audit-branch\nSELECT * FROM prod.db.table VERSION AS OF 'audit-branch';\n\n-- time travel to the snapshot referenced by the tag historical-snapshot\nSELECT * FROM prod.db.table VERSION AS OF 'historical-snapshot';\n</code></pre></li> <li>Incremental Reads: Perform incremental data ingestion by scanning snapshots for added or removed files. This is particularly useful for change data capture (CDC) scenarios. See Spark Incremental Reads for more details (Trino doesn't support incremental reads).     <pre><code>// get the data added after start-snapshot-id (10963874102873L) until end-snapshot-id (63874143573109L)\nspark.read\n     .format(\"iceberg\")\n     .option(\"start-snapshot-id\", \"10963874102873\")\n     .option(\"end-snapshot-id\", \"63874143573109\")\n     .load(\"path/to/table\")    \n</code></pre></li> <li>Metadata Queries: Access metadata tables such as history, snapshots, and files to gain insights into table versions, schema changes, and data file statistics. See Spark Metadata Queries and Trino Metadata Queries for more details.     <pre><code>-- Spark SQL\nSELECT * FROM prod.db.table.history;\nSELECT * from prod.db.table.metadata_log_entries;\nSELECT * FROM prod.db.table.snapshots;\nSELECT * FROM prod.db.table.entries;\nSELECT * FROM prod.db.table.files;\nSELECT * FROM prod.db.table.manifests;\nSELECT * FROM prod.db.table.partitions;\nSELECT * FROM prod.db.table.refs;\nSHOW TBLPROPERTIES prod.db.table;\nSELECT * from prod.db.table.position_deletes;\n</code></pre> <pre><code>-- Trino SQL\nSELECT * FROM \"test_table$history\";\nSELECT * FROM \"test_table$metadata_log_entries\";\nSELECT * FROM \"test_table$snapshots\";\nSELECT * FROM \"test_table$entries\";\nSELECT * FROM \"test_table$files\";\nSELECT * FROM \"test_table$manifests\";\nSELECT * FROM \"test_table$partitions\";\nSELECT * FROM \"test_table$refs\";\nSELECT * FROM \"test_table$properties\";\n</code></pre></li> </ul> <p>While Apache Hudi provides specialized query types like Read Optimized and CDC Incremental queries tailored for streaming data, Apache Iceberg's approach focuses on providing a consistent and flexible framework for both batch and streaming workloads, with strong support for schema and partition evolution and concurrent writes.</p>","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"blog/apache-iceberg-overview/#spark-procedures","title":"Spark Procedures","text":"","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"blog/apache-iceberg-overview/#snapshot-management","title":"Snapshot Management","text":"<p>Snapshot management in Apache Iceberg is a powerful feature that allows users to manipulate table snapshots, enabling time travel, rollback, and other advanced operations. Spark provides a set of procedures to manage snapshots effectively.</p> <ul> <li><code>rollback_to_snapshot</code>: Roll back a table to a specific snapshot ID.     <pre><code>CALL catalog_name.system.rollback_to_snapshot('db.sample', 1);\n</code></pre></li> <li><code>rollback_to_timestamp</code>: Roll back a table to the snapshot that was current at some time.     <pre><code>CALL catalog_name.system.rollback_to_timestamp('db.sample', TIMESTAMP '2021-06-30 00:00:00.000');\n</code></pre></li> <li><code>set_current_snapshot</code>: Sets the current snapshot ID for a table.     <pre><code>CALL catalog_name.system.set_current_snapshot('db.sample', 1);\nCALL catalog_name.system.set_current_snapshot(table =&gt; 'db.sample', ref =&gt; 's1');\n</code></pre></li> <li><code>cherrypick_snapshot</code>: Cherry-picks changes from a snapshot into the current table state. Cherry-picking creates a new snapshot from an existing     <pre><code>CALL catalog_name.system.cherrypick_snapshot('my_table', 1);\nCALL catalog_name.system.cherrypick_snapshot(snapshot_id =&gt; 1, table =&gt; 'my_table' );\n</code></pre></li> <li><code>publish_changes</code>: Publish changes from a staged WAP ID into the current table state.     <pre><code>CALL catalog_name.system.publish_changes('my_table', 'wap_id_1');\nCALL catalog_name.system.publish_changes(wap_id =&gt; 'wap_id_2', table =&gt; 'my_table');\n</code></pre></li> <li><code>fast_forward</code>: Fast-forward the current snapshot of one branch to the latest snapshot of another.     <pre><code>CALL catalog_name.system.fast_forward('my_table', 'main', 'audit-branch');\n</code></pre></li> </ul>","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"blog/apache-iceberg-overview/#metadata-management","title":"Metadata Management","text":"<p>Iceberg provides procedures to manage metadata files, including rewriting manifests, removing orphan files, and expiring snapshots. These procedures help maintain the integrity and performance of Iceberg tables.</p> <ul> <li><code>expire_snapshots</code>: Remove older snapshots and their files which are no longer needed.     <pre><code>CALL hive_prod.system.expire_snapshots('db.sample', TIMESTAMP '2021-06-30 00:00:00.000', 100);\nCALL hive_prod.system.expire_snapshots(table =&gt; 'db.sample', snapshot_ids =&gt; ARRAY(123));\n</code></pre></li> <li><code>remove_orphan_files</code>: Remove files which are not referenced in any metadata files of an Iceberg table and can thus be considered \"orphaned\".     <pre><code>CALL catalog_name.system.remove_orphan_files(table =&gt; 'db.sample', dry_run =&gt; true);\nCALL catalog_name.system.remove_orphan_files(table =&gt; 'db.sample', location =&gt; 'tablelocation/data');\nCALL catalog_name.system.remove_orphan_files(table =&gt; 'db.sample', prefix_mismatch_mode =&gt; 'IGNORE');\nCALL catalog_name.system.remove_orphan_files(table =&gt; 'db.sample', prefix_mismatch_mode =&gt; 'DELETE');\nCALL catalog_name.system.remove_orphan_files(table =&gt; 'db.sample', equal_schemes =&gt; map('file', 'file1'));\nCALL catalog_name.system.remove_orphan_files(table =&gt; 'db.sample', equal_authorities =&gt; map('ns1', 'ns2'));\n</code></pre></li> <li><code>rewrite_data_files</code>: Rewrite the data files of a table to optimize storage layout.     <pre><code>CALL catalog_name.system.rewrite_data_files('db.sample');\nCALL catalog_name.system.rewrite_data_files(table =&gt; 'db.sample', strategy =&gt; 'sort', sort_order =&gt; 'id DESC NULLS LAST,name ASC NULLS FIRST');\nCALL catalog_name.system.rewrite_data_files(table =&gt; 'db.sample', strategy =&gt; 'sort', sort_order =&gt; 'zorder(c1,c2)');\nCALL catalog_name.system.rewrite_data_files(table =&gt; 'db.sample', options =&gt; map('min-input-files', '2', 'remove-dangling-deletes', 'true'));\nCALL catalog_name.system.rewrite_data_files(table =&gt; 'db.sample', where =&gt; 'id = 3 and name = \"foo\"');\n</code></pre></li> <li><code>rewrite_manifests</code>: Rewrite manifests for a table to optimize scan planning.     <pre><code>CALL catalog_name.system.rewrite_manifests('db.sample');\nCALL catalog_name.system.rewrite_manifests('db.sample', false);\n</code></pre></li> <li><code>rewrite_position_delete_files</code>: Rewrite position delete files for a table to optimize storage layout.     <pre><code>CALL catalog_name.system.rewrite_position_delete_files('db.sample');\nCALL catalog_name.system.rewrite_position_delete_files(table =&gt; 'db.sample', options =&gt; map('rewrite-all', 'true'));\nCALL catalog_name.system.rewrite_position_delete_files(table =&gt; 'db.sample', options =&gt; map('min-input-files','2'));\n</code></pre></li> </ul>","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"blog/apache-iceberg-overview/#table-migration","title":"Table Migration","text":"<ul> <li><code>snapshot</code>: Create a light-weight temporary copy of a table for testing, without changing the source table.     <pre><code>CALL catalog_name.system.snapshot('db.sample', 'db.snap');\nCALL catalog_name.system.snapshot('db.sample', 'db.snap', '/tmp/temptable/');\n</code></pre></li> <li><code>migrate</code>: Replace a table with an Iceberg table, loaded with the source's data files. Table schema, partitioning, properties, and location will be copied from the source table.     <pre><code>CALL catalog_name.system.migrate('spark_catalog.db.sample', map('foo', 'bar'));\nCALL catalog_name.system.migrate('db.sample');\n</code></pre></li> <li><code>add_files</code>: Attempts to directly add files from a Hive or file based table into a given Iceberg table.     <pre><code>CALL spark_catalog.system.add_files(\n    table =&gt; 'db.tbl',\n    source_table =&gt; 'db.src_tbl',\n    partition_filter =&gt; map('part_col_1', 'A')\n);\n\nCALL spark_catalog.system.add_files(\n    table =&gt; 'db.tbl',\n    source_table =&gt; '`parquet`.`path/to/table`'\n);\n</code></pre></li> <li><code>register_table</code>: Creates a catalog entry for a metadata.json file which already exists but does not have a corresponding catalog identifier.     <pre><code>CALL spark_catalog.system.register_table(\n    table =&gt; 'db.tbl',\n    metadata_file =&gt; 'path/to/metadata/file.json'\n);\n</code></pre></li> </ul>","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"blog/apache-iceberg-overview/#others","title":"Others","text":"<ul> <li><code>ancestors_of</code>: Report the live snapshot IDs of parents of a specified snapshot</li> <li><code>create_changelog_view</code>: Creates a view that contains the changes from a given table.</li> <li><code>compute_table_stats</code>: Calculates the Number of Distinct Values (NDV) statistics for a specific table.</li> <li><code>rewrite_table_path</code>: Stages a copy of the Iceberg table's metadata files where every absolute path source prefix is replaced by the specified target prefix.</li> </ul> <p>See Spark Procedures for more details on the available procedures and their usage.</p>","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"blog/apache-iceberg-overview/#references","title":"References","text":"<ul> <li>Apache Iceberg Copy-On-Write (COW) vs Merge-On-Read (MOR): A Deep Dive</li> <li>Exploring the Architecture of Apache Iceberg, Delta Lake, and Apache Hudi</li> <li>Comparison of Data Lake Table Formats (Apache Iceberg, Apache Hudi and Delta Lake)</li> <li>Table Format Governance and Community Contributions: Apache Iceberg, Apache Hudi, and Delta Lake</li> <li>Table Format Partitioning Comparison: Apache Iceberg, Apache Hudi, and Delta Lake</li> <li>Tampa Bay DE Meetup: The Who, What and Why of Data Lake Table Formats (Iceberg, Hudi, Delta Lake)</li> <li>Hudi vs Iceberg vs Delta Lake: Data Lake Table Formats Compared</li> <li>What Is a Lakehouse?</li> </ul>","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"blog/from-data-lakes-to-data-lakehouses/","title":"The Lakehouse Series: From Data Lakes to Data Lakehouses","text":"<p>TLDR</p> <p>After reading this article, you will learn:</p> <ul> <li>What limitations traditional data lakes face</li> <li>How data lakehouses merge the flexibility of data lakes with the structured management of data warehouses</li> <li>What enterprise-grade capabilities define lakehouse architecture</li> <li>What the major open-source lakehouse formats are</li> </ul> Data Architecture Evolution","tags":["The Lakehouse Series"]},{"location":"blog/from-data-lakes-to-data-lakehouses/#challenges-in-traditional-data-lakes","title":"Challenges in Traditional Data Lakes","text":"<p>In the 2010s, Data Lakes emerged as a revolutionary concept, enabling enterprises to store vast amounts of raw data for on-demand analysis. This \"store everything in a pool and let users extract what they need\" approach fundamentally differed from the traditional data warehouse paradigm of \"ETL first, then store.\" While many companies implemented Data Lakes using Hadoop/HDFS, they still couldn't resolve critical issues around data governance, transactional support, and query performance. Over time, the limitations of Data Lakes became increasingly apparent, particularly in scenarios requiring efficient querying and robust data governance. Traditional data lakes often suffer from issues such as:</p> <ul> <li>Absence of Transactional Support: Data lakes cannot guarantee atomicity, consistency, isolation, and durability (ACID) properties, making it challenging to handle concurrent operations reliably.</li> <li>Poor Data Quality Enforcement: Without built-in validation mechanisms, data lakes struggle to maintain data integrity and prevent the ingestion of corrupt or invalid data.</li> <li>Consistency and Isolation Issues: The inability to properly isolate read and write operations creates conflicts when mixing batch processing, streaming workloads, and real-time queries.</li> <li>Limited Interoperability: Data lakes are frequently bound to particular processing engines, restricting the choice of analytical tools and creating vendor lock-in situations. For instance, traditional data lake implementations (such as Hive Metastore with HDFS) were primarily designed for Spark or Hive ecosystems.</li> </ul> <p>These pitfalls highlight the need for a more robust solution to manage and query data effectively.</p>","tags":["The Lakehouse Series"]},{"location":"blog/from-data-lakes-to-data-lakehouses/#the-emergence-of-data-lakehouses","title":"The Emergence of Data Lakehouses","text":"<p>Data lakehouses represent the natural evolution from traditional data lakes, addressing their inherent limitations while preserving their core benefits. In 2020, Databricks introduced the lakehouse concept as a hybrid architecture that merges the flexible storage capabilities of data lakes with the structured management and transactional guarantees of data warehouses. This convergence enables organizations to maintain their existing data lake investments while gaining enterprise-grade features previously exclusive to warehouse environments.</p> <p>The lakehouse architecture is characterized by several key capabilities:</p> <ul> <li>ACID Transactional Capabilities: Enterprise lakehouses require robust support for concurrent read and write operations across multiple data pipelines without compromising data integrity.</li> <li>Data Structure Validation and Evolution: The platform must enforce data schemas while allowing flexible evolution, accommodating traditional warehouse patterns like star and snowflake designs.</li> <li>Direct Business Intelligence Integration: Lakehouses allow analytical tools to query source data directly without requiring intermediate data movement or transformation.</li> <li>Independent Scaling of Storage and Processing: By separating storage from computational resources, the architecture can accommodate unlimited concurrent users and handle massive datasets efficiently.</li> <li>Standards-Based Accessibility: Built on open formats like Parquet with standardized APIs, enabling seamless integration with diverse analytical tools, ML frameworks, and programming environments.</li> <li>Multi-Modal Data Handling: Accommodates everything from raw, unstructured content to highly organized structured datasets within a unified platform.</li> <li>Versatile Processing Capabilities: Supports varied analytical needs including data science workflows, machine learning model development, SQL analytics, and business reporting.</li> <li>Native Real-Time Data Processing: Enables continuous data ingestion and immediate availability for live dashboards and real-time enterprise reporting requirements.</li> </ul> <p>Since Databricks introduced the Data Lakehouse concept in 2020, cloud providers have rapidly adopted and implemented these capabilities across their platforms. This widespread adoption reflects the growing recognition of lakehouses as the next evolution in data architecture. The momentum behind this shift is evidenced by Forrester Wave's 2024 report, which evaluated cloud vendors' Data Lakehouse offerings across 24 comprehensive dimensions. Based on assessments of both current capabilities and strategic vision, three companies emerged as Leaders: Databricks, Google, and Snowflake. The report reinforces that Data Lakehouse represents an inevitable evolution in the data landscape.</p> <p>In the open source realm, projects like Apache Hudi, Apache Iceberg, and Delta Lake continue to evolve, providing their respective solutions to implement Data Lakehouse functionality. These projects are constantly enhancing their capabilities. Even in 2025, DuckDB launched DuckLake, attempting to define a new Data Lakehouse standard that uses databases for catalog and metadata management, rather than the blob storage management approach employed by the other three formats.</p>","tags":["The Lakehouse Series"]},{"location":"blog/from-data-lakes-to-data-lakehouses/#data-lakehouse-open-formats","title":"Data Lakehouse Open Formats","text":"<p>Table formats like Apache Hudi, Apache Iceberg, and Delta Lake address these challenges by introducing:</p> <ul> <li>ACID Transactions: Ensuring data consistency and reliability during writes and updates.</li> <li>Schema Evolution Support: Allowing seamless changes to data schemas without breaking downstream processes.</li> <li>Efficient Metadata Management: Leveraging manifest or log files to optimize query planning and execution.</li> <li>Multi-Engine Compatibility: Enabling interoperability across various processing engines like Spark, Flink, and Trino.</li> </ul> <p>These lakehouse formats bridge the gap between raw data storage and structured data management, enabling organizations to build scalable, high-performance, and reliable lakehouse architectures.</p> <p>While each format follows its own design philosophy and implementation approach, they all share a common goal: overcoming the limitations of traditional data lakes and providing a unified foundation for analytics and machine learning workloads.</p> <p>I really enjoyed Alex Merced\u2019s presentation on \"The Who, What and Why of Data Lake Table Formats (Iceberg, Hudi, Delta Lake)\". As the co-author of \"Apache Iceberg: The Definitive Guide\", he offered a great overview of the three formats. You can watch the full talk below:</p>","tags":["The Lakehouse Series"]},{"location":"blog/from-data-lakes-to-data-lakehouses/#references","title":"References","text":"<ul> <li>What Is a Lakehouse?</li> <li>Tampa Bay DE Meetup - The Who, What and Why of Data Lake Table Formats (Iceberg, Hudi, Delta Lake)</li> <li>The Forrester Wave\u2122: Data Lakehouses, Q2 2024</li> <li>DuckLake: SQL as a Lakehouse Format</li> </ul>","tags":["The Lakehouse Series"]},{"location":"blog/spark-query-speedup/","title":"5 Practical Ways to Speed Up Your Apache Spark Queries","text":"<p>TLDR</p> <p>After reading this article, you will learn how to:</p> <ul> <li>Apply filters before joins to reduce data shuffling</li> <li>Avoid premature <code>collect()</code> actions that cause memory bottlenecks</li> <li>Replace UDFs with built-in functions for better performance</li> <li>Optimize duplicate removal using efficient methods</li> <li>Implement broadcast joins for small table operations</li> </ul> <p>Apache Spark is a powerful data processing framework that allows you to handle both batch and streaming data. It also provides high-level APIs that make it easier to work with datasets intuitively, without needing to worry too much about the underlying details of distributed systems.</p> <p>However, as your data scales up significantly, you eventually have to start thinking about how those operations are actually executed under the hood.</p> <p>So in this article, I'd like to share five commonly used tips that can help you write more efficient Spark queries and avoid consuming excessive Spark resources. For each tip, I'll start by explaining what an inefficient query looks like and why it performs poorly. Then, I'll walk through how you can rewrite it for better performance. That way, you'll get a clearer sense of the difference between the two approaches.</p>","tags":["Apache Spark"]},{"location":"blog/spark-query-speedup/#filter-early-before-joins","title":"Filter Early Before Joins","text":"<p>Let's say you're working with an e-commerce platform where you have two main tables: a <code>users</code> table containing customer information (around 1 million records) and an <code>events</code> table that logs all user activities like page views, clicks, and purchases (potentially billions of records). Your goal is to find all users who have made purchases, so you need to join these tables and filter for purchase events specifically.</p> <p>Slow Query</p> <pre><code>users.join(events, on=\"user_id\").filter(events[\"event_type\"] == \"purchase\")\n</code></pre> <p>This code performs a full join operation first before applying the filter condition. When <code>events</code> contains a large amount of data, this approach causes excessive shuffle operations and memory consumption. The filter condition could have been applied earlier through predicate pushdown optimization, but this optimization doesn't occur in this scenario.</p> <p>Fast Query</p> <pre><code>purchase_events = events.filter(events[\"event_type\"] == \"purchase\")\nusers.join(purchase_events, on=\"user_id\")\n</code></pre> <p>By filtering <code>events</code> before the join operation, we significantly reduce the amount of data that needs to be shuffled during the join. This technique implements filter pushdown and early pruning concepts, which minimize the computational overhead and improve query performance.</p>","tags":["Apache Spark"]},{"location":"blog/spark-query-speedup/#avoid-premature-collect-actions","title":"Avoid Premature <code>collect()</code> Actions","text":"<p>One of the most common mistakes in Spark development is using <code>collect()</code> when you only need to inspect or sample your data. This operation forces all data from distributed executors back to the driver node, creating a severe bottleneck and potentially causing out-of-memory errors.</p> <p>Let's say you're analyzing customer purchase patterns in your e-commerce platform. You have a large <code>orders</code> table with millions of records, and you want to quickly examine some sample data to understand the structure before writing more complex analytics queries.</p> <p>Slow Query</p> <pre><code># This brings all data back to the Driver\norders.collect()\n</code></pre> <p>The <code>collect()</code> operation pulls the entire DataFrame data back to the Driver's memory, which can cause out-of-memory errors. This approach prevents distributed execution and creates a bottleneck at the Driver node, negating the benefits of Spark's distributed computing architecture.</p> <p>Fast Query</p> <pre><code># Use aggregation or sampling instead\norders.sample(fraction=0.1).show()\n</code></pre> <p>The <code>show()</code> operation only displays the first few records and completes the computation on the Executors. By using operations like <code>limit()</code>, <code>sample()</code>, or other batch processing methods, we can examine data incrementally while reducing pressure on the Driver node.</p>","tags":["Apache Spark"]},{"location":"blog/spark-query-speedup/#use-built-in-functions-over-udfs","title":"Use Built-in Functions Over UDFs","text":"<p>Let's say you want to calculate discounted prices for your e-commerce orders. You have an <code>orders</code> table with <code>price</code> and <code>discount_rate</code> columns, and you need to compute the final price after applying the discount.</p> <p>Slow Query</p> <pre><code>from pyspark.sql.functions import udf\nfrom pyspark.sql.types import DoubleType\n\n@udf(DoubleType())\ndef calculate_discounted_price(price, discount_rate):\n    return price * (1 - discount_rate)\n\norders.withColumn(\"final_price\", calculate_discounted_price(orders[\"price\"], orders[\"discount_rate\"]))\n</code></pre> <p>User Defined Functions (UDFs) act as black boxes that the Spark Catalyst Optimizer cannot optimize. During execution, there's significant overhead from switching between the JVM and Python environments. Additionally, UDFs bypass Spark's native optimization mechanisms, often resulting in much slower performance.</p> <p>Fast Query</p> <pre><code>from pyspark.sql.functions import col\n\norders.withColumn(\"final_price\", col(\"price\") * (1 - col(\"discount_rate\")))\n</code></pre> <p>Using built-in functions allows the Catalyst Optimizer to recognize and optimize operations through vectorized computations. This approach eliminates the need for JVM-Python context switching and leverages Spark's internal optimization capabilities.</p>","tags":["Apache Spark"]},{"location":"blog/spark-query-speedup/#choose-broadcast-joins-for-small-tables","title":"Choose Broadcast Joins for Small Tables","text":"<p>Slow Query</p> <pre><code>df1.join(df2, \"id\")\n</code></pre> <p>Spark doesn't know which Data Frame is smaller, so it defaults to using shuffle hash join. Even when <code>df2</code> is clearly small, the absence of broadcast hints forces Spark to perform unnecessary shuffle operations, impacting performance significantly.</p> <p>Fast Query</p> <pre><code>from pyspark.sql.functions import broadcast\ndf1.join(broadcast(df2), \"id\")\n</code></pre> <p>Using broadcast join sends the smaller table directly to each executor, eliminating shuffle operations entirely. This approach is ideal for tables smaller than 10MB or the configured <code>spark.sql.autoBroadcastJoinThreshold</code> value.</p> <p>Spark \u6703\u6839\u64da <code>spark.sql.autoBroadcastJoinThreshold</code>\uff0c\u81ea\u52d5\u5224\u65b7\u662f\u5426\u5ee3\u64ad\uff1a</p>","tags":["Apache Spark"]},{"location":"blog/spark-query-speedup/#optimize-duplicate-removal","title":"Optimize Duplicate Removal","text":"<p>Slow Query</p> <pre><code>df.select(\"user_id\").distinct().count()\n</code></pre> <p>The <code>distinct()</code> operation is equivalent to a <code>groupBy</code> with aggregation, which triggers shuffle operations. When dealing with large datasets, this can cause severe network I/O bottlenecks and memory pressure across the cluster.</p> <p>Fast Query</p> <pre><code>df = df.repartition(\"user_id\").dropDuplicates([\"user_id\"])\n</code></pre> <p>Using <code>repartition()</code> with a specific key reduces the shuffle volume by creating local data ordering. The <code>dropDuplicates()</code> method provides more semantic clarity for multi-column operations compared to <code>distinct()</code>, enabling better optimization by the Catalyst optimizer.</p>","tags":["Apache Spark"]},{"location":"blog/whats-new-in-apache-airflow-3/","title":"What's New in Apache Airflow 3","text":"<p>Apache Airflow 3 was officially released in April 2025, marking one of the most significant updates in Airflow's history. This release introduces numerous new features and improvements aimed at enhancing developer experience and system performance. Let's dive into these exciting updates.</p>","tags":["Apache Airflow"]},{"location":"blog/whats-new-in-apache-airflow-3/#the-new-user-interface","title":"The New User Interface","text":"<p>One of the most striking changes in Airflow 3 is the completely revamped user interface. Now built with React, the UI boasts a modern design, including native support for dark mode and seamless integration with Airflow 3's new asset concept. These updates not only improve usability but also provide a more visually appealing experience.</p> <p>To begin with, Airflow 3 introduces a brand-new Home Page, which replaces the DAG List View as the default landing page. The Home Page now features health indicators that allow users to monitor the status of Airflow system components at a glance. Additionally, it provides quick access to Failed, Running, and Active DAGs, as well as execution statistics for DAGs and tasks over specific time intervals.</p> Home Page in Airflow 3 <p>Moving on to the DAG List View, Airflow 3 replaces the circular indicators from Airflow 2 with visual bar charts that summarize recent run outcomes. These charts use color to represent success or failure and bar length to indicate execution time, making it easier for users to quickly understand the status of recent runs.</p> DAG List View in Airflow 2 DAG List View in Airflow 3 <p>The DAG Details Page has also undergone significant improvements. In Airflow 2, the page was cluttered with excessive colors and text, and the Grid View and Graph View were displayed in separate sections, making it difficult to view detailed information alongside the graph. </p> <p>Airflow 3 addresses these issues by integrating the Grid View and Graph View into a single left-side panel. This allows users to simultaneously view detailed information about DAG runs while exploring the grid or graph. The interface has also been simplified, removing unnecessary elements to help users focus on execution details.</p> DAG Details Page in Airflow 2 DAG Details Page in Airflow 3 Graph View in DAG Details Page in Airflow 3 <p>Furthermore, Airflow 3 enhances the DAG Run View by allowing users to inspect task instance details while simultaneously viewing the Grid or Graph View. Users can even switch between these views to better understand task execution statuses in a graphical format.</p> DAG Run View in Airflow 3 Task Instance View in Airflow 3 <p>In previous versions, Airflow 2 introduced the concept of Datasets, which allowed users to track the flow of data between different DAGs. This was a significant step towards understanding data lineage and dependencies. However, with the release of Airflow 3, the Dataset feature has been upgraded and rebranded as the Asset concept, providing a more comprehensive and flexible approach to data and task management.</p> <p>The new Asset List View offers a global overview of all known assets, organizing them by name and displaying which tasks produce them and which DAGs consume them. This clear organization helps users quickly understand the relationships and dependencies between different assets across the platform.</p> Datasets List View in Airflow 2 Asset List View in Airflow 3 <p>The Asset Graph View takes this a step further by providing a contextual map of an asset's lineage. Users can explore upstream producers and downstream consumers of an asset, trigger asset events manually, and inspect recent asset events and the DAG runs they initiated. This comprehensive view offers deep insights into data flow and dependencies.</p> Asset Graph View in Airflow 3 <p>Finally, when working within a DAG, the DAG Graph View with Asset Overlays integrates asset awareness directly into the task dependency graph. This means users can visualize how assets flow between DAGs and uncover asset-triggered dependencies, all within the familiar DAG graph view. This seamless integration completes the picture of how data moves throughout workflows, from task-level execution to broader asset relationships.</p> Graph Overlays in DAG Graph View in Airflow 3 <p>The reimagined Asset system in Airflow 3 provides users with powerful tools to manage and understand their data workflows better than ever before.</p>","tags":["Apache Airflow"]},{"location":"blog/whats-new-in-apache-airflow-3/#architecture-changes-and-task-isolation","title":"Architecture Changes and Task Isolation","text":"<p>In Airflow 2, every system component, including the Scheduler, Workers, Webserver, and CLI, could directly communicate with the Metadata Database. This design had a clear advantage: it was simple and straightforward. All components coordinated through a single source of truth, ensuring strong consistency across the system. The Scheduler could write task states directly, Workers could report results immediately, and the Webserver could display the most up-to-date DAG and task information. As a result, users only needed to maintain one reliable database to keep the entire system running smoothly.</p> <p>However, this architecture also introduced several challenges. Since all components relied on the same database, it could quickly become a performance bottleneck as the number of DAGs and tasks grew. Under high concurrency, competition for database locks between Schedulers and Workers could lead to delays or even deadlocks. In addition, task code had the ability to access the database directly, which reduced isolation and increased the risk of bugs or malicious code affecting the entire system. Finally, the tight coupling between the database schema and all components made upgrades more difficult and increased operational costs, while also limiting observability and extensibility.</p> <p>With Airflow 3, these issues were addressed through the introduction of a dedicated API Server. Instead of allowing components to connect to the Metadata Database directly, all interactions now flow through the API Server. This reduces the load on the database, prevents lock contention, and improves system scalability. It also strengthens security by ensuring that task code can no longer modify the core database directly, thereby improving isolation. At the same time, the new API Server, built on FastAPI, delivers higher performance and more consistent interfaces. It also enables new capabilities such as the Task Execution API and multi-language SDKs. Overall, Airflow 3's API Server resolves the bottlenecks and risks present in the earlier architecture while laying the foundation for a more modern and extensible system.</p> Source: Astronomer","tags":["Apache Airflow"]},{"location":"blog/whats-new-in-apache-airflow-3/#remote-execution","title":"Remote Execution","text":"<p>One of the most important innovations in Airflow 3.0 is the introduction of a Task Execution API and an accompanying Task SDK. </p> <p>These components enable tasks to be defined and executed independently of Airflow\u2019s core runtime engine.</p> <p>Environment decoupling: Tasks can now run in isolated, remote, or containerized environments, separate from the scheduler and workers</p> <p>Edge Executor</p> <p>Why using Edge Worker? The Edge Worker is a execution option that allows you to run Airflow tasks on edge devices. The Edge Worker is designed to be lightweight and easy to deploy. </p> <p>apache-airflow-providers-edge3</p>","tags":["Apache Airflow"]},{"location":"blog/whats-new-in-apache-airflow-3/#dag-versioning","title":"DAG Versioning","text":"<p>DAG Bundle</p>","tags":["Apache Airflow"]},{"location":"blog/whats-new-in-apache-airflow-3/#backfills","title":"Backfills","text":"","tags":["Apache Airflow"]},{"location":"blog/whats-new-in-apache-airflow-3/#assets","title":"Assets","text":"","tags":["Apache Airflow"]},{"location":"blog/whats-new-in-apache-airflow-3/#event-driven-scheduling-inference-execution","title":"Event Driven Scheduling, Inference Execution","text":"<p>AIP-83</p>","tags":["Apache Airflow"]},{"location":"recaps/","title":"Monthly Recaps","text":"<p>Each month, I focus on learning specific technology domains and document my learning journey and outcomes. This practice helps me systematically master important skills while sharing my insights to grow together with the community.</p> <p>Through monthly reviews, I can track my progress, maintain learning momentum, and prepare for future technical challenges.</p> <ul> <li> <p> September 2025</p> <p>Deep diving into system observability and SRE practices, hands-on with OpenTelemetry, Prometheus, Grafana, and Trino to enhance monitoring, tracing, and reliability of distributed systems.</p> <p>\u2192 Observability &amp; SRE Deep-diving Month</p> </li> <li> <p> August 2025</p> <p>Learning advanced system performance tuning techniques for data-intensive applications.</p> <p>\u2192 System Performance Tuning Month</p> </li> <li> <p> July 2025</p> <p>Developing systems thinking and architectural design patterns for scalable solutions.</p> <p>\u2192 System Architect Thinking Month</p> </li> <li> <p> June 2025</p> <p>Building modern data lakehouse architectures with unified analytics platforms.</p> <p>\u2192 Data Lakehouse Building Month</p> </li> <li> <p> May 2025</p> <p>Creating robust MLOps pipelines for automated machine learning workflows.</p> <p>\u2192 MLOps Pipeline Crafting Month</p> </li> </ul>"},{"location":"recaps/202505/","title":"May 2025","text":""},{"location":"recaps/202505/#highlight-of-the-month","title":"Highlight of the Month","text":"<p>Summarize my biggest breakthrough, project, or insight in this month:</p> <p>This month I mainly focused on my side project Fraud Detection: From DataOps to MLOps, and successfully integrated lots of interesting tools I've never tried before, like Feast, MLflow, Ray and KServe! It helped me better understand how to connect data pipelines with real-time model serving and got so much fun during the process.</p>"},{"location":"recaps/202505/#what-i-created-or-tried","title":"What I Created or Tried","text":"<p>What I built, experimented with, or wrote:</p> <ul> <li> Set up an end-to-end Data2MLOps pipeline using dbt, Feast, MLflow, Ray and KServe.</li> <li> Published a series of posts on my side project Fraud Detection: From DataOps to MLOps.</li> <li> Tried out multi-stage Docker builds with <code>uv</code> to optimize my Python environment setup.</li> <li> Experimented with MinIO as an S3-compatible object store on Kubernetes.</li> <li> Tried out GitHub Copilot's configuraiton best practices and its agent mode for more efficient coding.</li> </ul>"},{"location":"recaps/202505/#what-i-learned","title":"What I Learned","text":"<p>Short reflections on what I actually learned or became more confident in:</p> <ul> <li> Grasped how multi-stage Docker builds work and how they can significantly reduce image size and build time</li> <li> Deepened my understanding of how Ray Tune integrates with Optuna to perform distributed hyperparameter tuning, enhancing both speed and efficiency in machine learning tasks</li> <li> Learned the pros and cons between REST API and gRPC for model serving, and how to use KServe to deploy models with both protocols</li> </ul>"},{"location":"recaps/202505/#reflections-beyond-just-tech","title":"Reflections - Beyond Just Tech","text":"<p>Soft-skill insights or workflow/communication/process reflections:</p> <ul> <li> Recognized that in the AI era, mastering individual tools is easy, but engineers add value by excelling in multi-tool integration and architectural design.</li> <li> Realized that relying solely on LLMs for poorly documented tools can lead to inefficiencies. It's crucial to combine LLM assistance with thorough manual exploration and testing.</li> <li> Noticed that GitHub Copilot can significantly speed up coding, but it requires careful management to avoid code quality issues.</li> </ul>"},{"location":"recaps/202505/#what-i-consumed","title":"What I Consumed","text":"<p>A list of articles, papers, courses, or videos I read/watched/completed:</p>"},{"location":"recaps/202505/#read","title":"Read","text":"<ul> <li> Real-time Fraud Detection on GCP with Feast</li> <li> Hyperparameter Tuning with MLflow and Optuna</li> <li> Deploy MLflow models with InferenceService</li> <li> Running Tune experiments with Optuna</li> <li> ihower's Facebook Post about MCP</li> <li> Python Tooling at Scale: LlamaIndex\u2019s Monorepo Overhaul</li> <li> Using uv in Docker</li> <li> Docker \u6559\u5b78\uff1a\u7528 Multi-stage build \u5efa\u7acb Poetry \u865b\u64ec\u74b0\u5883</li> <li> Python \u5957\u4ef6\u7ba1\u7406\u5668\u2014\u2014Poetry \u5b8c\u5168\u5165\u9580\u6307\u5357</li> </ul>"},{"location":"recaps/202505/#watched","title":"Watched","text":"<ul> <li> Claude 4 ADVANCED AI Coding: How I PARALLELIZE Claude Code with Git Worktrees</li> <li> Code with Claude Opening Keynote</li> </ul>"},{"location":"recaps/202505/#completed-courses","title":"Completed Courses","text":"<ul> <li> GitHub Copilot \u5354\u4f5c\u958b\u767c\u5be6\u6230</li> <li> MLflow in Action - Master the art of MLOps using MLflow tool</li> <li> Real-world End to End Machine Learning Ops on Google Cloud</li> </ul>"},{"location":"recaps/202505/#goals-for-next-month","title":"Goals for Next Month","text":"<p>Set 2\u20133 simple goals to stay focused and accountable:</p> <ul> <li> Explore Airflow 3.0 and its new features.</li> <li> Try building my first MCP server.</li> <li> Accelarate my developer experience with GitHub Copilot and its agent mode.</li> <li> Write and publish my thoughts on why I use <code>mkdocs-material</code> for my blog and how I set it up.</li> </ul>"},{"location":"recaps/202506/","title":"June 2025","text":""},{"location":"recaps/202506/#highlight-of-the-month","title":"Highlight of the Month","text":"<p>Summarize my biggest breakthrough, project, or insight in this month:</p> <p>In this month, I mainly focused on my side projects - Retail Lakehouse with Debezium, Kafka, Iceberg, and Trino and A Unified SQL-based Data Pipeline. Not only implementing these projects, I also watched and read some videos and articles about what are the best practices of deploying, managing and maintaining Trino, Iceberg and Kafka Cluster.</p>"},{"location":"recaps/202506/#what-i-created-or-tried","title":"What I Created or Tried","text":"<p>What I built, experimented with, or implemented:</p> <ul> <li> Built a side project: Retail Lakehouse with Debezium, Kafka, Iceberg, and Trino</li> <li> Built a side project: A Unified SQL-based Data Pipeline</li> <li> Published a blog post: The Lakehouse Series: OLTP vs. OLAP (A Parquet Primer)</li> <li> Published a blog post: How to Use MkDocs to Integrate GitHub Actions and Git Submodule for Cross-repo Documentation</li> <li> Experimented with <code>kubectl-ai</code><ul> <li>I've tried <code>kubectl-ai</code> as a MCP server to test the integration with VS Code Copilot Agent Mode, but nothing special happened.</li> </ul> </li> <li> Experimented with Speechify<ul> <li>I really like the ability to listen to articles and papers while doing other tasks. It helps me consume more content without feeling overwhelmed. What I like the most is that they have Snoop Dogg as a voice option, which adds a fun twist to the experience! Could you imagine listening to a data lakehouse article narrated by Snoop Dogg? \u2620\ufe0f</li> </ul> </li> </ul>"},{"location":"recaps/202506/#what-i-learned","title":"What I Learned","text":"<p>Short reflections on what I actually learned or became more confident in:</p> <ul> <li>Focus on OUTPUT, not INPUT. Nowadays, it's information overload everywhere, and it's easy to get lost in the sea of content. Instead of just consuming more information, I should focus on creating something meaningful with the knowledge I gain (Talk is cheap, show me the code).</li> </ul>"},{"location":"recaps/202506/#reflections-beyond-just-tech","title":"Reflections \u2013 Beyond Just Tech","text":"<p>Soft-skill insights or workflow/communication/process reflections:</p> <ul> <li>The ability to have a small talk with the speakers in the social gathering is a great way to build connections and learn from their experiences. In the past, I over focused on the technical aspects, but now I realize that soft skills and networking are equally important in the tech industry.</li> <li>My dad decided to give up active cancer treatment this month and to focus on quality of life instead. After this month of reflection, I realized that there are still many things I want to do and achieve in my life, not only in my career but also in my personal life. Life is too short to be only focused on work.</li> </ul>"},{"location":"recaps/202506/#what-i-consumed","title":"What I Consumed","text":"<p>A list of articles, papers, courses, or videos I read/watched/completed:</p>"},{"location":"recaps/202506/#read","title":"Read","text":"<ul> <li> \u5982\u4f55\u5efa\u7acb\u7368\u4e00\u7121\u4e8c\u7684 GitHub Profile\uff01\u8207\u4e09\u500b\u5f88\u9177\u7684\u8a2d\u8a08\u53ca\u61c9\u7528 \ud83d\ude80</li> <li> Databricks\u8cb7\u4e0bTabular\uff0c\u4f01\u5716\u6539\u5584\u8cc7\u6599\u76f8\u5bb9\u6027</li> <li> Databricks\u5c07\u4ee510\u5104\u7f8e\u5143\u8cb7\u4e0b\u958b\u6e90\u96f2\u7aef\u8cc7\u6599\u5eab\u65b0\u5275Neon</li> <li> What Is a Lakebase?<ul> <li>Openness</li> <li>Separation of storage and compute (the most important feature imo)</li> <li>Serverless</li> <li>Modern development workflow</li> <li>Built for AI agents</li> <li>Lakehouse integration</li> </ul> </li> <li> What Is a Lakehouse? | Databricks Blog</li> <li> \u611b\u597d AI Engineer \u96fb\u5b50\u5831 \ud83d\ude80 \u6a21\u578b\u4e0a\u4e0b\u6587\u5354\u5b9a MCP \u61c9\u7528\u958b\u767c #27<ul> <li>I really liked how the author described two different ways of building agents: one that relies on a customizable framework, and another that's more lightweight and built using just the core features of the programming language. It instantly reminded me of the old debates between TensorFlow 1.0 and PyTorch.</li> <li>After reading this article, I realized that the strength of senior engineers lies in their ability to quickly pick up new technologies and analyze different approaches logically with their own keen insights. This is a skill that I aspire to develop.</li> </ul> </li> <li> How Agoda manages 1.8 trillion Events per day on Kafka<ul> <li>2-step logging approach.</li> <li>Multiple smaller Kafka clusters instead of 1 Large Kafka cluster per Data Center</li> <li>Agoda employs a robust Kafka auditing system by aggregating message counts via background threads in client libraries, routing audits to a dedicated Kafka cluster, and implementing monitoring and alerting mechanisms for audit messages.</li> <li>Agoda calculates cluster capacity by comparing each resource\u2019s usage against its upper limit and taking the highest percentage to represent the dominant constraint at that moment.</li> <li>Agoda attributes cost back to teams, which transformed team mindsets, driving proactive cost management and accountability across Agoda</li> <li>The new auth system empowers the Kafka team to control access, manage credentials, and protect sensitive data through fine-grained ACLs</li> <li>Operational scalability is ensured through automated tooling that streamlines and simplifies system management.</li> </ul> </li> <li> Scaling Kafka to Support PayPal\u2019s Data Growth<ul> <li>Cluster Management: Kafka Config Service, ACLs, PayPal Kafka Libraries, QA Environment</li> <li>Monitoring and Alerting</li> <li>Configuration Management</li> <li>Enhancements and Automation: Patching security vulnerabilities, Security Enhancements, Topic Onboarding, MirrorMaker Onboarding, Repartition Assignment Enhancements, </li> </ul> </li> <li> Pyright \u4e0a\u624b\u6307\u5357\uff1aPython \u578b\u5225\u6aa2\u67e5\u7684\u65b0\u9078\u64c7</li> <li> DuckLake: SQL as a Lakehouse Format<ul> <li>It simplifies lakehouses by using a standard SQL database for all metadata while still storing data in open formats like Parquet, just like BigQuery with Spanner and Snowflake with FoundationDB.</li> </ul> </li> </ul>"},{"location":"recaps/202506/#watched","title":"Watched","text":"<ul> <li> Spark Structured Streaming with Kafka</li> <li> Understand RAFT without breaking your brain</li> <li> Data Lake at Wise powered by Trino and Iceberg</li> <li> I'm an ex-Google interviewer. You're doing LeetCode wrong.</li> <li> Data News: Snowflake/Databricks Announcements, Iceberg V3<ul> <li>Semantic Layer in Snowflake (Semantic Views) and Databricks (Unity Catalog metric views)</li> <li>Snowflake Openflow</li> </ul> </li> <li> Data News: DuckLake, Confluent\u2019s TableFlow, New Book!<ul> <li>Me, personally I think DuckLake is a game-changer for lakehouses, but the speaker didn't think so.</li> <li>Perceived benefits of DuckLake's pproach:<ul> <li>Eliminates separate catalog abstraction</li> <li>Offloads scan planning</li> <li>Easier to get started</li> </ul> </li> <li>Concerns and skepticism about DuckLake:<ul> <li>Reintroducing database overhead</li> <li>Scaling concerns:<ul> <li>Shared resources for scan planning</li> <li>Scalability of the central database</li> </ul> </li> <li>Limits innovation and discovery</li> <li>Unclear details on scan planning</li> </ul> </li> </ul> </li> <li> Introducing Pyrefly: A new type checker and IDE experience for Python</li> <li> Why build Event-Driven AI systems?</li> <li> Why MCP really is a big deal.<ul> <li>MCP offers pluggable, discoverable, and composable solutions that simplify complex integrations.</li> </ul> </li> <li> Why Everyone\u2019s Talking About MCP?<ul> <li>It addresses the issue faced by \\(M\\) AI vendors, where implementing \\(N\\) tools results in an \\(M \\times N\\) complexity problem. Instead, it simplifies the problem to an \\(M+N\\) complexity solution.</li> <li>Five primitives of MCP: resources, tools, prompts, roots and sampling.</li> </ul> </li> </ul>"},{"location":"recaps/202506/#completed-courses","title":"Completed Courses","text":"<ul> <li> GitHub Copilot \u9032\u968e\u958b\u767c\u5be6\u6230<ul> <li> Customize chat responses in VS Code<ul> <li>Instruction files and Prompt files are used to customize the chat responses in VS Code.</li> </ul> </li> <li> Prompt engineering for Copilot Chat</li> </ul> </li> <li> MCP Course</li> </ul>"},{"location":"recaps/202506/#goals-for-next-month","title":"Goals for Next Month","text":"<p>Set 2\u20133 simple goals to stay focused and accountable:</p> <ul> <li>Started to submit my resume to some companies, so I need to prepare for interviews.</li> <li>Publish a series of blog posts about the best practices of deploying, managing and maintaining Trino Cluster and Kafka Cluster and how the big companies use them in production environment.</li> <li>Focus on verbal output, not just written output.</li> <li>Host a series of mock interviews with people in the same community to practice my soft skills and networking abilities.</li> </ul>"},{"location":"recaps/202507/","title":"July 2025","text":""},{"location":"recaps/202507/#highlight-of-the-month","title":"Highlight of the Month","text":"<p>Summarize my biggest breakthrough, project, or insight in this month:</p> <p>This month, I've been focusing on system design practice, and it's been an incredibly valuable experience. Preparing for these interviews has helped me strengthen my ability to break down ambiguous problems, structure my thoughts clearly, and communicate trade-offs effectively. Most importantly, I've started to think more like a systems thinker \u2014 considering scalability, availability, and reliability in every decision.</p>"},{"location":"recaps/202507/#attended","title":"Attended","text":"<ul> <li> Kubernetes Community Days Taipei 2025<ul> <li>Kepler</li> <li>Crossplane</li> <li>Envoy Gateway</li> </ul> </li> <li> GDG Chunghua Meetup</li> <li> System Design Mock Interview with Kevin and Shirley<ul> <li> Design a Rate Limiter</li> <li> Design a Ride-Sharing Service like Uber</li> <li> Design a Ticket Booking Service like Ticket Master</li> <li> Design a Distributed Message Queue</li> <li> Design a Top K Heavy Hitters Service</li> <li> Design a Messaging Service like WhatsApp</li> </ul> </li> </ul>"},{"location":"recaps/202507/#what-i-created-or-tried","title":"What I Created or Tried","text":"<ul> <li> Published a blog post: The Lakhouse Series: Apache Iceberg Overview</li> <li> Published a blog post: The Lakhouse Series: Apache Hudi Overview</li> <li> Published a blog post: The Lakehouse Series: From Data Lakes to Data Lakehouses</li> </ul>"},{"location":"recaps/202507/#what-i-learned","title":"What I Learned","text":"<p>Short reflections on what I actually learned or became more confident in:</p>"},{"location":"recaps/202507/#reflections-beyond-just-tech","title":"Reflections \u2013 Beyond Just Tech","text":"<p>Soft-skill insights or workflow/communication/process reflections:</p> <ul> <li>This month I had interviews with three companies. While my resume optimization has been effective in getting me to the interview stage, my actual capabilities still need improvement to secure job offers.<ul> <li>The first company (Dcard) was for a Data Engineer position, where I was tested on OOP concepts and asked to design a queue class. My implementation skills need strengthening, particularly in using abstract classes and interfaces.</li> <li>The second company (Houzz) was for a Data Infra Engineer role. The third round focused on Spark and Airflow system architecture and optimization strategies. I realized I need to deepen my understanding of system component operations and optimization methods.</li> <li>The third company (Treasure Data) was for a Data Engineer position where dbt is heavily used internally. I reached the fourth round's behavioral interview but was completely unprepared for English behavioral interviews and was eliminated. This made me realize that beyond technical skills, behavioral interview preparation is crucial, especially for English interviews.</li> </ul> </li> <li>When preparing for interviews, I can't afford to be selective. I must comprehensively improve all my capabilities.</li> </ul>"},{"location":"recaps/202507/#what-i-consumed","title":"What I Consumed","text":"<p>A list of articles, papers, courses, or videos I read/watched/completed:</p>"},{"location":"recaps/202507/#read","title":"Read","text":"<ul> <li> Design a Ride-Sharing Service Like Uber | Hello Interview</li> <li> Consistent Hashing | Hello Interview</li> <li> DynamoDB | Hello Interview</li> <li> Kafka | Hello Interview</li> <li> Evolution to the Data Lakehouse</li> <li> What is a data lakehouse? | Databricks Docs</li> <li> Hudi vs Iceberg vs Delta Lake: Data Lake Table Formats Compared</li> <li> GitHub MCP Exploited: Accessing private repositories via MCP</li> </ul>"},{"location":"recaps/202507/#watched","title":"Watched","text":"<ul> <li> Introducing Lakebase - Databricks Co-founder &amp; Chief Architect Reynold Xin</li> <li> CAP Theorem Simplified</li> <li> System Design Was HARD - Until You Knew the Trade-Offs</li> <li> 7 System Design Concepts Explained in 10 Minutes</li> <li> Tampa Bay DE Meetup: The Who, What and Why of Data Lake Table Formats (Iceberg, Hudi, Delta Lake)</li> <li> Watch a Complete NOOB Try DuckDB and DuckLake for the first time</li> </ul>"},{"location":"recaps/202507/#goals-for-next-month","title":"Goals for Next Month","text":"<p>Set 2\u20133 simple goals to stay focused and accountable:</p>"},{"location":"recaps/202508/","title":"August 2025","text":""},{"location":"recaps/202508/#highlight-of-the-month","title":"Highlight of the Month","text":"<p>Summarize my biggest breakthrough, project, or insight in this month:</p> <p>This month I mainly focused on 2 things. First, I dove deep into Apache Iceberg performance tuning techniques, learning how to optimize both write and read operations, manage table compactions, and implement effective partitioning strategies. This knowledge is crucial for maintaining efficient data lakes and ensuring high query performance. Second, I dedicated time to preparing for technical interviews, specific for data lakehouse engineer role.</p>"},{"location":"recaps/202508/#what-i-built-published-or-experimented-with","title":"What I Built, Published, or Experimented with","text":"<ul> <li> Published Best Practices for Optimizing Apache Iceberg Workloads in AWS</li> <li> Published Deep Dive into Kafka Connect Icerberg Sink Connector</li> <li> Published Exactly Once Semantics in Kafka</li> <li> Published What's New in Apache Airflow 3</li> <li> Published 5 Practical Ways to Speed Up Your Apache Spark Queries</li> <li> Experimented with Pyrefly</li> <li> Experimented with <code>colima</code> for replacing Docker Desktop on Mac (and it was great!)</li> <li> Experimented with Claude Code.</li> </ul>"},{"location":"recaps/202508/#what-i-learned","title":"What I Learned","text":"<p>Short reflections on what I actually learned or became more confident in:</p> <ul> <li> Learned the performance tuning techniques for maintaining Iceberg tables, including write optimization, read optimization, compaction, partitioning strategies, etc.</li> </ul>"},{"location":"recaps/202508/#reflections-beyond-just-tech","title":"Reflections \u2013 Beyond Just Tech","text":"<p>Soft-skill insights or workflow/communication/process reflections:</p> <p>After interviewed several times for data engineering roles and finally got a job offer, I realized that</p> <ul> <li>Architecture diagram is super important for communicating my design ideas effectively and it catch interviewers' attention right away</li> <li>Preparing some materials based on the interviewer's introduction to the team and company after the first round interview is very helpful for the follow-up interviews, as it shows my enthusiasm and interest in the role and company.</li> <li>Side projects are definitely a plus, as it demonstrates my passion and commitment to learning and growing in the field.</li> </ul>"},{"location":"recaps/202508/#what-i-consumed","title":"What I Consumed","text":"<p>A list of articles, papers, courses, or videos I read/watched/completed:</p>"},{"location":"recaps/202508/#performance-tuning-optimization","title":"Performance Tuning &amp; Optimization","text":"<ul> <li> Spark Performance Tuning | Spark Docs</li> <li> Ch7 Optimizing and Tuning Spark Applications | Learning Spark</li> <li> Ch4 Optimizing the Performance of Apache Iceberg | The Definitive Guide of Apache Iceberg</li> <li> Best practices for optimizing Apache Iceberg workloads | AWS Docs</li> </ul>"},{"location":"recaps/202508/#real-time-data-processing-risingwave-debezium-flink-cdc","title":"Real-time Data Processing (RisingWave, Debezium, Flink CDC)","text":"<ul> <li> RisingWave vs. Apache Flink: Which one to choose?</li> </ul>"},{"location":"recaps/202508/#sqlmesh","title":"SQLMesh","text":"<ul> <li> SQLMesh sets a new precedent with support for multi-engine projects</li> <li> Multi-Repo guide</li> <li> Virtual Data Environments</li> <li> SQLMesh and virtual data environments</li> <li> Environments</li> </ul>"},{"location":"recaps/202508/#airflow","title":"Airflow","text":"<ul> <li> Apache Airflow\u00ae 3 is Generally Available!</li> <li> Apache Airflow 3.0 Is Here: The Most Significant Release Yet</li> </ul>"},{"location":"recaps/202508/#iceberg-hive","title":"Iceberg &amp; Hive","text":"<ul> <li> Migrating existing tables to Iceberg</li> <li> Optimizing read performance | Using Apache Iceberg on AWS</li> </ul>"},{"location":"recaps/202508/#goals-for-next-month","title":"Goals for Next Month","text":"<p>Set 2\u20133 simple goals to stay focused and accountable:</p> <ul> <li>Explore observability and SRE practices, hands-on with OpenTelemetry, Prometheus, Grafana</li> <li>Get more familiar with Trino in terms of performane tuning</li> </ul>"},{"location":"recaps/202509/","title":"September 2025","text":""},{"location":"recaps/202509/#highlight-of-the-month","title":"Highlight of the Month","text":"<p>Summarize my biggest breakthrough, project, or insight in this month:</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>"},{"location":"recaps/202509/#what-i-built-published-or-experimented-with","title":"What I Built, Published, or Experimented with","text":"<p>What I built, experimented with, or implemented:</p> <ul> <li> Published ____</li> <li> Built ____</li> <li> Experimented with ____</li> </ul>"},{"location":"recaps/202509/#what-i-learned","title":"What I Learned","text":"<p>Short reflections on what I actually learned or became more confident in:</p> <ul> <li> Grasped the pros and cons of ____</li> <li> Recognized how ____</li> <li> Learned ____</li> <li> Understood ____</li> </ul>"},{"location":"recaps/202509/#reflections-beyond-just-tech","title":"Reflections \u2013 Beyond Just Tech","text":"<p>Soft-skill insights or workflow/communication/process reflections:</p> <ul> <li> Realized that ____</li> <li> Noticed that ____</li> <li> Started ____</li> </ul>"},{"location":"recaps/202509/#what-i-attended","title":"What I Attended","text":"<p>Events, webinars, meetups, or conferences I attended:</p> <ul> <li> Lorem ipsum dolor sit amet</li> <li> Lorem ipsum dolor sit amet</li> <li> Lorem ipsum dolor sit amet</li> </ul>"},{"location":"recaps/202509/#what-i-consumed","title":"What I Consumed","text":"<p>A list of articles, papers, courses, or videos I read/watched/completed:</p>"},{"location":"recaps/202509/#trino","title":"Trino","text":"<ul> <li> Running Trino as exabyte-scale data warehouse</li> <li> Many clusters and only one gateway - Starburst, Naver, and Bloomberg at Trino Summit 2023</li> <li> Visualizing Trino with Superset - Preset at Trino Summit 2023</li> <li> Trino workload management - Airbnb at Trino Summit 2023</li> </ul>"},{"location":"recaps/202509/#iceberg","title":"Iceberg","text":"<ul> <li> Best practices and insights when migrating to Apache Iceberg for data engineers</li> <li> Supercharging Wise's Data Lake with Apache Iceberg</li> <li> Architecting an Iceberg Lakehouse</li> </ul>"},{"location":"recaps/202509/#zzz","title":"ZZZ","text":"<ul> <li> Lorem ipsum dolor sit amet</li> <li> Lorem ipsum dolor sit amet</li> <li> Lorem ipsum dolor sit amet</li> </ul>"},{"location":"recaps/202509/#goals-for-next-month","title":"Goals for Next Month","text":"<p>Set 2\u20133 simple goals to stay focused and accountable:</p> <ul> <li> Explore ____</li> <li> Write and publish ____</li> <li> Try ____</li> </ul>"},{"location":"recaps/template/","title":"Template","text":""},{"location":"recaps/template/#highlight-of-the-month","title":"Highlight of the Month","text":"<p>Summarize my biggest breakthrough, project, or insight in this month:</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>"},{"location":"recaps/template/#what-i-built-published-or-experimented-with","title":"What I Built, Published, or Experimented with","text":"<p>What I built, experimented with, or implemented:</p> <ul> <li> Published ____</li> <li> Built ____</li> <li> Experimented with ____</li> </ul>"},{"location":"recaps/template/#what-i-learned","title":"What I Learned","text":"<p>Short reflections on what I actually learned or became more confident in:</p> <ul> <li> Grasped the pros and cons of ____</li> <li> Recognized how ____</li> <li> Learned ____</li> <li> Understood ____</li> </ul>"},{"location":"recaps/template/#reflections-beyond-just-tech","title":"Reflections \u2013 Beyond Just Tech","text":"<p>Soft-skill insights or workflow/communication/process reflections:</p> <ul> <li> Realized that ____</li> <li> Noticed that ____</li> <li> Started ____</li> </ul>"},{"location":"recaps/template/#what-i-attended","title":"What I Attended","text":"<p>Events, webinars, meetups, or conferences I attended:</p> <ul> <li> Lorem ipsum dolor sit amet</li> <li> Lorem ipsum dolor sit amet</li> <li> Lorem ipsum dolor sit amet</li> </ul>"},{"location":"recaps/template/#what-i-consumed","title":"What I Consumed","text":"<p>A list of articles, papers, courses, or videos I read/watched/completed:</p>"},{"location":"recaps/template/#xxx","title":"XXX","text":"<ul> <li> Lorem ipsum dolor sit amet</li> <li> Lorem ipsum dolor sit amet</li> <li> Lorem ipsum dolor sit amet</li> </ul>"},{"location":"recaps/template/#yyy","title":"YYY","text":"<ul> <li> Lorem ipsum dolor sit amet</li> <li> Lorem ipsum dolor sit amet</li> <li> Lorem ipsum dolor sit amet</li> </ul>"},{"location":"recaps/template/#zzz","title":"ZZZ","text":"<ul> <li> Lorem ipsum dolor sit amet</li> <li> Lorem ipsum dolor sit amet</li> <li> Lorem ipsum dolor sit amet</li> </ul>"},{"location":"recaps/template/#goals-for-next-month","title":"Goals for Next Month","text":"<p>Set 2\u20133 simple goals to stay focused and accountable:</p> <ul> <li> Explore ____</li> <li> Write and publish ____</li> <li> Try ____</li> </ul>"},{"location":"shortcuts/","title":"Shortcuts","text":"<p>Keys and shortcuts for various tools.</p> <p>For how to edit this document, see Keys | PyMarkdown Extension. </p>"},{"location":"shortcuts/#helix","title":"Helix","text":"<p>Vim-like modal editor with multiple selection support and built-in language server integration.</p>"},{"location":"shortcuts/#how-helix-works","title":"How Helix Works","text":"<p>Helix operates through different modes that change how your keyboard input is interpreted:</p> <ul> <li>Normal Mode (default): Your keyboard becomes a command interface. Letters like <code>w</code>, <code>b</code>, <code>d</code> are commands for navigation and text manipulation, not typing</li> <li>Insert Mode: Standard typing mode where letters appear as text on screen</li> <li>Select Mode: Extends text selections as you move the cursor, similar to holding Shift in other editors</li> <li>Command Mode: Execute ex-style commands (like <code>:quit</code>, <code>:save</code>) by typing them out</li> </ul> <p>The key insight: you spend most time in Normal Mode navigating and commanding, briefly entering Insert Mode to type, then immediately returning to Normal Mode. This feels awkward initially but becomes incredibly efficient.</p>"},{"location":"shortcuts/#scenario-1-opening-and-exploring-a-project-normal-mode","title":"Scenario 1: Opening and Exploring a Project (Normal Mode)","text":"<p>Imagine you're starting work on a new codebase. Launch Helix and you'll be in Normal mode - think of this as your command dashboard. Your first task is finding the main file. Press Space, then F - this opens the file picker. You'll see a fuzzy finder where you can type part of a filename. Use J/K to navigate the list, then Enter to open your selection.</p> <p>Let's say you're now looking at <code>main.py</code> but need to check <code>config.py</code> too. Instead of opening another file picker, press Space, then B to see your buffer list - files you already have open. This becomes invaluable when juggling multiple files.</p> <p>The file is 500 lines long - intimidating! But navigation is easy: G, G jumps to line 1 instantly. Shift+G takes you to the very end. For quick scrolling, Ctrl+U jumps up half a screen, Ctrl+D jumps down.</p> <p>Essential shortcuts:</p> <ul> <li>Space, F: Open file picker</li> <li>Space, B: Switch between open buffers  </li> <li>G, G: Jump to start of file</li> <li>Shift+G: Jump to end of file</li> <li>Ctrl+U / Ctrl+D: Page up/down</li> </ul>"},{"location":"shortcuts/#scenario-2-making-your-first-code-change-normal-insert-normal","title":"Scenario 2: Making Your First Code Change (Normal \u2192 Insert \u2192 Normal)","text":"<p>You found a function that needs a new parameter. Place your cursor at the end of the parameter list using arrow keys or H/J/K/L. Now you need to start typing - but you're in Normal mode where letters are commands, not text!</p> <p>To enter Insert mode, press A (append after cursor). Notice your status bar shows \"INSERT\" - now you can type like any regular editor. Add <code>, debug=False</code> to the parameter list.</p> <p>When done typing, press Esc to return to Normal mode. This two-mode dance becomes second nature: Normal for navigation/commands, Insert for actual typing.</p> <p>Essential shortcuts:</p> <ul> <li>I: Insert before cursor</li> <li>A: Insert after cursor</li> <li>O: New line below and insert</li> <li>Shift+O: New line above and insert</li> <li>Esc: Return to Normal mode</li> </ul>"},{"location":"shortcuts/#scenario-3-refactoring-code-normal-mode","title":"Scenario 3: Refactoring Code (Normal Mode)","text":"<p>You spot a poorly named variable <code>x</code> that should be <code>user_count</code>. Place your cursor on any instance of <code>x</code>. In Normal mode, press Space, then R. Helix analyzes your code and highlights ALL occurrences of this variable across your entire project! Type the new name <code>user_count</code> and hit Enter - every instance updates automatically. This is language-server powered renaming.</p> <p>But what if you want to see what this variable does? Place your cursor on <code>user_count</code> and press G, then R. A picker shows every place this variable is referenced. Select one with J/K and Enter to jump there.</p> <p>Essential shortcuts:</p> <ul> <li>Space, R: Rename symbol everywhere</li> <li>G, D: Go to definition</li> <li>G, R: Go to references</li> <li>Space, A: Apply code action</li> </ul>"},{"location":"shortcuts/#scenario-4-debugging-with-diagnostics-normal-mode","title":"Scenario 4: Debugging with Diagnostics (Normal Mode)","text":"<p>Your code has syntax errors - the status bar shows \"2 errors, 1 warning\". Press ], then D to jump to the first diagnostic. Helix positions your cursor exactly on the problematic code and shows the error message.</p> <p>Want to auto-fix it? Press Space, then A for code actions. Helix might offer \"Add missing import\" or \"Fix syntax error\". Select the fix with J/K and Enter.</p> <p>Essential shortcuts:</p> <ul> <li>], D: Next diagnostic</li> <li>[, D: Previous diagnostic</li> <li>Space, A: Show code actions</li> </ul>"},{"location":"shortcuts/#scenario-5-finding-code-across-the-project-normal-mode","title":"Scenario 5: Finding Code Across the Project (Normal Mode)","text":"<p>You remember seeing a function called <code>calculate_tax</code> somewhere but can't find it. Press Space, then S - this opens the symbol picker showing all functions/classes in the current file. Not there? Try Space, then / for global search across your entire project. Type \"calculate_tax\" and Helix searches every file, showing matches with context.</p> <p>If your cursor is already on a word you want to search for, Space, then ? searches for that word everywhere instantly.</p> <p>Essential shortcuts:</p> <ul> <li>Space, S: Symbol picker (current file)</li> <li>Space, /: Global search in project</li> <li>Space, ?: Search word under cursor</li> <li>/: Search in current file</li> <li>N / Shift+N: Next/previous match</li> </ul>"},{"location":"shortcuts/#scenario-6-working-with-multiple-selections-normalselect-mode","title":"Scenario 6: Working with Multiple Selections (Normal/Select Mode)","text":"<p>You have a list of email addresses that all need \".com\" appended. Select one email by pressing W to select the word. Now press V to enter Select mode - this extends your selection as you move with J/K/L.</p> <p>Here's the magic: with text selected, press Alt+C. Helix finds ALL identical selections in your file and gives you multiple cursors! Now type \".com\" and it appears after every email simultaneously. Press ; to collapse back to one cursor.</p> <p>Essential shortcuts:</p> <ul> <li>X: Select current line</li> <li>W: Select word forward</li> <li>V: Enter Select mode (extend selection)</li> <li>Alt+C: Select all matching text</li> <li>;: Collapse to single cursor</li> </ul>"},{"location":"shortcuts/#scenario-7-side-by-side-editing-normal-window-mode","title":"Scenario 7: Side-by-Side Editing (Normal \u2192 Window Mode)","text":"<p>You're comparing two config files. Press Ctrl+W - you're now in Window mode (notice the status change). Press V for vertical split. Now you have two panes! Press Ctrl+W, then H/L to move between them.</p> <p>Open different files in each pane using Space, F as usual. When done with splits, press Ctrl+W, then Q to close the current pane.</p> <p>Essential shortcuts:</p> <ul> <li>Ctrl+W: Enter Window mode</li> <li>V: Vertical split (in Window mode)</li> <li>S: Horizontal split (in Window mode)</li> <li>H/J/K/L: Navigate between panes (in Window mode)</li> <li>Q: Close current pane (in Window mode)</li> </ul>"},{"location":"shortcuts/#micro","title":"Micro","text":"<p>A terminal-based text editor that aims to be easy to use and intuitive.</p> <ul> <li>Ctrl+</li> </ul>"},{"location":"side-projects/","title":"Side Projects","text":"<p>I love exploring new ideas and building things in my spare time. Here, you'll find a collection of side projects that showcase my interests, technical skills, and curiosity beyond work. Each project reflects my hands-on approach to learning and creating.</p> <ul> <li> <p> Data2ML Ops</p> <p>Tools and workflows for bridging data engineering and machine learning.</p> <p>\u2192 Data2ML Ops</p> </li> <li> <p> Retail Lakehouse</p> <p>Building a Data Lakehouse architecture for real-time retail analytics using Debezium, Apache Kafka, Apache Iceberg, AWS Glue and Trino.</p> <p>\u2192 Retail Lakehouse</p> </li> <li> <p> Unified SQL-based Data Pipelines</p> <p>Developing a unified SQL-based data pipeline architecture on BigQuery for cross-cloud data integration and analytics.</p> <p>\u2192 Unified SQL-based Data Pipelines</p> </li> <li> <p> Trending Content Prediction</p> <p>Predicting popular content using machine learning and analytics.</p> <p>\u2192 Trending Content Prediction</p> </li> </ul>"},{"location":"side-projects/sql-based-rag-application/","title":"SQL-based RAG Application","text":""},{"location":"side-projects/sql-based-rag-application/#create-a-bucket-in-gcs-and-upload-a-pdf-file","title":"Create a Bucket in GCS and upload a pdf file","text":""},{"location":"side-projects/sql-based-rag-application/#create-a-object-table-in-bigquery","title":"Create a Object Table in BigQuery","text":"<pre><code>create or replace external table `us_test2.pdf`\nwith connection `us.bg-object-tables`\noptions(\n  object_metadata = 'SIMPLE',\n  uris = ['gs://kcl-us-test/scf23.pdf']\n);\n</code></pre>"},{"location":"side-projects/sql-based-rag-application/#create-a-layout-parser-type-of-processor-in-document-ai-and-a-remote-model-corresponding-to-the-processor","title":"Create a Layout Parser type of Processor in Document AI and a Remote Model corresponding to the processor","text":"<pre><code>create or replace model `us_test2.doc_parser`\nremote with connection `us.document_ai`\noptions(\n  remote_service_type='CLOUD_AI_DOCUMENT_V1',\n  document_processor='ec023753643cb1be'\n);\n</code></pre>"},{"location":"side-projects/sql-based-rag-application/#create-a-embeddings-remote-model-in-bigquery","title":"Create a embeddings Remote Model in BigQuery","text":"<pre><code>create or replace model `us_test2.embedding_model`\nremote with connection `us.vertex_ai`\noptions (\n  endpoint='text-embedding-004'\n)\n</code></pre>"},{"location":"side-projects/sql-based-rag-application/#create-a-generative-text-remote-model-in-bigquery","title":"Create a generative text Remote Model in BigQuery","text":"<pre><code>create or replace model `us_test2.text_model`\nremote with connection `us.vertex_ai`\noptions(\n  endpoint = 'gemini-1.5-flash-002'\n)\n</code></pre>"},{"location":"side-projects/arxiv-analytics-platform/","title":"arXiv Analytics Platform","text":"<p>Project for HackMD Data Engineer 2025</p>"},{"location":"side-projects/data-mesh/","title":"Unified SQL-based Data Pipelines","text":""},{"location":"side-projects/data-mesh/#highlights","title":"\ud83d\udca1 Highlights","text":"<p>Highlights</p> <ul> <li> Built cross-cloud pipelines (AWS + GCP) to process both structured/unstructured data by integrating Object and BigLake Tables.</li> <li> Enabled SQL-based ML inference for annotating images by integrating Remote Models and federated query capabilities.</li> </ul> <p></p> <p>In this project, we demonstrate how to build a unified SQL-based data pipeline that processes both structured and unstructured data across AWS and GCP for both ML and non-ML workloads. We leverage BigQuery's Object Table functionality to access images stored in Google Cloud Storage, and we also use Remote Model functionality to perform image annotation using the Cloud Vision API. Notably, we utilize BigLake Tables to access structured data stored in AWS S3, allowing us to join product information with image annotations seamlessly.</p> <p>The pipeline is designed to be cross-cloud, enabling data teams to work with data stored in different cloud environments without the need for complex data movement or transformation processes. This approach reduces context switching and maintenance costs associated with using multiple languages and tools, as SQL serves as a common language for both data and BI teams.</p>"},{"location":"side-projects/data-mesh/#scenario-e-commerce-company-with-a-cross-cloud-architecture","title":"Scenario: E-commerce Company with a Cross-Cloud Architecture","text":"<p>Imagine you're working at an e-commerce company where different teams have made independent decisions about their tech stack based on their specific needs.</p> <p>On the backend side, the product-related transactional data\u2014such as product names, prices, categories, and units sold\u2014is stored in Amazon RDS. To build a centralized data lake and enable downstream analytics, the data engineering team uses AWS DMS to continuously replicate this structured data into AWS S3.</p> <p>Meanwhile, the media team is responsible for handling product images. For ease of use and integration with existing tools, they chose Google Cloud Storage (GCS) to store all product-related media assets like images and thumbnails. As a result, your organization ends up with product metadata on AWS and product images on GCP.</p> <p>Now, the data team is tasked with analyzing both the structured product information and the unstructured image data to generate insights, such as:</p> <ul> <li>What visual characteristics are common among high-performing products?</li> <li>Can we automatically annotate and categorize product images?</li> <li>How do product descriptions align with image content?</li> </ul> <p>But there's a problem.</p> <p>The company don\u2019t want to maintain multiple pipelines using Python for image annotation and Spark for joining data, nor do they want to transfer large image files between clouds just to run ML models.</p> <p>The company want to keep things simple, scalable, and SQL-driven, so that data analysts and BI teams can contribute directly without learning new tools.</p> <p>That\u2019s where BigQuery becomes your central processing engine:</p> <ul> <li>We can use BigLake Tables to query structured product data directly from AWS S3.</li> <li>We can use Object Tables to access images stored in GCS without copying them.</li> <li>We can integrate Remote Models to call the Cloud Vision API for image annotation, directly within SQL.</li> </ul> <p>This architecture allows your team to process both structured and unstructured data, across AWS and GCP, using just SQL\u2014without building and maintaining separate pipelines or duplicating data across cloud environments.</p>"},{"location":"side-projects/data-mesh/#when-not-to-use-this-architecture","title":"When Not to Use This Architecture?","text":"<p>While a unified SQL-based, cross-cloud pipeline on BigQuery offers simplicity and power, it's not always the right choice. Consider alternatives if:</p> <ul> <li>To avoid vendor lock-in to BigQuery's query engine or proprietary features like Remote Models and Object Tables. While they\u2019re powerful, they tightly couple your pipelines to GCP.</li> <li>BigQuery costs are a concern. If your workloads involve large-scale image annotation or frequent joins over huge datasets, query costs can grow rapidly.</li> <li>Teams prefer full Python-based workflows. If data teams are more comfortable with notebooks, Python SDKs, and want flexibility outside SQL, it might be better to build pipelines in tools like Airflow or Prefect, using Python-native ML libraries.</li> <li>You need real-time processing. This architecture is designed for batch or scheduled workflows. If your use case requires low-latency streaming, a dedicated streaming engine like Apache Flink or Kafka Streams may be more appropriate.</li> </ul>"},{"location":"side-projects/data-mesh/combined/","title":"Analyze Visual Cues That Drive Purchases","text":"<pre><code>with products as (\n  select\n    *\n  from `velano-collective-ac8f.products_aws.products`\n),\nimages_annotated as (\n  select\n    *\n  from ml.annotate_image(\n    model `velano-collective-ac8f.products.vision`,\n    table `velano-collective-ac8f.products.images`,\n    struct(['LABEL_DETECTION'] as vision_features)\n  )\n),\nimages_anntotated_extracted as (\n  select\n    cast(\n      split(\n        split(uri, \"/\")[offset(array_length(split(uri, '/')) - 1)], -- 1.png\n        '.'\n      )[offset(0)] -- 1\n    as int64\n    ) as product_id,\n    array(\n      select as struct\n        json_value(entry, '$.description') as description,\n        cast(json_value(entry, '$.score') as float64) as score\n        cast(json_value(entry, '$.topicality') as float64) as topicality\n      from unnest(json_query_array(ml_annotate_image_result.label_annotations)) as entry\n    ) as label_annotations\n  from images_annotated\n)\n\nselect\n  p.*,\n  i.label_annotations\nfrom products as p\nleft join images_anntotated_extracted as i\non p.id = i.product_id\n</code></pre>"},{"location":"side-projects/data-mesh/images-gcs-object-table/","title":"Annotate Product Images","text":"Annotate product images using BigQuery and Cloud Vision"},{"location":"side-projects/data-mesh/images-gcs-object-table/#create-a-cloud-storage-bucket-and-upload-images","title":"Create a Cloud Storage Bucket and Upload Images","text":"<p>Create a Google Cloud Storage bucket to store product images. The bucket should be in the <code>us</code> region and named <code>velano-collectives-n1y3</code>. You can use the following Terraform code to create the bucket:</p> <pre><code>resource \"google_storage_bucket\" \"velano_collectives\" {\n    name     = \"velano-collectives-${random_string.velano_collectives_suffix.result}\"\n    location = \"US\"\n}\n</code></pre> <p>After creating the bucket, upload your product images to it. You can use the <code>gsutil</code> command-line tool to copy images from your local machine to the Cloud Storage bucket:</p> <pre><code>gsutil cp velano-collectives/data/images/*.png gs://velano-collectives-n1y3/images/\n</code></pre>"},{"location":"side-projects/data-mesh/images-gcs-object-table/#create-a-dataset","title":"Create a Dataset","text":"<pre><code>create schema if not exists products\noptions(\n  location=\"us\"\n)\n</code></pre>"},{"location":"side-projects/data-mesh/images-gcs-object-table/#create-an-object-table","title":"Create an Object Table","text":"<p>Create a BigQuery connection to the Cloud Storage bucket containing the images:</p> <pre><code>resource \"google_bigquery_connection\" \"gcs\" {\n  connection_id = \"gcs\"\n  location      = \"US\"\n  cloud_resource {}\n}\n\nmodule \"bq_conn__gcs__project_iam\" {\n  source  = \"terraform-google-modules/iam/google//modules/projects_iam\"\n  version = \"~&gt; 8.0\"\n\n  projects = [module.project.project_id]\n  mode     = \"additive\"\n\n  bindings = {\n    \"roles/storage.objectViewer\" = [\n      \"serviceAccount:${google_bigquery_connection.gcs.cloud_resource[0].service_account_id}\"\n    ]\n    \"roles/serviceusage.serviceUsageConsumer\" = [\n      \"serviceAccount:${google_bigquery_connection.gcs.cloud_resource[0].service_account_id}\"\n    ]\n    \"roles/documentai.viewer\" = [\n      \"serviceAccount:${google_bigquery_connection.gcs.cloud_resource[0].service_account_id}\"\n    ]\n  }\n}\n</code></pre> <p>The connection's service account must have the following roles:</p> <ul> <li><code>roles/storage.objectViewer</code></li> <li><code>roles/serviceusage.serviceUsageConsumer</code></li> </ul> <p>Create an object table in BigQuery to reference the images stored in the Cloud Storage bucket:</p> <pre><code>create external table `velano-collective-ac8f.products.images`\nwith connection `velano-collective-ac8f.us.gcs`\noptions(\n  object_metadata = 'SIMPLE',\n  uris = ['gs://velano-collectives-n1y3/images/*.png']\n)\n</code></pre> <p></p> <p></p>"},{"location":"side-projects/data-mesh/images-gcs-object-table/#create-a-remote-model","title":"Create a Remote Model","text":"<p>Create a BigQuery connection to the Cloud Vision API:</p> <pre><code>resource \"google_bigquery_connection\" \"cloud_vision\" {\n  connection_id = \"cloud-vision\"\n  location      = \"US\"\n  cloud_resource {}\n}\n</code></pre> <p>After creating the connection, you can create a remote model that uses the Cloud Vision API to annotate images:</p> <pre><code>create or replace model `velano-collective-ac8f.products.vision`\nremote with connection `velano-collective-ac8f.us.cloud-vision`\noptions(\n    remote_service_type='CLOUD_AI_VISION_V1'\n)\n</code></pre> <p></p>"},{"location":"side-projects/data-mesh/images-gcs-object-table/#annotate-images","title":"Annotate images","text":"<p>When you have the object table and remote model set up, you can use the <code>ML.ANNOTATE_IMAGE</code> function to annotate the images. This function allows you to specify which vision features you want to extract from the images.</p> <pre><code>select\n  *\nfrom ml.annotate_image(\n  model `velano-collective-ac8f.products.vision`,\n  table `velano-collective-ac8f.products.images`,\n  struct(['LABEL_DETECTION'] AS vision_features)\n)\n</code></pre> <p></p>"},{"location":"side-projects/data-mesh/images-gcs-object-table/#references","title":"References","text":"<ul> <li>Annotate images with the <code>ML.ANNOTATE_IMAGE</code> function</li> <li>Create object tables</li> <li>The ML.ANNOTATE_IMAGE function</li> </ul>"},{"location":"side-projects/data-mesh/products-s3-biglake-table/","title":"Amazon S3 BigLake External Table","text":"Cross-cloud Data Pipelines with Amazon S3 BigLake"},{"location":"side-projects/data-mesh/products-s3-biglake-table/#create-amazon-s3-bucket-and-upload-data","title":"Create Amazon S3 Bucket and Upload Data","text":"<p>Create an Amazon S3 bucket in the <code>us-west-2</code> to store the data that you want to query with BigQuery. </p> aws-s3.tf<pre><code>resource \"random_string\" \"velano_collectives_suffix\" {\n  length  = 4\n  special = false\n  numeric = true\n  lower   = true\n  upper   = false\n}\n\nmodule \"aws_s3_bucket_velano_collectives\" {\n  source  = \"terraform-aws-modules/s3-bucket/aws\"\n  version = \"~&gt; 4.10.1\"\n\n  bucket = \"velano-collectives-${random_string.velano_collectives_suffix.result}\"\n}\n\nresource \"aws_s3_object\" \"products\" {\n  bucket = module.aws_s3_bucket_velano_collectives.s3_bucket_id\n  key    = \"products.csv\"\n  source = \"../data/products.csv\"\n}\n</code></pre> <p><code>products.csv</code> is a sample data file that you can upload to the S3 bucket.</p>"},{"location":"side-projects/data-mesh/products-s3-biglake-table/#connect-to-amazon-s3","title":"Connect to Amazon S3","text":"<p>In order to connect BigQuery to Amazon S3, you need to create a BigQuery connection that allows BigQuery to access data stored in S3. This involves creating an AWS IAM policy and role, configuring a trust relationship, and setting up the BigQuery connection.</p> <p>Here are the steps to follow:</p> <ol> <li>Create an AWS IAM Policy and Role for BigQuery</li> <li>Create a BigQuery connection</li> <li>Add a trust relationship to the AWS role<ul> <li>Add a trust policy to the AWS role</li> <li>Configure a custom AWS identity provider</li> </ul> </li> </ol> <p>See Connect to Amazon S3 for more details.</p> <p>After completing these steps, the AWS IAM policy and role would look like this:</p> aws-iam.tf<pre><code>resource \"aws_iam_policy\" \"bigquery\" {\n  name = \"bigquery-policy\"\n\n  policy = &lt;&lt;-EOF\n            {\n              \"Version\": \"2012-10-17\",\n              \"Statement\": [\n                  {\n                      \"Sid\": \"BucketLevelAccess\",\n                      \"Effect\": \"Allow\",\n                      \"Action\": [\"s3:ListBucket\"],\n                      \"Resource\": [\"${module.aws_s3_bucket_velano_collectives.s3_bucket_arn}\"]\n                  },\n                  {\n                      \"Sid\": \"ObjectLevelAccess\",\n                      \"Effect\": \"Allow\",\n                      \"Action\": [\"s3:GetObject\",\"s3:PutObject\"],\n                      \"Resource\": [\n                          \"${module.aws_s3_bucket_velano_collectives.s3_bucket_arn}\",\n                          \"${module.aws_s3_bucket_velano_collectives.s3_bucket_arn}/*\"\n                      ]\n                  }\n              ]\n            }\n            EOF\n}\n\nresource \"aws_iam_role\" \"bigquery\" {\n  name                 = \"bigquery-role\"\n  max_session_duration = 43200\n\n  assume_role_policy = &lt;&lt;-EOF\n    {\n      \"Version\": \"2012-10-17\",\n      \"Statement\": [\n        {\n          \"Effect\": \"Allow\",\n          \"Principal\": {\n            \"Federated\": \"accounts.google.com\"\n          },\n          \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n          \"Condition\": {\n            \"StringEquals\": {\n              \"accounts.google.com:sub\": \"${google_bigquery_connection.aws.aws[0].access_role[0].identity}\"\n            }\n          }\n        }\n      ]\n    }\n    EOF\n}\n\nresource \"aws_iam_role_policy_attachment\" \"main\" {\n  role       = aws_iam_role.bigquery.name\n  policy_arn = aws_iam_policy.bigquery.arn\n}\n</code></pre> <p>And the BigQuery connection would look like this:</p> gcp-bq-conns.tf<pre><code>resource \"google_bigquery_connection\" \"aws\" {\n  connection_id = \"aws\"\n  friendly_name = \"aws\"\n  description   = \"Created by Terraform\"\n\n  location = \"aws-us-west-2\"\n  aws {\n    access_role {\n      # This must be constructed as a string instead of referencing the\n      # AWS resources directly to avoid a resource dependency cycle\n      # in Terraform.\n      iam_role_id = \"arn:aws:iam::545757050262:role/bigquery-role\"\n    }\n  }\n}\n</code></pre>"},{"location":"side-projects/data-mesh/products-s3-biglake-table/#create-amazon-s3-biglake-tables","title":"Create Amazon S3 BigLake Tables","text":""},{"location":"side-projects/data-mesh/products-s3-biglake-table/#create-a-dataset","title":"Create a Dataset","text":"<pre><code>create schema if not exists products_aws\noptions (\n  location = 'aws-us-west-2'\n)\n</code></pre>"},{"location":"side-projects/data-mesh/products-s3-biglake-table/#create-a-table","title":"Create a Table","text":"<pre><code>create external table products_aws.products\nwith connection `aws-us-west-2.aws`\noptions (\n  format = \"csv\",\n  uris = [\"s3://velano-collectives-n1y3/products.csv\"]\n)\n</code></pre>"},{"location":"side-projects/data-mesh/products-s3-biglake-table/#query-amazon-s3-biglake-tables","title":"Query Amazon S3 BigLake Tables","text":"<p>In order to query the Amazon S3 BigLake table, you need to have the following roles assigned to your user or service account:</p> <ul> <li>BigQuery Connection User (<code>roles/bigquery.connectionUser</code>)</li> <li>BigQuery Data Viewer (<code>roles/bigquery.dataViewer</code>)</li> <li>BigQuery User (<code>roles/bigquery.user</code>)</li> </ul> <p>After you have the necessary permissions, you can query the table to see the data:</p> <pre><code>select\n  *,\n  _FILE_NAME AS file_name\nfrom `velano-collective-ac8f.products_aws.products`\n</code></pre> <p></p>"},{"location":"side-projects/data-mesh/products-s3-biglake-table/#references","title":"References","text":"<ul> <li>Connect to Amazon S3</li> <li>Create Amazon S3 BigLake external tables</li> <li>Query Amazon S3 data</li> </ul>"},{"location":"side-projects/data-mesh/rag/","title":"RAG Applications","text":"<ul> <li>Parse PDFs in a retrieval-augmented generation pipeline</li> <li>Perform semantic search and retrieval-augmented generation</li> </ul>"},{"location":"side-projects/data2ml-ops/","title":"Fraud Detection: from DataOps to MLOps","text":"","tags":["dbt","Feast","Ray","MLflow","KServe","MLOps","Kubernetes"]},{"location":"side-projects/data2ml-ops/#highlights","title":"\ud83d\udca1 Highlights","text":"Data &amp; FeaturesAutoMLOrchestration &amp; InfrastructureExperiment TrackingReal-Time Inference <p>Highlights</p> <ul> <li> Built modular, testable SQL pipelines with dbt, enabling reproducible and version-controlled feature generation</li> <li> Registered features to Feast (open source feature store) for consistent usage in both batch training and real-time serving</li> <li> Enabled feature backfilling and time-travel queries, supporting point-in-time correctness for fraud detection models</li> </ul> <p>Highlights</p> <ul> <li> Performed distributed Bayesian hyperparameter optimization using Ray Tune + Optuna, accelerating tuning efficiency at scale</li> <li> Handled imbalanced datasets using imbalanced-learn to dynamically apply over- and under-sampling strategies, improving model prediction performance</li> <li> Ensured reproducibility by tracking fixed random seeds, stratified sampling, and consistent data splits across trials</li> </ul> <p>Highlights</p> <ul> <li> Deployed the entire pipeline on Kubernetes, enabling scalable, containerized execution of distributed services</li> <li> (WIP) Orchestrated pipeline stages with Airflow, improving automation, observability, and task dependency management</li> <li> Integrated MinIO (S3-compatible) storage for storing intermediate features and trained models across components</li> </ul> <p>Highlights</p> <ul> <li> Integrated MLflow to auto-log training parameters, metrics, and artifacts, enabling experiment reproducibility and traceability</li> <li> Versioned models and experiments using MLflow\u2019s tracking server, enabling full auditability and rollback</li> <li> Stored model artifacts in remote object storage (MinIO), making them accessible for downstream deployment</li> </ul> <p>Highlights</p> <ul> <li> Deployed models as gRPC and REST endpoints using KServe, supporting diverse integration requirements</li> <li> Ensured compatibility between training-time and serving-time features via Feast\u2019s online store integration</li> <li> Enabled autoscaling and scale-to-zero, optimizing cost for infrequently used models</li> <li> Configured A/B testing traffic split, allowing controlled experimentation in production deployments</li> </ul> <p>In real-world machine learning projects, managing the workflow from data transformation to model deployment is often fragmented, error-prone, and hard to scale.</p> <p>This project demonstrates how to streamline and automate the entire lifecycle\u2014from feature engineering to hyperparameter tuning, model tracking, and deployment\u2014using modern open source tools and running fully on Kubernetes.</p> <p>The use case for this project is fraud detection, a high-impact and time-sensitive problem where real-time inference is critical. It serves as a practical demo of how to operationalize machine learning pipelines that are version-controlled, reproducible, and ready for production.</p> <p>It\u2019s designed to be:</p> <ul> <li> Reproducible \u2013 All data transformations, features, and models are versioned via dbt, Feast, and MLflow</li> <li> Scalable \u2013 Built on Kubernetes, enabling distributed training and resource orchestration across services</li> <li> Modular \u2013 Each stage is decoupled and replaceable, promoting clear responsibility and reuse</li> <li> Open Source \u2013 Fully built on open source tools like dbt, Feast, Ray, Optuna, MLflow, and KServe</li> <li> Portable \u2013 Easily adapted to other use cases beyond fraud detection</li> </ul> <p>Whether you're a data engineer, ML practitioner, or platform builder, this project offers a clear, working example of how to bridge DataOps and MLOps on a scalable, production-ready foundation.</p>","tags":["dbt","Feast","Ray","MLflow","KServe","MLOps","Kubernetes"]},{"location":"side-projects/data2ml-ops/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"Architecture (Click to Enlarge) dbtSQLMeshFeastAirflowRayMLflowKServe <p>Highlights</p> <ul> <li> Developed incremental models to process data in minibatches, improving pipeline efficiency and reducing compute cost</li> <li> Implemented test coverage and schema validation, ensuring data quality across transformations</li> <li> Generated documentation automatically from dbt models, enhancing maintainability and team collaboration</li> </ul> <p>Highlights</p> <p>Work In Progress</p> <p>Highlights</p> <ul> <li> Materialized online features to Redis, enabling real-time feature retrieval for low-latency inference</li> <li> Supported both batch and online inference by separating offline and online stores</li> <li> Enabled time-travel and point-in-time feature retrieval, ensuring training-serving consistency for fraud detection</li> </ul> <p>Highlights</p> <p>Work In Progress</p> <p>Highlights</p> <ul> <li> Performed distributed Bayesian hyperparameter tuning using Ray Tune and Optuna, accelerating model search and training time</li> <li> Integrated imbalanced-learn to automatically select appropriate over- and under-sampling strategies, improving performance on imbalanced datasets</li> <li> Scaled training across nodes on Kubernetes, leveraging Ray cluster for efficient resource utilization</li> </ul> <p>Highlights</p> <ul> <li> Integrated MLflow to auto-log parameters, metrics, and artifacts during training, enabling experiment tracking and auditability</li> <li> Logged final model as a versioned artifact, facilitating reproducible deployment and rollback</li> <li> Enabled reproducibility across environments by centralizing tracking and storage in a MinIO-based S3-compatible backend</li> </ul> <p>Highlights</p> <ul> <li> Deployed models as gRPC and REST endpoints using KServe, supporting diverse integration requirements</li> <li> Ensured compatibility between training-time and serving-time features via Feast\u2019s online store integration</li> <li> Enabled autoscaling and scale-to-zero, optimizing cost for infrequently used models</li> <li> Configured A/B testing traffic split, allowing controlled experimentation in production deployments</li> </ul>","tags":["dbt","Feast","Ray","MLflow","KServe","MLOps","Kubernetes"]},{"location":"side-projects/data2ml-ops/#whats-inside","title":"\ud83d\uddc2\ufe0f What's Inside?","text":"<pre><code>.\n\u251c\u2500\u2500 dbt/       - Transform raw data into feature tables\n\u251c\u2500\u2500 sqlmesh/   - (Work In Progress)\n\u251c\u2500\u2500 feast/     - Define and manage features with Feast\n\u251c\u2500\u2500 airflow/   - (Work In Progress)\n\u251c\u2500\u2500 ray/       - Run distributed hyperparameter tuning with Ray and Optuna\n\u251c\u2500\u2500 mlflow/    - Track experiments and log models with MLflow\n\u251c\u2500\u2500 kserve/    - Deploy trained models using KServe\n\u251c\u2500\u2500 minio/     - Configure MinIO (S3-compatible) for model/data storage\n</code></pre>","tags":["dbt","Feast","Ray","MLflow","KServe","MLOps","Kubernetes"]},{"location":"side-projects/data2ml-ops/courese-notes/","title":"Courese notes","text":""},{"location":"side-projects/data2ml-ops/courese-notes/#course-notes","title":"Course Notes","text":"<pre><code>gcloud iam service-accounts create SERVICE_ACCOUNT_NAME \\\n  --description=\"DESCRIPTION\" \\\n  --display-name=\"DISPLAY_NAME\"\n\ngcloud projects add-iam-policy-binding udemy-mlops \\\n    --member=serviceAccount:vertexai-sa@udemy-mlops.iam.gserviceaccount.com \\\n    --role=roles/aiplatform.customCodeServiceAgent\n\ngcloud projects add-iam-policy-binding udemy-mlops \\\n    --member=serviceAccount:vertexai-sa@udemy-mlops.iam.gserviceaccount.com \\\n    --role=roles/aiplatform.admin\n\ngcloud projects add-iam-policy-binding udemy-mlops \\\n    --member=serviceAccount:vertexai-sa@udemy-mlops.iam.gserviceaccount.com \\\n    --role=roles/storage.objectAdmin\n</code></pre>"},{"location":"side-projects/data2ml-ops/courese-notes/#hands-on-vertex-ai-custom-training-gcloud-cli","title":"hands-on Vertex AI Custom Training gcloud cli","text":"<ol> <li>Create a service account and grant the necessary permissions</li> </ol> <pre><code>gcloud iam service-accounts create SERVICE_ACCOUNT_NAME \\\n  --description=\"DESCRIPTION\" \\\n  --display-name=\"DISPLAY_NAME\"\n\ngcloud projects add-iam-policy-binding udemy-mlops \\\n    --member=serviceAccount:vertexai-sa@udemy-mlops.iam.gserviceaccount.com \\\n    --role=roles/aiplatform.customCodeServiceAgent\n\ngcloud projects add-iam-policy-binding udemy-mlops \\\n    --member=serviceAccount:vertexai-sa@udemy-mlops.iam.gserviceaccount.com \\\n    --role=roles/aiplatform.admin\n\ngcloud projects add-iam-policy-binding udemy-mlops \\\n    --member=serviceAccount:vertexai-sa@udemy-mlops.iam.gserviceaccount.com \\\n    --role=roles/storage.objectAdmin\n</code></pre> <ol> <li>Build the image locally</li> </ol> <pre><code>docker build -t vertex-bikeshare-model .\n</code></pre> <ol> <li>Tag the image locally</li> </ol> <pre><code>docker tag vertex-bikeshare-model gcr.io/udemy-mlops/vertex-bikeshare-model\n</code></pre> <ol> <li>Push the image to GCR</li> </ol> <pre><code>docker push gcr.io/udemy-mlops/vertex-bikeshare-model\n</code></pre> <ol> <li>Submit a custom model training job using gcloud</li> </ol> <pre><code>gcloud ai custom-jobs create --region=us-central1 \\\n--project=udemy-mlops \\\n--worker-pool-spec=replica-count=1,machine-type='n1-standard-4',container-image-uri='gcr.io/udemy-mlops/vertex-bikeshare-model' \\\n--service-account=SERVICE_ACCOUNT\n--display-name=bike-sharing-model-training\n</code></pre>"},{"location":"side-projects/data2ml-ops/courese-notes/#section-5","title":"Section 5","text":"<ol> <li>Create bucket</li> <li>Upload dataset</li> <li>Enable Secret Manager API in order to create Cloud Builds v2 Repositories</li> <li>Create Cloud Build v2 Repositories connection and link repo</li> <li>Create Cloud Build Trigger</li> <li>create github repo</li> <li></li> </ol> <pre><code># Assign Service account user role to the service account \ngcloud projects add-iam-policy-binding udemy-mlops \\\n--member=serviceAccount:1090925531874@cloudbuild.gserviceaccount.com --role=roles/aiplatform.admin\n</code></pre> <p>Cloud Build</p> <ol> <li>Build Docker Image</li> <li>Push Docker Image To GCR</li> <li>Execute Tests</li> <li>Submit Custom Training Job and wait until succedded</li> <li>Upload Model to Model Registry</li> <li>Fetch Model ID</li> <li>Create Endpoint</li> <li>Deploy Model Endpoint</li> </ol>"},{"location":"side-projects/data2ml-ops/courese-notes/#model_training_codepy","title":"model_training_code.py","text":"<ol> <li>load data</li> <li>preprocess data (rename columns, drop columns, one-hot-encodings)</li> <li>train test split</li> <li>train model</li> <li>dump model(\"gs://sid-vertex-mlops/bike-share-rf-regression-artifact/model.joblib\")</li> </ol>"},{"location":"side-projects/data2ml-ops/courese-notes/#section-6-kubefloe-pipeline","title":"Section 6 Kubefloe Pipeline","text":"<ul> <li>Upload in-vehicle coupon recommendation file</li> <li>kubeflow <code>@component</code><ul> <li><code>validate_input_ds()</code></li> <li><code>custom_training_job_component()</code></li> </ul> </li> <li>kubeflow <code>@dsl.pipeline</code><ul> <li><code>pipeline()</code></li> </ul> </li> <li><code>compiler()</code> -&gt; compile.json</li> <li> <p><code>from google.cloud.aiplatform import pipeline_jobs</code></p> <ul> <li><code>pipeline_jobs.PipelineJob().run()</code></li> </ul> </li> <li> <p><code>kfp.dsl.Metrics</code>: An artifact for storing key-value scalar metrics.</p> </li> <li><code>kfp.dsl.Output</code></li> </ul>"},{"location":"side-projects/data2ml-ops/prerequisites/","title":"Prerequisites","text":"<p>Before getting started, ensure your environment is properly set up with the following installations:</p> <ul> <li>Homebrew</li> <li>Minikube</li> <li>Docker Desktop</li> <li>kubectl</li> </ul> <pre><code>brew --version\n\nHomebrew 4.5.1\n</code></pre> <pre><code>minikube version\n\nminikube version: v1.33.1\ncommit: 248d1ec5b3f9be5569977749a725f47b018078ff\n</code></pre> <pre><code>docker version\n\nClient:\n Version:           28.1.1\n API version:       1.49\n Go version:        go1.23.8\n Git commit:        4eba377\n Built:             Fri Apr 18 09:49:45 2025\n OS/Arch:           darwin/arm64\n Context:           desktop-linux\n\nServer: Docker Desktop 4.41.1 (191279)\n Engine:\n  Version:          28.1.1\n  API version:      1.49 (minimum version 1.24)\n  Go version:       go1.23.8\n  Git commit:       01f442b\n  Built:            Fri Apr 18 09:52:08 2025\n  OS/Arch:          linux/arm64\n  Experimental:     false\n containerd:\n  Version:          1.7.27\n  GitCommit:        05044ec0a9a75232cad458027ca83437aae3f4da\n runc:\n  Version:          1.2.5\n  GitCommit:        v1.2.5-0-g59923ef\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n</code></pre> <pre><code>kubectl version\n\nClient Version: v1.30.2\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\nServer Version: v1.30.0\n</code></pre> <p>This series of articles provides detailed explanations and step-by-step guides. If you prefer to focus on the practical implementation, follow the sequence below to build the architecture step by step.</p> <ul> <li>Modeling Data</li> <li>Modeling Features</li> <li>Deploy Feast on Kubernetes</li> <li>Deploy MinIO on Kubernetes</li> <li>Deploy MLflow on Kubernetes</li> <li>Deploy Ray Cluster on Kubernetes Using KubeRay</li> <li>Integrate Ray Tune with Optuna, Imblearn, MLflow and MinIO</li> <li>Submit a Ray Tune Job to Your Ray Cluster</li> <li>Install KServe in Serverless Mode</li> <li>Feast Transformer</li> <li>Deploy Your Model on Kubernetes Using Kserve InferenceService</li> </ul>"},{"location":"side-projects/data2ml-ops/dbt/how-it-works/","title":"How It Works?","text":"<p>__</p>"},{"location":"side-projects/data2ml-ops/dbt/how-it-works/#core-concepts","title":"Core Concepts","text":""},{"location":"side-projects/data2ml-ops/dbt/how-it-works/#architecture-components","title":"Architecture Components","text":""},{"location":"side-projects/data2ml-ops/dbt/how-it-works/#key-modules-and-their-roles","title":"Key Modules and Their Roles","text":""},{"location":"side-projects/data2ml-ops/dbt/how-it-works/#behind-the-scenes","title":"Behind the Scenes","text":""},{"location":"side-projects/data2ml-ops/dbt/in-the-bigger-picture/","title":"In the Bigger Picture","text":""},{"location":"side-projects/data2ml-ops/dbt/in-the-bigger-picture/#history-current-status","title":"History &amp; Current Status","text":"<p>dbt (data build tool) is an open-source command-line tool that enables data analysts and engineers to transform data within their warehouses using SQL. Developed by dbt Labs (formerly Fishtown Analytics), the project was initiated in 2016 at RJMetrics and open-sourced in 2018. dbt focuses on the transformation component of the ELT (Extract, Load, Transform) process, allowing users to write modular, version-controlled SQL queries that can be tested and documented. It supports various data warehouses, including Snowflake, BigQuery, and Redshift.  \ufffc \ufffc \ufffc</p> <p>As of May 2025, dbt has over 10,800 stars on GitHub, with more than 325 contributors.  The dbt Community has grown to over 80,000 members, featuring active Slack channels, community forums, and regular meetups in 26 countries.  dbt is utilized by over 25,000 companies, including Microsoft, JetBlue, GitLab, and Sequoia Capital, highlighting its widespread adoption in the industry.  </p>"},{"location":"side-projects/data2ml-ops/dbt/in-the-bigger-picture/#alternatives","title":"Alternatives","text":""},{"location":"side-projects/data2ml-ops/dbt/in-the-bigger-picture/#integration-points","title":"Integration Points","text":""},{"location":"side-projects/data2ml-ops/dbt/in-the-bigger-picture/#kubernetes","title":"Kubernetes","text":""},{"location":"side-projects/data2ml-ops/dbt/in-the-bigger-picture/#cicd","title":"CI/CD","text":""},{"location":"side-projects/data2ml-ops/dbt/modeling-data/","title":"Modeling Data","text":""},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#installation","title":"Installation","text":"<p>Install with pip | dbt Docs</p> <pre><code>python3.10 -m venv dbt\n</code></pre> <pre><code>source ~/.venvs/dbt/bin/activate\n</code></pre> requirements.txt<pre><code>dbt-core==1.9.4\ndbt-bigquery==1.9.1\nsqlfluff==3.4.0\n</code></pre> <pre><code>pip install -r requirements.txt\n</code></pre> <pre><code>dbt --version\nCore:\n  - installed: 1.9.4\n  - latest:    1.9.4 - Up to date!\n\nPlugins:\n  - bigquery: 1.9.1 - Up to date!\n</code></pre>"},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#sources","title":"Sources","text":"_sources.yml<pre><code>version: 2\n\nsources:\n  - name: feast\n    description: &gt;\n      This source contains the raw data for the fraud detection project. It includes \n      transaction records, user account features, and labels indicating whether a \n      transaction was fraudulent.\n    project: feast-oss\n    dataset: fraud_tutorial\n    meta:\n      domain: fraud\n      owner: \"@kclai\"\n    tags:\n      - fraud\n      - pii\n    tables:\n      - name: transactions\n      - name: user_account_features\n      - name: user_has_fraudulent_transactions\n</code></pre>"},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#staging","title":"Staging","text":"<ul> <li>naming convention: <code>stg__&lt;src&gt;__&lt;tbl&gt;</code></li> </ul>"},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#stg__feast__transactions","title":"stg__feast__transactions","text":"stg__feast__transactions.sql<pre><code>select\n  src_account as src_account_id,\n  dest_account as dest_account_id,\n  amount,\n  case\n    when is_fraud = 1 then true\n    when is_fraud = 0 then false\n    else null\n  end as is_fraud,\n  timestamp as created_at\nfrom {{ source('feast', 'transactions') }}\n</code></pre> stg__feast__transactions.yml<pre><code>version: 2\n\nmodels:\n  - name: stg__feast__transactions\n    description: &gt;\n      This staging model standardizes raw transaction data for downstream fraud analysis. \n      It includes source and destination accounts, transaction amount, timestamp, and \n      a binary fraud label. Used as a foundational layer for detecting fraudulent behavior.\n    config:\n      event_time: created_at\n      contract:\n        enforced: true\n    columns:\n      - name: src_account_id\n        data_type: string\n        description: &gt;\n          The unique identifier of the source account that initiated the transaction.\n\n      - name: dest_account_id\n        data_type: string\n        description: &gt;\n          The unique identifier of the destination account that received the transaction.\n\n      - name: amount\n        data_type: float\n        description: &gt;\n          The monetary amount of the transaction. Assumes a consistent currency.\n\n      - name: is_fraud\n        data_type: boolean\n        description: &gt;\n          A boolean flag indicating whether the transaction was identified as fraudulent.\n          `true` indicates a confirmed fraudulent transaction.\n\n      - name: created_at\n        data_type: timestamp\n        description: &gt;\n          The timestamp when the transaction was created. Used for time-based analysis and \n          feature extraction (e.g., fraud trends over time).\n</code></pre>"},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#stg__feast__acct_fraud_7d","title":"stg__feast__acct_fraud_7d","text":"stg__feast__acct_fraud_7d.sql<pre><code>select\n  user_id as account_id,\n  case\n    when user_has_fraudulent_transactions_7d = 1 then true\n    when user_has_fraudulent_transactions_7d = 0 then false\n    else null\n  end as has_fraud_7d,\n  feature_timestamp\nfrom {{ source('feast', 'user_has_fraudulent_transactions') }}\n</code></pre> stg__feast__acct_fraud_7d.yml<pre><code>version: 2\n\nmodels:\n  - name: stg__feast__acct_fraud_7d\n    description: &gt;\n      This staging model identifies whether a user has been involved in any fraudulent transactions\n      within a specified time window (7 days). It is typically used as a feature in fraud detection\n      pipelines or real-time inference systems.\n    config:\n      contract:\n        enforced: true\n    columns:\n      - name: account_id\n        data_type: string\n        description: &gt;\n          The unique identifier of the user. This is used as the primary key to track fraud status\n          per user.\n\n      - name: has_fraud_7d\n        data_type: boolean\n        description: &gt;\n          A binary flag (true or false) indicating whether the user has had any fraudulent transactions \n          in the past 7 days. Can be used as a feature in ML models.\n\n      - name: feature_timestamp\n        data_type: timestamp\n        description: &gt;\n          The timestamp representing the point-in-time when the feature was calculated. Useful for\n          point-in-time correctness in feature stores or backtesting.\n</code></pre>"},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#stg__feast__acct_profiles","title":"stg__feast__acct_profiles","text":"stg__feast__acct_profiles.sql<pre><code>select\n  user_id as account_id,\n  credit_score,\n  account_age_days,\n  case\n    when user_has_2fa_installed = 1 then true\n    when user_has_2fa_installed = 0 then false\n    else null\n  end as has_2fa_installed,\n  feature_timestamp\nfrom {{ source('feast', 'user_account_features') }}\n</code></pre> stg__feast__acct_profiles.yml<pre><code>version: 2\n\nmodels:\n  - name: stg__feast__acct_profiles\n    description: \"\"\n    config:\n      contract:\n        enforced: true\n    columns:\n      - name: account_id\n        data_type: string\n        description: \"\"\n\n      - name: credit_score\n        data_type: int\n        description: \"\"\n\n      - name: account_age_days\n        data_type: int\n        description: \"\"\n\n      - name: has_2fa_installed\n        data_type: boolean\n        description: \"\"\n\n      - name: feature_timestamp\n        data_type: timestamp\n        description: \"\"\n</code></pre>"},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#intermediate","title":"Intermediate","text":""},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#int__feast__acct_num_txns","title":"int__feast__acct_num_txns","text":"int__feast__acct_num_txns.sql<pre><code>{{ config(\n    materialized='incremental',\n    incremental_strategy='microbatch',\n    event_time='transaction_date',\n    begin='2025-04-20',\n    batch_size='day',\n    partition_by={\n      \"field\": \"transaction_date\",\n      \"data_type\": \"date\",\n      \"granularity\": \"day\"\n    },\n    on_schema_change='append_new_columns'\n) }}\n\nwith transactions as (\n\n    -- this ref will be auto-filtered\n    select\n      {{ dbt_utils.star(\n        ref('fraud', 'stg__feast__transactions')\n      ) }}\n    from {{ ref('fraud', 'stg__feast__transactions') }}\n\n)\n\nselect\n  src_account_id as account_id,\n  date(created_at) as transaction_date,\n  count(*) as num_transactions,\n  sum(amount) as total_amount,\n  sum(case when amount &gt; 0 then amount else 0 end) as total_deposits,\nfrom transactions\ngroup by account_id, date(created_at)\n</code></pre>"},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#int__feast__acct_num_txns__extented","title":"int__feast__acct_num_txns__extented","text":"int__feast__acct_num_txns__extented.sql<pre><code>select\n  *,\n  sum(num_transactions) over (\n    partition by account_id\n    order by unix_date(transaction_date)\n    range between 6 preceding\n    and current row\n  ) as num_transactions_7d\nfrom {{ ref('fraud', 'int__feast__acct_num_txns') }}\n</code></pre>"},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#marts","title":"Marts","text":""},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#feat_acct_num_txns_7d","title":"feat_acct_num_txns_7d","text":"feat_acct_num_txns_7d.sql<pre><code>select\n  account_id as entity_id,\n  num_transactions_7d,\n  cast(transaction_date as timestamp) as feature_timestamp\nfrom {{ ref(\"fraud\", \"int__feast__acct_num_txns__extented\") }}\n</code></pre>"},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#feat_acct_fraud_7d","title":"feat_acct_fraud_7d","text":"feat_acct_fraud_7d.sql<pre><code>select\n  account_id as entity_id,\n  has_fraud_7d,\n  feature_timestamp\nfrom {{ ref('fraud', 'stg__feast__acct_fraud_7d') }}\n</code></pre>"},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#feat_acct_profiles","title":"feat_acct_profiles","text":"feat_acct_profiles.sql<pre><code>select\n  account_id as entity_id,\n  credit_score,\n  account_age_days,\n  has_2fa_installed,\n  feature_timestamp\nfrom {{ ref('fraud', 'stg__feast__acct_profiles') }}\n</code></pre>"},{"location":"side-projects/data2ml-ops/dbt/modeling-data/#exposures","title":"Exposures","text":"exposures.yml<pre><code>version: 2\n\nexposures:\n  - name: account_features\n    description:\n      This exposure contains features related to account activity and fraud detection.\n    type: ml\n    maturity: medium\n    owner:\n      name: KC Lai\n      email: kuanchoulai10@gmail.com\n    label: Account-related features\n    url: https://example.com\n    depends_on:\n      - ref('feat_acct_fraud_7d')\n      - ref('feat_acct_num_txns_7d')\n      - ref('feat_acct_profiles')\n</code></pre>"},{"location":"side-projects/data2ml-ops/dbt/what-why-when/","title":"What, Why, When","text":""},{"location":"side-projects/data2ml-ops/dbt/what-why-when/#what-is-x","title":"What is X?","text":""},{"location":"side-projects/data2ml-ops/dbt/what-why-when/#why-x","title":"Why X?","text":"<ul> <li>Data Engineer</li> <li>Data Analyst</li> <li>Data Scientist</li> <li>Machine Learning Engineer</li> </ul>"},{"location":"side-projects/data2ml-ops/dbt/what-why-when/#when-to-use-x","title":"When to Use X?","text":""},{"location":"side-projects/data2ml-ops/feast/deployment/","title":"Deploy Feast on Kubernetes","text":"<ul> <li>Offline Store: BigQuery</li> <li>Registry: postgresql</li> <li>Online Store: Redis</li> </ul> Architecture (Click to Enlarge) <pre><code>minikube start --cpus=4 --memory=7000 --driver=docker\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/deployment/#build-feast-docker-image-locally","title":"Build Feast Docker Image Locally","text":"<p>go to <code>feast/</code> folder. \u5c31\u6703\u770b\u5230\u4ee5\u4e0b\u6a94\u6848</p> Dockerfile<pre><code># Use Python 3.10 slim as the base image\nFROM python:3.10-slim\n\n# Set the working directory\nWORKDIR /app\n\n# Install Python dependencies\nCOPY requirements.txt /app/requirements.txt\nRUN pip install --no-cache-dir -r requirements.txt\nCOPY entrypoint.sh /app/entrypoint.sh\nRUN chmod +x entrypoint.sh\n\nCOPY feature_store.yaml /app/feature_store.yaml\nCOPY account_features.py /app/account_features.py\n\n# Set the entrypoint to run Feast\nENTRYPOINT [\"./entrypoint.sh\"]\n</code></pre> feature_store.yaml<pre><code>project: test\nregistry: # https://docs.feast.dev/reference/registries/sql\n  registry_type: sql\n  path: postgresql+psycopg://postgres:password@registry.feast.svc.cluster.local:5432\n  cache_ttl_seconds: 60\n  sqlalchemy_config_kwargs:\n    echo: false\n    pool_pre_ping: true\nentity_key_serialization_version: 3\nprovider: local \nonline_store: # https://docs.feast.dev/reference/online-stores/redis\n  type: redis\n  connection_string: \"online-store.feast.svc.cluster.local:6379\"\noffline_store: # https://docs.feast.dev/reference/offline-stores/bigquery\n  type: bigquery\n  dataset: feast\n  project_id: mlops-437709\n  billing_project_id: mlops-437709\n  location: US\n</code></pre> entrypoint.sh<pre><code>#!/bin/bash\nset -e\n\necho \"Running feast apply...\"\nfeast apply\n\necho \"Starting feast server...\"\nexec feast serve --host 0.0.0.0 --port 8080\n</code></pre> <p>Build image</p> <pre><code>docker buildx build \\\n  --platform linux/amd64 \\\n  -t feast:v0.1.0 \\\n  -t feast:latest \\\n  .\n</code></pre> <p>\u6211\u662f\u4f7f\u7528MacOS\uff0c\u6211\u5011\u4f7f\u7528<code>docker buildx</code>\uff0cfor amd64\u67b6\u69cb</p> <p>\u78ba\u8a8d\u662f\u5426\u5efa\u7f6e\u6210\u529f</p> <pre><code>docker images --filter=reference=\"feast*\"\n</code></pre> <pre><code>REPOSITORY   TAG       IMAGE ID       CREATED          SIZE\nfeast        v0.1.0    436abcf371e9   55 seconds ago   596MB\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/deployment/#load-image-into-minikube-cluster","title":"Load Image into Minikube Cluster","text":"<p>\u5728Local\u9032\u5165minikube</p> <pre><code>minikube ssh\n</code></pre> <p>\u67e5\u770b\u76ee\u524dminikube\u88e1\u6709\u7684images\uff0c\u6c92\u6709feast <pre><code>docker@minikube:~$ docker images\n</code></pre></p> <p>Expected Output</p> <pre><code>REPOSITORY                                TAG        IMAGE ID       CREATED         SIZE\nregistry.k8s.io/kube-apiserver            v1.30.0    181f57fd3cdb   12 months ago   112MB\nregistry.k8s.io/kube-controller-manager   v1.30.0    68feac521c0f   12 months ago   107MB\nregistry.k8s.io/kube-proxy                v1.30.0    cb7eac0b42cc   12 months ago   87.9MB\nregistry.k8s.io/kube-scheduler            v1.30.0    547adae34140   12 months ago   60.5MB\nregistry.k8s.io/etcd                      3.5.12-0   014faa467e29   15 months ago   139MB\nregistry.k8s.io/coredns/coredns           v1.11.1    2437cf762177   21 months ago   57.4MB\nregistry.k8s.io/pause                     3.9        829e9de338bd   2 years ago     514kB\ngcr.io/k8s-minikube/storage-provisioner   v5         ba04bb24b957   4 years ago     29MB\ndocker@minikube:~$ \n</code></pre> <p>\u5728local\u958b\u555f\u53e6\u500bterminal\uff0c\u5c07local\u7684docker image\u8f09\u5165\u5230minikube cluster\u88e1</p> <pre><code>minikube image load feast:v0.1.0\n</code></pre> <p>\u56de\u5230minikube\u88e1\uff0c\u67e5\u770bimages\uff0c\u6709feast\u4e86</p> <pre><code>docker@minikube:~$ docker images\n</code></pre> <p>Expected Output</p> <pre><code>REPOSITORY                                TAG        IMAGE ID       CREATED          SIZE\nfeast                                     v0.1.0     436abcf371e9   33 minutes ago   596MB\nregistry.k8s.io/kube-apiserver            v1.30.0    181f57fd3cdb   12 months ago    112MB\nregistry.k8s.io/kube-controller-manager   v1.30.0    68feac521c0f   12 months ago    107MB\nregistry.k8s.io/kube-scheduler            v1.30.0    547adae34140   12 months ago    60.5MB\nregistry.k8s.io/kube-proxy                v1.30.0    cb7eac0b42cc   12 months ago    87.9MB\nregistry.k8s.io/etcd                      3.5.12-0   014faa467e29   15 months ago    139MB\nregistry.k8s.io/coredns/coredns           v1.11.1    2437cf762177   21 months ago    57.4MB\nregistry.k8s.io/pause                     3.9        829e9de338bd   2 years ago      514kB\ngcr.io/k8s-minikube/storage-provisioner   v5         ba04bb24b957   4 years ago      29MB\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/deployment/#k8s","title":"K8S","text":""},{"location":"side-projects/data2ml-ops/feast/deployment/#registry","title":"Registry","text":"registry.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: registry\n  namespace: feast\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: registry\n  template:\n    metadata:\n      labels:\n        app: registry\n    spec:\n      containers:\n        - name: registry\n          image: postgres\n          env:\n            - name: POSTGRES_DB\n              value: feast\n            - name: POSTGRES_USER\n              value: postgres\n            - name: POSTGRES_PASSWORD\n              value: password\n          ports:\n            - containerPort: 5432\n          volumeMounts:\n            - name: storage\n              mountPath: /var/lib/postgresql/data\n      volumes:\n        - name: storage\n          hostPath:\n            path: /home/docker/data/feast/registry\n            type: DirectoryOrCreate          \n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: registry\n  namespace: feast\nspec:\n  selector:\n    app: registry\n  type: ClusterIP\n  ports:\n    - port: 5432\n      targetPort: 5432\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/deployment/#online-store","title":"Online Store","text":"online-store.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: online-store\n  namespace: feast\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: online-store\n  template:\n    metadata:\n      labels:\n        app: online-store\n    spec:\n      containers:\n        - name: online-store\n          image: redis\n          ports:\n            - containerPort: 6379\n              protocol: TCP\n          volumeMounts:\n            - name: storage\n              mountPath: /data\n      restartPolicy: Always\n      volumes:\n        - name: storage\n          hostPath:\n            path: /home/docker/data/feast/online-store\n            type: DirectoryOrCreate\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: online-store\n  namespace: feast\nspec:\n  selector:\n    app: online-store\n  type: ClusterIP\n  ports:\n    - port: 6379\n      targetPort: 6379\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/deployment/#secret","title":"Secret","text":""},{"location":"side-projects/data2ml-ops/feast/deployment/#create-gcp-service-account-key","title":"Create GCP Service Account Key","text":"<p>\u78ba\u8a8dProject id</p> <pre><code>gcloud config get-value project\n</code></pre> <p>Expected Output</p> <pre><code>mlops-437709\n</code></pre> <p>\u5efa\u7acbService Account</p> <pre><code>gcloud iam service-accounts create feast-sa \\\n    --display-name \"Feast Service Account\"\n</code></pre> <p>Expected Output</p> <pre><code>Reauthentication required.\nPlease enter your password:\nReauthentication successful.\nCreated service account [feast-sa].\n</code></pre> <p>\u66ffService Account\u52a0\u4e0aBigQuery\u6b0a\u9650</p> <pre><code>gcloud projects add-iam-policy-binding mlops-437709 \\\n    --member=\"serviceAccount:feast-sa@mlops-437709.iam.gserviceaccount.com\" \\\n    --role=\"roles/bigquery.admin\"\n</code></pre> Expected Output <pre><code>[1] EXPRESSION=request.time &lt; timestamp(\"2025-04-09T07:42:05.596Z\"), TITLE=cloudbuild-connection-setup\n[2] None\n[3] Specify a new condition\nThe policy contains bindings with conditions, so specifying a condition is required when adding a binding. Please specify a condition.:  2\n\nUpdated IAM policy for project [mlops-437709].\nbindings:\n- members:\n  - serviceAccount:service-362026176730@gcp-sa-aiplatform-cc.iam.gserviceaccount.com\n  role: roles/aiplatform.customCodeServiceAgent\n- members:\n  - serviceAccount:service-362026176730@gcp-sa-vertex-op.iam.gserviceaccount.com\n  role: roles/aiplatform.onlinePredictionServiceAgent\n- members:\n  - serviceAccount:service-362026176730@gcp-sa-aiplatform.iam.gserviceaccount.com\n  role: roles/aiplatform.serviceAgent\n- members:\n  - serviceAccount:service-362026176730@gcp-sa-artifactregistry.iam.gserviceaccount.com\n  role: roles/artifactregistry.serviceAgent\n- members:\n  - serviceAccount:feast-sa@mlops-437709.iam.gserviceaccount.com\n  role: roles/bigquery.admin\n- members:\n  - serviceAccount:362026176730@cloudbuild.gserviceaccount.com\n  role: roles/cloudbuild.builds.builder\n- members:\n  - serviceAccount:service-362026176730@gcp-sa-cloudbuild.iam.gserviceaccount.com\n  role: roles/cloudbuild.serviceAgent\n- members:\n  - serviceAccount:service-362026176730@containerregistry.iam.gserviceaccount.com\n  role: roles/containerregistry.ServiceAgent\n- members:\n  - serviceAccount:362026176730-compute@developer.gserviceaccount.com\n  role: roles/editor\n- members:\n  - serviceAccount:service-362026176730@gcp-sa-firestore.iam.gserviceaccount.com\n  role: roles/firestore.serviceAgent\n- members:\n  - serviceAccount:362026176730@cloudbuild.gserviceaccount.com\n  role: roles/iam.serviceAccountUser\n- members:\n  - serviceAccount:service-362026176730@cloud-ml.google.com.iam.gserviceaccount.com\n  role: roles/ml.serviceAgent\n- members:\n  - user:edison@kcl10.com\n  role: roles/owner\n- members:\n  - serviceAccount:service-362026176730@gcp-sa-pubsub.iam.gserviceaccount.com\n  role: roles/pubsub.serviceAgent\n- members:\n  - serviceAccount:362026176730@cloudbuild.gserviceaccount.com\n  role: roles/run.admin\n- members:\n  - serviceAccount:service-362026176730@serverless-robot-prod.iam.gserviceaccount.com\n  role: roles/run.serviceAgent\n- condition:\n    expression: request.time &lt; timestamp(\"2025-04-09T07:42:05.596Z\")\n    title: cloudbuild-connection-setup\n  members:\n  - serviceAccount:service-362026176730@gcp-sa-cloudbuild.iam.gserviceaccount.com\n  role: roles/secretmanager.admin\netag: BwY0his6v7Y=\nversion: 3\n</code></pre> <p>\u5efa\u7acbKey</p> <pre><code>gcloud iam service-accounts keys create feast-gcp-key.json \\\n    --iam-account=feast-sa@mlops-437709.iam.gserviceaccount.com\n</code></pre> <p>Expected Output</p> <pre><code>created key [a2609fffff05f5fdf311de233f1a2e1e89288ab5] of type [json] as [feast-gcp-key.json] for [feast-sa@mlops-437709.iam.gserviceaccount.com]\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/deployment/#create-k8s-secret","title":"Create K8S Secret","text":"<pre><code>cat feast-gcp-key.json | base64\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: feast-gcp-key\n  namespace: feast\ntype: Opaque\ndata:\n  key.json: &lt;base64 encoded string&gt;\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/deployment/#online-feature-server","title":"Online Feature Server","text":"online-feature-server.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: online-feature-server\n  namespace: feast\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: online-feature-server\n  template:\n    metadata:\n      labels:\n        app: online-feature-server\n    spec:\n      containers:\n        - name: online-feature-server\n          image: feast:v0.1.8\n          ports:\n            - containerPort: 8080\n          env:\n            - name: FEAST_FEATURE_SERVER_CONFIG_PATH\n              value: /app/config/feature_store.yaml\n            - name: GOOGLE_APPLICATION_CREDENTIALS\n              value: /var/secrets/google/key.json\n          volumeMounts:\n            - name: gcp-sa-key\n              mountPath: /var/secrets/google\n              readOnly: true\n      volumes:\n        - name: gcp-sa-key\n          secret:\n            secretName: gcp-sa-key\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: online-feature-server\n  namespace: feast\nspec:\n  selector:\n    app: online-feature-server\n  type: NodePort\n  ports:\n    - port: 8080\n      targetPort: 8080\n      nodePort: 30000\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/deployment/#deploy-your-feast-architecture","title":"Deploy Your Feast Architecture","text":"install.sh<pre><code>#!/bin/bash\n\necho \"Step 0: Create feast Namespace\"\nkubectl create ns feast\n\necho \"Step 1: Deploy Online Store\"\nkubectl apply -f online-store.yaml\nkubectl rollout status deployment/online-store -n feast\n\necho \"Step 2: Deploy Registry\"\nkubectl apply -f registry.yaml\nkubectl rollout status deployment/registry -n feast\n\necho \"Step 3: Deploy Feast Online Feature Server\"\nkubectl apply -f secret.yaml\nkubectl apply -f online-feature-server.yaml\nkubectl rollout status deployment/online-feature-server -n feast\n</code></pre> <pre><code>./install.sh\n</code></pre> <p>Expected Output</p> <pre><code>Step 0: Create feast Namespace\nnamespace/feast created\n\nStep 1: Deploy Online Store\ndeployment.apps/online-store created\nservice/online-store created\nWaiting for deployment \"online-store\" rollout to finish: 0 of 1 updated replicas are available...\ndeployment \"online-store\" successfully rolled out\n\nStep 2: Deploy Registry\ndeployment.apps/registry created\nservice/registry created\nWaiting for deployment \"registry\" rollout to finish: 0 of 1 updated replicas are available...\ndeployment \"registry\" successfully rolled out\n\nStep 3: Deploy Feast Online Feature Server\nsecret/gcp-sa-key created\ndeployment.apps/online-feature-server created\nservice/online-feature-server created\nWaiting for deployment \"online-feature-server\" rollout to finish: 0 of 1 updated replicas are available...\ndeployment \"online-feature-server\" successfully rolled out\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/deployment/#test-your-feast-online-feature-server","title":"Test Your Feast Online Feature Server","text":"<p>\u9996\u5148\u8981\u5148\u5c07service \u900f\u904e<code>minikube service</code> output\u7d66Local\u4f7f\u7528</p> <pre><code>minikube service online-feature-server -n feast\n</code></pre> <p>Expected Output</p> <pre><code>|-----------|-----------------------|-------------|---------------------------|\n| NAMESPACE |         NAME          | TARGET PORT |            URL            |\n|-----------|-----------------------|-------------|---------------------------|\n| feast     | online-feature-server |        8080 | http://192.168.49.2:30000 |\n|-----------|-----------------------|-------------|---------------------------|\n\ud83c\udfc3  Starting tunnel for service online-feature-server.\n|-----------|-----------------------|-------------|------------------------|\n| NAMESPACE |         NAME          | TARGET PORT |          URL           |\n|-----------|-----------------------|-------------|------------------------|\n| feast     | online-feature-server |             | http://127.0.0.1:52316 |\n|-----------|-----------------------|-------------|------------------------|\n\ud83c\udf89  Opening service feast/online-feature-server in default browser...\n\u2757  Because you are using a Docker driver on darwin, the terminal needs to be open to run it.\n</code></pre> <p><code>http://127.0.0.1:52316</code>\u5c31\u662f\u6211\u5011\u5728localhost\u53ef\u4ee5access\u5f97\u5230\u7684URL\uff0c\u5f85\u6703\u6e2c\u8a66\u6642\u90fd\u6703\u9700\u8981\u6253\u9019\u500bURL</p>"},{"location":"side-projects/data2ml-ops/feast/deployment/#materialize-features-to-online-store","title":"Materialize Features to Online Store","text":""},{"location":"side-projects/data2ml-ops/feast/deployment/#materialize","title":"<code>/materialize</code>","text":"<p>Materialize features within a specified time range</p> <pre><code>curl -X POST http://127.0.0.1:52316/materialize \\\n     -H \"Content-Type: application/json\" \\\n     -d @request-materialize.json\n</code></pre> <p>where <code>request-materialize.json</code> is</p> <pre><code>{\"start_ts\": \"2024-08-07T00:00:00Z\", \"end_ts\": \"2024-08-08T00:00:00Z\"}\n</code></pre> <p>by running <code>kubectl logs pod</code>, you can see the progress of materialization:</p> <p>Expected Output</p> <pre><code>Materializing 3 feature views from 2024-08-07 00:00:00+00:00 to 2024-08-08 00:00:00+00:00 into the redis online store.\n\nacct_fraud_7d:\n0it [00:00, ?it/s]\nacct_profiles:\n0it [00:00, ?it/s]\nacct_num_txns_7d:\n0it [00:00, ?it/s]\n10.244.0.1:39638 - \"POST /materialize HTTP/1.1\" 200\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/deployment/#materialize-incremental","title":"<code>/materialize-incremental</code>","text":"<p>Incrementally materialize features up to a specified timestamp</p> <pre><code>curl -X POST http://127.0.0.1:52316/materialize-incremental \\\n     -H \"Content-Type: application/json\" \\\n     -d @request-materialize-incremental.json\n</code></pre> <p>where <code>request-materialize-incremental.json</code> is</p> <pre><code>{\"end_ts\": \"2025-05-27T07:00:00\"}\n</code></pre> <p>by running <code>kubectl logs pod</code>, you can see the progress of materialization:</p> <p>Expected Output</p> <pre><code>acct_fraud_7d from 2025-05-11 11:56:07+00:00 to 2025-05-27 07:00:00+00:00:\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9944/9944 [00:02&lt;00:00, 3984.00it/s]\nacct_profiles from 2025-05-11 11:56:07+00:00 to 2025-05-27 07:00:00+00:00:\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9944/9944 [00:00&lt;00:00, 14760.64it/s]\nacct_num_txns_7d from 2025-05-11 11:56:07+00:00 to 2025-05-27 07:00:00+00:00:\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9944/9944 [00:00&lt;00:00, 17575.82it/s]\n10.244.0.1:23621 - \"POST /materialize-incremental HTTP/1.1\" 200\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/deployment/#get-online-features","title":"Get Online Features","text":""},{"location":"side-projects/data2ml-ops/feast/deployment/#get-online-features_1","title":"<code>/get-online-features</code>","text":"<p>Get online features from the feature store:</p> <pre><code>curl -X POST http://127.0.0.1:52316/get-online-features \\\n     -H \"Content-Type: application/json\" \\\n     -d @request-get-online-features.json | jq\n</code></pre> <p>where <code>request-get-online-features.json</code> is </p> <pre><code>{\n  \"feature_service\": \"fraud_detection_v1\",\n  \"entities\": {\n    \"entity_id\": [\"v5zlw0\", \"000q95\"]\n  }\n}\n</code></pre> <p>Expected Output</p> <pre><code>{\n  \"metadata\": {\n    \"feature_names\": [\n      \"entity_id\",\n      \"has_fraud_7d\",\n      \"num_transactions_7d\",\n      \"account_age_days\",\n      \"credit_score\",\n      \"has_2fa_installed\"\n    ]\n  },\n  \"results\": [\n    {\n      \"values\": [\n        \"v5zlw0\",\n        \"000q95\"\n      ],\n      \"statuses\": [\n        \"PRESENT\",\n        \"PRESENT\"\n      ],\n      \"event_timestamps\": [\n        \"1970-01-01T00:00:00Z\",\n        \"1970-01-01T00:00:00Z\"\n      ]\n    },\n    {\n      \"values\": [\n        false,\n        false\n      ],\n      \"statuses\": [\n        \"PRESENT\",\n        \"PRESENT\"\n      ],\n      \"event_timestamps\": [\n        \"2025-05-26T22:00:37Z\",\n        \"2025-05-26T22:00:37Z\"\n      ]\n    },\n    {\n      \"values\": [\n        7,\n        6\n      ],\n      \"statuses\": [\n        \"PRESENT\",\n        \"PRESENT\"\n      ],\n      \"event_timestamps\": [\n        \"2025-05-26T00:00:00Z\",\n        \"2025-05-26T00:00:00Z\"\n      ]\n    },\n    {\n      \"values\": [\n        655,\n        236\n      ],\n      \"statuses\": [\n        \"PRESENT\",\n        \"PRESENT\"\n      ],\n      \"event_timestamps\": [\n        \"2025-05-19T22:00:27Z\",\n        \"2025-05-19T22:00:27Z\"\n      ]\n    },\n    {\n      \"values\": [\n        480,\n        737\n      ],\n      \"statuses\": [\n        \"PRESENT\",\n        \"PRESENT\"\n      ],\n      \"event_timestamps\": [\n        \"2025-05-19T22:00:27Z\",\n        \"2025-05-19T22:00:27Z\"\n      ]\n    },\n    {\n      \"values\": [\n        true,\n        true\n      ],\n      \"statuses\": [\n        \"PRESENT\",\n        \"PRESENT\"\n      ],\n      \"event_timestamps\": [\n        \"2025-05-19T22:00:27Z\",\n        \"2025-05-19T22:00:27Z\"\n      ]\n    }\n  ]\n}\n</code></pre> <p>https://docs.feast.dev/reference/feature-servers/python-feature-server</p>"},{"location":"side-projects/data2ml-ops/feast/how-it-works/","title":"How It Works?","text":"<p>__</p>"},{"location":"side-projects/data2ml-ops/feast/how-it-works/#core-concepts","title":"Core Concepts","text":""},{"location":"side-projects/data2ml-ops/feast/how-it-works/#architecture-components","title":"Architecture Components","text":""},{"location":"side-projects/data2ml-ops/feast/how-it-works/#key-modules-and-their-roles","title":"Key Modules and Their Roles","text":""},{"location":"side-projects/data2ml-ops/feast/how-it-works/#behind-the-scenes","title":"Behind the Scenes","text":""},{"location":"side-projects/data2ml-ops/feast/in-the-bigger-picture/","title":"In the Bigger Picture","text":""},{"location":"side-projects/data2ml-ops/feast/in-the-bigger-picture/#history-current-status","title":"History &amp; Current Status","text":"<p>Feast (Feature Store) is an open-source feature store for machine learning, originally developed by Gojek in collaboration with Google Cloud. The project was open-sourced in 2019, aiming to provide a centralized platform for managing and serving machine learning features in production environments. Feast has since evolved into a widely adopted solution, with contributions from a growing community of developers and organizations.</p> <p>As of May 2025, Feast has garnered over 6,000 stars on GitHub, reflecting its popularity and active development. The community includes more than 1,000 contributors and over 5,500 members on Slack . Feast hosts biweekly community calls and annual events, such as the Feast Summit, to foster collaboration and share advancements. Notable adopters of Feast include companies like Robinhood, NVIDIA, Discord, Cloudflare, Walmart, Shopify, Salesforce, Twitter, IBM, Capital One, Red Hat, Expedia, HelloFresh, Adyen, and SeatGeek.</p>"},{"location":"side-projects/data2ml-ops/feast/in-the-bigger-picture/#alternatives","title":"Alternatives","text":""},{"location":"side-projects/data2ml-ops/feast/in-the-bigger-picture/#integration-points","title":"Integration Points","text":""},{"location":"side-projects/data2ml-ops/feast/in-the-bigger-picture/#kubernetes","title":"Kubernetes","text":""},{"location":"side-projects/data2ml-ops/feast/in-the-bigger-picture/#cicd","title":"CI/CD","text":""},{"location":"side-projects/data2ml-ops/feast/modeling-features/","title":"Modeling Features","text":"<ul> <li>three data sources</li> <li>one entity</li> <li>three feature views</li> <li>one feature service</li> </ul>"},{"location":"side-projects/data2ml-ops/feast/modeling-features/#prerequisites","title":"Prerequisites","text":"<p>Virtual environment prepared</p> requirements.txt<pre><code>feast[gcp,redis]==0.49.0\npsycopg[binary]==3.2.7\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/modeling-features/#packages","title":"Packages","text":"account_features.py<pre><code>from datetime import timedelta\n\nfrom feast import (BigQuerySource, Entity, FeatureService, FeatureView,\n                   ValueType, Field)\nfrom feast.types import String, Int64, Bool\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/modeling-features/#data-sources","title":"Data Sources","text":"account_features.py<pre><code># Data Sources\n# https://rtd.feast.dev/en/latest/index.html#feast.infra.offline_stores.bigquery_source.BigQuerySource\nds_acct_fraud_7d = BigQuerySource(\n    table=f\"mlops-437709.dbt_kclai.feat_acct_fraud_7d\",\n    timestamp_field=\"feature_timestamp\"\n)\n\nds_acct_num_txns_7d = BigQuerySource(\n    table=f\"mlops-437709.dbt_kclai.feat_acct_num_txns_7d\",\n    timestamp_field=\"feature_timestamp\"\n)\n\nds_acct_profiles = BigQuerySource(\n    table=f\"mlops-437709.dbt_kclai.feat_acct_profiles\",\n    timestamp_field=\"feature_timestamp\"\n)\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/modeling-features/#entity","title":"Entity","text":"account_features.py<pre><code># Entity\naccount_entity = Entity(\n    name=\"Account\",\n    description=\"A user that has executed a transaction or received a transaction\",\n    value_type=ValueType.STRING,\n    join_keys=[\"entity_id\"]\n)\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/modeling-features/#feature-views","title":"Feature Views","text":"account_features.py<pre><code># Feature Views\nfv_acct_fraud_7d = FeatureView(\n    name=\"acct_fraud_7d\",\n    entities=[account_entity],\n    schema=[\n        Field(name=\"has_fraud_7d\", dtype=Bool)\n    ],\n    ttl=timedelta(weeks=52),\n    source=ds_acct_fraud_7d\n)\n\n\nfv_acct_num_txns_7d = FeatureView(\n    name=\"acct_num_txns_7d\",\n    entities=[account_entity],\n    schema=[\n        Field(name=\"num_transactions_7d\", dtype=Int64)\n    ],\n    ttl=timedelta(weeks=1),\n    source=ds_acct_num_txns_7d\n)\n\nfv_acct_profiles = FeatureView(\n    name=\"acct_profiles\",\n    entities=[account_entity],\n    schema=[\n        Field(name=\"credit_score\", dtype=Int64),\n        Field(name=\"account_age_days\", dtype=Int64),\n        Field(name=\"has_2fa_installed\", dtype=Bool)\n    ],\n    ttl=timedelta(weeks=52),\n    source=ds_acct_profiles\n)\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/modeling-features/#feature-services","title":"Feature Services","text":"account_features.py<pre><code># Feature Services\n# Versioning features that power ML models:\n# https://docs.feast.dev/master/how-to-guides/running-feast-in-production#id-3.2-versioning-features-that-power-ml-models\nfs_fraud_detection_v1 = FeatureService(\n    name=\"fraud_detection_v1\",\n    features=[\n        fv_acct_fraud_7d,\n        fv_acct_num_txns_7d[[\"num_transactions_7d\"]],\n        fv_acct_profiles\n    ]\n)\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/vertex-ai-feature-store-implementation/","title":"Vertex AI Feature Store Implementation (Hands-on)","text":"main.yaml<pre><code># Feature Online Store\nresource \"google_vertex_ai_feature_online_store\" \"demo_store\" {\n  name   = \"demo\"\n  region = \"us-central1\"\n\n  bigtable {\n    auto_scaling {\n      min_node_count         = 1\n      max_node_count         = 1\n      cpu_utilization_target = 70\n    }\n  }\n}\n</code></pre> main.yaml<pre><code># Template for feature group and features\nlocals {\n  feature_groups = {\n    credit_request = [\n      \"credit_amount\", \"credit_duration\", \"installment_commitment\", \"credit_score\"\n    ]\n    customer_financial_profile = [\n      \"checking_balance\", \"savings_balance\", \"existing_credits\", \"job_history\"\n    ]\n    credit_context = [\n      \"purpose\", \"other_parties\", \"credit_standing\", \"assets\", \"other_payment_plans\"\n    ]\n    customer_demographics = [\n      \"age\", \"num_dependents\", \"residence_since\", \"sex\"\n    ]\n  }\n}\n</code></pre> main.yaml<pre><code># Feature Groups\nresource \"google_vertex_ai_feature_group\" \"feature_groups\" {\n  for_each    = local.feature_groups\n  name        = each.key\n  region      = \"us-central1\"\n  description = \"Feature group for ${each.key}\"\n\n  big_query {\n    big_query_source {\n      input_uri = \"bq://mlops-437709.featurestore_demo.credit_files_with_timestamp\"\n    }\n    entity_id_columns = [\"credit_request_id\"]\n  }\n}\n</code></pre> main.yaml<pre><code># Feature Group - Features\nresource \"google_vertex_ai_feature_group_feature\" \"credit_request_features\" {\n  for_each = toset(local.feature_groups.credit_request)\n\n  name          = each.value\n  region        = \"us-central1\"\n  feature_group = google_vertex_ai_feature_group.feature_groups[\"credit_request\"].name\n  description   = \"Feature for ${each.value}\"\n}\n\nresource \"google_vertex_ai_feature_group_feature\" \"customer_financial_profile_features\" {\n  for_each = toset(local.feature_groups.customer_financial_profile)\n\n  name          = each.value\n  region        = \"us-central1\"\n  feature_group = google_vertex_ai_feature_group.feature_groups[\"customer_financial_profile\"].name\n  description   = \"Feature for ${each.value}\"\n}\n\nresource \"google_vertex_ai_feature_group_feature\" \"credit_context_features\" {\n  for_each = toset(local.feature_groups.credit_context)\n\n  name          = each.value\n  region        = \"us-central1\"\n  feature_group = google_vertex_ai_feature_group.feature_groups[\"credit_context\"].name\n  description   = \"Feature for ${each.value}\"\n}\n\nresource \"google_vertex_ai_feature_group_feature\" \"customer_demographics_features\" {\n  for_each = toset(local.feature_groups.customer_demographics)\n\n  name          = each.value\n  region        = \"us-central1\"\n  feature_group = google_vertex_ai_feature_group.feature_groups[\"customer_demographics\"].name\n  description   = \"Feature for ${each.value}\"\n}\n</code></pre> main.yaml<pre><code># Feature Online Store FeatureView\nresource \"google_vertex_ai_feature_online_store_featureview\" \"main\" {\n  name                 = \"main\"\n  region               = \"us-central1\"\n  feature_online_store = google_vertex_ai_feature_online_store.demo_store.name\n  sync_config {\n    cron = \"*/10 * * * *\"\n  }\n  feature_registry_source {\n\n    feature_groups { \n        feature_group_id = google_vertex_ai_feature_group.feature_groups[\"credit_request\"].name\n        feature_ids      = [google_vertex_ai_feature_group_feature.credit_request_features[\"credit_amount\"].name]\n       }\n  }\n}\n</code></pre>"},{"location":"side-projects/data2ml-ops/feast/vertex-ai-feature-store-introduction/","title":"Vertex AI Feature Store Introduction","text":""},{"location":"side-projects/data2ml-ops/feast/what-why-when/","title":"What, Why, When","text":""},{"location":"side-projects/data2ml-ops/feast/what-why-when/#what-is-x","title":"What is X?","text":""},{"location":"side-projects/data2ml-ops/feast/what-why-when/#why-x","title":"Why X?","text":"<ul> <li>Data Engineer</li> <li>Data Analyst</li> <li>Data Scientist</li> <li>Machine Learning Engineer</li> </ul>"},{"location":"side-projects/data2ml-ops/feast/what-why-when/#when-to-use-x","title":"When to Use X?","text":""},{"location":"side-projects/data2ml-ops/kserve/deployment-with-transformer/","title":"Deploy Your Model on Kubernetes with Feast Transformer","text":""},{"location":"side-projects/data2ml-ops/kserve/deployment/","title":"Deploy Your Model on Kubernetes","text":""},{"location":"side-projects/data2ml-ops/kserve/deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li>MinIO must be running and contain a bucket named <code>mlflow</code>.  <ul> <li>If you haven\u2019t set it up, see MinIO Deployment for instructions.</li> </ul> </li> <li>A Ray Tune job must have completed and logged the best model to MLflow, which was stored in MinIO. If you haven\u2019t done this yet, refer to the following guides:<ul> <li>MLflow Deployment: Set up the MLflow tracking server and configure MinIO as the artifact store.</li> <li>Ray Deployment: Deploy a Ray cluster on Kubernetes.</li> <li>Ray Tune Integration Guide: Learn how to integrate Ray Tune with MLflow, Optuna imbalanced-learn, XGBoost and MinIO.</li> <li>Ray Tune Job Submission: Run the tuning job and log the best model to MLflow.</li> </ul> </li> <li><code>gRPCurl</code> installed     <pre><code>brew install grpcurl\n</code></pre></li> </ul> <p>This guide walks you through how to deploy the model you previously trained with Ray and logged to MinIO via MLflow. You'll learn how to serve it using KServe with both REST and gRPC endpoints, and enable autoscaling\u2014including scale-to-zero support.</p>"},{"location":"side-projects/data2ml-ops/kserve/deployment/#grant-kserve-permission-to-load-models-from-minio","title":"Grant KServe Permission to Load Models from MinIO <sup>1</sup>","text":"<p>To allow KServe to pull models from MinIO (or any S3-compatible storage), you'll need to provide access credentials via a Kubernetes <code>Secret</code> and bind it to a <code>ServiceAccount</code>. Then, reference that service account in your <code>InferenceService</code>.</p> <p>Start by creating a <code>Secret</code> that holds your S3 credentials. This secret should include your access key and secret key, and be annotated with the MinIO endpoint and settings to disable HTTPS if you're working in a local or test environment.</p> secret.yaml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: s3creds\n  annotations:\n     serving.kserve.io/s3-endpoint: minio-api.minio.svc.cluster.local:9000\n     serving.kserve.io/s3-usehttps: \"0\" # by default 1, if testing with minio you can set to 0\ntype: Opaque\nstringData: # This is for raw credential string. For base64 encoded string, use `data` instead\n  AWS_ACCESS_KEY_ID: minio_user\n  AWS_SECRET_ACCESS_KEY: minio_password\n</code></pre> <p>These values should match what you specified when deploying MinIO on Kubernetes. For more details, refer to the configuration section below or revisit this article.</p> Info minio.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio\n  namespace: minio\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n        - name: minio\n          image: minio/minio:RELEASE.2025-04-22T22-12-26Z\n          args:\n            - server\n            - /data\n            - --console-address\n            - :9001\n          env:\n            - name: MINIO_ROOT_USER\n              value: minio_user\n            - name: MINIO_ROOT_PASSWORD\n              value: minio_password\n          ports:\n            - containerPort: 9000\n              protocol: TCP\n            - containerPort: 9001\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /minio/health/live\n              port: 9000\n            initialDelaySeconds: 30\n            periodSeconds: 20\n            timeoutSeconds: 15\n            failureThreshold: 6\n          readinessProbe:\n            httpGet:\n              path: /minio/health/ready\n              port: 9000\n            initialDelaySeconds: 15\n            periodSeconds: 10\n            timeoutSeconds: 10\n            failureThreshold: 3\n          volumeMounts:\n            - name: storage\n              mountPath: /data\n      volumes:\n        - name: storage\n          hostPath:\n            path: /home/docker/data/minio\n            type: DirectoryOrCreate\n      restartPolicy: Always\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: minio-console\n  namespace: minio\nspec:\n  selector:\n    app: minio\n  type: NodePort\n  ports:\n    - name: console\n      port: 9001\n      targetPort: 9001\n      nodePort: 30901\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: minio-api\n  namespace: minio\nspec:\n  selector:\n    app: minio\n  type: ClusterIP\n  ports:\n    - name: api\n      port: 9000\n      targetPort: 9000\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: minio-create-bucket\n  namespace: minio\nspec:\n  backoffLimit: 6\n  completions: 1\n  template:\n    metadata:\n      labels:\n        job: minio-create-bucket\n    spec:\n      initContainers:\n        - name: wait-for-minio\n          image: busybox\n          command:\n            - sh\n            - -c\n            - |\n              until nc -z minio-api.minio.svc.cluster.local 9000; do\n                echo \"Waiting for MinIO...\"\n                sleep 2\n              done\n              echo \"MinIO is ready!\"\n      containers:\n        - name: minio-create-buckets\n          image: minio/mc\n          command:\n            - sh\n            - -c\n            - |\n              mc alias set minio http://minio-api.minio.svc.cluster.local:9000 minio_user minio_password &amp;&amp;\n              for bucket in mlflow dbt sqlmesh ray; do\n                if ! mc ls minio/$bucket &gt;/dev/null 2&gt;&amp;1; then\n                  echo \"Creating bucket: $bucket\"\n                  mc mb minio/$bucket\n                  echo \"Bucket created: $bucket\"\n                else\n                  echo \"Bucket already exists: $bucket\"\n                fi\n              done\n      restartPolicy: OnFailure\n      terminationGracePeriodSeconds: 30\n</code></pre> <p>Next, create a <code>ServiceAccount</code> that references the secret. This will allow KServe to inject the credentials when pulling models.</p> sa.yaml<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: sa\nsecrets:\n- name: s3creds\n</code></pre> <p>Finally, define an <code>InferenceService</code> that uses the <code>ServiceAccount</code> and points to the model artifact stored in MinIO. In this example, we are deploying a model saved in MLflow format using the v2 inference protocol.</p> RESTgRPC inference-service-rest-v2.yaml<pre><code>apiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"fraud-detection-rest\"\nspec:\n  predictor:\n    minReplicas: 0\n    scaleTarget: 1\n    scaleMetric: qps\n    model:\n      modelFormat:\n        name: mlflow\n      protocolVersion: v2\n      storageUri: s3://miflow/2/5ccd7dcabc1f49c1bc45f1f94d945dd6/artifacts/model\n    serviceAccountName: sa\n</code></pre> inference-service-grpc-v2.yaml<pre><code>apiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"fraud-detection-grpc\"\nspec:\n  predictor:\n    minReplicas: 0\n    scaleTarget: 1\n    scaleMetric: qps\n    model:\n      modelFormat:\n        name: mlflow\n      protocolVersion: v2\n      storageUri: s3://miflow/2/5ccd7dcabc1f49c1bc45f1f94d945dd6/artifacts/model\n      ports:\n        - containerPort: 9000\n          name: h2c\n          protocol: TCP\n    serviceAccountName: sa\n</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/deployment/#deploy-the-fraud-detection-mlflow-model-with-inferenceservice","title":"Deploy the Fraud Detection MLflow Model with InferenceService<sup>2</sup><sup>3</sup>","text":"<p>This example shows how to deploy your trained MLflow model on KServe using both the REST and gRPC protocol. The <code>InferenceService</code> configuration specifies the model format (<code>mlflow</code>), the s2 inference protocol, and the S3 URI where the model is stored. The <code>serviceAccountName</code> field allows KServe to access the model stored in MinIO using the credentials provided earlier.</p> RESTgRPC inference-service-rest-v2.yaml<pre><code>apiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"fraud-detection-rest\"\nspec:\n  predictor:\n    minReplicas: 0\n    scaleTarget: 1\n    scaleMetric: qps\n    model:\n      modelFormat:\n        name: mlflow\n      protocolVersion: v2\n      storageUri: s3://miflow/2/5ccd7dcabc1f49c1bc45f1f94d945dd6/artifacts/model\n    serviceAccountName: sa\n</code></pre> <p>Apply the configuration using <code>kubectl</code>:</p> <pre><code>kubectl apply -f inference-service-rest-v2.yaml\n</code></pre> <p>Once deployed, KServe will expose a REST endpoint where you can send inference requests. You can verify the service status using:</p> <pre><code>kubectl get inferenceservice fraud-detection-http\n</code></pre> inference-service-grpc-v2.yaml<pre><code>apiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"fraud-detection-grpc\"\nspec:\n  predictor:\n    minReplicas: 0\n    scaleTarget: 1\n    scaleMetric: qps\n    model:\n      modelFormat:\n        name: mlflow\n      protocolVersion: v2\n      storageUri: s3://miflow/2/5ccd7dcabc1f49c1bc45f1f94d945dd6/artifacts/model\n      ports:\n        - containerPort: 9000\n          name: h2c\n          protocol: TCP\n    serviceAccountName: sa\n</code></pre> <p>Apply the configuration using <code>kubectl</code>:</p> <pre><code>kubectl apply -f inference-service-grpc-v2.yaml\n</code></pre> <p>Once deployed, KServe will expose a gRPC endpoint where you can send inference requests. You can verify the service status using: </p> <pre><code>kubectl get inferenceservice fraud-detection-grpc\n</code></pre> <p>Make sure the <code>storageUri</code> matches the path where your model is saved.</p> <p>During deployment, you may encounter a few common issues:</p> <ol> <li>The model fails to load inside the pod during the storage initialization phase<sup>4</sup>. This is usually a permission issue\u2014make sure your access credentials are correctly configured as shown in the section above.</li> <li>Sometimes the model loads successfully into the model server, but inference requests still fail. This could be due to:<ul> <li>A mismatch between the model version and the model server runtime. In this case, try explicitly setting the <code>runtimeVersion</code><sup>5</sup>.</li> <li>Incorrect port settings, which prevent the server from responding properly.</li> <li>Architecture mismatch\u2014for example, if you trained the model on a Mac (ARM64) but are using an x86-based KServe runtime image.</li> <li>Deployment in a control plane namespace. Namespaces labeled with <code>control-plane</code> are skipped by KServe\u2019s webhook to avoid privilege escalation. This prevents the storage initializer from being injected into the pod, leading to errors like: <code>No such file or directory: '/mnt/models'</code>.</li> </ul> </li> </ol>"},{"location":"side-projects/data2ml-ops/kserve/deployment/#test-the-endpoints","title":"Test the Endpoints","text":"<p>Determine the ingress IP and port:</p> <pre><code>export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\nexport INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\"http2\")].port}')\n</code></pre> <p>This step retrieves the external IP address of the Istio Ingress Gateway and stores it in <code>INGRESS_HOST</code>, and extracts the port named <code>http2</code> to set as <code>INGRESS_PORT</code>, allowing you to construct the full service endpoint for sending inference requests.</p> RESTgRPC <p>Set the required environment variables for the HTTP inference request:</p> <pre><code>export INPUT_PATH=input-example-rest-v2.json\nexport SERVICE_HOSTNAME=$(kubectl get inferenceservice fraud-detection-rest -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n</code></pre> input-example-rest-v2.json input-example-rest-v2.json<pre><code>{\n  \"parameters\": {\n    \"content_type\": \"pd\"\n  },\n  \"inputs\": [\n    {\n      \"name\": \"has_fraud_7d\",\n      \"shape\": [2],\n      \"datatype\": \"BOOL\",\n      \"data\": [false, false]\n    },\n    {\n      \"name\": \"num_transactions_7d\",\n      \"shape\": [2],\n      \"datatype\": \"INT64\",\n      \"data\": [7, 6]\n    },\n    {\n      \"name\": \"account_age_days\",\n      \"shape\": [2],\n      \"datatype\": \"INT64\",\n      \"data\": [655, 236]\n    },\n    {\n      \"name\": \"credit_score\",\n      \"shape\": [2],\n      \"datatype\": \"INT64\",\n      \"data\": [680, 737]\n    },\n    {\n      \"name\": \"has_2fa_installed\",\n      \"shape\": [2],\n      \"datatype\": \"BOOL\",\n      \"data\": [true, true]\n    }\n  ]\n}\n</code></pre> test-commands.txt<pre><code># -v                       Enable verbose output for debugging\n# -H \"Host:...\"            Set the Host header to route through the ingress gateway\n# -H \"Content-Type:...\"    Specify the request content type as JSON\n# -d @...                  Provide the input payload from the specified JSON file\n# http://${INGRESS_HOST}... Target the model's inference endpoint\ncurl -v \\\n  -H \"Host: ${SERVICE_HOSTNAME}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d @${INPUT_PATH} \\\n  http://${INGRESS_HOST}:${INGRESS_PORT}/v2/models/fraud-detection-http/infer\n</code></pre> <p>Expected Output</p> <pre><code>*   Trying 127.0.0.1:80...\n* Connected to 127.0.0.1 (127.0.0.1) port 80\n&gt; POST /v2/models/mlflow-apple-demand/infer HTTP/1.1\n&gt; Host: mlflow-apple-demand.default.127.0.0.1.sslip.io\n&gt; User-Agent: curl/8.7.1\n&gt; Accept: */*\n&gt; Content-Type: application/json\n&gt; Content-Length: 1089\n&gt; \n* upload completely sent off: 1089 bytes\n&lt; HTTP/1.1 200 OK\n&lt; ce-endpoint: mlflow-apple-demand\n&lt; ce-id: 9ddc841e-a8d4-405f-a7e4-73f7aa9bab09\n&lt; ce-inferenceservicename: mlserver\n&lt; ce-modelid: mlflow-apple-demand\n&lt; ce-namespace: default\n&lt; ce-requestid: 9ddc841e-a8d4-405f-a7e4-73f7aa9bab09\n&lt; ce-source: io.seldon.serving.deployment.mlserver.default\n&lt; ce-specversion: 0.3\n&lt; ce-type: io.seldon.serving.inference.response\n&lt; content-length: 240\n&lt; content-type: application/json\n&lt; date: Fri, 02 May 2025 04:06:58 GMT\n&lt; server: istio-envoy\n&lt; x-envoy-upstream-service-time: 247\n&lt; \n* Connection #0 to host 127.0.0.1 left intact\n{\"model_name\":\"mlflow-apple-demand\",\"id\":\"9ddc841e-a8d4-405f-a7e4-73f7aa9bab09\",\"parameters\":{\"content_type\":\"np\"},\"outputs\":[{\"name\":\"output-1\",\"shape\":[1,1],\"datatype\":\"FP32\",\"parameters\":{\"content_type\":\"np\"},\"data\":[1486.56298828125]}]}\n</code></pre> <p>Download the <code>open_inference_grpc.proto</code> file:</p> <pre><code>curl -O https://raw.githubusercontent.com/kserve/open-inference-protocol/main/specification/protocol/open_inference_grpc.proto\n</code></pre> open_inference_grpc.proto open_inference_grpc.proto<pre><code>// Copyright 2023 The KServe Authors.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//    http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\nsyntax = \"proto3\";\npackage inference;\n\n// Inference Server GRPC endpoints.\nservice GRPCInferenceService\n{\n  // The ServerLive API indicates if the inference server is able to receive \n  // and respond to metadata and inference requests.\n  rpc ServerLive(ServerLiveRequest) returns (ServerLiveResponse) {}\n\n  // The ServerReady API indicates if the server is ready for inferencing.\n  rpc ServerReady(ServerReadyRequest) returns (ServerReadyResponse) {}\n\n  // The ModelReady API indicates if a specific model is ready for inferencing.\n  rpc ModelReady(ModelReadyRequest) returns (ModelReadyResponse) {}\n\n  // The ServerMetadata API provides information about the server. Errors are \n  // indicated by the google.rpc.Status returned for the request. The OK code \n  // indicates success and other codes indicate failure.\n  rpc ServerMetadata(ServerMetadataRequest) returns (ServerMetadataResponse) {}\n\n  // The per-model metadata API provides information about a model. Errors are \n  // indicated by the google.rpc.Status returned for the request. The OK code \n  // indicates success and other codes indicate failure.\n  rpc ModelMetadata(ModelMetadataRequest) returns (ModelMetadataResponse) {}\n\n  // The ModelInfer API performs inference using the specified model. Errors are\n  // indicated by the google.rpc.Status returned for the request. The OK code \n  // indicates success and other codes indicate failure.\n  rpc ModelInfer(ModelInferRequest) returns (ModelInferResponse) {}\n}\n\nmessage ServerLiveRequest {}\n\nmessage ServerLiveResponse\n{\n  // True if the inference server is live, false if not live.\n  bool live = 1;\n}\n\nmessage ServerReadyRequest {}\n\nmessage ServerReadyResponse\n{\n  // True if the inference server is ready, false if not ready.\n  bool ready = 1;\n}\n\nmessage ModelReadyRequest\n{\n  // The name of the model to check for readiness.\n  string name = 1;\n\n  // The version of the model to check for readiness. If not given the\n  // server will choose a version based on the model and internal policy.\n  string version = 2;\n}\n\nmessage ModelReadyResponse\n{\n  // True if the model is ready, false if not ready.\n  bool ready = 1;\n}\n\nmessage ServerMetadataRequest {}\n\nmessage ServerMetadataResponse\n{\n  // The server name.\n  string name = 1;\n\n  // The server version.\n  string version = 2;\n\n  // The extensions supported by the server.\n  repeated string extensions = 3;\n}\n\nmessage ModelMetadataRequest\n{\n  // The name of the model.\n  string name = 1;\n\n  // The version of the model to check for readiness. If not given the\n  // server will choose a version based on the model and internal policy.\n  string version = 2;\n}\n\nmessage ModelMetadataResponse\n{\n  // Metadata for a tensor.\n  message TensorMetadata\n  {\n    // The tensor name.\n    string name = 1;\n\n    // The tensor data type.\n    string datatype = 2;\n\n    // The tensor shape. A variable-size dimension is represented\n    // by a -1 value.\n    repeated int64 shape = 3;\n  }\n\n  // The model name.\n  string name = 1;\n\n  // The versions of the model available on the server.\n  repeated string versions = 2;\n\n  // The model's platform. See Platforms.\n  string platform = 3;\n\n  // The model's inputs.\n  repeated TensorMetadata inputs = 4;\n\n  // The model's outputs.\n  repeated TensorMetadata outputs = 5;\n\n  // Optional Model Properties\n  map&lt;string, string&gt; properties = 6;\n}\n\nmessage ModelInferRequest\n{\n  // An input tensor for an inference request.\n  message InferInputTensor\n  {\n    // The tensor name.\n    string name = 1;\n\n    // The tensor data type.\n    string datatype = 2;\n\n    // The tensor shape.\n    repeated int64 shape = 3;\n\n    // Optional inference input tensor parameters.\n    map&lt;string, InferParameter&gt; parameters = 4;\n\n    // The tensor contents using a data-type format. This field must\n    // not be specified if \"raw\" tensor contents are being used for\n    // the inference request.\n    InferTensorContents contents = 5;\n  }\n\n  // An output tensor requested for an inference request.\n  message InferRequestedOutputTensor\n  {\n    // The tensor name.\n    string name = 1;\n\n    // Optional requested output tensor parameters.\n    map&lt;string, InferParameter&gt; parameters = 2;\n  }\n\n  // The name of the model to use for inferencing.\n  string model_name = 1;\n\n  // The version of the model to use for inference. If not given the\n  // server will choose a version based on the model and internal policy.\n  string model_version = 2;\n\n  // Optional identifier for the request. If specified will be\n  // returned in the response.\n  string id = 3;\n\n  // Optional inference parameters.\n  map&lt;string, InferParameter&gt; parameters = 4;\n\n  // The input tensors for the inference.\n  repeated InferInputTensor inputs = 5;\n\n  // The requested output tensors for the inference. Optional, if not\n  // specified all outputs produced by the model will be returned.\n  repeated InferRequestedOutputTensor outputs = 6;\n\n  // The data contained in an input tensor can be represented in \"raw\"\n  // bytes form or in the repeated type that matches the tensor's data\n  // type. To use the raw representation 'raw_input_contents' must be\n  // initialized with data for each tensor in the same order as\n  // 'inputs'. For each tensor, the size of this content must match\n  // what is expected by the tensor's shape and data type. The raw\n  // data must be the flattened, one-dimensional, row-major order of\n  // the tensor elements without any stride or padding between the\n  // elements. Note that the FP16 and BF16 data types must be represented as\n  // raw content as there is no specific data type for a 16-bit float type.\n  //\n  // If this field is specified then InferInputTensor::contents must\n  // not be specified for any input tensor.\n  repeated bytes raw_input_contents = 7;\n}\n\nmessage ModelInferResponse\n{\n  // An output tensor returned for an inference request.\n  message InferOutputTensor\n  {\n    // The tensor name.\n    string name = 1;\n\n    // The tensor data type.\n    string datatype = 2;\n\n    // The tensor shape.\n    repeated int64 shape = 3;\n\n    // Optional output tensor parameters.\n    map&lt;string, InferParameter&gt; parameters = 4;\n\n    // The tensor contents using a data-type format. This field must\n    // not be specified if \"raw\" tensor contents are being used for\n    // the inference response.\n    InferTensorContents contents = 5;\n  }\n\n  // The name of the model used for inference.\n  string model_name = 1;\n\n  // The version of the model used for inference.\n  string model_version = 2;\n\n  // The id of the inference request if one was specified.\n  string id = 3;\n\n  // Optional inference response parameters.\n  map&lt;string, InferParameter&gt; parameters = 4;\n\n  // The output tensors holding inference results.\n  repeated InferOutputTensor outputs = 5;\n\n  // The data contained in an output tensor can be represented in\n  // \"raw\" bytes form or in the repeated type that matches the\n  // tensor's data type. To use the raw representation 'raw_output_contents'\n  // must be initialized with data for each tensor in the same order as\n  // 'outputs'. For each tensor, the size of this content must match\n  // what is expected by the tensor's shape and data type. The raw\n  // data must be the flattened, one-dimensional, row-major order of\n  // the tensor elements without any stride or padding between the\n  // elements. Note that the FP16 and BF16 data types must be represented as\n  // raw content as there is no specific data type for a 16-bit float type.\n  //\n  // If this field is specified then InferOutputTensor::contents must\n  // not be specified for any output tensor.\n  repeated bytes raw_output_contents = 6;\n}\n\n// An inference parameter value. The Parameters message describes a \n// \u201cname\u201d/\u201dvalue\u201d pair, where the \u201cname\u201d is the name of the parameter\n// and the \u201cvalue\u201d is a boolean, integer, or string corresponding to \n// the parameter.\nmessage InferParameter\n{\n  // The parameter value can be a string, an int64, a boolean\n  // or a message specific to a predefined parameter.\n  oneof parameter_choice\n  {\n    // A boolean parameter value.\n    bool bool_param = 1;\n\n    // An int64 parameter value.\n    int64 int64_param = 2;\n\n    // A string parameter value.\n    string string_param = 3;\n\n    // A double parameter value.\n    double double_param = 4;\n\n    // A uint64 parameter value.\n    uint64 uint64_param = 5;\n  }\n}\n\n// The data contained in a tensor represented by the repeated type\n// that matches the tensor's data type. Protobuf oneof is not used\n// because oneofs cannot contain repeated fields.\nmessage InferTensorContents\n{\n  // Representation for BOOL data type. The size must match what is\n  // expected by the tensor's shape. The contents must be the flattened,\n  // one-dimensional, row-major order of the tensor elements.\n  repeated bool bool_contents = 1;\n\n  // Representation for INT8, INT16, and INT32 data types. The size\n  // must match what is expected by the tensor's shape. The contents\n  // must be the flattened, one-dimensional, row-major order of the\n  // tensor elements.\n  repeated int32 int_contents = 2;\n\n  // Representation for INT64 data types. The size must match what\n  // is expected by the tensor's shape. The contents must be the\n  // flattened, one-dimensional, row-major order of the tensor elements.\n  repeated int64 int64_contents = 3;\n\n  // Representation for UINT8, UINT16, and UINT32 data types. The size\n  // must match what is expected by the tensor's shape. The contents\n  // must be the flattened, one-dimensional, row-major order of the\n  // tensor elements.\n  repeated uint32 uint_contents = 4;\n\n  // Representation for UINT64 data types. The size must match what\n  // is expected by the tensor's shape. The contents must be the\n  // flattened, one-dimensional, row-major order of the tensor elements.\n  repeated uint64 uint64_contents = 5;\n\n  // Representation for FP32 data type. The size must match what is\n  // expected by the tensor's shape. The contents must be the flattened,\n  // one-dimensional, row-major order of the tensor elements.\n  repeated float fp32_contents = 6;\n\n  // Representation for FP64 data type. The size must match what is\n  // expected by the tensor's shape. The contents must be the flattened,\n  // one-dimensional, row-major order of the tensor elements.\n  repeated double fp64_contents = 7;\n\n  // Representation for BYTES data type. The size must match what is\n  // expected by the tensor's shape. The contents must be the flattened,\n  // one-dimensional, row-major order of the tensor elements.\n  repeated bytes bytes_contents = 8;\n}\n</code></pre> <p>Downloading this <code>.proto</code> file gives you the standard gRPC interface definition for the Open Inference Protocol. It allows your client or server to communicate with ML model servers using a unified API.</p> <p>Set the required environment variables for the gRPC inference request:</p> <pre><code>export PROTO_FILE=open_inference_grpc.proto\nexport INPUT_PATH=input-example-grpc-v2.json\nexport SERVICE_HOSTNAME=$(kubectl get inferenceservice fraud-detection-grpc -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n</code></pre> <p>These variables specify the protobuf schema for gRPC, the input payload to send, and the target hostname for routing the request through the ingress gateway.</p> input-example-grpc-v2.json input-example-grpc-v2.json<pre><code>{\n  \"model_name\": \"fraud-detection-grpc\",\n  \"inputs\": [\n    {\n      \"name\": \"has_fraud_7d\",\n      \"shape\": [2],\n      \"datatype\": \"BOOL\",\n      \"contents\": {\n        \"bool_contents\": [false, false]\n      }\n    },\n    {\n      \"name\": \"num_transactions_7d\",\n      \"shape\": [2],\n      \"datatype\": \"INT64\",\n      \"contents\": {\n        \"int64_contents\": [7, 6]\n      }\n    },\n    {\n      \"name\": \"account_age_days\",\n      \"shape\": [2],\n      \"datatype\": \"INT64\",\n      \"contents\": {\n        \"int64_contents\": [655, 236]\n      }\n    },\n    {\n      \"name\": \"credit_score\",\n      \"shape\": [2],\n      \"datatype\": \"INT64\",\n      \"contents\": {\n        \"int64_contents\": [680, 737]\n      }\n    },\n    {\n      \"name\": \"has_2fa_installed\",\n      \"shape\": [2],\n      \"datatype\": \"BOOL\",\n      \"contents\": {\n        \"bool_contents\": [true, true]\n      }\n    }\n  ]\n}\n</code></pre> test-commands.txt<pre><code># -vv                Verbose output for debugging\n# -plaintext         Use plaintext (non-TLS) connection\n# -proto             Path to the .proto file describing the gRPC service\n# -authority         Sets the HTTP/2 authority header (useful for ingress routing)\n# -d                 Read the request body from stdin\n# ${INGRESS_HOST}... Target host and port of the gRPC server\n# inference.GRPC...  Fully-qualified gRPC method to call\n# &lt;&lt;&lt;...             Provide JSON request body from file as stdin\ngrpcurl -vv \\\n  -plaintext \\\n  -proto ${PROTO_FILE} \\\n  -authority ${SERVICE_HOSTNAME} \\\n  -d @ \\\n  ${INGRESS_HOST}:${INGRESS_PORT} \\\n  inference.GRPCInferenceService.ModelInfer \\\n  &lt;&lt;&lt; $(cat \"$INPUT_PATH\")\n</code></pre> <p>Expected Output</p> <pre><code>TODO: Add expected output for gRPC inference request\n</code></pre> <ol> <li> <p>Deploy InferenceService with a saved model on S3 | KServe \u21a9</p> </li> <li> <p>Deploy MLflow models with InferenceService | KServe \u21a9</p> </li> <li> <p>Develop ML model with MLflow and deploy to Kubernetes | MLflow \u21a9</p> </li> <li> <p>Storage Initializer fails to download model | KServe Debugging Guide \u21a9</p> </li> <li> <p>Explicitly Specify a Runtime Version \u21a9</p> </li> </ol>"},{"location":"side-projects/data2ml-ops/kserve/feast-transformer/","title":"Write a Custom KServe Transformer with Feast Integration","text":"<p>This guide demonstrates how to integrate Feast with KServe, enabling feature retrieval and transformation during inference.</p>"},{"location":"side-projects/data2ml-ops/kserve/feast-transformer/#extend-the-kserve-model-class","title":"Extend the KServe Model Class","text":"<p><code>KServe.Model</code> base class mainly defines three handlers <code>preprocess</code>, <code>predict</code> and <code>postprocess</code>, these handlers are executed in sequence where the output of the <code>preprocess</code> handler is passed to the <code>predict</code> handler as the input.</p> <p>To create a custom transformer that retrieves features from Feast, you will extend the <code>KServe.Model</code> class and implement the <code>preprocess</code> method. This method will dynamically fetch features based on the input data and append them to the inference request.</p> feast_transformer.py<pre><code>class FeastTransformer(kserve.Model):\n    \"\"\"A class object for the data handling activities of driver ranking\n    Task and returns a KServe compatible response.\n\n    Args:\n        kserve (class object): The Model class from the KServe\n        module is passed here.\n    \"\"\"\n\n    def __init__(\n        self,\n        feast_url: str,\n        feast_entity_id: str,\n        feature_service: str,\n        model_name: str,\n        predictor_host: str,\n        predictor_protocol: str,\n    ):\n        \"\"\"Initialize the model name, predictor host, Feast serving URL,\n           entity IDs, and feature references\n\n        Args:\n            feast_url (str): URL for the Feast online feature server, in the name of &lt;host_name&gt;:&lt;port&gt;.\n            feast_entity_id (str): Name of the entity ID key for feature store lookups.\n            feature_service (str): Name of the feature service to retrieve from the feature store.\n            model_name (str): Name of the model used on the endpoint path (default is \"model\").\n            predictor_host (str): Hostname for calling the predictor from the transformer (default is None).\n            predictor_protocol (str): Inference protocol for the predictor (default is \"v1\").\n        \"\"\"\n        super().__init__(\n            name=model_name,\n            predictor_config=PredictorConfig(\n                predictor_host=predictor_host,\n                predictor_protocol=predictor_protocol,\n            )\n        )\n        self.feast_url = feast_url\n        self.feast_entity_id = feast_entity_id\n        self.feature_service = feature_service\n</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/feast-transformer/#logic-of-the-preprocess-method","title":"Logic of the <code>preprocess()</code> Method","text":"<p>The <code>preprocess()</code> method plays a pivotal role in the inference pipeline by preparing the input data for prediction. It begins by parsing the payload to extract entity IDs (<code>extract_entity_ids()</code>), which are essential for querying the Feast feature store.</p> <p>Once the entity IDs are identified, the method sends a request to the Feast online feature server to fetch the corresponding features. </p> <p>Finally, it processes the response from Feast, transforming the retrieved features into a format that aligns with the requirements of the predictor (<code>create_inference_request</code>). This ensures a seamless flow of data, enabling accurate and efficient predictions.</p> <p>Since the transformer is designed to handle multiple protocols, the <code>preprocess()</code> method checks the request type and processes it accordingly. It supports REST v1, REST v2, and gRPC protocols, ensuring compatibility with various KServe deployments. Additionally, the method processes the response received from Feast based on the protocol used by the predictor. By tailoring both the incoming request handling and the outgoing response formatting, this Feast transformer becomes a highly versatile and adaptable component within the KServe ecosystem.</p> feast_transformer.py<pre><code>    def preprocess(\n        self, payload: Union[Dict, InferRequest], headers: Dict[str, str] = None\n    ) -&gt; Union[Dict, InferRequest]:\n        \"\"\"Pre-process activity of the driver input data.\n\n        Args:\n            payload (Dict|InferRequest): Body of the request, v2 endpoints pass InferRequest.\n            headers (Dict): Request headers.\n\n        Returns:\n            output (Dict|InferRequest): Transformed payload to ``predict`` handler or return InferRequest for predictor call.\n        \"\"\"\n        logger.info(f\"Headers: {headers}\")\n        logger.info(f\"Type of payload: {type(payload)}\")\n        logger.info(f\"Payload: {payload}\")\n\n        # Prepare and send a request for the Feast online feature server\n        entites = self.extract_entity_ids(payload)\n        logger.info(f\"Extracted entities: {entites}\")\n        feast_response = requests.post(\n            f\"{self.feast_url}/get-online-features\",\n            data=json.dumps({\n                \"feature_service\": self.feature_service,\n                \"entities\": entites\n            }),\n            headers={\n                \"Content-Type\": \"application/json\",\n                \"Accept\": \"application/json\"\n            }\n        )\n\n        # Parse the response from the Feast online feature server\n        if feast_response.status_code != 200:\n            logger.error(\n                f\"Error fetching features from Feast: {feast_results}\")\n            raise Exception(\n                f\"Error fetching features from Feast: {feast_results}\")\n        feast_results = feast_response.json()\n        logger.info(f\"Feast results: {feast_results}\")\n\n        output = self.create_inference_request(feast_results)\n        logger.info(f\"Type of output: {type(output)}\")\n        logger.info(f\"Output of preprocess: {output}\")\n        return output\n</code></pre> feast_transformer.py<pre><code>    def extract_entity_ids(self, payload: Union[Dict, InferRequest]) -&gt; Dict:\n        \"\"\"Extract entity IDs from the input payload.\n\n        This method processes the input payload to extract entity IDs based on the \n        protocol (REST v1, REST v2, or gRPC v2) and returns them in a dictionary format.\n\n        Args:\n            payload (Dict|InferRequest): The input payload containing entity IDs.\n\n        Returns:\n            entites (Dict): A dictionary with the extracted entity IDs. For example:\n            {\n            \"entity_id\": [\"v5zlw0\", \"000q95\"]\n            }\n        \"\"\"\n        # The protocol here refers to the protocol used by the transformer deployment\n        # v2\n        if isinstance(payload, InferRequest):\n            infer_input = payload.inputs[0]\n            entity_ids = [\n                # Decode each element based on the protocol: gRPC uses raw bytes, REST uses base64-encoded strings\n                d.decode(\n                    'utf-8') if payload.from_grpc else base64.b64decode(d).decode('utf-8')\n                for d in infer_input.data\n            ]\n        # REST v1, type(payload) = Dict\n        else:\n            entity_ids = [\n                instance[self.feast_entity_id]\n                for instance in payload[\"instances\"]\n            ]\n\n        return {self.feast_entity_id: entity_ids}\n</code></pre> feast_transformer.py<pre><code>    def create_inference_request(self, feast_results: Dict) -&gt; Union[Dict, InferRequest]:\n        \"\"\"Create the inference request for all entities and return it as a dict.\n\n        Args:\n            feast_results (Dict): entity feast_results extracted from the feature store\n\n        Returns:\n            output (Dict|InferRequest): Returns the entity ids with feast_results\n        \"\"\"\n        feature_names = feast_results[\"metadata\"][\"feature_names\"]\n        results = feast_results[\"results\"]\n        num_datapoints = len(results[0][\"values\"])\n        num_features = len(feature_names)\n\n        # for v1 predictor protocol, we can directly pass the dict\n        if self.protocol == PredictorProtocol.REST_V1.value:\n            output = {\n                \"instances\": [\n                    {\n                        feature_names[j]: results[j]['values'][i] for j in range(num_features) if feature_names[j] != \"entity_id\"\n                    }\n                    for i in range(num_datapoints)\n                ]\n            }\n        # for v2 predictor protocol, we need to build an InferRequest\n        else:\n            # TODO: find a way to not hardcode the data types\n            type_map = {\n                \"has_fraud_7d\": \"BOOL\",\n                \"num_transactions_7d\": \"INT64\",\n                \"credit_score\": \"INT64\",\n                \"account_age_days\": \"INT64\",\n                \"has_2fa_installed\": \"BOOL\",\n            }\n            map_datatype = lambda feature_name: type_map.get(feature_name, \"BYTES\")\n\n            output = InferRequest(\n                model_name=self.name,\n                parameters={\n                    \"content-type\": \"pd\"\n                },\n                infer_inputs=[\n                    InferInput(\n                        name=feature_names[j],\n                        datatype=map_datatype(feature_names[j]),\n                        shape=[num_datapoints],\n                        data=[\n                            results[j][\"values\"][i]\n                            for i in range(num_datapoints)\n                        ]\n                    )\n                    for j in range(num_features)\n                    if feature_names[j] != \"entity_id\"\n                ]\n            )\n\n        return output\n</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/feast-transformer/#package-the-custom-transformer","title":"Package the Custom Transformer","text":"<p>Packaging the custom transformer is essential to ensure it can be easily deployed and reused across different environments. By organizing the code into a well-defined structure and containerizing it, we create a portable and self-contained solution that can be seamlessly integrated into any KServe deployment. This approach not only simplifies dependency management but also ensures consistency and reliability, making it easier to scale and maintain the transformer in production.</p> <p>First, create a directory structure for your custom transformer:</p> <pre><code>.\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 __main__.py\n\u2514\u2500\u2500 feast_transformer.py\n</code></pre> <p>Then, extend the <code>__main__.py</code> file to include custom arguments that allow us to easily inject Feast-related information into the KServe transformer. This makes it convenient to specify details like the Feast URL, entity ID, and feature service directly in the inference service YAML file. By doing so, we ensure that the transformer remains flexible and configurable, adapting seamlessly to different deployment environments.</p> __main__.py<pre><code>parser = argparse.ArgumentParser(parents=[kserve.model_server.parser])\nparser.add_argument(\n    \"--feast_url\",\n    type=str,\n    help=\"URL for the Feast online feature server, in the name of &lt;host_name&gt;:&lt;port&gt;\",\n    required=True,\n)\nparser.add_argument(\n    \"--feast_entity_id\",\n    type=str,\n    help=\"Name of the entity ID key for feature store lookups.\",\n    required=True,\n)\nparser.add_argument(\n    \"--feature_service\",\n    type=str,\n    help=\"Name of the feature service to retrieve from the feature store.\",\n    required=True,\n)\n</code></pre> <p>When you run the transformer, you can specify the Feast URL, entity ID, and feature service as command-line arguments. This allows the transformer to dynamically connect to the Feast feature store and retrieve the necessary features for inference.</p> <pre><code>python -m feast_transformer \\\n  --feast_url http://example.com:6565 \\\n  --feast_entity_id entity_id \\\n  --feature-service fraud_detection_v1\n</code></pre> <p>After the arguments are parsed, the <code>main()</code> function initializes the KServe model with the provided Feast configuration and starts the model server.</p> __main__.py<pre><code>if __name__ == \"__main__\":\n    if args.configure_logging:\n        logging.configure_logging(args.log_config_file)\n    transformer = FeastTransformer(\n        feast_url=args.feast_url,\n        feast_entity_id=args.feast_entity_id,\n        feature_service=args.feature_service,\n        model_name=args.model_name,\n        predictor_host=args.predictor_host,\n        predictor_protocol=args.predictor_protocol,\n    )\n    server = kserve.ModelServer()\n    server.start(models=[transformer])\n</code></pre> <p>Here's a sequence diagram when we start the KServe Model Server:</p> <pre><code>sequenceDiagram\n    participant User as User\n    participant ModelServer as ModelServer\n    participant ModelRepo as Model Repository\n    participant RESTServer as REST Server\n    participant GRPCServer as GRPC Server\n    participant EventLoop as Event Loop\n    participant SignalHandler as Signal Handler\n\n    User-&gt;&gt;ModelServer: Instantiate ModelServer\n    ModelServer-&gt;&gt;ModelRepo: Initialize Model Repository\n    ModelServer-&gt;&gt;EventLoop: Setup Event Loop\n    ModelServer-&gt;&gt;SignalHandler: Register Signal Handlers\n\n    User-&gt;&gt;ModelServer: Call start(models)\n    ModelServer-&gt;&gt;ModelRepo: Register Models\n    alt At least one model is ready\n        ModelServer-&gt;&gt;RESTServer: Start REST Server\n        RESTServer--&gt;&gt;ModelServer: REST Server Running\n        ModelServer-&gt;&gt;GRPCServer: Start GRPC Server\n        GRPCServer--&gt;&gt;ModelServer: GRPC Server Running\n    else No models are ready\n        ModelServer--&gt;&gt;User: Raise NoModelReady Exception\n    end\n    ModelServer-&gt;&gt;EventLoop: Run Event Loop\n\n    User-&gt;&gt;ModelServer: Call stop(sig)\n    ModelServer-&gt;&gt;RESTServer: Stop REST Server\n    RESTServer--&gt;&gt;ModelServer: REST Server Stopped\n    ModelServer-&gt;&gt;GRPCServer: Stop GRPC Server\n    GRPCServer--&gt;&gt;ModelServer: GRPC Server Stopped\n    ModelServer-&gt;&gt;ModelRepo: Unload Models\n    ModelRepo--&gt;&gt;ModelServer: Models Unloaded\n    ModelServer--&gt;&gt;User: Server Stopped</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/feast-transformer/#containerize-the-transformer","title":"Containerize the Transformer","text":"<pre><code>kserve/docker\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 uv.lock\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 .dockerignore\n\u251c\u2500\u2500 feast_transformer\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 __main__.py\n\u2514   \u2514\u2500\u2500 feast_transformer.py\n</code></pre> <p>First, create a <code>pyproject.toml</code> file to define the dependencies for your custom transformer. This file will be used by the <code>uv</code> package manager to install the necessary libraries.</p> pyproject.toml<pre><code>[project]\nname = \"feast-transformer\"\nversion = \"0.1.0\"\ndescription = \"Feast Transformer for KServe\"\nrequires-python = \"~=3.11\"\nauthors = [\n    { email = \"kuanchoulai10@gmail.com\" }\n]\nreadme = \"README.md\"\n\ndependencies = [\n    \"kserve==0.15.1\",\n    \"requests&gt;=2.22.0\",\n    \"numpy&gt;=1.16.3\"\n]\n\n\n[build-system]\nrequires = [\"setuptools&gt;=80.0.0\"]\nbuild-backend = \"setuptools.build_meta\"\n</code></pre> <p>Next, use the <code>uv</code> package manager to install the dependencies and generate a lock file. The lock file ensures that the same versions of the dependencies are used across different environments, providing consistency and reliability.</p> <pre><code>uv lock\n</code></pre> <p>Then, create a <code>Dockerfile</code> to build the custom transformer image. This Dockerfile uses a multi-stage build process to ensure that the final image is lightweight and contains only the necessary components.</p> Dockerfile<pre><code># Stage 1: Dependency installation using uv\n# https://docs.astral.sh/uv/guides/integration/docker/#available-images\n# https://github.com/astral-sh/uv-docker-example/blob/main/multistage.Dockerfile\nFROM ghcr.io/astral-sh/uv:python3.11-bookworm-slim AS builder\n\nENV UV_COMPILE_BYTECODE=1 UV_LINK_MODE=copy UV_PYTHON_DOWNLOADS=0\n\nWORKDIR /app\n\n# Install build tools required for psutil\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\\n    gcc python3-dev &amp;&amp; \\\n    apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=uv.lock,target=uv.lock \\\n    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \\\n    uv sync --locked --no-install-project --no-dev\nCOPY . /app\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv sync --locked --no-dev\n\n# Stage 2: Final image\nFROM python:3.11-slim-bookworm\n\nWORKDIR /app\n\nCOPY --from=builder --chown=app:app /app /app\n\nENV PATH=\"/app/.venv/bin:$PATH\"\n\n# Set the entry point for the container\nENTRYPOINT [\"python\", \"-m\", \"feast_transformer\"]\n</code></pre> <p>The first stage (<code>builder</code>) uses the <code>uv</code> package manager to install all dependencies defined in the <code>pyproject.toml</code> file. It also caches dependencies to speed up subsequent builds and compiles Python bytecode for better performance. Additionally, it installs build tools like <code>gcc</code> and <code>python3-dev</code> to handle dependencies requiring compilation, such as <code>psutil</code>.</p> <p>The <code>uv sync</code> command is run twice in this stage. The first <code>uv sync</code> installs only the dependencies without the project files, ensuring that the dependency installation is cached and reused across builds. The second <code>uv sync</code> installs the project files and finalizes the environment. This two-step process minimizes rebuild times by leveraging cached dependencies while still ensuring the application code is up-to-date.</p> <p>The second stage creates the final image. It copies the application and its dependencies from the <code>builder</code> stage into a minimal Python base image. This ensures the final image is small and optimized for production use. The <code>PATH</code> environment variable is updated to include the virtual environment created by <code>uv</code>, and the entry point is set to run the transformer using the <code>python -m feast_transformer</code> command.</p> <p>This approach ensures a clean, efficient, and portable container image for deploying the Feast transformer.</p> <p>Finally, build the Docker image and load it into Minikube. This step is crucial for deploying the transformer in a local Kubernetes environment, allowing you to test and validate the integration with KServe and Feast.</p> <pre><code>docker buildx build \\\n  --platform linux/amd64 \\\n  -t feast-transformer:v0.1.0 \\\n  .\n</code></pre> <pre><code>minikube image load feast-transformer:v0.1.0\n</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/how-it-works/","title":"How It Works?","text":"<p>KServe can be installed in two modes:</p> <ol> <li>Serverless Mode (Recommended): Powered by Knative and Istio, this mode offers benefits such as automatic scaling, enhanced security, simplified traffic management, and seamless integration with serverless workflows.<sup>13</sup></li> <li>RawDeployment Mode: Utilizes native Kubernetes resources like Deployment, Service, Ingress, and Horizontal Pod Autoscaler, providing a more traditional approach to model serving.<sup>14</sup></li> </ol> KServe Architecture on Serverless Mode<sup>1</sup>"},{"location":"side-projects/data2ml-ops/kserve/how-it-works/#architecture-components","title":"Architecture Components","text":""},{"location":"side-projects/data2ml-ops/kserve/how-it-works/#control-plane","title":"Control Plane","text":"KServe Controller <p>Handles the creation of services, ingress resources, model server containers, and model agent containers to facilitate request/response logging, batching, and model retrieval.<sup>17</sup></p> Ingress Gateway <p>Acts as an entry point for directing external or internal traffic to the appropriate services.<sup>17</sup></p> <p>If operating in Serverless mode, the following additional components are included:</p> Knative Serving Controller <p>Manages service revisions, sets up network routing configurations, and provisions serverless containers with a queue proxy to handle traffic metrics and enforce concurrency limits.<sup>17</sup></p> Knative Activator <p>Responsible for reviving pods that have been scaled down to zero and routing incoming requests to them.<sup>17</sup></p> Knative Autoscaler (KPA) <p>Monitors application traffic and dynamically adjusts the number of replicas based on predefined metrics.<sup>17</sup></p>"},{"location":"side-projects/data2ml-ops/kserve/how-it-works/#data-plane","title":"Data Plane","text":"InferenceService <p>An InferenceService in KServe is a Kubernetes custom resource designed to simplify the deployment and management of machine learning models for inference. It integrates components such as predictors, transformers, and explainers, offering capabilities like autoscaling, version control, and traffic splitting to optimize model serving in production.<sup>18</sup></p> Predictor <p>The predictor is the core component of the InferenceService, responsible for hosting the model and exposing it through a network endpoint for inference requests.<sup>18</sup></p> Explainer <p>The explainer provides an optional feature that generates model explanations alongside predictions, offering insights into the model's decision-making process.<sup>18</sup></p> Transformer <p>The transformer allows users to define custom pre-processing and post-processing steps, enabling data transformations before predictions or explanations are generated.<sup>18</sup></p>"},{"location":"side-projects/data2ml-ops/kserve/how-it-works/#serving-runtimes","title":"Serving Runtimes","text":"<p>KServe utilizes two types of Custom Resource Definitions (CRDs) to define model serving environments: <code>ServingRuntimes</code> and <code>ClusterServingRuntimes</code>. The primary distinction between them is their scope\u2014<code>ServingRuntimes</code> are namespace-scoped, while <code>ClusterServingRuntimes</code> are cluster-scoped.</p> ServingRuntime / ClusterServingRuntime <p>These CRDs specify templates for Pods capable of serving one or more specific model formats. Each ServingRuntime includes essential details such as the runtime's container image and the list of supported model formats<sup>2</sup>.</p> <p>KServe provides several pre-configured ClusterServingRuntimes, enabling users to deploy popular model formats without the need to manually define the runtimes.</p> Support Model Formats<sup>2</sup><sup>3</sup>"},{"location":"side-projects/data2ml-ops/kserve/how-it-works/#core-concepts","title":"Core Concepts","text":""},{"location":"side-projects/data2ml-ops/kserve/how-it-works/#inference-service","title":"Inference Service","text":"<p>The following example demonstrates the minimum setup required to deploy an InferenceService in KServe<sup>4</sup>:</p> <pre><code>apiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"sklearn-iris\"\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: sklearn\n      storageUri: \"gs://kfserving-examples/models/sklearn/1.0/model\"\n</code></pre> <p>Predictor:</p> <ul> <li>Model Format: Specifies the framework or format of the model being served. In this example, the model format is <code>sklearn</code>.</li> <li>Storage URI: Indicates the location of the model file. Here, the model is stored in a Google Cloud Storage bucket at <code>gs://kfserving-examples/models/sklearn/1.0/model</code>.</li> </ul> <p>To ensure that the pod can successfully load the model, proper permissions must be configured.<sup>5</sup><sup>6</sup></p> <p>Once applied, your InferenceService will be successfully deployed:</p> <pre><code>kubectl get inferenceservices sklearn-iris -n kserve-test\n</code></pre> <pre><code>NAME           URL                                                 READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                    AGE\nsklearn-iris   http://sklearn-iris.kserve-test.example.com         True           100                              sklearn-iris-predictor-default-47q2g   7d23h\n</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/how-it-works/#inference-protocol","title":"Inference Protocol","text":"<p>KServe's data plane protocol provides a framework-agnostic inference API that works seamlessly across various ML/DL frameworks and model servers. It supports two versions:</p> <ul> <li>v1: Offers support exclusively for REST APIs.<sup>15</sup></li> <li>v2: Extends support to both REST and gRPC APIs.<sup>16</sup></li> </ul> <p>To use v2 REST protocol for inference with the deployed model, you set the <code>protocolVersion</code> field to v2<sup>7</sup>:</p> <pre><code>apiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"mlflow-v2-wine-classifier\"\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: mlflow\n      protocolVersion: v2\n      storageUri: \"gs://kfserving-examples/models/mlflow/wine\"\n</code></pre> <p>Not all server runtimes support v2 inference protocol and gRPC protocol, you should check here<sup>3</sup> for more information.</p> Supported Protocols for Each Server Runtime<sup>3</sup> <p>To use v2 gRPC protocol for inference with the deployed model, you set the container port to be <code>8081</code> and the name of port to be <code>h2c</code><sup>8</sup>(this setup is not for TensorFlow and PyTorch, which have their own settings).</p> <pre><code>apiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"sklearn-v2-iris-grpc\"\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: sklearn\n      protocolVersion: v2\n      runtime: kserve-sklearnserver\n      storageUri: \"gs://kfserving-examples/models/sklearn/1.0/model\"\n      ports:\n        - name: h2c     # knative expects grpc port name to be 'h2c'\n          protocol: TCP\n          containerPort: 8081\n</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/how-it-works/#autoscaling","title":"Autoscaling","text":"<p>Set up based on what condition to scale up the predictor using <code>scaleTarget</code> and  <code>scaleMetric</code>.<sup>9</sup></p> <pre><code>apiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"flowers-sample\"\nspec:\n  predictor:\n    scaleTarget: 1\n    scaleMetric: concurrency # \"qps\" is an option as well\n    model:\n      modelFormat:\n        name: tensorflow\n      storageUri: \"gs://kfserving-examples/models/tensorflow/flowers\"\n</code></pre> <p>Scale to zero using <code>minReplicas: 0</code>.<sup>9</sup></p> <pre><code>apiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"flowers-sample\"\nspec:\n  predictor:\n    minReplicas: 0\n    model:\n      modelFormat:\n        name: tensorflow\n      storageUri: \"gs://kfserving-examples/models/tensorflow/flowers\"\n</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/how-it-works/#canary-rollout","title":"Canary Rollout","text":"<p>KServe supports canary rollout, a deployment strategy that allows you to gradually shift traffic between different versions of a model. This approach minimizes risks by enabling you to test new versions (canary models) with a small percentage of traffic before fully rolling them out.</p> <p>Promote the canary model or roll back to the previous model using the <code>canaryTrafficPercent</code> field. In addition, you can use the <code>serving.kserve.io/enable-tag-routing</code> annotation to route traffic explicitly. This allows you to direct traffic to the canary model (model v2) or the previous model (model v1) by including a tag in the request URL.<sup>10</sup></p>"},{"location":"side-projects/data2ml-ops/kserve/how-it-works/#inference-graph","title":"Inference Graph","text":"<p>Modern ML inference systems are increasingly complex, often requiring multiple models to generate a single prediction. KServe simplifies this process by supporting <code>InferenceGraph</code>, allowing users to define and deploy intricate ML inference pipelines in a declarative and scalable manner for production use.<sup>11</sup></p> Inference Graph<sup>11</sup> InferenceGraph <p>Composed of routing <code>Nodes</code>, each containing a series of routing <code>Steps</code>. Each <code>Step</code> can direct traffic to either an InferenceService or another <code>Node</code> within the graph, making the <code>InferenceGraph</code> highly modular. The <code>InferenceGraph</code> supports four types of Routing <code>Nodes</code>: Sequence, Switch, Ensemble, and Splitter.<sup>11</sup></p> Sequence Node <p>Enables users to define a series of <code>Steps</code> where each <code>Step</code> routes to an <code>InferenceService</code> or another <code>Node</code> in a sequential manner. The output of one <code>Step</code> can be configured to serve as the input for the next <code>Step</code>.<sup>11</sup></p> Ensemble Node <p>Facilitates model ensembles by running multiple models independently and combining their outputs into a single prediction. Various methods, such as majority voting for classification or averaging for regression, can be used to aggregate the results.<sup>11</sup></p> Splitter Node <p>Distributes traffic across multiple targets based on a specified weighted distribution.<sup>11</sup></p> Switch Node <p>Allows users to specify routing conditions to determine which <code>Step</code> to execute. The response is returned as soon as a condition is met. If no conditions are satisfied, the graph returns the original request.<sup>11</sup></p> <p>Example<sup>12</sup></p> <pre><code>apiVersion: \"serving.kserve.io/v1alpha1\"\nkind: \"InferenceGraph\"\nmetadata:\n  name: \"dog-breed-pipeline\"\nspec:\n  nodes:\n    root:\n      routerType: Sequence\n      steps:\n      - serviceName: cat-dog-classifier\n        name: cat_dog_classifier # step name\n      - serviceName: dog-breed-classifier\n        name: dog_breed_classifier\n        data: $request\n        condition: \"[@this].#(predictions.0==\\\"dog\\\")\"\n  resources:\n    requests:\n      cpu: 100m\n      memory: 256Mi\n    limits:\n      cpu: 1\n      memory: 1Gi\n</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/how-it-works/#behind-the-scenes","title":"Behind the Scenes","text":"<p>When you apply an <code>InferenceService</code> using <code>kubectl apply</code>, the following steps occur behind the scenes (in serverless mode):</p> <ol> <li>The KServe Controller receives the request and deploys a Knative Service.</li> <li>A Knative Revision is prepared to manage versioning and traffic routing.</li> <li>The Transformer and Predictor Pods are deployed, with autoscaling configurations set up via the Knative Autoscaler.</li> <li>The Predictor Pod uses an InitContainer (Storage Initializer) to fetch the model from a storage location (e.g., GCS, S3).</li> <li>Once the model is retrieved, the Predictor Pod deploys the model using the specified Server Runtime.</li> <li>The Predictor Pod exposes its endpoint through a Queue Proxy, which handles traffic metrics and concurrency limits. The endpoint is then made accessible externally via a Service.</li> <li>The Transformer Pod, which handles pre-processing and post-processing logic, does not require a storage initializer. It simply deploys the transformer container.</li> <li>Similar to the Predictor Pod, the Transformer Pod exposes its endpoint through a Queue Proxy, making it accessible externally via a Service.</li> <li>Finally, the backend of your AI application can call the <code>InferenceService</code> endpoints to execute pre-processing, prediction, and post-processing. The system dynamically scales up or down based on the configured autoscaling metrics.</li> </ol> <p>This seamless orchestration ensures efficient and scalable model serving for your AI applications.</p> <ol> <li> <p>KServe Docs \u21a9</p> </li> <li> <p>Serving Runtimes | Concepts \u21a9\u21a9</p> </li> <li> <p>Model Serving Runtimes | Supported Model Frameworks/Formats \u21a9\u21a9\u21a9</p> </li> <li> <p>First InferenceService \u21a9</p> </li> <li> <p>Deploy InferenceService with a saved model on S3 \u21a9</p> </li> <li> <p>Deploy InferenceService with a saved model on GCS \u21a9</p> </li> <li> <p>Deploy MLflow models with InferenceService\u00b6 \u21a9</p> </li> <li> <p>Deploy Scikit-learn models with InferenceService \u21a9</p> </li> <li> <p>Autoscale InferenceService with Knative Autoscaler \u21a9\u21a9</p> </li> <li> <p>Canary Rollout Example \u21a9</p> </li> <li> <p>Inference Graph \u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Deploy Image Processing Inference pipeline with InferenceGraph \u21a9</p> </li> <li> <p>Serverless Installation Guide \u21a9</p> </li> <li> <p>Kubernetes Deployment Installation Guide \u21a9</p> </li> <li> <p>V1 Inference Protocol \u21a9</p> </li> <li> <p>V2 Inference Protocol \u21a9</p> </li> <li> <p>Control Plane \u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Data Plane \u21a9\u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"side-projects/data2ml-ops/kserve/in-the-bigger-picture/","title":"In the Bigger Picture","text":""},{"location":"side-projects/data2ml-ops/kserve/in-the-bigger-picture/#history-current-status","title":"History &amp; Current Status","text":"<p>KServe is an open-source, serverless machine learning inference platform designed to run on Kubernetes. Originally developed in 2019 by teams from Google, IBM, Bloomberg, NVIDIA, and Seldon, it was initially released as KFServing. In September 2021, the project was rebranded to KServe and transitioned to an independent GitHub organization. KServe is currently incubated under the LF AI &amp; Data Foundation.  \ufffc \ufffc \ufffc</p> <p>As of May 2025, KServe has over 4,000 stars on GitHub and more than 1,000 contributors. The community is active, hosting biweekly public meetings and maintaining comprehensive documentation. KServe is utilized by organizations such as Amazon Web Services, Bloomberg, Gojek, IBM, NVIDIA, and Samsung SDS.</p>"},{"location":"side-projects/data2ml-ops/kserve/in-the-bigger-picture/#alternatives","title":"Alternatives","text":"<p>We have researched the alternatives in the previous article. See here for more.</p>"},{"location":"side-projects/data2ml-ops/kserve/installation/","title":"Install Kserve in Serverless Mode","text":"<p>To install KServe in serverless mode, you need to first install three components: Knative Serving, Networking Layer, and Cert Manager.</p> <p>KServe provides a recommended version matrix for Knative Serving and Istio based on your Kubernetes version.</p> Recommended Version Matrix<sup>1</sup> <p>Since my Kubernetes version is 1.30, I will install the following versions:</p> <ul> <li>Knative Serving v1.15.0</li> <li>Istio v1.22.8</li> <li>Cert Manager v1.17.2</li> <li>KServe v0.15.1</li> </ul>"},{"location":"side-projects/data2ml-ops/kserve/installation/#install-knative-serving","title":"Install Knative Serving<sup>2</sup>","text":"<p>Before install, nothing</p> <pre><code>kubectl get ns\n</code></pre> <p>Expected Output</p> <pre><code>NAME              STATUS   AGE\ndefault           Active   15d\nkube-node-lease   Active   15d\nkube-public       Active   15d\nkube-system       Active   15d\n</code></pre> <p>Install the required custom resources by running the command:</p> <pre><code>kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.15.0/serving-crds.yaml\n</code></pre> Expected Output <pre><code>customresourcedefinition.apiextensions.k8s.io/certificates.networking.internal.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/configurations.serving.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/clusterdomainclaims.networking.internal.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/domainmappings.serving.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/ingresses.networking.internal.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/metrics.autoscaling.internal.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/podautoscalers.autoscaling.internal.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/revisions.serving.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/routes.serving.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/serverlessservices.networking.internal.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/services.serving.knative.dev created\ncustomresourcedefinition.apiextensions.k8s.io/images.caching.internal.knative.dev created\n</code></pre> <p>Deploy the core components of Knative Serving to you Kubernetes cluster (namespace <code>knative-serving</code>) by running the command:</p> <pre><code>kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.15.0/serving-core.yaml\n</code></pre> Expected Output <pre><code>namespace/knative-serving created\nrole.rbac.authorization.k8s.io/knative-serving-activator created\nclusterrole.rbac.authorization.k8s.io/knative-serving-activator-cluster created\nclusterrole.rbac.authorization.k8s.io/knative-serving-aggregated-addressable-resolver created\nclusterrole.rbac.authorization.k8s.io/knative-serving-addressable-resolver created\nclusterrole.rbac.authorization.k8s.io/knative-serving-namespaced-admin created\nclusterrole.rbac.authorization.k8s.io/knative-serving-namespaced-edit created\nclusterrole.rbac.authorization.k8s.io/knative-serving-namespaced-view created\nclusterrole.rbac.authorization.k8s.io/knative-serving-core created\nclusterrole.rbac.authorization.k8s.io/knative-serving-podspecable-binding created\nserviceaccount/controller created\nclusterrole.rbac.authorization.k8s.io/knative-serving-admin created\nclusterrolebinding.rbac.authorization.k8s.io/knative-serving-controller-admin created\nclusterrolebinding.rbac.authorization.k8s.io/knative-serving-controller-addressable-resolver created\nserviceaccount/activator created\nrolebinding.rbac.authorization.k8s.io/knative-serving-activator created\nclusterrolebinding.rbac.authorization.k8s.io/knative-serving-activator-cluster created\ncustomresourcedefinition.apiextensions.k8s.io/images.caching.internal.knative.dev unchanged\ncertificate.networking.internal.knative.dev/routing-serving-certs created\ncustomresourcedefinition.apiextensions.k8s.io/certificates.networking.internal.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/configurations.serving.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/clusterdomainclaims.networking.internal.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/domainmappings.serving.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/ingresses.networking.internal.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/metrics.autoscaling.internal.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/podautoscalers.autoscaling.internal.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/revisions.serving.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/routes.serving.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/serverlessservices.networking.internal.knative.dev unchanged\ncustomresourcedefinition.apiextensions.k8s.io/services.serving.knative.dev unchanged\nimage.caching.internal.knative.dev/queue-proxy created\nconfigmap/config-autoscaler created\nconfigmap/config-certmanager created\nconfigmap/config-defaults created\nconfigmap/config-deployment created\nconfigmap/config-domain created\nconfigmap/config-features created\nconfigmap/config-gc created\nconfigmap/config-leader-election created\nconfigmap/config-logging created\nconfigmap/config-network created\nconfigmap/config-observability created\nconfigmap/config-tracing created\nhorizontalpodautoscaler.autoscaling/activator created\npoddisruptionbudget.policy/activator-pdb created\ndeployment.apps/activator created\nservice/activator-service created\ndeployment.apps/autoscaler created\nservice/autoscaler created\ndeployment.apps/controller created\nservice/controller created\nhorizontalpodautoscaler.autoscaling/webhook created\npoddisruptionbudget.policy/webhook-pdb created\ndeployment.apps/webhook created\nservice/webhook created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/config.webhook.serving.knative.dev created\nmutatingwebhookconfiguration.admissionregistration.k8s.io/webhook.serving.knative.dev created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/validation.webhook.serving.knative.dev created\nsecret/webhook-certs created\n</code></pre> <p>Once the deployment finished, you can see <code>knative-serving</code> namespace created and all the resources inside:</p> <pre><code>kubectl get ns\n</code></pre> <p>Expected Output</p> <pre><code>NAME              STATUS   AGE\ndefault           Active   15d\nknative-serving   Active   17s\nkube-node-lease   Active   15d\nkube-public       Active   15d\nkube-system       Active   15d\n</code></pre> <pre><code>kubectl get all -n knative-serving\n</code></pre> <p>Expected Output</p> <pre><code>kubectl get all -n knative-serving\nNAME                              READY   STATUS    RESTARTS   AGE\npod/activator-bccd57594-l5n5m     1/1     Running   0          3m16s\npod/autoscaler-7c6d8b8456-lwn6b   1/1     Running   0          3m16s\npod/controller-6458dc4845-bv7st   1/1     Running   0          3m16s\npod/webhook-68b5b4c69-8jv7q       1/1     Running   0          3m16s\n\nNAME                                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                   AGE\nservice/activator-service            ClusterIP   10.110.50.161    &lt;none&gt;        9090/TCP,8008/TCP,80/TCP,81/TCP,443/TCP   3m16s\nservice/autoscaler                   ClusterIP   10.105.23.244    &lt;none&gt;        9090/TCP,8008/TCP,8080/TCP                3m16s\nservice/autoscaler-bucket-00-of-01   ClusterIP   10.108.159.205   &lt;none&gt;        8080/TCP                                  2m55s\nservice/controller                   ClusterIP   10.110.203.67    &lt;none&gt;        9090/TCP,8008/TCP                         3m16s\nservice/webhook                      ClusterIP   10.104.187.107   &lt;none&gt;        9090/TCP,8008/TCP,443/TCP                 3m16s\n\nNAME                         READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/activator    1/1     1            1           3m16s\ndeployment.apps/autoscaler   1/1     1            1           3m16s\ndeployment.apps/controller   1/1     1            1           3m16s\ndeployment.apps/webhook      1/1     1            1           3m16s\n\nNAME                                    DESIRED   CURRENT   READY   AGE\nreplicaset.apps/activator-bccd57594     1         1         1       3m16s\nreplicaset.apps/autoscaler-7c6d8b8456   1         1         1       3m16s\nreplicaset.apps/controller-6458dc4845   1         1         1       3m16s\nreplicaset.apps/webhook-68b5b4c69       1         1         1       3m16s\n\nNAME                                            REFERENCE              TARGETS               MINPODS   MAXPODS   REPLICAS   AGE\nhorizontalpodautoscaler.autoscaling/activator   Deployment/activator   cpu: &lt;unknown&gt;/100%   1         20        1          3m16s\nhorizontalpodautoscaler.autoscaling/webhook     Deployment/webhook     cpu: &lt;unknown&gt;/100%   1         5         1          3m16s\n</code></pre> <p>\u90e8\u7f72\u5b8c\u6210\u5f8c\uff0c\u53ef\u4ee5\u770b\u5230\u6709\u4ee5\u4e0b\u9019\u56db\u500bpods\uff0c\u5206\u5225\u8ca0\u8cac</p> <ul> <li>activator</li> <li>autoscaler</li> <li>controller</li> <li>webhook</li> </ul>"},{"location":"side-projects/data2ml-ops/kserve/installation/#install-networking-layer-istio","title":"Install Networking Layer - Istio<sup>3</sup>","text":"<p>Install Istio 1.22.8 on the home directory</p> <pre><code>cd $HOME\ncurl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.22.8 sh -\n</code></pre> Expected Output <pre><code>% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                Dload  Upload   Total   Spent    Left  Speed\n100   102  100   102    0     0    350      0 --:--:-- --:--:-- --:--:--   351\n100  5124  100  5124    0     0   6834      0 --:--:-- --:--:-- --:--:-- 13343\n\nDownloading istio-1.22.8 from https://github.com/istio/istio/releases/download/1.22.8/istio-1.22.8-osx-arm64.tar.gz ...\n\nIstio 1.22.8 download complete!\n\nThe Istio release archive has been downloaded to the istio-1.22.8 directory.\n\nTo configure the istioctl client tool for your workstation,\nadd the /Users/kcl/istio-1.22.8/bin directory to your environment path variable with:\n    export PATH=\"$PATH:/Users/kcl/istio-1.22.8/bin\"\n\nBegin the Istio pre-installation check by running:\n    istioctl x precheck \n\nTry Istio in ambient mode\n    https://istio.io/latest/docs/ambient/getting-started/\nTry Istio in sidecar mode\n    https://istio.io/latest/docs/setup/getting-started/\nInstall guides for ambient mode\n    https://istio.io/latest/docs/ambient/install/\nInstall guides for sidecar mode\n    https://istio.io/latest/docs/setup/install/\n\nNeed more information? Visit https://istio.io/latest/docs/\n</code></pre> <p>Add the <code>$HOME/istio-1.22.8/bin</code> directory to your environment path variable</p> <pre><code>export PATH=\"$PATH:$HOME/istio-1.22.8/bin\"\n</code></pre> <p>Check the versions</p> <pre><code>istioctl version\n</code></pre> <p>Expected Output</p> <pre><code>client version: 1.22.8\ncontrol plane version: 1.22.8\ndata plane version: 1.22.8 (1 proxies)\n</code></pre> <p>You can easily install and customize your Istio installation with <code>istioctl</code>. It will deploy the resources to your kubernetes cluster in the namespace <code>istio-system</code></p> <pre><code>istioctl install -y\n</code></pre> <p>Expected Output</p> <pre><code>WARNING: Istio 1.22.0 may be out of support (EOL) already: see https://istio.io/latest/docs/releases/supported-releases/ for supported releases\n\u2714 Istio core installed                       \n\u2714 Istiod installed                           \n\u2714 Ingress gateways installed\n\u2714 Installation complete\nMade this installation the default for injection and validation.\n</code></pre> <p>Check all the deployed resources</p> <pre><code>kubectl get ns\n</code></pre> <p>Expected Output</p> <pre><code>NAME              STATUS   AGE\ndefault           Active   15d\nistio-system      Active   62s\nknative-serving   Active   18m\nkube-node-lease   Active   15d\nkube-public       Active   15d\nkube-system       Active   15d\n</code></pre> <pre><code>kubectl get all -n istio-system\n</code></pre> <p>Expected Output</p> <pre><code>NAME                                        READY   STATUS    RESTARTS   AGE\npod/istio-ingressgateway-767ff7b4b6-7wxzk   1/1     Running   0          5m52s\npod/istiod-7bc77d764b-vh66z                 1/1     Running   0          6m18s\n\nNAME                           TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                      AGE\nservice/istio-ingressgateway   LoadBalancer   10.97.252.211   &lt;pending&gt;     15021:32213/TCP,80:31540/TCP,443:30462/TCP   5m52s\nservice/istiod                 ClusterIP      10.96.206.177   &lt;none&gt;        15010/TCP,15012/TCP,443/TCP,15014/TCP        6m18s\n\nNAME                                   READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/istio-ingressgateway   1/1     1            1           5m52s\ndeployment.apps/istiod                 1/1     1            1           6m19s\n\nNAME                                              DESIRED   CURRENT   READY   AGE\nreplicaset.apps/istio-ingressgateway-767ff7b4b6   1         1         1       5m52s\nreplicaset.apps/istiod-7bc77d764b                 1         1         1       6m18s\n\nNAME                                                       REFERENCE                         TARGETS              MINPODS   MAXPODS   REPLICAS   AGE\nhorizontalpodautoscaler.autoscaling/istio-ingressgateway   Deployment/istio-ingressgateway   cpu: &lt;unknown&gt;/80%   1         5         1          5m52s\nhorizontalpodautoscaler.autoscaling/istiod                 Deployment/istiod                 cpu: &lt;unknown&gt;/80%   1         5         1          6m18s\n</code></pre> <p>\u8aaa\u660e</p> <ul> <li>istio-ingressgateway</li> <li>istiod</li> <li>external-ip pending</li> </ul>"},{"location":"side-projects/data2ml-ops/kserve/installation/#integrate-istio-with-knative-serving","title":"Integrate Istio with Knative Serving<sup>3</sup>","text":"<p>To integrate Istio with Knative Serving install the Knative Istio controller by running the command</p> <pre><code>kubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.15.0/net-istio.yaml\n</code></pre> <p>Expected Output</p> <pre><code>clusterrole.rbac.authorization.k8s.io/knative-serving-istio created\ngateway.networking.istio.io/knative-ingress-gateway created\ngateway.networking.istio.io/knative-local-gateway created\nservice/knative-local-gateway created\nconfigmap/config-istio created\npeerauthentication.security.istio.io/webhook created\npeerauthentication.security.istio.io/net-istio-webhook created\ndeployment.apps/net-istio-controller created\ndeployment.apps/net-istio-webhook created\nsecret/net-istio-webhook-certs created\nservice/net-istio-webhook created\nmutatingwebhookconfiguration.admissionregistration.k8s.io/webhook.istio.networking.internal.knative.dev created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/config.webhook.istio.networking.internal.knative.dev created\n</code></pre> <p>Verify the integration</p> <pre><code>kubectl get all -n knative-serving\n</code></pre> <p>Expected Output</p> <pre><code>NAME                                        READY   STATUS    RESTARTS   AGE\npod/activator-bccd57594-l5n5m               1/1     Running   0          28m\npod/autoscaler-7c6d8b8456-lwn6b             1/1     Running   0          28m\npod/controller-6458dc4845-bv7st             1/1     Running   0          28m\npod/net-istio-controller-6b847d477f-5vtcd   1/1     Running   0          102s\npod/net-istio-webhook-856498bfc7-tswxz      1/1     Running   0          102s\npod/webhook-68b5b4c69-8jv7q                 1/1     Running   0          28m\n\nNAME                                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                   AGE\nservice/activator-service            ClusterIP   10.110.50.161    &lt;none&gt;        9090/TCP,8008/TCP,80/TCP,81/TCP,443/TCP   28m\nservice/autoscaler                   ClusterIP   10.105.23.244    &lt;none&gt;        9090/TCP,8008/TCP,8080/TCP                28m\nservice/autoscaler-bucket-00-of-01   ClusterIP   10.108.159.205   &lt;none&gt;        8080/TCP                                  27m\nservice/controller                   ClusterIP   10.110.203.67    &lt;none&gt;        9090/TCP,8008/TCP                         28m\nservice/net-istio-webhook            ClusterIP   10.97.168.104    &lt;none&gt;        9090/TCP,8008/TCP,443/TCP                 102s\nservice/webhook                      ClusterIP   10.104.187.107   &lt;none&gt;        9090/TCP,8008/TCP,443/TCP                 28m\n\nNAME                                   READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/activator              1/1     1            1           28m\ndeployment.apps/autoscaler             1/1     1            1           28m\ndeployment.apps/controller             1/1     1            1           28m\ndeployment.apps/net-istio-controller   1/1     1            1           102s\ndeployment.apps/net-istio-webhook      1/1     1            1           102s\ndeployment.apps/webhook                1/1     1            1           28m\n\nNAME                                              DESIRED   CURRENT   READY   AGE\nreplicaset.apps/activator-bccd57594               1         1         1       28m\nreplicaset.apps/autoscaler-7c6d8b8456             1         1         1       28m\nreplicaset.apps/controller-6458dc4845             1         1         1       28m\nreplicaset.apps/net-istio-controller-6b847d477f   1         1         1       102s\nreplicaset.apps/net-istio-webhook-856498bfc7      1         1         1       102s\nreplicaset.apps/webhook-68b5b4c69                 1         1         1       28m\n\nNAME                                            REFERENCE              TARGETS               MINPODS   MAXPODS   REPLICAS   AGE\nhorizontalpodautoscaler.autoscaling/activator   Deployment/activator   cpu: &lt;unknown&gt;/100%   1         20        1          28m\nhorizontalpodautoscaler.autoscaling/webhook     Deployment/webhook     cpu: &lt;unknown&gt;/100%   1         5         1          28m\n</code></pre> <p>Verify the installation for Istio</p> <pre><code>kubectl get all -n istio-system\n</code></pre> <p>Expected Output</p> <pre><code>kubectl get all -n istio-system\nNAME                                        READY   STATUS    RESTARTS   AGE\npod/istio-ingressgateway-767ff7b4b6-7wxzk   1/1     Running   0          12m\npod/istiod-7bc77d764b-vh66z                 1/1     Running   0          13m\n\nNAME                            TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                      AGE\nservice/istio-ingressgateway    LoadBalancer   10.97.252.211   &lt;pending&gt;     15021:32213/TCP,80:31540/TCP,443:30462/TCP   12m\nservice/istiod                  ClusterIP      10.96.206.177   &lt;none&gt;        15010/TCP,15012/TCP,443/TCP,15014/TCP        13m\nservice/knative-local-gateway   ClusterIP      10.101.124.46   &lt;none&gt;        80/TCP,443/TCP                               4m27s\n\nNAME                                   READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/istio-ingressgateway   1/1     1            1           12m\ndeployment.apps/istiod                 1/1     1            1           13m\n\nNAME                                              DESIRED   CURRENT   READY   AGE\nreplicaset.apps/istio-ingressgateway-767ff7b4b6   1         1         1       12m\nreplicaset.apps/istiod-7bc77d764b                 1         1         1       13m\n\nNAME                                                       REFERENCE                         TARGETS              MINPODS   MAXPODS   REPLICAS   AGE\nhorizontalpodautoscaler.autoscaling/istio-ingressgateway   Deployment/istio-ingressgateway   cpu: &lt;unknown&gt;/80%   1         5         1          12m\nhorizontalpodautoscaler.autoscaling/istiod                 Deployment/istiod                 cpu: &lt;unknown&gt;/80%   1         5         1          13m\n</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/installation/#configure-dns","title":"Configure DNS<sup>4</sup>","text":"<p>You can configure DNS to prevent the need to run <code>curl</code> commands with a host header. Knative provides a Kubernetes Job called default-domain that configures Knative Serving to use sslip.io as the default DNS suffix. This will only work if the cluster LoadBalancer Service exposes an IPv4 address or hostname, so it will not work with IPv6 clusters or local setups like minikube unless <code>minikube tunnel</code> is running.</p> <p>First, run the <code>minikube tunnel</code> command:</p> <pre><code>minikube tunnel\n</code></pre> <p>Expected Output</p> <pre><code>\u2705  Tunnel successfully started\n\n\ud83d\udccc  NOTE: Please do not close this terminal as this process must stay alive for the tunnel to be accessible ...\n\n\u2757  The service/ingress istio-ingressgateway requires privileged ports to be exposed: [80 443]\n\ud83d\udd11  sudo permission will be asked for it.\n\ud83c\udfc3  Starting tunnel for service istio-ingressgateway.\n</code></pre> <p>You can see that the Istio Ingress Gayeway now has external IP:</p> <pre><code>kubectl get svc istio-ingressgateway -n istio-system\n</code></pre> <p>Expected Output</p> <pre><code>NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                      AGE\nistio-ingressgateway   LoadBalancer   10.97.200.129   127.0.0.1     15021:31297/TCP,80:32665/TCP,443:30210/TCP   71m\n</code></pre> <p>Then we run the <code>default-domain</code> job to configure Knative Serving to use sslip.io as the default DNS suffix</p> <pre><code>kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.15.0/serving-default-domain.yaml\n</code></pre> <p>Expected Output</p> <pre><code>job.batch/default-domain created\nservice/default-domain-service created\n</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/installation/#install-cert-manager","title":"Install Cert Manager<sup>5</sup>","text":"<p>\u5b89\u88dd\u524d\uff0cnamespaces\u88e1\u6c92\u6709<code>cert-manager</code></p> <pre><code>kubectl get ns\n</code></pre> <pre><code>NAME              STATUS   AGE\ndefault           Active   15d\nistio-system      Active   35m\nknative-serving   Active   53m\nkube-node-lease   Active   15d\nkube-public       Active   15d\nkube-system       Active   15d\n</code></pre> <p>\u90e8\u7f72Cert Manager</p> <pre><code>kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.17.2/cert-manager.yaml\n</code></pre> Expected Output <pre><code>namespace/cert-manager created\ncustomresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created\ncustomresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created\ncustomresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created\ncustomresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created\ncustomresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created\ncustomresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created\nserviceaccount/cert-manager-cainjector created\nserviceaccount/cert-manager created\nserviceaccount/cert-manager-webhook created\nclusterrole.rbac.authorization.k8s.io/cert-manager-cainjector created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created\nclusterrole.rbac.authorization.k8s.io/cert-manager-cluster-view created\nclusterrole.rbac.authorization.k8s.io/cert-manager-view created\nclusterrole.rbac.authorization.k8s.io/cert-manager-edit created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests created\nclusterrole.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created\nrole.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created\nrole.rbac.authorization.k8s.io/cert-manager:leaderelection created\nrole.rbac.authorization.k8s.io/cert-manager-tokenrequest created\nrole.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created\nrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created\nrolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection created\nrolebinding.rbac.authorization.k8s.io/cert-manager-cert-manager-tokenrequest created\nrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created\nservice/cert-manager-cainjector created\nservice/cert-manager created\nservice/cert-manager-webhook created\ndeployment.apps/cert-manager-cainjector created\ndeployment.apps/cert-manager created\ndeployment.apps/cert-manager-webhook created\nmutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created\n</code></pre> <p>\u4f86\u770b\u770b\u90e8\u7f72\u4e86\u54ea\u4e9b\u6771\u897f</p> <pre><code>kubectl get ns\n</code></pre> <p>Expected Output</p> <pre><code>NAME              STATUS   AGE\ncert-manager      Active   2m59s\ndefault           Active   15d\nistio-system      Active   40m\nknative-serving   Active   58m\nkube-node-lease   Active   15d\nkube-public       Active   15d\nkube-system       Active   15d\n</code></pre> <pre><code>kubectl get all -n cert-manager\n</code></pre> <p>Expected Output</p> <pre><code>NAME                                           READY   STATUS    RESTARTS   AGE\npod/cert-manager-6687d8765c-v8zfd              1/1     Running   0          3m55s\npod/cert-manager-cainjector-764498cfc8-m7rs2   1/1     Running   0          3m55s\npod/cert-manager-webhook-74c74b87d7-dsz9x      1/1     Running   0          3m55s\n\nNAME                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)            AGE\nservice/cert-manager              ClusterIP   10.96.219.58     &lt;none&gt;        9402/TCP           3m55s\nservice/cert-manager-cainjector   ClusterIP   10.109.144.231   &lt;none&gt;        9402/TCP           3m55s\nservice/cert-manager-webhook      ClusterIP   10.107.111.91    &lt;none&gt;        443/TCP,9402/TCP   3m55s\n\nNAME                                      READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/cert-manager              1/1     1            1           3m55s\ndeployment.apps/cert-manager-cainjector   1/1     1            1           3m55s\ndeployment.apps/cert-manager-webhook      1/1     1            1           3m55s\n\nNAME                                                 DESIRED   CURRENT   READY   AGE\nreplicaset.apps/cert-manager-6687d8765c              1         1         1       3m55s\nreplicaset.apps/cert-manager-cainjector-764498cfc8   1         1         1       3m55s\nreplicaset.apps/cert-manager-webhook-74c74b87d7      1         1         1       3m55s\n</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/installation/#install-kserve","title":"Install KServe<sup>6</sup>","text":"<pre><code>kubectl create ns kserve\n</code></pre> <p>Expected Output</p> <pre><code>namespace/kserve created\n</code></pre> <p>Install KServe CRDs</p> <pre><code>helm install kserve-crd oci://ghcr.io/kserve/charts/kserve-crd \\\n  --version v0.15.1 \\\n  -n kserve\n</code></pre> <p>Expected Output</p> <pre><code>Pulled: ghcr.io/kserve/charts/kserve-crd:v0.15.1\nDigest: sha256:b5f4f22fae8fa747ef839e1b228e74e97a78416235eb5f35da49110d25b3d1e7\nNAME: kserve-crd\nLAST DEPLOYED: Thu May 22 22:02:37 2025\nNAMESPACE: kserve\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <p>Deploy KServe</p> <pre><code>helm install kserve oci://ghcr.io/kserve/charts/kserve \\\n  --version v0.15.1 \\\n  -n kserve\n</code></pre> <p>Expected Output</p> <pre><code>Pulled: ghcr.io/kserve/charts/kserve:v0.15.1\nDigest: sha256:e65039d9e91b16d429f5fb56528e15a4695ff106a41eeae07f1f697abe974bd5\nNAME: kserve\nLAST DEPLOYED: Thu May 22 22:02:52 2025\nNAMESPACE: kserve\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <pre><code>kubectl get all -n kserve\n</code></pre> <p>Expected Output</p> <pre><code>NAME                                             READY   STATUS    RESTARTS   AGE\npod/kserve-controller-manager-6cb87dcc55-2zrgm   2/2     Running   0          2m38s\npod/modelmesh-controller-6f5bdb97db-878bb        1/1     Running   0          2m38s\n\nNAME                                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE\nservice/kserve-controller-manager-service   ClusterIP   10.97.113.121    &lt;none&gt;        8443/TCP                     2m38s\nservice/kserve-webhook-server-service       ClusterIP   10.103.116.193   &lt;none&gt;        443/TCP                      2m38s\nservice/modelmesh-serving                   ClusterIP   None             &lt;none&gt;        8033/TCP,8008/TCP,2112/TCP   2m4s\nservice/modelmesh-webhook-server-service    ClusterIP   10.97.175.193    &lt;none&gt;        9443/TCP                     2m38s\n\nNAME                                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/kserve-controller-manager          1/1     1            1           2m38s\ndeployment.apps/modelmesh-controller               1/1     1            1           2m38s\ndeployment.apps/modelmesh-serving-mlserver-1.x     0/0     0            0           2m4s\ndeployment.apps/modelmesh-serving-ovms-1.x         0/0     0            0           2m4s\ndeployment.apps/modelmesh-serving-torchserve-0.x   0/0     0            0           2m4s\ndeployment.apps/modelmesh-serving-triton-2.x       0/0     0            0           2m4s\n\nNAME                                                          DESIRED   CURRENT   READY   AGE\nreplicaset.apps/kserve-controller-manager-6cb87dcc55          1         1         1       2m38s\nreplicaset.apps/modelmesh-controller-6f5bdb97db               1         1         1       2m38s\nreplicaset.apps/modelmesh-serving-mlserver-1.x-57d65d9fdd     0         0         0       2m4s\nreplicaset.apps/modelmesh-serving-ovms-1.x-5488c8f4f9         0         0         0       2m4s\nreplicaset.apps/modelmesh-serving-torchserve-0.x-67f9485cb9   0         0         0       2m4s\nreplicaset.apps/modelmesh-serving-triton-2.x-66756bc646       0         0         0       2m4s\n</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/installation/#summary","title":"Summary","text":"Install and Uninstall KServe Scripts install.sh<pre><code>#!/bin/bash\n\nset -e\n\necho \"\ud83d\ude80 Installing Knative + Istio + KServe...\"\n\n# Install Knative Serving CRDs and Core\nkubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.15.0/serving-crds.yaml\nkubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.15.0/serving-core.yaml\n\n# Download and install Istio\ncurl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.22.8 DESTDIR=$HOME sh -\nexport PATH=\"$PATH:$HOME/istio-1.22.8/bin\"\n\n# Add Istio to .zshrc if not already present\ngrep -qxF 'export PATH=\"$PATH:$HOME/istio-1.22.8/bin\"' ~/.zshrc || echo 'export PATH=\"$PATH:$HOME/istio-1.22.8/bin\"' &gt;&gt; ~/.zshrc\n\n# Install Istio components\n$HOME/istio-1.22.8/bin/istioctl install -y\n\n# Install Knative net-istio integration\nkubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.15.0/net-istio.yaml\n\n# Install cert-manager\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.17.2/cert-manager.yaml\n\n# Create namespace for KServe (if not already exists)\nkubectl create ns kserve --dry-run=client -o yaml | kubectl apply -f -\n\n# Install KServe CRDs and components\nhelm install kserve-crd oci://ghcr.io/kserve/charts/kserve-crd \\\n  --version v0.15.1 \\\n  -n kserve\n\nhelm install kserve oci://ghcr.io/kserve/charts/kserve \\\n  --version v0.15.1 \\\n  -n kserve\n\necho \"\u2705 Installation complete. You may want to run:\"\necho \"   source ~/.zshrc\"\n</code></pre> uninstall.sh<pre><code>#!/bin/bash\n\nset -e\n\necho \"Uninstalling KServe, Knative, Istio, and Cert-Manager...\"\n\n# Uninstall KServe\nhelm uninstall kserve -n kserve || echo \"kserve not found\"\nhelm uninstall kserve-crd -n kserve || echo \"kserve-crd not found\"\nkubectl delete ns kserve --ignore-not-found\n\n# Uninstall cert-manager\nkubectl delete -f https://github.com/cert-manager/cert-manager/releases/download/v1.17.2/cert-manager.yaml || echo \"cert-manager not installed\"\nkubectl delete ns cert-manager --ignore-not-found\n\n# Uninstall Knative net-istio and serving\nkubectl delete -f https://github.com/knative/net-istio/releases/download/knative-v1.15.0/net-istio.yaml || echo \"net-istio not installed\"\nkubectl delete -f https://github.com/knative/serving/releases/download/knative-v1.15.0/serving-core.yaml || echo \"knative-core not installed\"\nkubectl delete -f https://github.com/knative/serving/releases/download/knative-v1.15.0/serving-crds.yaml || echo \"knative-crds not installed\"\n\n# Uninstall Istio\n$HOME/istio-1.22.8/bin/istioctl uninstall --purge -y || echo \"istioctl uninstall failed\"\nkubectl delete ns istio-system --ignore-not-found\n\n# Optional: Remove istioctl binary and path from .zshrc\necho \"Cleaning up istioctl binary and .zshrc entry...\"\nsed -i '' '/istio-1.22.8\\/bin/d' ~/.zshrc 2&gt;/dev/null || true\nrm -rf $HOME/istio-1.22.8\n\necho \"Uninstallation completed.\"\n</code></pre> <ol> <li> <p>Serverless Installation Guide \u21a9</p> </li> <li> <p>Installing Knative Serving using YAML files \u21a9</p> </li> <li> <p>Installing Istio for Knative \u21a9\u21a9</p> </li> <li> <p>Configure DNS \u21a9</p> </li> <li> <p>Install Cert Manager \u21a9</p> </li> <li> <p>Install KServe using Helm \u21a9</p> </li> </ol>"},{"location":"side-projects/data2ml-ops/kserve/transformer-advantages/","title":"Move Feature Retrieval to KServe Transformer","text":"<p>In real-time fraud detection, response time and consistency are critical. When serving machine learning models on Kubernetes, the way you integrate feature lookups into your inference flow can have significant architectural implications. Let\u2019s explore two setups and highlight the benefits of offloading feature retrieval to KServe\u2019s built-in Transformer component by using the Feast online feature server.</p>"},{"location":"side-projects/data2ml-ops/kserve/transformer-advantages/#two-approaches-to-feature-retrieval","title":"Two Approaches to Feature Retrieval","text":""},{"location":"side-projects/data2ml-ops/kserve/transformer-advantages/#option-1-feature-lookup-in-backend-no-transformer","title":"Option 1: Feature Lookup in Backend (No Transformer)","text":"<p>In this setup, the Backend is responsible for querying the Online Feature Server (OFS), retrieving features based on an input ID (e.g., <code>entity_id</code>), and passing the features to the predictor.</p> <pre><code>sequenceDiagram\n    participant User as User\n    participant FE as Frontend\n    participant BE as Backend\n    participant Predictor as InferenceService:Predictor\n    participant OFS as Online Feature Server\n\n    User-&gt;&gt;FE: Trigger prediction\n    FE-&gt;&gt;BE: Send request (entity_id)\n    BE-&gt;&gt;OFS: Lookup features by entity_id (PREPROCESSING)\n    OFS--&gt;&gt;BE: Return features (PREPROCESSING)\n    BE-&gt;&gt;Predictor: Send features for prediction (PREDICTION)\n    Predictor--&gt;&gt;BE: Return prediction result (PREDICTION)\n    BE--&gt;&gt;FE: Return prediction\n    FE--&gt;&gt;User: Display result</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/transformer-advantages/#option-2-feature-lookup-via-transformer","title":"Option 2: Feature Lookup via Transformer","text":"<p>Here, the backend simply passes the raw ID to the InferenceService. The Transformer performs the feature lookup before passing features to the predictor.</p> <pre><code>sequenceDiagram\n    participant User as User\n    participant FE as Frontend\n    participant BE as Backend\n    participant Transformer as InferenceService:Transformer\n    participant Predictor as InferenceService:Predictor\n    participant OFS as Online Feature Server\n\n    User-&gt;&gt;FE: Trigger prediction\n    FE-&gt;&gt;BE: Send request (entity_id)\n    BE-&gt;&gt;Transformer: Send request (entity_id)\n    Transformer-&gt;&gt;OFS: Lookup features by entity_id (PREPROCESSING)\n    OFS--&gt;&gt;Transformer: Return features (PREPROCESSING)\n    Transformer-&gt;&gt;Predictor: Send features for prediction (PREDICTION)\n    Predictor--&gt;&gt;Transformer: Return prediction result (PREDICTION)\n    Transformer--&gt;&gt;BE: Return prediction result\n    BE--&gt;&gt;FE: Return prediction\n    FE--&gt;&gt;User: Display result</code></pre>"},{"location":"side-projects/data2ml-ops/kserve/transformer-advantages/#why-move-feature-retrieval-logic-from-backend-to-transformer","title":"Why Move Feature Retrieval Logic from Backend to Transformer?","text":"<p>Switching to the second architecture\u2014embedding the feature lookup within KServe\u2019s inference pipeline using its Transformer\u2014brings several advantages:</p>"},{"location":"side-projects/data2ml-ops/kserve/transformer-advantages/#simpler-backend-logic","title":"Simpler Backend Logic","text":"<p>In the first setup, the backend must:</p> <ul> <li>Know which features to request.</li> <li>Understand how to call the online feature server.</li> <li>Deal with errors or retries.</li> </ul> <p>In the second setup, the backend becomes a simple pass-through: it just sends a entity_id. The InferenceService encapsulates all feature logic, making the backend more lightweight and maintainable.</p>"},{"location":"side-projects/data2ml-ops/kserve/transformer-advantages/#better-reusability-and-portability","title":"Better Reusability and Portability","text":"<p>Decoupling the feature logic from the backend means multiple clients (mobile, web, partner APIs) can use the same InferenceService without duplicating feature retrieval logic. You can also deploy the same model in different environments without rewriting backend code.</p>"},{"location":"side-projects/data2ml-ops/kserve/transformer-advantages/#stronger-data-security-in-backend","title":"Stronger Data Security in Backend","text":"<p>By shifting feature retrieval logic to the KServe transformer, you ensure that the backend never directly accesses or handles sensitive feature data. This separation minimizes data exposure risks. For example, in fraud detection, features like historical average transaction amounts may be sensitive and should be kept within the model serving infrastructure.</p>"},{"location":"side-projects/data2ml-ops/kserve/transformer-advantages/#summary","title":"Summary","text":"<p>Moving feature retrieval from the backend to a transformer in KServe\u2019s InferenceService creates a cleaner, more consistent, and scalable architecture for real-time fraud detection. With Feast and KServe, it\u2019s easier than ever to centralize feature logic and productionize your models efficiently.</p>"},{"location":"side-projects/data2ml-ops/kserve/what-why-when/","title":"What, Why, When","text":""},{"location":"side-projects/data2ml-ops/kserve/what-why-when/#what-is-kserve","title":"What is KServe?","text":"<p>KServe is an open-source, Kubernetes-native platform designed to streamline the deployment and management of machine learning (ML) models at scale. It provides a standardized interface for serving models across various ML frameworks, including TensorFlow, PyTorch, XGBoost, scikit-learn, ONNX, and even large language models (LLMs)<sup>1</sup>.</p> <p>Built upon Kubernetes and Knative, KServe offers serverless capabilities such as autoscaling (including scaling down to zero)<sup>2</sup>, canary rollouts<sup>3</sup>, and model versioning. This architecture abstracts the complexities of infrastructure management, allowing data scientists and ML engineers to focus on developing and deploying models without delving into the intricacies of Kubernetes configurations.</p> <p>For a comprehensive introduction to KServe, consider watching the following video:</p>"},{"location":"side-projects/data2ml-ops/kserve/what-why-when/#why-kserve","title":"Why KServe?","text":"<p>KServe caters to various roles within the ML lifecycle, offering tailored benefits:</p> <p>For Data Scientists, With KServe's standardized APIs and support for multiple ML frameworks, data scientists can deploy models without worrying about the underlying infrastructure. Features like model explainability<sup>4</sup> and inference graphs aid in understanding and refining model behavior.</p> <p>For ML Engineers, KServe provides advanced deployment strategies, including canary rollouts and traffic splitting<sup>3</sup>, facilitating safe and controlled model updates. Its integration with monitoring tools like Prometheus and Grafana ensures observability and performance tracking<sup>5</sup><sup>6</sup>.</p> <p>For MLOps Teams, By leveraging Kubernetes' scalability and KServe's serverless capabilities, MLOps teams can manage model deployments efficiently across different environments, ensuring high availability and reliability.</p>"},{"location":"side-projects/data2ml-ops/kserve/what-why-when/#when-to-use-kserve","title":"When to Use KServe?","text":""},{"location":"side-projects/data2ml-ops/kserve/what-why-when/#deploying-models-across-diverse-frameworks","title":"Deploying Models Across Diverse Frameworks","text":"<p>When working with a variety of ML frameworks, KServe's standardized serving interface<sup>7</sup> allows for consistent deployment practices, reducing the overhead of managing different serving solutions.</p>"},{"location":"side-projects/data2ml-ops/kserve/what-why-when/#scaling-inference-services-based-on-demand","title":"Scaling Inference Services Based on Demand","text":"<p>For applications with fluctuating traffic patterns, KServe's autoscaling features, including scaling down to zero during idle periods, ensure cost-effective resource utilization while maintaining responsiveness<sup>2</sup>.</p>"},{"location":"side-projects/data2ml-ops/kserve/what-why-when/#implementing-safe-and-controlled-model-updates","title":"Implementing Safe and Controlled Model Updates","text":"<p>In scenarios requiring gradual model rollouts, KServe's support for canary deployments and traffic splitting enables testing new model versions with a subset of traffic before full-scale deployment<sup>3</sup>.</p>"},{"location":"side-projects/data2ml-ops/kserve/what-why-when/#managing-complex-inference-pipelines","title":"Managing Complex Inference Pipelines","text":"<p>When dealing with intricate inference workflows involving preprocessing, postprocessing<sup>8</sup>, or chaining multiple models, KServe's inference graph<sup>9</sup> feature allows for the composition of such pipelines, enhancing modularity and maintainability.</p> <ol> <li> <p>Model Serving Runtimes | KServe Docs \u21a9</p> </li> <li> <p>KServe GitHub Repository \u21a9\u21a9</p> </li> <li> <p>Exploring ML Model Serving with KServe (YouTube) \u21a9\u21a9\u21a9</p> </li> <li> <p>KServe: Highly Scalable Machine Learning Deployment with Kubernetes \u21a9</p> </li> <li> <p>KServe: Streamlining Machine Learning Model Serving in Kubernetes \u21a9</p> </li> <li> <p>Grafana Dashboards \u21a9</p> </li> <li> <p>Open Inference Protocol (V2 Inference Protocol) \u21a9</p> </li> <li> <p>How to write a custom transformer \u21a9</p> </li> <li> <p>Inference Graph \u21a9</p> </li> </ol>"},{"location":"side-projects/data2ml-ops/minio/deployment/","title":"Deploy MinIO on Kubernetes","text":"Architecture (Click to Enlarge)"},{"location":"side-projects/data2ml-ops/minio/deployment/#deployment","title":"Deployment","text":"minio.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio\n  namespace: minio\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n        - name: minio\n          image: minio/minio:RELEASE.2025-04-22T22-12-26Z\n          args:\n            - server\n            - /data\n            - --console-address\n            - :9001\n          env:\n            - name: MINIO_ROOT_USER\n              value: minio_user\n            - name: MINIO_ROOT_PASSWORD\n              value: minio_password\n          ports:\n            - containerPort: 9000\n              protocol: TCP\n            - containerPort: 9001\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /minio/health/live\n              port: 9000\n            initialDelaySeconds: 30\n            periodSeconds: 20\n            timeoutSeconds: 15\n            failureThreshold: 6\n          readinessProbe:\n            httpGet:\n              path: /minio/health/ready\n              port: 9000\n            initialDelaySeconds: 15\n            periodSeconds: 10\n            timeoutSeconds: 10\n            failureThreshold: 3\n          volumeMounts:\n            - name: storage\n              mountPath: /data\n      volumes:\n        - name: storage\n          hostPath:\n            path: /home/docker/data/minio\n            type: DirectoryOrCreate\n      restartPolicy: Always\n</code></pre>"},{"location":"side-projects/data2ml-ops/minio/deployment/#job","title":"Job","text":"minio.yaml<pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: minio-create-bucket\n  namespace: minio\nspec:\n  backoffLimit: 6\n  completions: 1\n  template:\n    metadata:\n      labels:\n        job: minio-create-bucket\n    spec:\n      initContainers:\n        - name: wait-for-minio\n          image: busybox\n          command:\n            - sh\n            - -c\n            - |\n              until nc -z minio-api.minio.svc.cluster.local 9000; do\n                echo \"Waiting for MinIO...\"\n                sleep 2\n              done\n              echo \"MinIO is ready!\"\n      containers:\n        - name: minio-create-buckets\n          image: minio/mc\n          command:\n            - sh\n            - -c\n            - |\n              mc alias set minio http://minio-api.minio.svc.cluster.local:9000 minio_user minio_password &amp;&amp;\n              for bucket in mlflow dbt sqlmesh ray; do\n                if ! mc ls minio/$bucket &gt;/dev/null 2&gt;&amp;1; then\n                  echo \"Creating bucket: $bucket\"\n                  mc mb minio/$bucket\n                  echo \"Bucket created: $bucket\"\n                else\n                  echo \"Bucket already exists: $bucket\"\n                fi\n              done\n      restartPolicy: OnFailure\n      terminationGracePeriodSeconds: 30\n</code></pre>"},{"location":"side-projects/data2ml-ops/minio/deployment/#services","title":"Services","text":"minio.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: minio-console\n  namespace: minio\nspec:\n  selector:\n    app: minio\n  type: NodePort\n  ports:\n    - name: console\n      port: 9001\n      targetPort: 9001\n      nodePort: 30901\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: minio-api\n  namespace: minio\nspec:\n  selector:\n    app: minio\n  type: ClusterIP\n  ports:\n    - name: api\n      port: 9000\n      targetPort: 9000\n</code></pre>"},{"location":"side-projects/data2ml-ops/minio/what-why-when/","title":"What, Why, When","text":""},{"location":"side-projects/data2ml-ops/mlflow/deployment/","title":"Deploy MLflow on Kubernetes","text":"Architecture (Click to Enlarge)"},{"location":"side-projects/data2ml-ops/mlflow/deployment/#artifact-store-minio","title":"Artifact Store (MinIO)","text":"<p>First deploy an S3-compatible object store - MinIO for our MLflow artifact store to store artifacts like figures, models, reports, etc. See here for deploying MinIO.</p> <p>After deploying MinIO and the <code>mlflow</code> bucket created, in MLflow's helm chart, we could specify artifact store's configuration</p> values.yaml<pre><code>artifactStore:\n  name: minio-api # API Service name for MinIO\n  namespace: minio\n  user: minio_user\n  password: minio_password\n  apiPort: 9000\n  bucketName: mlflow\n  hostPath: /home/docker/data/minio\n  mountPath: /data\n</code></pre>"},{"location":"side-projects/data2ml-ops/mlflow/deployment/#backend-store","title":"Backend Store","text":"values.yaml<pre><code>backendStore:\n  name: backend-store\n  db: mlflow\n  user: user\n  password: password\n  host: postgres\n  port: 5432\n  hostPath: /home/docker/data/mlflow/backend-store\n  mountPath: /var/lib/postgresql/data\n</code></pre> backend-store.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ .Values.backendStore.name }}\n  namespace: {{ .Release.Namespace }}\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: {{ .Values.backendStore.name }}\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: {{ .Values.backendStore.name }}\n    spec:\n      containers:\n        - name: {{ .Values.backendStore.name }}\n          image: postgres:latest\n          env:\n            - name: POSTGRES_DB\n              value: {{ .Values.backendStore.db }}\n            - name: POSTGRES_USER\n              value: {{ .Values.backendStore.user }}\n            - name: POSTGRES_PASSWORD\n              value: {{ .Values.backendStore.password }}\n          ports:\n            - containerPort: {{ .Values.backendStore.port }}\n              protocol: TCP\n          volumeMounts:\n            - name: storage\n              mountPath: {{ .Values.backendStore.mountPath }}\n      restartPolicy: Always\n      volumes:\n        - name: storage\n          hostPath:\n            path: {{ .Values.backendStore.hostPath }}\n            type: DirectoryOrCreate\n</code></pre> backend-store.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: {{ .Values.backendStore.name }}\n  namespace: {{ .Release.Namespace }}\nspec:\n  selector:\n    app: {{ .Values.backendStore.name }}\n  type: ClusterIP\n  ports:\n    - port: {{ .Values.backendStore.port }}\n      targetPort: {{ .Values.backendStore.port }}\n</code></pre>"},{"location":"side-projects/data2ml-ops/mlflow/deployment/#tracking-server","title":"Tracking Server","text":"values.yaml<pre><code>trackingServer:\n  name: tracking-server\n  host: 0.0.0.0\n  port: 5000\n</code></pre> tracking-server.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ .Values.trackingServer.name }}\n  namespace: {{ .Release.Namespace }}\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: {{ .Values.trackingServer.name }}\n  template:\n    metadata:\n      labels:\n        app: {{ .Values.trackingServer.name }}\n    spec:\n      initContainers:\n        - name: wait-for-backend-store\n          image: busybox\n          command:\n            - sh\n            - -c\n            - |\n              until nc -z {{ .Values.backendStore.name }}.{{ .Release.Namespace }}.svc.cluster.local {{ .Values.backendStore.port }}; do\n                echo \"Waiting for backend store...\"\n                sleep 2\n              done\n              echo \"Backend store is ready!\"\n        - name: wait-for-artifact-store\n          image: busybox\n          command:\n            - sh\n            - -c\n            - |\n              until nc -z {{ .Values.artifactStore.name }}.{{ .Values.artifactStore.namespace }}.svc.cluster.local {{ .Values.artifactStore.apiPort }}; do\n                echo \"Waiting for artifact store...\"\n                sleep 2\n              done\n              echo \"Artifact store is ready!\"\n      containers:\n        - name: {{ .Values.trackingServer.name }}\n          image: bitnami/mlflow:2.22.0\n          env:\n            - name: MLFLOW_S3_ENDPOINT_URL\n              value: http://{{ .Values.artifactStore.name }}.{{ .Values.artifactStore.namespace }}.svc.cluster.local:{{ .Values.artifactStore.apiPort }}\n            - name: AWS_ACCESS_KEY_ID\n              value: {{ .Values.artifactStore.user }}\n            - name: AWS_SECRET_ACCESS_KEY\n              value: {{ .Values.artifactStore.password }}\n            - name: MLFLOW_S3_IGNORE_TLS\n              value: \"true\"\n          command: [\"mlflow\"]\n          args:\n            [\n              \"server\",\n              \"--backend-store-uri\", \"postgresql://{{ .Values.backendStore.user }}:{{ .Values.backendStore.password }}@{{ .Values.backendStore.name }}:{{ .Values.backendStore.port }}/{{ .Values.backendStore.db }}\",\n              \"--artifacts-destination\", \"s3://{{ .Values.artifactStore.bucketName }}\",\n              \"--host\", \"{{ .Values.trackingServer.host }}\",\n              \"--port\", \"{{ .Values.trackingServer.port }}\",\n            ]\n          ports:\n            - containerPort: {{ .Values.trackingServer.port }}\n</code></pre> tracking-server.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: {{ .Values.trackingServer.name }}\n  namespace: {{ .Release.Namespace }}\nspec:\n  type: NodePort\n  selector:\n    app: {{ .Values.trackingServer.name }}\n  ports:\n    - port: {{ .Values.trackingServer.port }}\n      targetPort: {{ .Values.trackingServer.port }}\n      nodePort: 30500\n</code></pre>"},{"location":"side-projects/data2ml-ops/mlflow/how-it-works/","title":"How It Works?","text":"<p>__</p>"},{"location":"side-projects/data2ml-ops/mlflow/how-it-works/#core-concepts","title":"Core Concepts","text":""},{"location":"side-projects/data2ml-ops/mlflow/how-it-works/#architecture-components","title":"Architecture Components","text":""},{"location":"side-projects/data2ml-ops/mlflow/how-it-works/#key-modules-and-their-roles","title":"Key Modules and Their Roles","text":""},{"location":"side-projects/data2ml-ops/mlflow/how-it-works/#behind-the-scenes","title":"Behind the Scenes","text":""},{"location":"side-projects/data2ml-ops/mlflow/hpo/","title":"Track Hyperparameter Optimization with Optuna and MLflow","text":"In\u00a0[1]: Copied! <pre>import math\nimport logging\nfrom datetime import datetime, timedelta\n\nimport numpy as np\nimport optuna\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nimport mlflow\n</pre> import math import logging from datetime import datetime, timedelta  import numpy as np import optuna import pandas as pd import xgboost as xgb from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split  import mlflow <pre>/Users/kcl/.venvs/feast/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre>MLFLOW_TRACKING_URI = \"http://127.0.0.1:50666\"\nmlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n</pre> MLFLOW_TRACKING_URI = \"http://127.0.0.1:50666\" mlflow.set_tracking_uri(MLFLOW_TRACKING_URI) In\u00a0[3]: Copied! <pre>logger = logging.getLogger(\"mlflow\")\nlogger.setLevel(logging.WARNING)\n</pre> logger = logging.getLogger(\"mlflow\") logger.setLevel(logging.WARNING) In\u00a0[4]: Copied! <pre>def generate_apple_sales_data_with_promo_adjustment(\n    base_demand: int = 1000,\n    n_rows: int = 5000,\n    competitor_price_effect: float = -50.0,\n):\n    \"\"\"\n    Generates a synthetic dataset for predicting apple sales demand with multiple\n    influencing factors.\n\n    This function creates a pandas DataFrame with features relevant to apple sales.\n    The features include date, average_temperature, rainfall, weekend flag, holiday flag,\n    promotional flag, price_per_kg, competitor's price, marketing intensity, stock availability,\n    and the previous day's demand. The target variable, 'demand', is generated based on a\n    combination of these features with some added noise.\n\n    Args:\n        base_demand (int, optional): Base demand for apples. Defaults to 1000.\n        n_rows (int, optional): Number of rows (days) of data to generate. Defaults to 5000.\n        competitor_price_effect (float, optional): Effect of competitor's price being lower\n                                                   on our sales. Defaults to -50.\n\n    Returns:\n        pd.DataFrame: DataFrame with features and target variable for apple sales prediction.\n\n    Example:\n        &gt;&gt;&gt; df = generate_apple_sales_data_with_promo_adjustment(base_demand=1200, n_rows=6000)\n        &gt;&gt;&gt; df.head()\n    \"\"\"\n\n    # Set seed for reproducibility\n    np.random.seed(9999)\n\n    # Create date range\n    dates = [datetime.now() - timedelta(days=i) for i in range(n_rows)]\n    dates.reverse()\n\n    # Generate features\n    df = pd.DataFrame(\n        {\n            \"date\": dates,\n            \"average_temperature\": np.random.uniform(10, 35, n_rows),\n            \"rainfall\": np.random.exponential(5, n_rows),\n            \"weekend\": [(date.weekday() &gt;= 5) * 1 for date in dates],\n            \"holiday\": np.random.choice([0, 1], n_rows, p=[0.97, 0.03]),\n            \"price_per_kg\": np.random.uniform(0.5, 3, n_rows),\n            \"month\": [date.month for date in dates],\n        }\n    )\n\n    # Introduce inflation over time (years)\n    df[\"inflation_multiplier\"] = 1 + (df[\"date\"].dt.year - df[\"date\"].dt.year.min()) * 0.03\n\n    # Incorporate seasonality due to apple harvests\n    df[\"harvest_effect\"] = np.sin(2 * np.pi * (df[\"month\"] - 3) / 12) + np.sin(\n        2 * np.pi * (df[\"month\"] - 9) / 12\n    )\n\n    # Modify the price_per_kg based on harvest effect\n    df[\"price_per_kg\"] = df[\"price_per_kg\"] - df[\"harvest_effect\"] * 0.5\n\n    # Adjust promo periods to coincide with periods lagging peak harvest by 1 month\n    peak_months = [4, 10]  # months following the peak availability\n    df[\"promo\"] = np.where(\n        df[\"month\"].isin(peak_months),\n        1,\n        np.random.choice([0, 1], n_rows, p=[0.85, 0.15]),\n    )\n\n    # Generate target variable based on features\n    base_price_effect = -df[\"price_per_kg\"] * 50\n    seasonality_effect = df[\"harvest_effect\"] * 50\n    promo_effect = df[\"promo\"] * 200\n\n    df[\"demand\"] = (\n        base_demand\n        + base_price_effect\n        + seasonality_effect\n        + promo_effect\n        + df[\"weekend\"] * 300\n        + np.random.normal(0, 50, n_rows)\n    ) * df[\"inflation_multiplier\"]  # adding random noise\n\n    # Add previous day's demand\n    df[\"previous_days_demand\"] = df[\"demand\"].shift(1)\n    df[\"previous_days_demand\"].fillna(method=\"bfill\", inplace=True)  # fill the first row\n\n    # Introduce competitor pricing\n    df[\"competitor_price_per_kg\"] = np.random.uniform(0.5, 3, n_rows)\n    df[\"competitor_price_effect\"] = (\n        df[\"competitor_price_per_kg\"] &lt; df[\"price_per_kg\"]\n    ) * competitor_price_effect\n\n    # Stock availability based on past sales price (3 days lag with logarithmic decay)\n    log_decay = -np.log(df[\"price_per_kg\"].shift(3) + 1) + 2\n    df[\"stock_available\"] = np.clip(log_decay, 0.7, 1)\n\n    # Marketing intensity based on stock availability\n    # Identify where stock is above threshold\n    high_stock_indices = df[df[\"stock_available\"] &gt; 0.95].index\n\n    # For each high stock day, increase marketing intensity for the next week\n    for idx in high_stock_indices:\n        df.loc[idx : min(idx + 7, n_rows - 1), \"marketing_intensity\"] = np.random.uniform(0.7, 1)\n\n    # If the marketing_intensity column already has values, this will preserve them;\n    #  if not, it sets default values\n    fill_values = pd.Series(np.random.uniform(0, 0.5, n_rows), index=df.index)\n    df[\"marketing_intensity\"].fillna(fill_values, inplace=True)\n\n    # Adjust demand with new factors\n    df[\"demand\"] = df[\"demand\"] + df[\"competitor_price_effect\"] + df[\"marketing_intensity\"]\n\n    # Drop temporary columns\n    df.drop(\n        columns=[\n            \"inflation_multiplier\",\n            \"harvest_effect\",\n            \"month\",\n            \"competitor_price_effect\",\n            \"stock_available\",\n        ],\n        inplace=True,\n    )\n\n    return df\n</pre> def generate_apple_sales_data_with_promo_adjustment(     base_demand: int = 1000,     n_rows: int = 5000,     competitor_price_effect: float = -50.0, ):     \"\"\"     Generates a synthetic dataset for predicting apple sales demand with multiple     influencing factors.      This function creates a pandas DataFrame with features relevant to apple sales.     The features include date, average_temperature, rainfall, weekend flag, holiday flag,     promotional flag, price_per_kg, competitor's price, marketing intensity, stock availability,     and the previous day's demand. The target variable, 'demand', is generated based on a     combination of these features with some added noise.      Args:         base_demand (int, optional): Base demand for apples. Defaults to 1000.         n_rows (int, optional): Number of rows (days) of data to generate. Defaults to 5000.         competitor_price_effect (float, optional): Effect of competitor's price being lower                                                    on our sales. Defaults to -50.      Returns:         pd.DataFrame: DataFrame with features and target variable for apple sales prediction.      Example:         &gt;&gt;&gt; df = generate_apple_sales_data_with_promo_adjustment(base_demand=1200, n_rows=6000)         &gt;&gt;&gt; df.head()     \"\"\"      # Set seed for reproducibility     np.random.seed(9999)      # Create date range     dates = [datetime.now() - timedelta(days=i) for i in range(n_rows)]     dates.reverse()      # Generate features     df = pd.DataFrame(         {             \"date\": dates,             \"average_temperature\": np.random.uniform(10, 35, n_rows),             \"rainfall\": np.random.exponential(5, n_rows),             \"weekend\": [(date.weekday() &gt;= 5) * 1 for date in dates],             \"holiday\": np.random.choice([0, 1], n_rows, p=[0.97, 0.03]),             \"price_per_kg\": np.random.uniform(0.5, 3, n_rows),             \"month\": [date.month for date in dates],         }     )      # Introduce inflation over time (years)     df[\"inflation_multiplier\"] = 1 + (df[\"date\"].dt.year - df[\"date\"].dt.year.min()) * 0.03      # Incorporate seasonality due to apple harvests     df[\"harvest_effect\"] = np.sin(2 * np.pi * (df[\"month\"] - 3) / 12) + np.sin(         2 * np.pi * (df[\"month\"] - 9) / 12     )      # Modify the price_per_kg based on harvest effect     df[\"price_per_kg\"] = df[\"price_per_kg\"] - df[\"harvest_effect\"] * 0.5      # Adjust promo periods to coincide with periods lagging peak harvest by 1 month     peak_months = [4, 10]  # months following the peak availability     df[\"promo\"] = np.where(         df[\"month\"].isin(peak_months),         1,         np.random.choice([0, 1], n_rows, p=[0.85, 0.15]),     )      # Generate target variable based on features     base_price_effect = -df[\"price_per_kg\"] * 50     seasonality_effect = df[\"harvest_effect\"] * 50     promo_effect = df[\"promo\"] * 200      df[\"demand\"] = (         base_demand         + base_price_effect         + seasonality_effect         + promo_effect         + df[\"weekend\"] * 300         + np.random.normal(0, 50, n_rows)     ) * df[\"inflation_multiplier\"]  # adding random noise      # Add previous day's demand     df[\"previous_days_demand\"] = df[\"demand\"].shift(1)     df[\"previous_days_demand\"].fillna(method=\"bfill\", inplace=True)  # fill the first row      # Introduce competitor pricing     df[\"competitor_price_per_kg\"] = np.random.uniform(0.5, 3, n_rows)     df[\"competitor_price_effect\"] = (         df[\"competitor_price_per_kg\"] &lt; df[\"price_per_kg\"]     ) * competitor_price_effect      # Stock availability based on past sales price (3 days lag with logarithmic decay)     log_decay = -np.log(df[\"price_per_kg\"].shift(3) + 1) + 2     df[\"stock_available\"] = np.clip(log_decay, 0.7, 1)      # Marketing intensity based on stock availability     # Identify where stock is above threshold     high_stock_indices = df[df[\"stock_available\"] &gt; 0.95].index      # For each high stock day, increase marketing intensity for the next week     for idx in high_stock_indices:         df.loc[idx : min(idx + 7, n_rows - 1), \"marketing_intensity\"] = np.random.uniform(0.7, 1)      # If the marketing_intensity column already has values, this will preserve them;     #  if not, it sets default values     fill_values = pd.Series(np.random.uniform(0, 0.5, n_rows), index=df.index)     df[\"marketing_intensity\"].fillna(fill_values, inplace=True)      # Adjust demand with new factors     df[\"demand\"] = df[\"demand\"] + df[\"competitor_price_effect\"] + df[\"marketing_intensity\"]      # Drop temporary columns     df.drop(         columns=[             \"inflation_multiplier\",             \"harvest_effect\",             \"month\",             \"competitor_price_effect\",             \"stock_available\",         ],         inplace=True,     )      return df In\u00a0[5]: Copied! <pre>df = generate_apple_sales_data_with_promo_adjustment(base_demand=1_000, n_rows=5000)\ndf\n</pre> df = generate_apple_sales_data_with_promo_adjustment(base_demand=1_000, n_rows=5000) df <pre>/var/folders/zs/82l0dwfx1rdgz3490g0n3_qw0000gn/T/ipykernel_3699/1129670120.py:85: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df[\"previous_days_demand\"].fillna(method=\"bfill\", inplace=True)  # fill the first row\n/var/folders/zs/82l0dwfx1rdgz3490g0n3_qw0000gn/T/ipykernel_3699/1129670120.py:85: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  df[\"previous_days_demand\"].fillna(method=\"bfill\", inplace=True)  # fill the first row\n/var/folders/zs/82l0dwfx1rdgz3490g0n3_qw0000gn/T/ipykernel_3699/1129670120.py:108: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df[\"marketing_intensity\"].fillna(fill_values, inplace=True)\n</pre> Out[5]: date average_temperature rainfall weekend holiday price_per_kg promo demand previous_days_demand competitor_price_per_kg marketing_intensity 0 2011-08-24 16:51:35.064975 30.584727 1.199291 0 0 1.726258 0 851.375336 851.276659 1.935346 0.098677 1 2011-08-25 16:51:35.064973 15.465069 1.037626 0 0 0.576471 0 906.855943 851.276659 2.344720 0.019318 2 2011-08-26 16:51:35.064971 10.786525 5.656089 0 0 2.513328 0 808.304909 906.836626 0.998803 0.409485 3 2011-08-27 16:51:35.064970 23.648154 12.030937 1 0 1.839225 0 1099.833810 857.895424 0.761740 0.872803 4 2011-08-28 16:51:35.064967 13.861391 4.303812 1 0 1.531772 0 1283.949061 1148.961007 2.123436 0.820779 ... ... ... ... ... ... ... ... ... ... ... ... 4995 2025-04-27 16:51:35.054780 21.643051 3.821656 1 0 2.391010 1 1875.882437 1880.799278 1.504432 0.756489 4996 2025-04-28 16:51:35.054778 13.808813 1.080603 0 1 0.898693 1 1596.870527 1925.125948 1.343586 0.742145 4997 2025-04-29 16:51:35.054775 11.698227 1.911000 0 0 2.839860 1 1271.065524 1596.128382 2.771896 0.742145 4998 2025-04-30 16:51:35.054772 18.052081 1.000521 0 0 1.188440 1 1681.886638 1320.323379 2.564075 0.742145 4999 2025-05-01 16:51:35.054738 17.017294 0.650213 0 0 2.131694 0 1289.584771 1681.144493 0.785727 0.833140 <p>5000 rows \u00d7 11 columns</p> In\u00a0[6]: Copied! <pre># Preprocess the dataset\nX = df.drop(columns=[\"date\", \"demand\"])\ny = df[\"demand\"]\ntrain_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.25)\ndtrain = xgb.DMatrix(train_x, label=train_y)\ndvalid = xgb.DMatrix(valid_x, label=valid_y)\n</pre> # Preprocess the dataset X = df.drop(columns=[\"date\", \"demand\"]) y = df[\"demand\"] train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.25) dtrain = xgb.DMatrix(train_x, label=train_y) dvalid = xgb.DMatrix(valid_x, label=valid_y) In\u00a0[7]: Copied! <pre>import matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndef plot_correlation_with_demand(df, save_path=None):  # noqa: D417\n    \"\"\"\n    Plots the correlation of each variable in the dataframe with the 'demand' column.\n\n    Args:\n    - df (pd.DataFrame): DataFrame containing the data, including a 'demand' column.\n    - save_path (str, optional): Path to save the generated plot. If not specified, plot won't be saved.\n\n    Returns:\n    - None (Displays the plot on a Jupyter window)\n    \"\"\"\n\n    # Compute correlations between all variables and 'demand'\n    correlations = df.corr()[\"demand\"].drop(\"demand\").sort_values()\n\n    # Generate a color palette from red to green\n    colors = sns.diverging_palette(10, 130, as_cmap=True)\n    color_mapped = correlations.map(colors)\n\n    # Set Seaborn style\n    sns.set_style(\n        \"whitegrid\", {\"axes.facecolor\": \"#c2c4c2\", \"grid.linewidth\": 1.5}\n    )  # Light grey background and thicker grid lines\n\n    # Create bar plot\n    fig = plt.figure(figsize=(12, 8))\n    plt.barh(correlations.index, correlations.values, color=color_mapped)\n\n    # Set labels and title with increased font size\n    plt.title(\"Correlation with Demand\", fontsize=18)\n    plt.xlabel(\"Correlation Coefficient\", fontsize=16)\n    plt.ylabel(\"Variable\", fontsize=16)\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=14)\n    plt.grid(axis=\"x\")\n\n    plt.tight_layout()\n\n    # Save the plot if save_path is specified\n    if save_path:\n        plt.savefig(save_path, format=\"png\", dpi=600)\n\n    # prevent matplotlib from displaying the chart every time we call this function\n    plt.close(fig)\n\n    return fig\n\n\n# Test the function\ncorrelation_plot = plot_correlation_with_demand(df, save_path=\"correlation_plot.png\")\n</pre> import matplotlib.pyplot as plt import seaborn as sns   def plot_correlation_with_demand(df, save_path=None):  # noqa: D417     \"\"\"     Plots the correlation of each variable in the dataframe with the 'demand' column.      Args:     - df (pd.DataFrame): DataFrame containing the data, including a 'demand' column.     - save_path (str, optional): Path to save the generated plot. If not specified, plot won't be saved.      Returns:     - None (Displays the plot on a Jupyter window)     \"\"\"      # Compute correlations between all variables and 'demand'     correlations = df.corr()[\"demand\"].drop(\"demand\").sort_values()      # Generate a color palette from red to green     colors = sns.diverging_palette(10, 130, as_cmap=True)     color_mapped = correlations.map(colors)      # Set Seaborn style     sns.set_style(         \"whitegrid\", {\"axes.facecolor\": \"#c2c4c2\", \"grid.linewidth\": 1.5}     )  # Light grey background and thicker grid lines      # Create bar plot     fig = plt.figure(figsize=(12, 8))     plt.barh(correlations.index, correlations.values, color=color_mapped)      # Set labels and title with increased font size     plt.title(\"Correlation with Demand\", fontsize=18)     plt.xlabel(\"Correlation Coefficient\", fontsize=16)     plt.ylabel(\"Variable\", fontsize=16)     plt.xticks(fontsize=14)     plt.yticks(fontsize=14)     plt.grid(axis=\"x\")      plt.tight_layout()      # Save the plot if save_path is specified     if save_path:         plt.savefig(save_path, format=\"png\", dpi=600)      # prevent matplotlib from displaying the chart every time we call this function     plt.close(fig)      return fig   # Test the function correlation_plot = plot_correlation_with_demand(df, save_path=\"correlation_plot.png\") In\u00a0[8]: Copied! <pre>def plot_residuals(model, dvalid, valid_y, save_path=None):  # noqa: D417\n    \"\"\"\n    Plots the residuals of the model predictions against the true values.\n\n    Args:\n    - model: The trained XGBoost model.\n    - dvalid (xgb.DMatrix): The validation data in XGBoost DMatrix format.\n    - valid_y (pd.Series): The true values for the validation set.\n    - save_path (str, optional): Path to save the generated plot. If not specified, plot won't be saved.\n\n    Returns:\n    - None (Displays the residuals plot on a Jupyter window)\n    \"\"\"\n\n    # Predict using the model\n    preds = model.predict(dvalid)\n\n    # Calculate residuals\n    residuals = valid_y - preds\n\n    # Set Seaborn style\n    sns.set_style(\"whitegrid\", {\"axes.facecolor\": \"#c2c4c2\", \"grid.linewidth\": 1.5})\n\n    # Create scatter plot\n    fig = plt.figure(figsize=(12, 8))\n    plt.scatter(valid_y, residuals, color=\"blue\", alpha=0.5)\n    plt.axhline(y=0, color=\"r\", linestyle=\"-\")\n\n    # Set labels, title and other plot properties\n    plt.title(\"Residuals vs True Values\", fontsize=18)\n    plt.xlabel(\"True Values\", fontsize=16)\n    plt.ylabel(\"Residuals\", fontsize=16)\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=14)\n    plt.grid(axis=\"y\")\n\n    plt.tight_layout()\n\n    # Save the plot if save_path is specified\n    if save_path:\n        plt.savefig(save_path, format=\"png\", dpi=600)\n\n    # Show the plot\n    plt.close(fig)\n\n    return fig\n</pre> def plot_residuals(model, dvalid, valid_y, save_path=None):  # noqa: D417     \"\"\"     Plots the residuals of the model predictions against the true values.      Args:     - model: The trained XGBoost model.     - dvalid (xgb.DMatrix): The validation data in XGBoost DMatrix format.     - valid_y (pd.Series): The true values for the validation set.     - save_path (str, optional): Path to save the generated plot. If not specified, plot won't be saved.      Returns:     - None (Displays the residuals plot on a Jupyter window)     \"\"\"      # Predict using the model     preds = model.predict(dvalid)      # Calculate residuals     residuals = valid_y - preds      # Set Seaborn style     sns.set_style(\"whitegrid\", {\"axes.facecolor\": \"#c2c4c2\", \"grid.linewidth\": 1.5})      # Create scatter plot     fig = plt.figure(figsize=(12, 8))     plt.scatter(valid_y, residuals, color=\"blue\", alpha=0.5)     plt.axhline(y=0, color=\"r\", linestyle=\"-\")      # Set labels, title and other plot properties     plt.title(\"Residuals vs True Values\", fontsize=18)     plt.xlabel(\"True Values\", fontsize=16)     plt.ylabel(\"Residuals\", fontsize=16)     plt.xticks(fontsize=14)     plt.yticks(fontsize=14)     plt.grid(axis=\"y\")      plt.tight_layout()      # Save the plot if save_path is specified     if save_path:         plt.savefig(save_path, format=\"png\", dpi=600)      # Show the plot     plt.close(fig)      return fig In\u00a0[9]: Copied! <pre>def plot_feature_importance(model, booster):  # noqa: D417\n    \"\"\"\n    Plots feature importance for an XGBoost model.\n\n    Args:\n    - model: A trained XGBoost model\n\n    Returns:\n    - fig: The matplotlib figure object\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(10, 8))\n    importance_type = \"weight\" if booster == \"gblinear\" else \"gain\"\n    xgb.plot_importance(\n        model,\n        importance_type=importance_type,\n        ax=ax,\n        title=f\"Feature Importance based on {importance_type}\",\n    )\n    plt.tight_layout()\n    plt.close(fig)\n\n    return fig\n</pre> def plot_feature_importance(model, booster):  # noqa: D417     \"\"\"     Plots feature importance for an XGBoost model.      Args:     - model: A trained XGBoost model      Returns:     - fig: The matplotlib figure object     \"\"\"     fig, ax = plt.subplots(figsize=(10, 8))     importance_type = \"weight\" if booster == \"gblinear\" else \"gain\"     xgb.plot_importance(         model,         importance_type=importance_type,         ax=ax,         title=f\"Feature Importance based on {importance_type}\",     )     plt.tight_layout()     plt.close(fig)      return fig In\u00a0[10]: Copied! <pre>def get_or_create_experiment(experiment_name):\n    \"\"\"\n    Retrieve the ID of an existing MLflow experiment or create a new one if it doesn't exist.\n\n    This function checks if an experiment with the given name exists within MLflow.\n    If it does, the function returns its ID. If not, it creates a new experiment\n    with the provided name and returns its ID.\n\n    Parameters:\n    - experiment_name (str): Name of the MLflow experiment.\n\n    Returns:\n    - str: ID of the existing or newly created MLflow experiment.\n    \"\"\"\n\n    if experiment := mlflow.get_experiment_by_name(experiment_name):\n        return experiment.experiment_id\n    else:\n        return mlflow.create_experiment(experiment_name)\n</pre> def get_or_create_experiment(experiment_name):     \"\"\"     Retrieve the ID of an existing MLflow experiment or create a new one if it doesn't exist.      This function checks if an experiment with the given name exists within MLflow.     If it does, the function returns its ID. If not, it creates a new experiment     with the provided name and returns its ID.      Parameters:     - experiment_name (str): Name of the MLflow experiment.      Returns:     - str: ID of the existing or newly created MLflow experiment.     \"\"\"      if experiment := mlflow.get_experiment_by_name(experiment_name):         return experiment.experiment_id     else:         return mlflow.create_experiment(experiment_name) In\u00a0[11]: Copied! <pre>experiment_id = get_or_create_experiment(\"Apples Demand\")\nexperiment_id\n</pre> experiment_id = get_or_create_experiment(\"Apples Demand\") experiment_id Out[11]: <pre>'1'</pre> In\u00a0[12]: Copied! <pre># Set the current active MLflow experiment\nmlflow.set_experiment(experiment_id=experiment_id)\n</pre> # Set the current active MLflow experiment mlflow.set_experiment(experiment_id=experiment_id) Out[12]: <pre>&lt;Experiment: artifact_location='mlflow-artifacts:/1', creation_time=1746089509195, experiment_id='1', last_update_time=1746089509195, lifecycle_stage='active', name='Apples Demand', tags={}&gt;</pre> In\u00a0[13]: Copied! <pre># override Optuna's default logging to ERROR only\noptuna.logging.set_verbosity(optuna.logging.ERROR)\n</pre> # override Optuna's default logging to ERROR only optuna.logging.set_verbosity(optuna.logging.ERROR) In\u00a0[14]: Copied! <pre># define a logging callback that will report on only new challenger parameter configurations if a\n# trial has usurped the state of 'best conditions'\ndef champion_callback(study, frozen_trial):\n    \"\"\"\n    Logging callback that will report when a new trial iteration improves upon existing\n    best trial values.\n\n    Note: This callback is not intended for use in distributed computing systems such as Spark\n    or Ray due to the micro-batch iterative implementation for distributing trials to a cluster's\n    workers or agents.\n    The race conditions with file system state management for distributed trials will render\n    inconsistent values with this callback.\n    \"\"\"\n\n    prev_best_value = study.user_attrs.get(\"prev_best_value\", None)\n\n    if study.best_value and prev_best_value != study.best_value:\n        study.set_user_attr(\"prev_best_value\", study.best_value)\n\n        # MLflow log when a new best trial is found\n        full_params = frozen_trial.user_attrs.get(\"full_params\", {})\n        mse = frozen_trial.user_attrs.get(\"mse\", None)\n        with mlflow.start_run(run_name=f\"best-trial-{frozen_trial.number}\", nested=True):\n            mlflow.log_params(full_params)\n            mlflow.log_metric(\"mse\", mse)\n            mlflow.log_metric(\"rmse\", math.sqrt(mse))\n\n        if prev_best_value:\n            improvement_percent = (abs(prev_best_value - study.best_value) / study.best_value) * 100\n            print(\n                f\"Trial {frozen_trial.number} achieved value: {frozen_trial.value} with \"\n                f\"{improvement_percent: .4f}% improvement\"\n            )\n        else:\n            print(f\"Initial trial {frozen_trial.number} achieved value: {frozen_trial.value}\")\n</pre> # define a logging callback that will report on only new challenger parameter configurations if a # trial has usurped the state of 'best conditions' def champion_callback(study, frozen_trial):     \"\"\"     Logging callback that will report when a new trial iteration improves upon existing     best trial values.      Note: This callback is not intended for use in distributed computing systems such as Spark     or Ray due to the micro-batch iterative implementation for distributing trials to a cluster's     workers or agents.     The race conditions with file system state management for distributed trials will render     inconsistent values with this callback.     \"\"\"      prev_best_value = study.user_attrs.get(\"prev_best_value\", None)      if study.best_value and prev_best_value != study.best_value:         study.set_user_attr(\"prev_best_value\", study.best_value)          # MLflow log when a new best trial is found         full_params = frozen_trial.user_attrs.get(\"full_params\", {})         mse = frozen_trial.user_attrs.get(\"mse\", None)         with mlflow.start_run(run_name=f\"best-trial-{frozen_trial.number}\", nested=True):             mlflow.log_params(full_params)             mlflow.log_metric(\"mse\", mse)             mlflow.log_metric(\"rmse\", math.sqrt(mse))          if prev_best_value:             improvement_percent = (abs(prev_best_value - study.best_value) / study.best_value) * 100             print(                 f\"Trial {frozen_trial.number} achieved value: {frozen_trial.value} with \"                 f\"{improvement_percent: .4f}% improvement\"             )         else:             print(f\"Initial trial {frozen_trial.number} achieved value: {frozen_trial.value}\") In\u00a0[15]: Copied! <pre>def objective(trial):\n    # Define hyperparameters\n    params = {\n        \"objective\": \"reg:squarederror\",\n        \"eval_metric\": \"rmse\",\n        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n    }\n\n    if params[\"booster\"] == \"gbtree\" or params[\"booster\"] == \"dart\":\n        params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n        params[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n        params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n        params[\"grow_policy\"] = trial.suggest_categorical(\n            \"grow_policy\", [\"depthwise\", \"lossguide\"]\n        )\n    \n    # Train XGBoost model\n    bst = xgb.train(params, dtrain)\n    pred_y = bst.predict(dvalid)\n    mse = mean_squared_error(valid_y, pred_y)\n\n    trial.set_user_attr(\"full_params\", params)\n    trial.set_user_attr(\"mse\", mse)\n\n    return mse\n</pre> def objective(trial):     # Define hyperparameters     params = {         \"objective\": \"reg:squarederror\",         \"eval_metric\": \"rmse\",         \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),         \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),         \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),     }      if params[\"booster\"] == \"gbtree\" or params[\"booster\"] == \"dart\":         params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)         params[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)         params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)         params[\"grow_policy\"] = trial.suggest_categorical(             \"grow_policy\", [\"depthwise\", \"lossguide\"]         )          # Train XGBoost model     bst = xgb.train(params, dtrain)     pred_y = bst.predict(dvalid)     mse = mean_squared_error(valid_y, pred_y)      trial.set_user_attr(\"full_params\", params)     trial.set_user_attr(\"mse\", mse)      return mse In\u00a0[16]: Copied! <pre># Initiate the parent run and call the hyperparameter tuning child run logic\nrun_name = \"fourth\"\nwith mlflow.start_run(experiment_id=experiment_id, run_name=run_name, nested=True):\n    # Initialize the Optuna study\n    study = optuna.create_study(direction=\"minimize\")\n\n    # Execute the hyperparameter optimization trials.\n    # Note the addition of the `champion_callback` inclusion to control our logging\n    study.optimize(objective, n_trials=100, callbacks=[champion_callback])\n\n    mlflow.log_params(study.best_params)\n    mlflow.log_metric(\"best_mse\", study.best_value)\n    mlflow.log_metric(\"best_rmse\", math.sqrt(study.best_value))\n\n    # Log tags\n    mlflow.set_tags(\n        tags={\n            \"project\": \"Apple Demand Project\",\n            \"optimizer_engine\": \"optuna\",\n            \"model_family\": \"xgboost\",\n            \"feature_set_version\": 1,\n        }\n    )\n\n    # Log a fit model instance\n    model = xgb.train(study.best_params, dtrain)\n\n    # Log the correlation plot\n    mlflow.log_figure(figure=correlation_plot, artifact_file=\"correlation_plot.png\")\n\n    # Log the feature importances plot\n    importances = plot_feature_importance(model, booster=study.best_params.get(\"booster\"))\n    mlflow.log_figure(figure=importances, artifact_file=\"feature_importances.png\")\n\n    # Log the residuals plot\n    residuals = plot_residuals(model, dvalid, valid_y)\n    mlflow.log_figure(figure=residuals, artifact_file=\"residuals.png\")\n\n    artifact_path = \"model\"\n\n    mlflow.xgboost.log_model(\n        xgb_model=model,\n        artifact_path=artifact_path,\n        input_example=train_x.iloc[[0]],\n        model_format=\"ubj\",\n        metadata={\"model_data_version\": 1},\n    )\n\n    # Get the logged model uri so that we can load it from the artifact store\n    model_uri = mlflow.get_artifact_uri(artifact_path)\n</pre> # Initiate the parent run and call the hyperparameter tuning child run logic run_name = \"fourth\" with mlflow.start_run(experiment_id=experiment_id, run_name=run_name, nested=True):     # Initialize the Optuna study     study = optuna.create_study(direction=\"minimize\")      # Execute the hyperparameter optimization trials.     # Note the addition of the `champion_callback` inclusion to control our logging     study.optimize(objective, n_trials=100, callbacks=[champion_callback])      mlflow.log_params(study.best_params)     mlflow.log_metric(\"best_mse\", study.best_value)     mlflow.log_metric(\"best_rmse\", math.sqrt(study.best_value))      # Log tags     mlflow.set_tags(         tags={             \"project\": \"Apple Demand Project\",             \"optimizer_engine\": \"optuna\",             \"model_family\": \"xgboost\",             \"feature_set_version\": 1,         }     )      # Log a fit model instance     model = xgb.train(study.best_params, dtrain)      # Log the correlation plot     mlflow.log_figure(figure=correlation_plot, artifact_file=\"correlation_plot.png\")      # Log the feature importances plot     importances = plot_feature_importance(model, booster=study.best_params.get(\"booster\"))     mlflow.log_figure(figure=importances, artifact_file=\"feature_importances.png\")      # Log the residuals plot     residuals = plot_residuals(model, dvalid, valid_y)     mlflow.log_figure(figure=residuals, artifact_file=\"residuals.png\")      artifact_path = \"model\"      mlflow.xgboost.log_model(         xgb_model=model,         artifact_path=artifact_path,         input_example=train_x.iloc[[0]],         model_format=\"ubj\",         metadata={\"model_data_version\": 1},     )      # Get the logged model uri so that we can load it from the artifact store     model_uri = mlflow.get_artifact_uri(artifact_path) <pre>\ud83c\udfc3 View run best-trial-0 at: http://127.0.0.1:50666/#/experiments/1/runs/25de8f9716124a0e953ec50a03a85bdd\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nInitial trial 0 achieved value: 64200.9377312124\n\ud83c\udfc3 View run best-trial-2 at: http://127.0.0.1:50666/#/experiments/1/runs/f8c4f4c28f7346f39e513beaef0b2eb2\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nTrial 2 achieved value: 64072.863216692545 with  0.1999% improvement\n\ud83c\udfc3 View run best-trial-7 at: http://127.0.0.1:50666/#/experiments/1/runs/a7c0d3ff781b4722bf6a8f4a1c29cc21\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nTrial 7 achieved value: 19370.33276815125 with  230.7783% improvement\n\ud83c\udfc3 View run best-trial-9 at: http://127.0.0.1:50666/#/experiments/1/runs/b50292d1b7f54e4ab43bcd950c901853\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nTrial 9 achieved value: 19280.226444123156 with  0.4674% improvement\n\ud83c\udfc3 View run best-trial-12 at: http://127.0.0.1:50666/#/experiments/1/runs/ec7572de81d74d2e814a246deb279585\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nTrial 12 achieved value: 19249.251703373608 with  0.1609% improvement\n\ud83c\udfc3 View run best-trial-15 at: http://127.0.0.1:50666/#/experiments/1/runs/3a5f94921c5f425599caf305b7bcea63\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nTrial 15 achieved value: 19143.276660193736 with  0.5536% improvement\n\ud83c\udfc3 View run best-trial-26 at: http://127.0.0.1:50666/#/experiments/1/runs/2d64a8f2a9d046379f79c8fd2ed84da8\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nTrial 26 achieved value: 19042.25204424367 with  0.5305% improvement\n\ud83c\udfc3 View run best-trial-40 at: http://127.0.0.1:50666/#/experiments/1/runs/0ff1b4a5bba7468abf7221b0c2567857\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nTrial 40 achieved value: 18255.62810065196 with  4.3089% improvement\n\ud83c\udfc3 View run best-trial-70 at: http://127.0.0.1:50666/#/experiments/1/runs/8688d93cb6b2495eaca05a8d831be39a\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nTrial 70 achieved value: 16690.920638134605 with  9.3746% improvement\n\ud83c\udfc3 View run best-trial-71 at: http://127.0.0.1:50666/#/experiments/1/runs/08f702060d76408a92ba803e459b9d38\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nTrial 71 achieved value: 15909.402041712121 with  4.9123% improvement\n\ud83c\udfc3 View run best-trial-72 at: http://127.0.0.1:50666/#/experiments/1/runs/e89e6529f4174a8a93d7f78c5a8f2a61\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nTrial 72 achieved value: 15573.390403470594 with  2.1576% improvement\n\ud83c\udfc3 View run best-trial-76 at: http://127.0.0.1:50666/#/experiments/1/runs/07bef624d7084837a4de95636ae72508\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nTrial 76 achieved value: 15331.608560002875 with  1.5770% improvement\n\ud83c\udfc3 View run best-trial-85 at: http://127.0.0.1:50666/#/experiments/1/runs/094dec77387f47ebad1869b6ca2487a8\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\nTrial 85 achieved value: 15330.17016465973 with  0.0094% improvement\n</pre> <pre>/Users/kcl/.venvs/feast/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values &lt;https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values&gt;`_ for more details.\n  warnings.warn(\n</pre> <pre>\ud83c\udfc3 View run fourth at: http://127.0.0.1:50666/#/experiments/1/runs/b1d6cefb8b9c434895e7627fe7529e4e\n\ud83e\uddea View experiment at: http://127.0.0.1:50666/#/experiments/1\n</pre> In\u00a0[17]: Copied! <pre>model_uri\n</pre> model_uri Out[17]: <pre>'mlflow-artifacts:/1/b1d6cefb8b9c434895e7627fe7529e4e/artifacts/model'</pre> In\u00a0[35]: Copied! <pre>loaded = mlflow.xgboost.load_model(model_uri)\n</pre> loaded = mlflow.xgboost.load_model(model_uri) <pre>Downloading artifacts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:22&lt;00:00,  3.18s/it]\n</pre> In\u00a0[36]: Copied! <pre>batch_dmatrix = xgb.DMatrix(X)\n\ninference = loaded.predict(batch_dmatrix)\n\ninfer_df = df.copy()\n\ninfer_df[\"predicted_demand\"] = inference\n</pre> batch_dmatrix = xgb.DMatrix(X)  inference = loaded.predict(batch_dmatrix)  infer_df = df.copy()  infer_df[\"predicted_demand\"] = inference In\u00a0[37]: Copied! <pre>infer_df\n</pre> infer_df Out[37]: date average_temperature rainfall weekend holiday price_per_kg promo demand previous_days_demand competitor_price_per_kg marketing_intensity predicted_demand 0 2011-08-22 16:36:16.256208 30.584727 1.199291 0 0 1.726258 0 851.375336 851.276659 1.935346 0.098677 938.926270 1 2011-08-23 16:36:16.256201 15.465069 1.037626 0 0 0.576471 0 906.855943 851.276659 2.344720 0.019318 1016.131104 2 2011-08-24 16:36:16.256200 10.786525 5.656089 0 0 2.513328 0 808.304909 906.836626 0.998803 0.409485 888.394958 3 2011-08-25 16:36:16.256198 23.648154 12.030937 0 0 1.839225 0 799.833810 857.895424 0.761740 0.872803 928.042908 4 2011-08-26 16:36:16.256197 13.861391 4.303812 0 0 1.531772 0 983.949061 848.961007 2.123436 0.820779 985.474487 ... ... ... ... ... ... ... ... ... ... ... ... ... 4995 2025-04-25 16:36:16.246942 21.643051 3.821656 0 0 2.391010 1 1449.882437 1454.799278 1.504432 0.756489 1287.835571 4996 2025-04-26 16:36:16.246940 13.808813 1.080603 1 1 0.898693 1 2022.870527 1499.125948 1.343586 0.742145 1742.767700 4997 2025-04-27 16:36:16.246939 11.698227 1.911000 1 0 2.839860 1 1697.065524 2022.128382 2.771896 0.742145 1757.431885 4998 2025-04-28 16:36:16.246935 18.052081 1.000521 0 0 1.188440 1 1681.886638 1746.323379 2.564075 0.742145 1473.662109 4999 2025-04-29 16:36:16.246853 17.017294 0.650213 0 0 2.131694 1 1573.584771 1681.144493 0.785727 0.833140 1346.723755 <p>5000 rows \u00d7 12 columns</p>"},{"location":"side-projects/data2ml-ops/mlflow/hpo/#track-hyperparameter-optimization-with-optuna-and-mlflow","title":"Track Hyperparameter Optimization with Optuna and MLflow\u00b6","text":""},{"location":"side-projects/data2ml-ops/mlflow/hpo/#import-packages","title":"Import Packages\u00b6","text":""},{"location":"side-projects/data2ml-ops/mlflow/hpo/#generate-synthetic-data","title":"Generate Synthetic Data\u00b6","text":""},{"location":"side-projects/data2ml-ops/mlflow/hpo/#define-plotting-functions","title":"Define Plotting Functions\u00b6","text":"<ul> <li><code>plot_correlation_with_demand</code></li> <li><code>plot_residuals</code></li> <li><code>plot_feature_importance</code></li> </ul>"},{"location":"side-projects/data2ml-ops/mlflow/hpo/#set-up-the-experiment","title":"Set up the Experiment\u00b6","text":""},{"location":"side-projects/data2ml-ops/mlflow/hpo/#create-a-callback-function","title":"Create a Callback Function\u00b6","text":""},{"location":"side-projects/data2ml-ops/mlflow/hpo/#create-a-objective-function","title":"Create a Objective Function\u00b6","text":""},{"location":"side-projects/data2ml-ops/mlflow/hpo/#load-the-model-and-run-batch-prediction","title":"Load the Model and Run Batch Prediction\u00b6","text":""},{"location":"side-projects/data2ml-ops/mlflow/in-the-bigger-picture/","title":"In the Bigger Picture","text":""},{"location":"side-projects/data2ml-ops/mlflow/in-the-bigger-picture/#history-current-status","title":"History &amp; Current Status","text":"<p>MLflow is an open-source platform for managing the end-to-end machine learning lifecycle, initially developed by Databricks and introduced in June 2018. It provides tools for experiment tracking, model packaging, deployment, and a centralized model registry, aiming to streamline the complexities of machine learning workflows. MLflow is designed to be library-agnostic, supporting various ML libraries and frameworks. \ufffc \ufffc</p> <p>As of May 2025, MLflow has garnered over 20,000 stars on GitHub and boasts a vibrant community with more than 1,000 contributors. The platform is actively maintained, with recent releases introducing features like support for OpenAI\u2019s Responses API, Gemini embeddings, and integration with Azure Data Lake Storage. MLflow is utilized by numerous organizations worldwide, including Microsoft, Meta, Toyota, Booking.com, and Accenture, underscoring its widespread adoption in the industry.</p>"},{"location":"side-projects/data2ml-ops/mlflow/in-the-bigger-picture/#alternatives","title":"Alternatives","text":""},{"location":"side-projects/data2ml-ops/mlflow/in-the-bigger-picture/#integration-points","title":"Integration Points","text":""},{"location":"side-projects/data2ml-ops/mlflow/in-the-bigger-picture/#kubernetes","title":"Kubernetes","text":""},{"location":"side-projects/data2ml-ops/mlflow/in-the-bigger-picture/#cicd","title":"CI/CD","text":""},{"location":"side-projects/data2ml-ops/mlflow/what-why-when/","title":"What, Why, When","text":""},{"location":"side-projects/data2ml-ops/mlflow/what-why-when/#what-is-x","title":"What is X?","text":""},{"location":"side-projects/data2ml-ops/mlflow/what-why-when/#why-x","title":"Why X?","text":"<ul> <li>Data Engineer</li> <li>Data Analyst</li> <li>Data Scientist</li> <li>Machine Learning Engineer</li> </ul>"},{"location":"side-projects/data2ml-ops/mlflow/what-why-when/#when-to-use-x","title":"When to Use X?","text":""},{"location":"side-projects/data2ml-ops/others/cert-manager-introduction/","title":"Cert Manager","text":""},{"location":"side-projects/data2ml-ops/others/istio-introduction/","title":"What Is Istio?","text":"Architecture <ul> <li>Istio is an open source service mesh that layers transparently onto existing distributed applications.</li> <li>Istio extends Kubernetes to establish a programmable, application-aware network. Working with both Kubernetes and traditional workloads, Istio brings standard, universal traffic management, telemetry, and security to complex deployments.</li> <li>It gives you:<ul> <li>Secure service-to-service communication in a cluster with mutual TLS encryption, strong identity-based authentication and authorization</li> <li>Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic</li> <li>Fine-grained control of traffic behavior with rich routing rules, retries, failovers, and fault injection</li> <li>A pluggable policy layer and configuration API supporting access controls, rate limits and quotas</li> <li>Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress</li> </ul> </li> </ul>","tags":["Istio"]},{"location":"side-projects/data2ml-ops/others/istio-introduction/#how-it-works","title":"How It Works?","text":"<ul> <li>The control plane takes your desired configuration, and its view of the services, and dynamically programs the proxy servers, updating them as the rules or the environment changes.</li> <li>The data plane is the communication between services. Without a service mesh, the network doesn\u2019t understand the traffic being sent over, and can\u2019t make any decisions based on what type of traffic it is, or who it is from or to. It supports two data planes:<ul> <li>sidecar mode, which deploys an Envoy proxy along with each pod that you start in your cluster, or running alongside services running on VMs.</li> <li>ambient mode, which uses a per-node Layer 4 proxy, and optionally a per-namespace Envoy proxy for Layer 7 features</li> </ul> </li> </ul>","tags":["Istio"]},{"location":"side-projects/data2ml-ops/others/istio-introduction/#sidecar-mode-vs-ambient-mode-in-istio","title":"Sidecar Mode vs. Ambient Mode in Istio","text":"Feature Sidecar Mode Ambient Mode Proxy location One per pod (sidecar) One per node (<code>ztunnel</code>), plus optional waypoint proxies Setup complexity Requires injection into each pod No sidecar; label namespace only Resource usage Higher Lower Feature maturity Very mature Newer, still evolving Transparency to app Yes Yes Traffic interception iptables per pod eBPF or iptables at node level","tags":["Istio"]},{"location":"side-projects/data2ml-ops/others/istio-introduction/#what-happens-after-installing-istio","title":"What happens after installing Istio","text":"<p>After completing the Istio installation, the system creates a namespace called <code>istio-system</code> in Kubernetes, which serves as the primary location where Istio's control plane components operate. Multiple core components are automatically deployed within this namespace, presented as services and pods, with Istio extending its own resource model through Custom Resource Definitions (CRDs).</p> <p>First, the most important control component is <code>istiod</code>, an integrated control plane component that has consolidated the previously separate Pilot, Mixer, and Citadel components since Istio 1.5. This pod manages service discovery, sidecar configuration distribution (XDS), mTLS certificate issuance, and security policy distribution. You'll see a pod named <code>istiod-xxxx</code> in the <code>istio-system</code> namespace, with the corresponding service typically being <code>istiod</code> or <code>istio-pilot</code>, depending on the version and installation method.</p> <p>Additionally, if you enable the Ingress Gateway (which most people do), you'll also see a deployment, service, and pod called <code>istio-ingressgateway</code>. This is an Envoy-based proxy server responsible for receiving external HTTP/TCP traffic, typically configured as a <code>NodePort</code> or <code>LoadBalancer</code> service type.</p> <p>Beyond the control components mentioned above, Istio installation also includes various extension resources. Most notably, it provides a series of CRDs such as <code>VirtualService</code>, <code>DestinationRule</code>, <code>Gateway</code>, <code>PeerAuthentication</code>, and <code>AuthorizationPolicy</code>. These are the resources you'll use when working with Istio for routing control, traffic splitting, and security policies.</p> <p>These resources form Istio's core foundation, enabling you to establish a controllable, secure, and traffic-observable service mesh on Kubernetes.</p>","tags":["Istio"]},{"location":"side-projects/data2ml-ops/others/istio-introduction/#why-istio","title":"Why Istio?","text":"<p>Simple and powerful</p> <p>Istio is characterized as \"simple and powerful\" for several key reasons:</p> <ul> <li>It provides comprehensive functionality, covering nearly all service mesh governance requirements</li> <li>Once established, it makes microservice operations and governance genuinely straightforward</li> <li>It abstracts underlying complexity through standardized proxy layers and policy definitions</li> <li>The newly introduced Ambient mode further reduces deployment and learning barriers</li> </ul> <p>Initially, I found Istio somewhat complex, given the numerous terminologies like sidecar, control plane, and mTLS. However, after truly understanding it, I believe there's solid reasoning behind calling it \"simple and powerful.\"</p> <p>The \"powerful\" aspect is that it handles virtually all network governance between microservices. Features like TLS encryption, zero-trust architecture, traffic splitting, error retries, fault injection, and A/B testing that previously required individual development, testing, and deployment can now be accomplished simply by defining a YAML configuration.</p> <p>The \"simple\" designation comes from a system-level perspective. Once you establish the service mesh with Istio, you'll genuinely feel that \"service management complexity\" has been significantly reduced. Particularly with the introduction of Ambient mode, there's no need for sidecar injection or constant pod spec modifications, making deployment cleaner and scaling easier.</p> <p>To be honest, Istio itself may not be truly \"beginner-friendly,\" but it makes subsequent operations simpler and more reliable.</p>","tags":["Istio"]},{"location":"side-projects/data2ml-ops/others/istio-introduction/#why-not-just-use-kubernetes","title":"Why Not Just Use Kubernetes?","text":"<p>First, regarding traffic control, Kubernetes' native Ingress can only handle requests entering the cluster from external sources and has no control over internal communication between services. It cannot adjust routing logic based on versions, headers, user identity, or other conditions. Istio, through <code>VirtualService</code> and <code>DestinationRule</code> resources, enables fine-grained specification of traffic distribution. For example, directing specific users to new versions, implementing A/B testing, or canary releases, all while achieving flexible routing without modifying applications.</p> <ul> <li><code>VirtualService</code>: How you route your traffic TO a given destination</li> <li><code>DestinationRule</code>: Configure what happens to traffic FOR that destination</li> </ul> <p>Next is the security aspect. While Kubernetes supports Role-Based Access Control (RBAC) and NetworkPolicy, it cannot ensure that communication between services is encrypted or verify the identities of communicating parties. Istio establishes encrypted channels between services through automated mutual TLS (mTLS) mechanisms, combined with authentication and authorization policies, ensuring that only authorized services can communicate with each other, thereby implementing the fundamental principles of zero-trust architecture.</p> <p>Regarding observability, while Kubernetes allows viewing Pod logs and some basic metrics, its native functionality is clearly insufficient for microservice tracing, latency analysis, and traffic bottleneck identification. Istio deploys proxies at each service edge, enabling automatic collection of detailed telemetry information, including request-level tracing, traffic metrics, and error rates, integrated with tools like Prometheus, Grafana, and Jaeger\u2014achieving comprehensive monitoring with minimal application modifications.</p> <p>Finally, in terms of flexibility and extensibility, Kubernetes cannot inject custom network processing logic for individual services. Istio, through sidecar mode (or the newer ambient mode), provides each service with its own network proxy and supports WebAssembly plugins, allowing dynamic insertion of authentication logic, data transformation, or even fault simulation, adapting network behavior to meet business requirements.</p>","tags":["Istio"]},{"location":"side-projects/data2ml-ops/others/istio-introduction/#references","title":"References","text":"<ul> <li>What is Istio? | Istio</li> <li>Sidecar or ambient? | Istio</li> </ul>","tags":["Istio"]},{"location":"side-projects/data2ml-ops/others/knative-introduction/","title":"What Is Knative?","text":"<ul> <li>Knative is an Open-Source Enterprise-level solution to build Serverless and Event Driven Applications</li> <li>Why serverless containers?<ul> <li>Simpler Abstractions</li> <li>Autoscaling (Scale down to zero and up from zero)</li> <li>Progressive Rollouts</li> <li>Event Integrations</li> <li>Handle Events</li> <li>Plugable</li> </ul> </li> </ul> Knative Serving and Eventing <ul> <li>Knative has two main components that empower teams working with Kubernetes. Serving and Eventing work together to automate and manage tasks and applications.</li> <li>serving and eventing\u5404\u81ea\u7368\u7acb\uff0c\u53ef\u5206\u5225\u5b89\u88dd\u4f7f\u7528</li> <li>Knative Serving: Run serverless containers in Kubernetes with ease. Knative takes care of the details of networking, autoscaling (even to zero), and revision tracking. Teams can focus on core logic using any programming language.</li> <li>Knative Eventing: Universal subscription, delivery and management of events. Build modern apps by attaching compute to a data stream with declarative event connectivity and developer friendly object models.</li> </ul>","tags":["Knative"]},{"location":"side-projects/data2ml-ops/others/knative-introduction/#knative-history","title":"Knative History","text":"<ul> <li>Knative \u6700\u521d\u7531 Google \u65bc 2018 \u5e74 7 \u6708\u767c\u8d77\uff0c\u4e26\u8207 IBM\u3001Red Hat\u3001VMware \u548c SAP \u7b49\u516c\u53f8\u5bc6\u5207\u5408\u4f5c\u958b\u767c\u3002</li> <li>\u767c\u5c55\u91cc\u7a0b\u7891<ul> <li>2018 \u5e74 7 \u6708\uff1aKnative \u9996\u6b21\u516c\u958b\u767c\u5e03\u3002</li> <li>2019 \u5e74 3 \u6708\uff1aBuild \u7d44\u4ef6\u6f14\u9032\u70ba Tekton\uff0c\u5c08\u6ce8\u65bc CI/CD\u3002</li> <li>2019 \u5e74 9 \u6708\uff1aServing API \u9054\u5230 v1 \u7248\u672c\u3002</li> <li>2020 \u5e74 7 \u6708\uff1aEventing API \u9054\u5230 v1 \u7248\u672c\u3002</li> <li>2021 \u5e74 11 \u6708\uff1aKnative \u767c\u5e03 1.0 \u7248\u672c\uff0c\u6a19\u8a8c\u8457\u5176\u7a69\u5b9a\u6027\u548c\u5546\u696d\u53ef\u7528\u6027\u3002</li> <li>2022 \u5e74 3 \u6708\uff1aKnative \u6210\u70ba CNCF \u7684\u5b75\u5316\u5c08\u6848\u3002</li> </ul> </li> </ul>","tags":["Knative"]},{"location":"side-projects/data2ml-ops/others/knative-introduction/#knative","title":"Knative \u9069\u5408\u4f7f\u7528\u7684\u60c5\u5883","text":"<ul> <li>\u4e8b\u4ef6\u89f8\u767c\u578b\u61c9\u7528\uff08Event-Driven Applications\uff09\u7576\u61c9\u7528\u7a0b\u5f0f\u53ea\u9700\u8981\u5728\u7279\u5b9a\u4e8b\u4ef6\u767c\u751f\u6642\u57f7\u884c\uff0c\u4f8b\u5982\uff1a<ul> <li>GitHub webhook: \u6709 push \u4e8b\u4ef6\u6642\u89f8\u767c\u81ea\u52d5\u90e8\u7f72\u6d41\u7a0b</li> <li>Kafka message: \u6709\u8a0a\u606f\u9032\u5165\u7279\u5b9a topic \u6642\u555f\u52d5\u8655\u7406\u908f\u8f2f</li> <li>IoT \u8cc7\u6599\u4e0a\u50b3: \u8a2d\u5099\u56de\u50b3\u8cc7\u6599\u5c31\u8655\u7406\u4e00\u6b21</li> <li>\u5b9a\u6642\u5de5\u4f5c: \u6bcf10\u5206\u9418\u57f7\u884c\u4e00\u6b21\u8cc7\u6599\u6e05\u6d17\u4efb\u52d9</li> </ul> </li> <li>\u4e0d\u9700\u8981\u4e00\u76f4\u904b\u884c\u7684\u61c9\u7528 (Scale-to-zero)<ul> <li>\u958b\u767c\u74b0\u5883 API: \u4e0d\u5e38\u88ab\u8abf\u7528\uff0c\u4f46\u4e0d\u80fd\u4e0b\u7dda  </li> <li>\u81ea\u52a9\u5831\u8868\u7522\u751f: \u4f7f\u7528\u8005\u9ede\u9078\u6642\u624d\u555f\u52d5\u7522\u751f\u7a0b\u5f0f  </li> <li>\u4f7f\u7528\u8005\u89f8\u767c\u7684\u4efb\u52d9: \u4f8b\u5982\u8cc7\u6599\u5c0e\u5165\u3001\u8f49\u63db\u7b49\u81e8\u6642\u4efb\u52d9  </li> </ul> </li> <li>\u85cd\u7da0\u90e8\u7f72\u8207\u7070\u968e\u767c\u5e03 (Blue-Green / Canary Release)<ul> <li>\u767c\u5e03\u65b0\u7248\u672c: \u53ef\u5c07 10% \u6d41\u91cf\u5c0e\u5411\u65b0\u7248\u672c  </li> <li>\u9010\u6b65\u64f4\u5927\u6d41\u91cf: \u6839\u64da\u5065\u5eb7\u72c0\u6cc1\u6162\u6162\u8f49\u79fb\u6d41\u91cf  </li> <li>\u5feb\u901f\u56de\u9000: \u65b0\u7248\u6709\u554f\u984c\u6642\u7acb\u5373\u5207\u56de\u820a\u7248  </li> </ul> </li> <li>\u7121\u4f3a\u670d\u5668\u51fd\u6578 (FaaS) \u5e73\u53f0\u5efa\u8a2d<ul> <li>\u5efa\u7acb\u4f01\u696d\u5167\u90e8 FaaS: \u958b\u767c\u8005\u53ea\u9700\u63d0\u4f9b container \u6620\u50cf\u5373\u53ef  </li> <li>\u81ea\u5b9a\u7fa9\u89f8\u767c\u689d\u4ef6: \u53ef\u7d81\u5b9a Kafka\u3001Webhook\u3001Cron \u7b49  </li> <li>\u6a19\u6e96\u5316\u4e8b\u4ef6\u683c\u5f0f: \u652f\u63f4 CloudEvents \u6a19\u6e96\u683c\u5f0f  </li> </ul> </li> <li>\u7d50\u5408 DevOps / GitOps \u7684\u5feb\u901f\u4ea4\u4ed8\u5834\u666f<ul> <li>\u81ea\u52d5\u90e8\u7f72 pipeline: Git push \u2192 Tekton build \u2192 Knative deploy  </li> <li>GitOps Workflow: Git \u8a2d\u5b9a\u8b8a\u66f4 \u2192 \u81ea\u52d5\u66f4\u65b0 revision  </li> <li>\u591a\u7248\u672c\u63a7\u7ba1\u8207\u5207\u63db: \u53ef\u5feb\u901f\u5207\u63db\u7248\u672c\u3001\u6e2c\u8a66\u3001\u56de\u9000  </li> </ul> </li> <li>\u8cc7\u6e90\u654f\u611f\u578b\u7684\u5fae\u670d\u52d9\u67b6\u69cb<ul> <li>\u4f7f\u7528\u8005\u5831\u8868\u670d\u52d9: \u767d\u5929\u7528\u91cf\u9ad8\u3001\u665a\u4e0a\u5e7e\u4e4e\u6c92\u4eba\u7528  </li> <li>\u884c\u92b7\u6d3b\u52d5\u7cfb\u7d71: \u6d3b\u52d5\u671f\u9593\u9ad8\u5cf0\uff0c\u5e73\u5e38\u7121\u4eba\u4f7f\u7528  </li> <li>Edge/IoT \u7bc0\u9ede: \u9700\u7bc0\u7701 CPU/Memory \u4f7f\u7528\u91cf  </li> </ul> </li> </ul>","tags":["Knative"]},{"location":"side-projects/data2ml-ops/others/knative-introduction/#knative_1","title":"\u4e0d\u9069\u5408 Knative \u7684\u5834\u666f","text":"<ul> <li>\u9ad8\u541e\u5410\u3001\u4f4e\u5ef6\u9072\u5e38\u99d0\u670d\u52d9: scale-to-zero \u6703\u9020\u6210\u51b7\u555f\u52d5\u5ef6\u9072</li> <li>WebSocket / gRPC Streaming: \u4e0d\u652f\u63f4\u9577\u9023\u7dda\u5354\u5b9a</li> <li>\u975e HTTP \u5354\u8b70: Knative Serving \u76ee\u524d\u53ea\u652f\u63f4 HTTP-based \u8acb\u6c42</li> <li>\u6709\u72c0\u614b\u670d\u52d9: Knative \u50c5\u652f\u63f4 stateless container</li> </ul>","tags":["Knative"]},{"location":"side-projects/data2ml-ops/others/knative-introduction/#eventing","title":"Eventing","text":"<ul> <li>Knative Eventing is a powerful Kubernetes-based framework that enables event-driven application development. It allows developers to build loosely coupled, reactive services that respond to events from various sources. By decoupling producers and consumers of events, Knative Eventing makes it easier to scale, update, and maintain modern cloud-native applications, especially in serverless environments.</li> <li>Knative Eventing uses standard HTTP POST requests to send and receive events between event producers and sinks.</li> <li>These events conform to the CloudEvents specifications, which enables creating, parsing, sending, and receiving events in any programming language.</li> </ul>","tags":["Knative"]},{"location":"side-projects/data2ml-ops/others/knative-introduction/#event-mesh","title":"Event Mesh","text":"Knative Event Mesh <ul> <li>An Event Mesh is dynamic, interconnected infrastructure which is designed to simplify distributing events from senders to recipients.</li> <li>provides asynchronous (store-and-forward) delivery of messages which allows decoupling senders and recipients in time</li> <li>Event Meshes also simplify the routing concerns of senders and recipients by decoupling them from the underlying event transport infrastructure (which may be a federated set of solutions like Kafka, RabbitMQ, or cloud provider infrastructure)</li> <li>The mesh transports events from producers to consumers via a network of interconnected event brokers across any environment, and even between clouds in a seamless and loosely coupled way.</li> <li>Event producers can publish all events to the mesh, which can route events to interested subscribers without needing the application to subdivide events to channels</li> <li>Event consumers can use mesh configuration to receive events of interest using fine-grained filter expressions rather than needing to implement multiple subscriptions and application-level event filtering to select the events of interest.</li> <li>the Broker API offers a discoverable endpoint for event ingress and the Trigger API completes the offering with its event filtering and delivery capabilities</li> </ul> <p>\u7576\u6211\u7b2c\u4e00\u6b21\u63a5\u89f8\u5230 Knative Eventing \u7684 \"Event Mesh\" \u6982\u5ff5\u6642\uff0c\u5176\u5be6\u6709\u9ede\u7591\u60d1\u3002\u7562\u7adf\u6211\u4ee5\u524d\u7fd2\u6163\u7528 Kafka \u9019\u7a2e\u6bd4\u8f03\u76f4\u63a5\u7684\u8a0a\u606f\u7cfb\u7d71\uff0cproducer \u628a\u8cc7\u6599\u4e1f\u5230\u67d0\u500b topic\uff0cconsumer \u8a02\u95b1\u90a3\u500b topic\uff0c\u5927\u5bb6\u5404\u53f8\u5176\u8077\u3001\u6e05\u695a\u660e\u77ad\u3002\u4e45\u4e86\u4e5f\u5c31\u7406\u6240\u7576\u7136\u5730\u63a5\u53d7\u4e86\u9019\u7a2e\u300c\u5927\u5bb6\u90fd\u8981\u77e5\u9053\u8a0a\u606f\u53bb\u54ea\u3001\u5f9e\u54ea\u4f86\u300d\u7684\u6a21\u5f0f\u3002</p> <p>\u4f46\u5f8c\u4f86\u6211\u958b\u59cb\u7406\u89e3 Event Mesh \u7684\u6642\u5019\uff0c\u8166\u4e2d\u6709\u4e00\u7a2e\u300c\u554a\uff0c\u539f\u4f86\u9084\u53ef\u4ee5\u9019\u6a23\u8a2d\u8a08\u300d\u7684\u611f\u89ba\u3002Event Mesh \u4e0d\u518d\u8981\u6c42 sender \u8ddf receiver \u90fd\u77e5\u9053\u8a0a\u606f\u901a\u904e\u4e86\u54ea\u689d\u901a\u9053\uff0c\u4e5f\u4e0d\u9700\u8981\u5927\u5bb6\u786c\u7d81\u5728\u540c\u4e00\u500b Kafka topic \u4e0a\u3002\u76f8\u53cd\u5730\uff0c\u5b83\u5f37\u8abf\u7684\u662f\u300c\u4e8b\u4ef6\u672c\u8eab\u300d\uff0c\u6bd4\u5982\u9019\u662f\u4e00\u500b\u4f86\u81ea\u67d0\u500b\u4f86\u6e90\u3001\u67d0\u7a2e\u985e\u578b\u3001\u767c\u751f\u5728\u67d0\u500b\u6642\u9593\u9ede\u7684\u4e8b\u4ef6\u2014\u2014\u9019\u4e9b\u5c6c\u6027\u624d\u662f\u5b83\u80fd\u4e0d\u80fd\u88ab\u8655\u7406\u7684\u95dc\u9375\u3002\u7cfb\u7d71\u6703\u6839\u64da\u9019\u4e9b\u5c6c\u6027\uff0c\u81ea\u52d5\u628a\u4e8b\u4ef6\u9001\u5230\u771f\u6b63\u9700\u8981\u5b83\u7684\u5730\u65b9\u3002\u9019\u4e2d\u9593\u7684\u8def\u600e\u9ebc\u8d70\uff0c\u4e0d\u518d\u662f\u61c9\u7528\u7a0b\u5f0f\u7684\u8cac\u4efb\uff0c\u800c\u662f Event Mesh \u5e6b\u4f60\u8655\u7406\u597d\u3002</p> <p>\u6700\u8b93\u6211\u9a5a\u8a1d\u7684\u662f\uff0c\u5b83\u751a\u81f3\u53ef\u4ee5\u5728\u80cc\u5f8c\u540c\u6642\u7528 Kafka\u3001RabbitMQ\uff0c\u751a\u81f3 cloud provider \u7684 Pub/Sub\uff0c\u4f5c\u70ba\u4e8b\u4ef6\u50b3\u8f38\u7684\u5e95\u5c64\u3002\u63db\u53e5\u8a71\u8aaa\uff0c\u4f60\u4e0d\u7528\u9078\u908a\u7ad9\uff0c\u4e5f\u4e0d\u9700\u8981\u5728\u8a2d\u8a08\u521d\u671f\u5c31\u7d81\u6b7b\u5728\u67d0\u500b\u6280\u8853\u4e0a\u3002\u53ea\u8981\u4e8b\u4ef6\u9001\u5f97\u51fa\u4f86\uff0c\u6709\u8208\u8da3\u7684\u670d\u52d9\u5c31\u6703\u6536\u5230\uff0c\u4e0d\u9700\u8981\u4e8b\u524d\u7d04\u597d topic\u3001\u4e5f\u4e0d\u9700\u8981\u7dad\u8b77\u4e00\u5806 subscriptions\u3002\u9019\u7a2e\u9b06\u8026\u5408\u7684\u8a2d\u8a08\u8b93\u7cfb\u7d71\u64f4\u5c55\u8d77\u4f86\u8f15\u9b06\u5f88\u591a\uff0c\u958b\u767c\u8d77\u4f86\u4e5f\u6bd4\u8f03\u81ea\u7531\uff0c\u7279\u5225\u9069\u5408\u5fae\u670d\u52d9\u6216 serverless \u67b6\u69cb\u3002</p> <p>\u5982\u679c\u4f60\u4e5f\u66fe\u7d93\u89ba\u5f97 Kafka \u7684 topic \u8a2d\u8a08\u5f88\u9748\u6d3b\u4f46\u8d8a\u4f86\u8d8a\u96e3\u7ba1\u7406\uff0c\u90a3\u6211\u771f\u7684\u5f88\u63a8\u85a6\u4f60\u770b\u770b Event Mesh \u7684\u505a\u6cd5\u3002\u5b83\u8b93\u4e8b\u4ef6\u6210\u70ba\u67b6\u69cb\u7684\u4e3b\u89d2\uff0c\u800c\u4e0d\u662f\u67d0\u500b\u5de5\u5177\u6216\u5e73\u53f0\u3002\u9019\u7a2e\u8f49\u8b8a\uff0c\u5c0d\u6211\u4f86\u8aaa\u4e0d\u53ea\u662f\u6280\u8853\u4e0a\u7684\u6f14\u9032\uff0c\u4e5f\u662f\u4e00\u7a2e\u601d\u7dad\u4e0a\u7684\u91cb\u653e\u3002</p> Event Sources <ul> <li>An event source is a Kubernetes custom resource (CR), created by a developer or cluster administrator, that acts as a link between an event producer and an event sink. A sink can be a k8s service, including Knative Services, a Channel, or a Broker that receives events from an event source.</li> <li>Apache Kafka, RabbitMQ, Amazon S3, Amazon SQS etc.</li> </ul> Brokers <p> Brokers are Kubernetes custom resources that define an event mesh for collecting a pool of events. Brokers provide a discoverable endpoint for event ingress, and use Triggers for event delivery. Event producers can send events to a broker by POSTing the event.</p>","tags":["Knative"]},{"location":"side-projects/data2ml-ops/others/knative-introduction/#triggers","title":"Triggers","text":"<p>A trigger represents a desire to subscribe to events from a specific broker.</p>","tags":["Knative"]},{"location":"side-projects/data2ml-ops/others/knative-introduction/#event-sinks","title":"Event Sinks","text":"<ul> <li>When you create an event source, you can specify a sink where events are sent to from the source. A sink is an Addressable or a Callable resource that can receive incoming events from other resources.</li> <li>Knative Services, Channels, and Brokers are all examples of sinks.</li> <li>Amazon S3, SNS, SQS, Kafka, Logger, Redis Sink,</li> </ul> <p>The core components of Knative Eventing include Event Sources, Brokers, Triggers and Sink. Event sources can originate from systems like GitHub (webhooks), Apache Kafka, CronJobs, Kubernetes API server events, or even custom containers that emit CloudEvents. A Broker acts as a central event mesh that receives and buffers incoming events. Triggers are routing rules that filter events from the Broker and forward them to services based on specific criteria. Currently, the most common event delivery mechanism is HTTP using the CloudEvents specification, typically in a push-based manner to HTTP endpoints such as Knative Services.</p> <p>For example, imagine you\u2019ve deployed Knative Eventing with a PingSource that emits an event every minute. This event is sent to a Broker, which acts as an event hub. A Trigger listens on that Broker and filters events based on attributes like the event type. When a matching event arrives, the Trigger forwards it to a Knative Service (a containerized HTTP handler). Behind the scenes, Knative handles service discovery, traffic routing, autoscaling (even from zero), and ensures that the container is activated just-in-time to handle the event. This creates a seamless, scalable, and efficient event-driven pipeline without needing to manage infrastructure manually.</p>","tags":["Knative"]},{"location":"side-projects/data2ml-ops/others/knative-introduction/#serving","title":"Serving","text":"","tags":["Knative"]},{"location":"side-projects/data2ml-ops/others/knative-introduction/#resources","title":"Resources","text":"Knative Serving Resources: Services, Routes, Configurations, Revisions <ul> <li>Service: The main entry point that manages the full lifecycle of your app.</li> <li>Route: Sends traffic to specific revisions, with support for splitting and naming.</li> <li>Configuration: Stores deployment settings; changes create new revisions.</li> <li>Revision: A read-only snapshot of code and config that auto-scales with traffic.</li> </ul>","tags":["Knative"]},{"location":"side-projects/data2ml-ops/others/knative-introduction/#componenets","title":"Componenets","text":"Knative Serving Architecture <ul> <li>Activator:<ul> <li>It is responsible to queue incoming requests (if a Knative Service is scaled-to-zero)</li> <li>It communicates with the autoscaler to bring scaled-to-zero Services back up and forward the queued requests.</li> <li>Activator can also act as a request buffer to handle traffic bursts.</li> </ul> </li> <li>Autoscaler: scale the Knative Services based on configuration, metrics and incoming requests.</li> <li>Controller: manages the state of Knative resources within the cluster</li> <li>Queue-proxy:<ul> <li>The Queue-Proxy is a sidecar container in the Knative Service's Pod.</li> <li>collect metrics and enforcing the desired concurrency when forwarding requests to the user's container</li> <li>a queue if necessary, similar to the Activator.</li> </ul> </li> <li>Webhooks: validate and mutate Knative Resources.</li> </ul>","tags":["Knative"]},{"location":"side-projects/data2ml-ops/others/knative-introduction/#networking-layer","title":"Networking Layer","text":"<ul> <li>Knative Serving depends on a Networking Layer that fulfils the Knative Networking Specification.</li> <li>Knative Serving defines an internal <code>KIngress</code> resource, which acts as an abstraction for different multiple pluggable networking layers</li> <li>Currently, three networking layers are available and supported by the community:<ul> <li>net-istio</li> <li>net-kourier</li> <li>net-contour</li> </ul> </li> </ul> <p>How does the network traffic flow?</p> Knative Serving Network Traffic Flow <ul> <li>The <code>Ingress Gateway</code> is used to route requests to the activator (proxy mode) or directly to a Knative Service Pod (serve mode), depending on the mode (proxy/serve, see here for more details).</li> <li>Each networking layer has a controller that is responsible to watch the KIngress resources and configure the Ingress Gateway accordingly.</li> <li>For the Ingress Gateway to be reachable outside the cluster, it must be exposed using a Kubernetes Service of <code>type: LoadBalancer</code> or <code>type: NodePort</code></li> </ul>","tags":["Knative"]},{"location":"side-projects/data2ml-ops/others/knative-introduction/#references","title":"References","text":"<ul> <li>Knative Serving | Knative</li> <li>Knative Serving Architecture | Knative</li> <li>Knative Eventing | Knative</li> <li>Event Mesh | Knative</li> <li>Event Sources | Knative</li> <li>About Sinks | Knative</li> <li>About Brokers | Knative</li> <li>Using Triggers | Knative</li> </ul>","tags":["Knative"]},{"location":"side-projects/data2ml-ops/others/mlserver-introduction/","title":"MLServer","text":"<ul> <li>MLServer is an open source inference server for your machine learning models.</li> <li>MLServer aims to provide an easy way to start serving your machine learning models through a REST and gRPC interface</li> </ul>"},{"location":"side-projects/data2ml-ops/others/mlserver-introduction/#inference-runtimes","title":"Inference Runtimes","text":"<ul> <li>Inference runtimes allow you to define how your model should be used within MLServer. You can think of them as the backend glue between MLServer and your machine learning framework of choice.</li> <li>Out of the box, MLServer comes with a set of pre-packaged runtimes which let you interact with a subset of common ML frameworks. This allows you to start serving models saved in these frameworks straight away.</li> </ul> MLServer Inference Runtime MLServer Supported Inference Runtime"},{"location":"side-projects/data2ml-ops/others/mlserver-introduction/#openapi-support","title":"OpenAPI Support","text":"<ul> <li>OpenAPI spec: dataplane.json</li> <li>MLServer follows the Open Inference Protocol (previously known as the \u201cV2 Protocol\u201d). fully compliant with KServing\u2019s V2 Dataplane spec.</li> <li>Support Swagger UI: <ul> <li>The autogenerated Swagger UI can be accessed under the <code>/v2/docs</code> endpoint.</li> <li>MLServer will also autogenerate a Swagger UI tailored to individual models, showing the endpoints available for each one. under the following endpoints:<ul> <li><code>/v2/models/{model_name}/docs</code></li> <li><code>/v2/models/{model_name}/versions/{model_version}/docs</code></li> </ul> </li> </ul> </li> </ul> MLServer Swagger UI MLServer Model Swagger UI"},{"location":"side-projects/data2ml-ops/others/mlserver-introduction/#parallel-inference","title":"Parallel Inference","text":"MLServer Parallel Inference <ul> <li>Python has some native issues</li> <li>The Global Interpreter Lock (GIL) is a mutex lock that exists in most Python interpreters (e.g. CPython). Its main purpose is to lock Python\u2019s execution so that it only runs on a single processor at the same time. This simplifies certain things to the interpreter. However, it also adds the limitation that a single Python process will never be able to leverage multiple cores.</li> <li>Out of the box, MLServer overcome the python native issue, to support to offload inference workloads to a pool of workers running in separate processes.</li> <li><code>parallel_workers</code> on the <code>settings.json</code> file</li> </ul>"},{"location":"side-projects/data2ml-ops/others/mlserver-introduction/#multi-model-servingmms","title":"Multi-Model Serving(MMs)","text":"<ul> <li>within a single instance of MLServer, you can serve multiple models under different paths. This also includes multiple versions of the same model.</li> </ul>"},{"location":"side-projects/data2ml-ops/others/mlserver-introduction/#adaptive-batching","title":"Adaptive Batching","text":"<ul> <li>MLServer includes support to batch requests together transparently on-the-fly. We refer to this as \u201cadaptive batching\u201d, although it can also be known as \u201cpredictive batching\u201d.</li> <li>Why? <ul> <li>Maximise resource usage</li> <li>Minimise any inference overhead</li> </ul> </li> <li>\u9700\u8981\u4ed4\u7d30\u8abf\u6574\uff0c\u56e0\u6b64MLServer won\u2019t enable by default adaptive batching on newly loaded models.</li> <li>Usage: <code>max_batch_size</code>, <code>max_batch_time</code> on the <code>model-settings.json</code> file</li> </ul>"},{"location":"side-projects/data2ml-ops/others/mlserver-introduction/#metrics","title":"Metrics","text":"<ul> <li>Out-of-the-box, MLServer exposes a set of metrics that help you monitor your machine learning workloads in production.</li> <li>On top of these, you can also register and track your own custom metrics as part of your custom inference runtimes.</li> <li>Default Metrics<ul> <li><code>model_infer_request_success</code>: Number of successful inference requests.</li> <li><code>model_infer_request_failure</code>: Number of failed inference requests.</li> <li><code>batch_request_queue</code>: Queue size for the adaptive batching queue.</li> <li><code>parallel_request_queue</code>: Queue size for the inference workers queue.</li> </ul> </li> <li>REST Metrics<ul> <li><code>[rest_server]_requests</code>: Number of REST requests, labelled by endpoint and status code.</li> <li><code>[rest_server]_requests_duration_seconds</code>: Latency of REST requests.</li> <li><code>[rest_server]_requests_in_progress</code>: Number of in-flight REST requests.</li> </ul> </li> <li>gRPC Metrics<ul> <li><code>grpc_server_handled</code>: Number of gRPC requests, labelled by gRPC code and method.</li> <li><code>grpc_server_started</code>: Number of in-flight gRPC requests.</li> </ul> </li> </ul>"},{"location":"side-projects/data2ml-ops/others/mlserver-introduction/#reference","title":"Reference","text":"<ul> <li>Inference Runtimes | MLServer</li> <li>Multi-Model Serving | MLServer</li> </ul>"},{"location":"side-projects/data2ml-ops/ray/deployment/","title":"Deploy Ray Cluster on Kubernetes Using KubeRay","text":"<p>KubeRay simplifies managing Ray clusters on Kubernetes by introducing three key Custom Resource Definitions (CRDs): RayCluster, RayJob, and RayService. These CRDs make it easy to tailor Ray clusters for different use cases.<sup>1</sup></p> <p>The KubeRay operator offers a Kubernetes-native approach to managing Ray clusters. A typical Ray cluster includes a head node pod and multiple worker node pods. With optional autoscaling, the operator can dynamically adjust the cluster size based on workload demands, adding or removing pods as needed.<sup>1</sup></p> What is KubeRay?<sup>1</sup> Architecture (Click to Enlarge) <p>Setting up KubeRay is straightforward. This guide will walk you through installing the KubeRay operator and deploying your first Ray cluster using Helm. By the end, you'll have a fully functional Ray environment running on your Kubernetes cluster.<sup>2</sup><sup>3</sup></p>"},{"location":"side-projects/data2ml-ops/ray/deployment/#install-kuberay-operator","title":"Install KubeRay Operator","text":"<p>Start by adding the KubeRay Helm repository to access the required charts:</p> <pre><code>helm repo add kuberay https://ray-project.github.io/kuberay-helm/\n</code></pre> <pre><code>\"kuberay\" has been added to your repositories\n</code></pre> <p>Update your local Helm chart list to ensure you're using the latest version:</p> <pre><code>helm repo update\n</code></pre> <pre><code>Hang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"kuberay\" chart repository\nUpdate Complete. \u2388Happy Helming!\u2388\n</code></pre> <p>Next, create a namespace to manage KubeRay resources:</p> <pre><code>kubectl create ns kuberay\n</code></pre> <pre><code>namespace/kuberay created\n</code></pre> <p>Now, install the KubeRay operator in the namespace. This sets up the controller to manage Ray clusters:</p> <pre><code>helm install kuberay-operator kuberay/kuberay-operator \\\n  --version 1.3.0 \\\n  -n kuberay\n</code></pre> <pre><code>NAME: kuberay-operator\nLAST DEPLOYED: Wed May 14 20:29:44 2025\nNAMESPACE: kuberay\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <p>Verify that the KubeRay operator pod is running:</p> <pre><code>kubectl get pods -n kuberay\n</code></pre> <pre><code>NAME                                READY   STATUS    RESTARTS   AGE\nkuberay-operator-66d848f5cd-5npp6   1/1     Running   0          23s\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/deployment/#deploy-a-ray-cluster","title":"Deploy a Ray Cluster","text":"<p>Export the default <code>values.yaml</code> file to customize memory settings. If you've encountered OOM issues, it's a good idea to increase memory allocation upfront.<sup>4</sup></p> <pre><code>helm show values kuberay/ray-cluster &gt; values.yaml\nnano values.yaml\n</code></pre> values.yaml values.yaml<pre><code># Default values for ray-cluster.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\n# The KubeRay community welcomes PRs to expose additional configuration\n# in this Helm chart.\n\nimage:\n  repository: rayproject/ray\n  tag: 2.41.0\n  pullPolicy: IfNotPresent\n\nnameOverride: \"kuberay\"\nfullnameOverride: \"\"\n\nimagePullSecrets: []\n  # - name: an-existing-secret\n\n# common defined values shared between the head and worker\ncommon:\n  # containerEnv specifies environment variables for the Ray head and worker containers.\n  # Follows standard K8s container env schema.\n  containerEnv: []\n  #  - name: BLAH\n  #    value: VAL\nhead:\n  # rayVersion determines the autoscaler's image version.\n  # It should match the Ray version in the image of the containers.\n  # rayVersion: 2.41.0\n  # If enableInTreeAutoscaling is true, the autoscaler sidecar will be added to the Ray head pod.\n  # Ray autoscaler integration is supported only for Ray versions &gt;= 1.11.0\n  # Ray autoscaler integration is Beta with KubeRay &gt;= 0.3.0 and Ray &gt;= 2.0.0.\n  # enableInTreeAutoscaling: true\n  # autoscalerOptions is an OPTIONAL field specifying configuration overrides for the Ray autoscaler.\n  # The example configuration shown below represents the DEFAULT values.\n  # autoscalerOptions:\n    # upscalingMode: Default\n    # idleTimeoutSeconds is the number of seconds to wait before scaling down a worker pod which is not using Ray resources.\n    # idleTimeoutSeconds: 60\n    # imagePullPolicy optionally overrides the autoscaler container's default image pull policy (IfNotPresent).\n    # imagePullPolicy: IfNotPresent\n    # Optionally specify the autoscaler container's securityContext.\n    # securityContext: {}\n    # env: []\n    # envFrom: []\n    # resources specifies optional resource request and limit overrides for the autoscaler container.\n    # For large Ray clusters, we recommend monitoring container resource usage to determine if overriding the defaults is required.\n    # resources:\n    #   limits:\n    #     cpu: \"500m\"\n    #     memory: \"512Mi\"\n    #   requests:\n    #     cpu: \"500m\"\n    #     memory: \"512Mi\"\n  labels: {}\n  # Note: From KubeRay v0.6.0, users need to create the ServiceAccount by themselves if they specify the `serviceAccountName`\n  # in the headGroupSpec. See https://github.com/ray-project/kuberay/pull/1128 for more details.\n  serviceAccountName: \"\"\n  restartPolicy: \"\"\n  rayStartParams: {}\n  # containerEnv specifies environment variables for the Ray container,\n  # Follows standard K8s container env schema.\n  containerEnv: []\n  # - name: EXAMPLE_ENV\n  #   value: \"1\"\n  envFrom: []\n    # - secretRef:\n    #     name: my-env-secret\n  # ports optionally allows specifying ports for the Ray container.\n  # ports: []\n  # resource requests and limits for the Ray head container.\n  # Modify as needed for your application.\n  # Note that the resources in this example are much too small for production;\n  # we don't recommend allocating less than 8G memory for a Ray pod in production.\n  # Ray pods should be sized to take up entire K8s nodes when possible.\n  # Always set CPU and memory limits for Ray pods.\n  # It is usually best to set requests equal to limits.\n  # See https://docs.ray.io/en/latest/cluster/kubernetes/user-guides/config.html#resources\n  # for further guidance.\n  resources:\n    limits:\n      cpu: \"1\"\n      # To avoid out-of-memory issues, never allocate less than 2G memory for the Ray head.\n      memory: \"4G\"\n    requests:\n      cpu: \"1\"\n      memory: \"4G\"\n  annotations: {}\n  nodeSelector: {}\n  tolerations: []\n  affinity: {}\n  # Pod security context.\n  podSecurityContext: {}\n  # Ray container security context.\n  securityContext: {}\n  # Optional: The following volumes/volumeMounts configurations are optional but recommended because\n  # Ray writes logs to /tmp/ray/session_latests/logs instead of stdout/stderr.\n  volumes:\n    - name: log-volume\n      emptyDir: {}\n  volumeMounts:\n    - mountPath: /tmp/ray\n      name: log-volume\n  # sidecarContainers specifies additional containers to attach to the Ray pod.\n  # Follows standard K8s container spec.\n  sidecarContainers: []\n  # See docs/guidance/pod-command.md for more details about how to specify\n  # container command for head Pod.\n  command: []\n  args: []\n  # Optional, for the user to provide any additional fields to the service.\n  # See https://pkg.go.dev/k8s.io/Kubernetes/pkg/api/v1#Service\n  headService: {}\n    # metadata:\n    #   annotations:\n    #     prometheus.io/scrape: \"true\"\n\n  # Custom pod DNS configuration\n  # See https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-dns-config\n  # dnsConfig:\n  #   nameservers:\n  #     - 8.8.8.8\n  #   searches:\n  #     - example.local\n  #   options:\n  #     - name: ndots\n  #       value: \"2\"\n  #     - name: edns0\n  topologySpreadConstraints: {}\n\n\nworker:\n  # If you want to disable the default workergroup\n  # uncomment the line below\n  # disabled: true\n  groupName: workergroup\n  replicas: 1\n  minReplicas: 1\n  maxReplicas: 3\n  labels: {}\n  serviceAccountName: \"\"\n  restartPolicy: \"\"\n  rayStartParams: {}\n  # containerEnv specifies environment variables for the Ray container,\n  # Follows standard K8s container env schema.\n  containerEnv: []\n  # - name: EXAMPLE_ENV\n  #   value: \"1\"\n  envFrom: []\n    # - secretRef:\n    #     name: my-env-secret\n  # ports optionally allows specifying ports for the Ray container.\n  # ports: []\n  # resource requests and limits for the Ray head container.\n  # Modify as needed for your application.\n  # Note that the resources in this example are much too small for production;\n  # we don't recommend allocating less than 8G memory for a Ray pod in production.\n  # Ray pods should be sized to take up entire K8s nodes when possible.\n  # Always set CPU and memory limits for Ray pods.\n  # It is usually best to set requests equal to limits.\n  # See https://docs.ray.io/en/latest/cluster/kubernetes/user-guides/config.html#resources\n  # for further guidance.\n  resources:\n    limits:\n      cpu: \"1\"\n      memory: \"3G\"\n    requests:\n      cpu: \"1\"\n      memory: \"3G\"\n  annotations: {}\n  nodeSelector: {}\n  tolerations: []\n  affinity: {}\n  # Pod security context.\n  podSecurityContext: {}\n  # Ray container security context.\n  securityContext: {}\n  # Optional: The following volumes/volumeMounts configurations are optional but recommended because\n  # Ray writes logs to /tmp/ray/session_latests/logs instead of stdout/stderr.\n  volumes:\n    - name: log-volume\n      emptyDir: {}\n  volumeMounts:\n    - mountPath: /tmp/ray\n      name: log-volume\n  # sidecarContainers specifies additional containers to attach to the Ray pod.\n  # Follows standard K8s container spec.\n  sidecarContainers: []\n  # See docs/guidance/pod-command.md for more details about how to specify\n  # container command for worker Pod.\n  command: []\n  args: []\n  topologySpreadConstraints: {}\n\n\n  # Custom pod DNS configuration\n  # See https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-dns-config\n  # dnsConfig:\n  #   nameservers:\n  #     - 8.8.8.8\n  #   searches:\n  #     - example.local\n  #   options:\n  #     - name: ndots\n  #       value: \"2\"\n  #     - name: edns0\n\n# The map's key is used as the groupName.\n# For example, key:small-group in the map below\n# will be used as the groupName\nadditionalWorkerGroups:\n  smallGroup:\n    # Disabled by default\n    disabled: true\n    replicas: 0\n    minReplicas: 0\n    maxReplicas: 3\n    labels: {}\n    serviceAccountName: \"\"\n    restartPolicy: \"\"\n    rayStartParams: {}\n    # containerEnv specifies environment variables for the Ray container,\n    # Follows standard K8s container env schema.\n    containerEnv: []\n      # - name: EXAMPLE_ENV\n      #   value: \"1\"\n    envFrom: []\n        # - secretRef:\n        #     name: my-env-secret\n    # ports optionally allows specifying ports for the Ray container.\n    # ports: []\n    # resource requests and limits for the Ray head container.\n    # Modify as needed for your application.\n    # Note that the resources in this example are much too small for production;\n    # we don't recommend allocating less than 8G memory for a Ray pod in production.\n    # Ray pods should be sized to take up entire K8s nodes when possible.\n    # Always set CPU and memory limits for Ray pods.\n    # It is usually best to set requests equal to limits.\n    # See https://docs.ray.io/en/latest/cluster/kubernetes/user-guides/config.html#resources\n    # for further guidance.\n    resources:\n      limits:\n        cpu: 1\n        memory: \"3G\"\n      requests:\n        cpu: 1\n        memory: \"3G\"\n    annotations: {}\n    nodeSelector: {}\n    tolerations: []\n    affinity: {}\n    # Pod security context.\n    podSecurityContext: {}\n    # Ray container security context.\n    securityContext: {}\n    # Optional: The following volumes/volumeMounts configurations are optional but recommended because\n    # Ray writes logs to /tmp/ray/session_latests/logs instead of stdout/stderr.\n    volumes:\n      - name: log-volume\n        emptyDir: {}\n    volumeMounts:\n      - mountPath: /tmp/ray\n        name: log-volume\n    sidecarContainers: []\n    # See docs/guidance/pod-command.md for more details about how to specify\n    # container command for worker Pod.\n    command: []\n    args: []\n\n    # Topology Spread Constraints for worker pods\n    # See: https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/\n    topologySpreadConstraints: {}\n\n    # Custom pod DNS configuration\n    # See https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-dns-config\n    # dnsConfig:\n    #   nameservers:\n    #     - 8.8.8.8\n    #   searches:\n    #     - example.local\n    #   options:\n    #     - name: ndots\n    #       value: \"2\"\n    #     - name: edns0\n\n# Configuration for Head's Kubernetes Service\nservice:\n  # This is optional, and the default is ClusterIP.\n  type: ClusterIP\n</code></pre> <p>Install the Ray cluster using the customized <code>values.yaml</code>. Here, we're using the image tag <code>2.46.0-py310-aarch64</code> for Python 3.10, Ray 2.46.0, and MacOS ARM architecture. You can find all supported Ray images on Docker Hub.<sup>5</sup></p> <pre><code>helm install raycluster kuberay/ray-cluster \\\n  --version 1.3.0 \\\n  --set 'image.tag=2.46.0-py310-aarch64' \\\n  -n kuberay \\\n  -f values.yaml\n</code></pre> <pre><code>NAME: raycluster\nLAST DEPLOYED: Wed May 14 20:31:53 2025\nNAMESPACE: kuberay\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <p>Once the RayCluster CR is created, you can check its status:</p> <pre><code>kubectl get rayclusters -n kuberay\n</code></pre> <pre><code>NAME                 DESIRED WORKERS   AVAILABLE WORKERS   CPUS   MEMORY   GPUS   STATUS   AGE\nraycluster-kuberay   1                                     2      4G       0               62s\n</code></pre> <p>To view the running pods in your Ray cluster, use:</p> <pre><code>kubectl get pods --selector=ray.io/cluster=raycluster-kuberay -n kuberay\n</code></pre> <pre><code>NAME                                          READY   STATUS    RESTARTS   AGE\nraycluster-kuberay-head-k6ktp                 1/1     Running   0          5m49s\nraycluster-kuberay-workergroup-worker-zrxbj   1/1     Running   0          5m49s\n</code></pre> <ol> <li> <p>Ray on Kubernetes | Ray Docs \u21a9\u21a9\u21a9</p> </li> <li> <p>KubeRay Operator Installation | Ray Docs \u21a9</p> </li> <li> <p>RayCluster Quickstart | Ray Docs \u21a9</p> </li> <li> <p>Out-Of-Memory Prevention | Ray Docs \u21a9</p> </li> <li> <p>rayproject/ray | Docker Hub \u21a9</p> </li> </ol>"},{"location":"side-projects/data2ml-ops/ray/how-it-works/","title":"How It Works?","text":""},{"location":"side-projects/data2ml-ops/ray/how-it-works/#key-modules-and-their-roles","title":"Key Modules and Their Roles","text":"<p>Ray Core is a powerful distributed computing framework that provides a small set of essential primitives (tasks, actors, and objects) for building and scaling distributed applications.<sup>1</sup></p> <p>On top of Ray Core, Ray provides different AI libraries for different ML workloads.</p> Key Modules<sup>2</sup> Module Description Details Ray Data Scalable datasets for ML Ray Data provides distributed data processing optimized for machine learning and AI workloads. It efficiently streams data through data pipelines.<sup>2</sup> Ray Train Distributed model training Ray Train makes distributed model training simple. It abstracts away the complexity of setting up distributed training across popular frameworks like PyTorch and TensorFlow.<sup>2</sup> Ray Tune Hyperparameter tuning at scale Ray Tune is a library for hyperparameter tuning at any scale. It automatically finds the best hyperparameters for your models with efficient distributed search algorithms.<sup>2</sup> Ray Serve Scalable model serving Ray Serve provides scalable and programmable serving for ML models and business logic. Deploy models from any framework with production-ready performance.<sup>2</sup> Ray RLlib Industry-grade reinforcement learning RLlib is a reinforcement learning (RL) library that offers high performance implementations of popular RL algorithms and supports various training environments. RLlib offers high scalability and unified APIs for a variety of industry- and research applications.<sup>2</sup>"},{"location":"side-projects/data2ml-ops/ray/how-it-works/#architecture-components","title":"Architecture Components","text":"Ray Cluster<sup>3</sup> Ray Cluster <p>A Ray cluster consists of a single head node and any number of connected worker nodes. Ray nodes are implemented as pods when running on Kubernetes.<sup>3</sup></p> Head Node <p>Every Ray cluster has one node which is designated as the head node of the cluster. The head node is identical to other worker nodes, except that it also runs singleton processes responsible for cluster management such as the autoscaler, GCS and the Ray driver processes which run Ray jobs.<sup>3</sup></p> Worker Node <p>Worker nodes do not run any head node management processes, and serve only to run user code in Ray tasks and actors.<sup>3</sup></p> Autoscaling <p>When the resource demands of the Ray workload exceed the current capacity of the cluster, the autoscaler will try to increase the number of worker nodes. When worker nodes sit idle, the autoscaler will remove worker nodes from the cluster.<sup>3</sup></p> Ray Jobs <p>A Ray job is a single application: it is the collection of Ray tasks, objects, and actors that originate from the same script. There are two ways to run a Ray job on a Ray cluster: (1) Ray Jobs API and (2) Run the driver script directly on the Ray cluster.<sup>3</sup></p> 2 Ways of running Ray Jobs<sup>3</sup>"},{"location":"side-projects/data2ml-ops/ray/how-it-works/#core-concepts","title":"Core Concepts","text":"Ray CoreRay DataRay TrainRay TuneRay Serve Tasks <p>Ray enables arbitrary functions to execute asynchronously on separate Python workers. These asynchronous Ray functions are called tasks. Ray enables tasks to specify their resource requirements in terms of CPUs, GPUs, and custom resources.<sup>7</sup></p> Actors <p>Actors extend the Ray API from functions (tasks) to classes. An actor is essentially a stateful worker (or a service).<sup>7</sup></p> Objects <p>Tasks and actors create objects and compute on objects. You can refer to these objects as remote objects. Ray caches remote objects in its distributed shared-memory object store.<sup>7</sup></p> Datasets <p><code>Dataset</code> is the main user-facing Python API. It represents a distributed data collection and define data loading and processing operations. The Dataset API is lazy. Each Dataset consists of blocks.<sup>4</sup></p> Blocks <p>Each Dataset consists of blocks. A block is a contiguous subset of rows from a dataset, which are distributed across the cluster and processed independently in parallel.<sup>4</sup></p> <p> Datasets and Blocks<sup>4</sup> </p> Training Function <p>The training function is a user-defined Python function that contains the end-to-end model training loop logic. When launching a distributed training job, each worker executes this training function.<sup>6</sup></p> Workers <p>Ray Train distributes model training compute to individual worker processes across the cluster. Each worker is a process that executes the training funciton.<sup>6</sup></p> Scaling Configuration <p>The <code>ScalingConfig</code> is the mechanism for defining the scale of the training job. Two common parameters are <code>num_workers</code> and <code>use_gpu</code>.<sup>6</sup></p> Trainer <p>The Trainer ties the previous three concepts together to launch distributed training jobs.<sup>6</sup></p> <p> Ray Tune Configuration<sup>5</sup> </p> Search Space <p>A search space defines valid values for your hyperparameters and can specify how these values are sampled. Tune offers various functions to define search spaces and sampling methods.<sup>5</sup><sup>8</sup></p> Search Algorithms <p>To optimize the hyperparameters of your training process, you use a Search Algorithm which suggests hyperparameter configurations. Tune has Search Algorithms that integrate with many popular optimization libraries, such as HyperOpt or Optuna.<sup>5</sup> Tune automatically converts the provided search space into the search spaces the search algorithms and underlying libraries expect.</p> Schedulers <p>In short, schedulers can stop, pause, or tweak the hyperparameters of running trials, potentially making your hyperparameter tuning process much faster. Tune includes distributed implementations of early stopping algorithms such as Median Stopping Rule, HyperBand, and ASHA. Tune also includes a distributed implementation of Population Based Training (PBT) and Population Based Bandits (PB2). When using schedulers, you may face compatibility issues<sup>5</sup></p> Trainables <p>In short, a Trainable is an object that you can pass into a Tune run. Ray Tune has two ways of defining a trainable, namely the Function API and the Class API. The Function API is generally recommended.<sup>5</sup></p> Trials <p>You use <code>Tuner.fit()</code> to execute and manage hyperparameter tuning and generate your trials. The Tuner.fit() function also provides many features such as logging, checkpointing, and early stopping.<sup>5</sup></p> Analyses <p><code>Tuner.fit()</code> returns an <code>ResultGrid</code> object which has methods you can use for analyzing your training.<sup>5</sup></p> Deployment <p>A deployment contains business logic or an ML model to handle incoming requests and can be scaled up to run across a Ray cluster. At runtime, a deployment consists of a number of replicas, which are individual copies of the class or function that are started in separate Ray Actors (processes).<sup>9</sup></p> Application <p>An application is the unit of upgrade in a Ray Serve cluster. An application consists of one or more deployments. One of these deployments is considered the \u201cingress\u201d deployment, which handles all inbound traffic.<sup>9</sup></p> DeploymentHandle (composing deployments) <p>Ray Serve enables flexible model composition and scaling by allowing multiple independent deployments to call into each other.<sup>9</sup></p> Ingress deployment (HTTP handling) <p>The ingress deployment defines the HTTP handling logic for the application.<sup>9</sup></p>"},{"location":"side-projects/data2ml-ops/ray/how-it-works/#behind-the-scenes","title":"Behind the Scenes","text":"Ray DataRay TrainRay Tune <p>Ray Data uses a two-phase planning process to execute operations efficiently:<sup>4</sup></p> <ul> <li>Logical plans consist of logical operators that describe what operation to perform.</li> <li>Physical plans consist of physical operators that describe how to execute the operation.</li> </ul> <p>The building blocks of these plans are operators:<sup>4</sup></p> <ul> <li>Logical plans consist of logical operators that describe what operation to perform.</li> <li>Physical plans consist of physical operators that describe how to execute the operation.</li> </ul> <p>Ray Data uses a streaming execution model to efficiently process large datasets. It can process data in a streaming fashion through a pipeline of operations.<sup>4</sup></p> <p> Streaming Topology<sup>4</sup> </p> <p>In the streaming execution model, operators are connected in a pipeline, with each operator\u2019s output queue feeding directly into the input queue of the next downstream operator. This creates an efficient flow of data through the execution plan.<sup>4</sup></p> <p>Calling the <code>Trainer.fit()</code> method executes the training job by<sup>6</sup>:</p> <ol> <li>Launching workers as defined by the <code>scaling_config</code>.</li> <li>Setting up the framework's distributed environment on all workers.</li> <li>Running the training function on all workers.</li> </ol> <p>Calling the <code>Trainer.fit()</code> method executes the tuning job by:</p> <ol> <li>The driver process launches and schedules trials across the Ray cluster based on the search space and resources defined.</li> <li>Each trial runs as a Ray task or actor on worker nodes, executing training functions in parallel.</li> <li>Results are collected, and once all trials finish (or meet stop criteria), tuner.fit() returns the best configs and metrics.</li> </ol> <ol> <li> <p>Ray Core | Ray Docs \u21a9</p> </li> <li> <p>Getting Started | Ray Docs \u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Key Concepts | Ray Cluster | Ray Docs \u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Key Concepts | Ray Data | Ray Docs \u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Key Concepts | Ray Tune | Ray Docs \u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Overview | Ray Train | Ray Docs \u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Key Concepts | Ray Core | Ray Docs \u21a9\u21a9\u21a9</p> </li> <li> <p>Tune Search Space API | Ray Docs \u21a9</p> </li> <li> <p>Key Concepts | Ray Tune | Ray Docs \u21a9\u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"side-projects/data2ml-ops/ray/in-the-bigger-picture/","title":"In the Bigger Picture","text":""},{"location":"side-projects/data2ml-ops/ray/in-the-bigger-picture/#history-current-status","title":"History &amp; Current Status","text":"<p>Ray was originally developed by UC Berkeley's RISELab (formerly AMPLab) and was first open-sourced in September 2017.<sup>1</sup> The project was designed to simplify distributed computing for machine learning and AI workloads. Later, the team spun off a company, Anyscale, Inc.<sup>2</sup>, to maintain and commercialize Ray. As of May 2025, Ray has over 37,000 stars on GitHub.<sup>3</sup> In April 2024, Thoughtworks included Ray in the Trial phase of its Technology Radar, indicating it is a promising and maturing tool worth evaluating in real-world projects.<sup>4</sup></p> <p>The community is active and growing, with more than 10,000 members on Slack<sup>5</sup> and over 1,100 contributors on GitHub. Ray is used by more than 23,000 developers and organizations worldwide, including OpenAI, Shopify, and Uber. An annual Ray Summit<sup>6</sup> brings the community together to share use cases, roadmap updates, and technical deep dives.</p> Ray Use Cases<sup>1</sup>"},{"location":"side-projects/data2ml-ops/ray/in-the-bigger-picture/#alternatives","title":"Alternatives","text":""},{"location":"side-projects/data2ml-ops/ray/in-the-bigger-picture/#distributed-data-processing","title":"Distributed Data Processing","text":"<p>(Generated by ChatGPT 4o Deep Research on 2025/05/20)</p> Ray DataApache SparkDaskDaft <p>An open-source unified framework designed to scale Python and AI workloads (such as machine learning) across distributed environments.</p> <p>Pros</p> <ul> <li>Python-native distributed computing framework with a flexible and simple interface, avoiding the overhead of the JVM. Especially suitable for data science and ML workloads.</li> <li>Comprehensive ecosystem including built-in libraries like Ray Tune, Ray Train, Ray Serve, and RLlib\u2014helpful for building large-scale ML applications.</li> <li>Abstracts distributed complexity by offering parallel computing primitives, enabling non-experts in distributed systems to easily get started.</li> </ul> <p>Cons</p> <ul> <li>Lacks high-level data processing interfaces: Ray Datasets only supports basic parallel processing; it lacks full ETL capabilities like rich querying, visualization, or aggregations. It's not positioned as a full data science ETL solution.</li> <li>Scheduling overhead for many small tasks: Ray may incur non-trivial overhead when managing a large number of tiny tasks (batching is often needed). Compared to mature big data engines, its stability and tuning experience are still evolving.</li> </ul> <p>When to Use</p> <ul> <li>Distributed Python computation / ML: Ideal when you need to scale arbitrary Python or ML pipelines (e.g., hyperparameter tuning, distributed training, reinforcement learning).</li> <li>Heterogeneous resource scheduling: Suited for workloads requiring GPUs, TPUs, or managing multiple concurrent tasks like data preprocessing and training.</li> </ul> <p>A unified analytics engine for large-scale data processing. Offers APIs in Scala, Java, Python, and supports batch, streaming, and ML workloads.</p> <p>Pros</p> <ul> <li>Strong big data processing capabilities: Stable and battle-tested, Spark is the de facto standard for distributed computing on large datasets.</li> <li>Robust ecosystem: A unified platform supporting SQL (Spark SQL), DataFrame APIs (pandas API on Spark), machine learning (MLlib), graph computation (GraphX), and streaming (Structured Streaming).</li> <li>Widespread adoption and support: Large community and broad industry usage, with extensive third-party integrations and enterprise support.</li> </ul> <p>Cons</p> <ul> <li>Indirect Python support: Spark runs on the JVM, and PySpark uses serialization to move data between Python and the JVM. Heavy use of Python UDFs may introduce performance overhead. Also lacks the interactive, dynamic experience native Python users are used to.</li> <li>High resource overhead: Cluster startup and scheduling can be expensive, especially for small-to-medium workloads. Lightweight alternatives may outperform Spark in such scenarios.</li> </ul> <p>When to Use</p> <ul> <li>Massive-scale batch data processing: Best suited for terabyte-to-petabyte-scale jobs, especially if Hadoop/Spark infrastructure is already in place. Ideal for complex ETL, warehousing, and batch analytics.</li> <li>All-in-one platform requirements: Excellent when you need a unified stack offering SQL, ML, and streaming in one platform.</li> </ul> <p>A flexible Python parallel computing library that extends the PyData ecosystem (like Pandas/NumPy) to handle distributed or out-of-core computation.</p> <p>Pros</p> <ul> <li>Familiar interface: Easy to adopt for Python users, with APIs that closely resemble Pandas. You can scale up existing Pandas/NumPy code with minimal changes.</li> <li>Lightweight and flexible: Can run on a single machine and scale to a cluster. More suitable than Spark for medium-sized data workloads with lower deployment overhead.</li> <li>General-purpose parallel computing: Supports not just DataFrames but also Dask Array, Dask Delayed, and Dask Futures\u2014enabling parallelization of arbitrary Python code and numerical operations.</li> </ul> <p>Cons</p> <ul> <li>Limited scalability at extreme data volumes: For jobs over 1TB, memory management and scheduling can become difficult. Spark may outperform Dask in such scenarios. Also lacks a mature query optimizer for complex joins and aggregations.</li> <li>Partial API limitations: Dask's data structures are immutable and do not support some in-place Pandas operations (like <code>.loc</code> assignment). You may need to use <code>map_partitions</code> and other patterns. Some Pandas edge behaviors may also differ, requiring a learning curve.</li> </ul> <p>When to Use</p> <ul> <li>Data science in Python with medium-scale data: Ideal for datasets from a few GBs to several TBs, especially if you're already using Pandas/Numpy and want to speed up existing code with parallelism.</li> <li>Flexible and custom computation: Useful for local development that later scales to clusters. Suitable when the workload involves custom Python functions and numerical computations, rather than pure SQL. Also great for scientific computing tasks that need fault tolerance and visualized scheduling.</li> </ul> <p>A new-generation unified data engine offering both Python DataFrame and SQL interfaces. Built in Rust for high performance, with seamless scaling from local to petabyte-scale distributed execution.</p> <p>Pros</p> <ul> <li>High performance and unified interface: Supports both DataFrame-style operations and SQL queries. Built in Rust and based on Apache Arrow memory format for fast local processing. Scales to distributed clusters via integration with Ray. Benchmarks show significant performance gains over Spark and Dask.</li> <li>Supports complex data types: Designed for multimodal data like images, videos, and embeddings, with built-in functions and type support to simplify processing of unstructured data.</li> <li>Query optimization and cloud integration: Includes a built-in query optimizer (e.g., predicate pushdown, cost estimation). Integrates with data catalogs like Iceberg/Delta and cloud storage like S3, achieving fast I/O in cloud environments.</li> </ul> <p>Cons</p> <ul> <li>Immature ecosystem: Still in early (0.x) development stages. Community and ecosystem are limited compared to Spark or Dask. Lacks extensive tooling and production-grade stability.</li> <li>Requires external tools for distributed execution: Daft focuses on data processing itself. Distributed resource management is delegated to Ray, which adds extra setup and operational complexity.</li> <li>Incomplete feature set: Some advanced analytics features are still under development. API stability and error messaging could be improved compared to mature engines.</li> </ul> <p>When to Use</p> <ul> <li>High-performance Python workloads with complex data: Great for handling unstructured data like images, videos, and embeddings at scale, especially when you want to stay in the Python ecosystem.</li> <li>Seamless local-to-distributed scaling: Ideal for teams looking to prototype locally using DataFrames and easily scale to clusters/cloud later. A good alternative to Spark/Dask for scenarios needing both SQL and Python transformations in a performant pipeline.</li> </ul>"},{"location":"side-projects/data2ml-ops/ray/in-the-bigger-picture/#distributed-hpo","title":"Distributed HPO","text":"<p>(Generated by ChatGPT 4o Deep Research on 2025/05/20)</p> Ray TuneKatib <p>A Python library for experiment execution and hyperparameter tuning at any scale.</p> <p>Pros</p> <ul> <li>Wide algorithm support:Supports a wide range of state-of-the-art search algorithms (e.g. PBT, HyperBand/ASHA) and integrates with many optimization libraries like Optuna, HyperOpt, Ax, Nevergrad, etc. This allows users to leverage cutting-edge tuning methods easily.</li> <li>Scalable and distributed:Can parallelize and distribute trials across multiple CPUs/GPUs and even multiple nodes using Ray's cluster architecture, often without any code changes. This makes it simple to scale up experiments from a laptop to a distributed cluster.</li> <li>Easy integration:Provides a simple Python API that works seamlessly with popular ML frameworks (TensorFlow, PyTorch, scikit-learn, etc.). It also offers convenient tools (like TensorBoard logging) for monitoring experiments, lowering the learning curve for new users.</li> </ul> <p>Cons</p> <ul> <li>Kubernetes integration not native:Ray Tune is not built into Kubernetes; running it on K8s requires deploying a Ray cluster (e.g. via the KubeRay operator), which is extra overhead compared to a K8s-native solution.</li> <li>Python-centric:It's primarily a Python library, so each trial runs in a Python process. This makes it less language-agnostic than a tool like Katib \u2013 non-Python workloads are not as straightforward to tune with Ray Tune.</li> <li>Resource overhead: Using Ray introduces additional background processes and resource usage (for the Ray runtime). For very small-scale or simple hyperparameter searches, this added complexity can be overkill when a lightweight tool (like Optuna alone) might suffice.</li> </ul> <p>When to Use</p> <ul> <li>Use Ray Tune when you want to perform hyperparameter search inside a Python workflow (e.g. in a Jupyter notebook or script) and easily scale it from local execution to distributed runs without changing your code.</li> <li>It is a good choice if you don't have a Kubernetes infrastructure or prefer not to rely on one. Ray Tune can manage distributed training on its own (on VMs or a Ray cluster) and thus suits scenarios where setting up Kubeflow/Katib would be too heavyweight.</li> </ul> <p>An open-source, Kubernetes-native hyperparameter tuning system that is framework-agnostic and supports AutoML features like early stopping and NAS.</p> <p>Pros</p> <ul> <li>Kubernetes-native: Designed to run as part of a K8s cluster, Katib treats hyperparameter tuning jobs as Kubernetes workloads. This tight integration makes it a natural fit for cloud-native environments and enables multi-tenant usage in production (multiple users can share the service). It seamlessly works with Kubeflow, allowing tuning to plug into pipelines and Kubernetes resource management.</li> <li>Framework/language agnostic: Katib can tune any model training code in any language or framework, as long as it's containerized. It's not tied to a specific ML library \u2013 whether you use TensorFlow, PyTorch, R, or even a custom script, Katib can run it because it launches jobs in containers.</li> <li>Rich algorithm library: Offers diverse hyperparameter search strategies out-of-the-box: random search, grid search, Bayesian optimization (via Skopt), Hyperband/ASHA, CMA-ES, Tree of Parzen Estimators (TPE), Population Based Training, and more. It's integrated with libraries like Hyperopt/Optuna to provide state-of-the-art optimization methods. Katib even supports advanced AutoML techniques such as neural architecture search (NAS) and early stopping criteria, which few other open-source tools provide.</li> </ul> <p>Cons</p> <ul> <li>Kubernetes dependency: Katib requires a Kubernetes environment (typically as part of Kubeflow) to run. This means it's not usable in a simple local setup and has a higher operational overhead if you don't already have a K8s cluster.</li> <li>Higher learning curve: Configuring Katib experiments involves writing Kubernetes CRD (YAML) configurations for experiments, parameters, and metrics. Users must understand Kubernetes concepts and Katib's CRD schema, which can be less straightforward than using a Python library API. This complexity can slow down initial experimentation, especially for those new to K8s.</li> <li>Beta-stage project: As of now, Katib is in beta status. While it is used in production by some, certain features or integrations may not be as mature or well-documented as more established libraries. It largely relies on the Kubeflow ecosystem, so using it outside of Kubeflow/Kubernetes contexts might not be practical.</li> </ul> <p>When to Use</p> <ul> <li>Use Katib when you are working in a Kubernetes-based ML platform (especially with Kubeflow) and want a hyperparameter tuning service that integrates naturally with your cluster jobs and pipelines.</li> <li>It is the go-to choice if you need to tune heterogeneous workloads or non-Python applications. For example, if your training code is in R, Java, or any framework outside of Python, Katib's container-based approach allows you to optimize those just as easily. Similarly, if you have an existing training pipeline (e.g., a Kubeflow TFJob/PyTorchJob), Katib can hook into it to adjust hyperparameters.</li> <li>Choose Katib for large-scale or collaborative scenarios: it supports multi-user (multi-tenant) usage and distributed training jobs natively. If you require advanced AutoML features like NAS or want a centrally managed HPO service for your team or organization, Katib provides these capabilities in a cloud-native way.</li> </ul>"},{"location":"side-projects/data2ml-ops/ray/in-the-bigger-picture/#model-serving","title":"Model Serving","text":"<p>(Generated by ChatGPT 4o Deep Research on 2025/05/20)</p> Ray ServeBentoMLKServeSeldon Core <p>Ray Serve is an open-source, scalable model serving library built on Ray, allowing you to serve machine learning models (and arbitrary Python business logic) as high-performance, resilient web services. It's framework-agnostic and can run anywhere Ray can (from a laptop to a Kubernetes cluster) with minimal code changes.</p> <p>Pros</p> <ul> <li>Flexible model composition and Python-first workflow: You can deploy multiple models and custom business logic in a single pipeline, all in Python code, which makes it easy to develop and test locally before scaling up. Ray Serve's integration with FastAPI also simplifies turning your code into RESTful endpoints.</li> <li>Scalable and resource-efficient: Built on the Ray distributed computing framework, it natively scales across CPU/GPU nodes and even multiple machines. It supports fine-grained resource control (e.g. fractional GPUs and auto-batching), enabling high throughput serving and efficient utilization of hardware. Ray's autoscaler can dynamically add or remove replicas based on load.</li> <li>Kubernetes integration optional: Unlike purely Kubernetes-focused tools, Ray Serve doesn't require K8s \u2013 you can start on a local machine or VM and later use KubeRay (Ray's K8s operator) to deploy to a cluster without major code changes. This lowers the barrier to entry while still allowing cloud-native deployment when needed.</li> </ul> <p>Cons</p> <ul> <li>Not a full ML platform: Ray Serve focuses on serving and lacks built-in model lifecycle management features (no native model registry/versioning UI or advanced monitoring dashboards). You may need to implement or integrate external tools for model tracking, canary testing, or extensive metrics.</li> <li>Operational overhead of Ray: Using Ray Serve means running a Ray cluster, which introduces additional complexity and resource overhead. Deploying on Kubernetes, for example, requires operating the Ray runtime (via an operator or manual setup), which can be overkill if you only need simple single-model serving. </li> <li>Limited traffic-splitting out-of-the-box: There's no built-in canary or A/B traffic management API (unlike KServe or Seldon's declarative canary support). Achieving A/B tests would require custom logic in your application (e.g. routing requests between Ray Serve deployments manually).</li> </ul> <p>When to Use</p> <ul> <li>Dynamic or complex inference pipelines: Use Ray Serve when your application involves multiple models or steps that need to be composed and scaled together (for example, an ensemble or a workflow of preprocessing \u2192 model A \u2192 model B). Its ability to handle distributed workflows and call one deployment from another shines in these scenarios.</li> <li>Scaling Python services from prototyping to production: If you want to prototype a model service locally (using pure Python) and later seamlessly scale it to a cluster or cloud environment without rewriting for a new serving stack, Ray Serve is an excellent choice. It's ideal for teams already using Ray for other tasks (training, tuning) who want to reuse that infrastructure for serving.</li> <li>Resource-intensive models with custom logic: When you need fine-grained control over resource allocation (e.g. serving large models with fractional GPU sharing) or have custom Python logic alongside predictions, Ray Serve provides the flexibility and performance to meet those needs. It's well-suited if your use case doesn't fit the one-model-per-container paradigm of other frameworks.</li> </ul> <p>BentoML is an open-source platform that simplifies packaging and deploying ML models at scale. It provides a unified, framework-agnostic way to containerize models (into a self-contained \u201cBento\u201d bundle) and deploy them to various environments with minimal coding, bridging the gap between model development and production deployment.</p> <p>Pros</p> <ul> <li>Easy and developer-friendly: BentoML offers a simple Python API to package models and serve them, which means you can deploy a model with just a few lines of code. It requires minimal setup and no deep Kubernetes knowledge to get started \u2013 great for small teams or startups to quickly ship models as microservices. The development workflow is very \u201cpythonic\u201d and supports local testing (e.g., you can run a Bento service locally like any Flask app).</li> <li>Framework-agnostic &amp; portable: It supports a wide range of ML frameworks (TensorFlow, PyTorch, Scikit-learn, Hugging Face transformers, etc.) and can deploy to multiple targets \u2013 from Docker containers on your infrastructure to serverless platforms or Kubernetes via Bento's operator/Yatai service. This flexibility lets you use the same packaged model artifact across different environments, avoiding lock-in to a specific serving backend.</li> <li>Built-in tooling for deployments: BentoML automatically generates REST/gRPC API servers for your model and includes basic observability features. For example, it has integrated logging and monitoring hooks \u2013 you can get inference metrics and logs, and even integrate with Prometheus/Grafana for live monitoring. It also supports versioning your models and services, which makes it possible to do A/B tests or canary releases by deploying multiple versions of a model and routing traffic accordingly.</li> </ul> <p>Cons</p> <ul> <li>Not ideal for extreme scale: While BentoML can handle moderate production workloads, it isn't as optimized for massive scale or ultra-low latency scenarios as some specialized serving systems. Horizontal scaling (e.g., running many replicas) isn't managed by BentoML itself \u2013 you'd rely on Kubernetes or an external orchestrator to scale out, which adds extra work. For very high-throughput or large numbers of models, the overhead of each Bento service and the absence of built-in autoscaling means it might not be the most resource-efficient choice.</li> <li>Lacks native Kubernetes integration: Unlike KServe or Seldon, BentoML doesn't deploy via a Kubernetes CRD by default. You either use BentoML's CLI/Yatai to build a container and deploy it, or manually handle the Kubernetes deployment (e.g., create your own Kubernetes Deployment/Service for the Bento container). This means features like canary routing or auto-scaling must be set up through Kubernetes or other tools, not toggled via BentoML configs.</li> <li>Limited built-in MLOps features: BentoML focuses on the serving container and basic logging/metrics. It doesn't natively provide advanced monitoring dashboards, data drift detection, or experiment tracking \u2013 you'd integrate external tools for those needs. Similarly, governance features (like role-based access, model approval workflows, etc.) are not part of the open-source BentoML, though some are offered in BentoCloud. In short, it's a lightweight serving tool rather than a full-fledged enterprise ML platform.</li> </ul> <p>When to Use</p> <ul> <li>Rapid prototyping and small-to-medium deployments: Use BentoML when you want to go from a trained model to a deployed service quickly with minimal overhead \u2013 for instance, a small team that needs to frequently deploy models for different projects. It shines in early-stage production scenarios where simplicity and speed are favored over complex infrastructure.</li> <li>Polyglot model environments: If your use case involves various model types (different frameworks or libraries) and you need a consistent way to package and deploy all of them, BentoML is a good fit. It provides one standardized workflow to containerize models from any framework and deploy to your environment of choice.</li> <li>Customization and control in Python: When you require custom pre- or post-processing logic, or want to integrate business logic into your prediction service, BentoML allows you to write that logic in Python alongside model inference. This is useful in scenarios where alternative tools (which often auto-launch models in generic servers) don't easily support custom code. In BentoML, you have full control to define the request handling, which can be handy for experimental features or complex input/output handling.</li> </ul> <p>KServe (formerly KFServing) is a Kubernetes-native model serving platform that provides a Custom Resource Definition (CRD) called <code>InferenceService</code> to deploy ML models on Kubernetes in a standardized way. It focuses on \u201cserverless\u201d inference \u2013 automatically scaling model servers up and down (even to zero) based on traffic \u2013 and supports many popular ML frameworks out-of-the-box.</p> <p>Pros</p> <ul> <li>Deep Kubernetes integration: KServe defines an <code>InferenceService</code> CRD for model deployments, which makes it feel like a natural extension of Kubernetes. You can deploy models by simply specifying the model artifact (e.g., a URI to a saved model on cloud storage) and selecting an appropriate runtime (TensorFlow Serving, TorchServe, scikit-learn, XGBoost, etc.), without needing to write custom serving code. This declarative approach is convenient for teams already comfortable with <code>kubectl</code> and YAML.</li> <li>Serverless autoscaling (Knative): KServe leverages Knative Serving under the hood to handle scaling. It can automatically scale up replicas based on request load and scale down to zero when no traffic is present. This on-demand scaling is efficient for cost and resource usage, and it works for CPU and GPU workloads alike. Built-in autoscaling metrics and concurrency controls help handle spikes in traffic.</li> <li>Advanced deployment strategies: It supports canary deployments and traffic splitting natively. You can deploy a new version of a model and direct a percentage of traffic to it (for A/B testing or gradual rollouts) using simple CRD fields. KServe also introduced inference graphs, allowing you to chain models or have ensembles within a single <code>InferenceService</code> (though this is somewhat basic compared to Seldon's graphs). Additionally, KServe supports standard ML endpoint protocols (like KFServing V2 and even the OpenAI API schema for generative models) for interoperability.</li> </ul> <p>Cons</p> <ul> <li>Setup complexity: Installing and managing KServe can be complex. It typically requires deploying Knative and Istio (for networking) on your Kubernetes cluster, which means extra moving parts. This overhead implies you need solid Kubernetes/cloud-native expertise to operate it. Teams without existing K8s infrastructure might find it heavyweight.</li> <li>Limited flexibility for custom code: KServe excels at serving pretrained models with standard predictors, but if you need custom pre-processing, post-processing, or arbitrary Python logic, it's less straightforward. You often have to build a custom docker image implementing KServe's SDK interfaces for transformers or predictors. In comparison, tools like Seldon or BentoML (or writing your own service) may offer more flexibility for custom inference logic.</li> <li>Fewer built-in MLOps features: While KServe covers the basics of scaling and canarying, it doesn't inherently provide some advanced features like data drift detection, out-of-the-box monitoring dashboards, or automatic logging of requests/responses \u2013 those would rely on integrating with other tools (e.g., Prometheus for metrics, Kubeflow or custom pipelines for data logging). In areas like explainability or outlier detection, KServe is more bare-bones than Seldon Core's extensive feature set.</li> </ul> <p>When to Use</p> <ul> <li>Kubernetes-first organizations: Choose KServe if your team is already invested in Kubernetes (possibly also using Kubeflow) and you want a model serving solution that fits natively into that ecosystem. It's ideal when you prefer deploying models via Kubernetes manifests and want the control of K8s primitives (like using HPA, etc.) with the convenience of not writing a serving application from scratch.</li> <li>Multiple models, multiple frameworks: KServe is well-suited when you have models in different frameworks and want a unified way to serve them. Because it provides built-in support for many framework servers, you can hand off a TensorFlow model or a PyTorch model to KServe in a similar fashion. This makes it easier to standardize deployment in a heterogeneous ML environment.</li> <li>Auto-scaling API endpoints: If your use case demands elastic scaling (including scale-to-zero) to handle sporadic or bursty traffic patterns, and you want to pay for/use resources only on-demand, KServe's Knative-based design is a strong advantage. It also allows gradual rollouts of new model versions easily. In summary, use KServe for production scenarios where you need robust scaling and traffic management for your model APIs, and you're okay with the operational overhead of maintaining the KServe/Knative stack.</li> </ul> <p>Seldon Core is an open-source MLOps framework for deploying, managing, and monitoring machine learning models at scale on Kubernetes. It converts your models into production-ready microservices and offers an advanced set of deployment features (from experiments and ensembles to explainers and outlier detectors) to support complex production ML use cases.</p> <p>Pros</p> <ul> <li>Rich feature set for production ML: Seldon Core provides many advanced capabilities out-of-the-box: you can do A/B testing and canary rollouts of models, incorporate explainability (e.g., SHAP values) and outlier detection in your deployments, and even define inference graphs to chain multiple models or preprocessing steps. These features make it possible to implement complex deployment strategies (multi-armed bandits, shadow deployments, etc.) with the framework's support rather than building those from scratch.</li> <li>Scales to enterprise needs: Designed to handle deployments of potentially thousands of models, Seldon is built with scalability in mind . It uses a CRD (<code>SeldonDeployment</code>) to deploy models on Kubernetes, and can manage scaling via standard Kubernetes mechanisms (e.g., Horizontal Pod Autoscalers). It also supports numerous frameworks and languages (you can deploy models from TensorFlow, PyTorch, Scikit-learn, or even custom predictors in languages like Java) giving it versatility in complex organizations.</li> <li>Integrated with monitoring and governance tools: Seldon has native integrations for metrics and logging \u2013 it works well with Prometheus/Grafana for monitoring and can emit detailed metrics about predictions. It also leverages Istio for traffic routing and security policies in Kubernetes. Moreover, it can integrate with workflow orchestrators (Airflow, Argo, Kubeflow Pipelines) and includes audit and logging components, which is useful for governance and compliance in enterprise settings.</li> </ul> <p>Cons</p> <ul> <li>High complexity and setup overhead: Seldon Core's powerful features come at the cost of complexity. Setting up Seldon Core involves deploying its operators and (often) integrating with Istio for ingress, plus configuring all the CRD specs properly \u2013 this requires significant Kubernetes expertise. The learning curve is steep, and for small projects the resource overhead (multiple pods for controllers, sidecars for logging/metrics, etc.) can be heavy.</li> <li>Documentation and usability challenges: Some of Seldon's more advanced features (like complex inference graphs or custom routers) are not trivial to implement and the documentation/examples can be lacking. This means that while those features exist, leveraging them may require considerable experimentation and community support. In contrast, simpler frameworks might get you from zero to serving faster if your needs are basic.</li> <li>Potential overkill for simple use cases: If you just need to deploy one or two models with straightforward inference, Seldon can be over-engineered. The overhead of its components and the requirement to containerize your model to fit Seldon's runtime might slow you down unnecessarily. In such cases, a lighter-weight solution (like BentoML or a simple Flask app) could be more appropriate. Seldon really shines in more demanding scenarios, not one-off model deploys.</li> </ul> <p>When to Use</p> <ul> <li>Enterprise-scale deployments with advanced requirements: Seldon Core is an excellent choice when you are dealing with large-scale ML systems in production \u2013 for example, a situation where you have many models or microservices and you need sophisticated routing (A/B tests, canaries) and monitoring on all of them. If reliability, traceability, and robust governance are top priorities (as in regulated industries or mission-critical ML), Seldon's comprehensive features are very valuable.</li> <li>Need for out-of-the-box MLOps features: If your use cases demand things like real-time explainability of model decisions, automatic outlier or drift detection, or complex inference pipelines (ensembles, cascades of models) and you prefer having these capabilities ready-made, Seldon Core provides them built-in. It's a suitable framework when you want to avoid implementing these features from scratch and are willing to invest time in mastering Seldon's framework.</li> <li>Kubernetes-heavy environments with expert DevOps: Use Seldon when you have a strong DevOps/MLOps team familiar with Kubernetes who can maintain the infrastructure. In scenarios where the ML platform is a first-class part of the engineering organization, Seldon Core gives a lot of control and can be tuned to very specific needs. It's often used in tandem with Kubeflow or other K8s tools in companies building out a full ML platform. If your team has the bandwidth to handle a more complex system for the sake of advanced functionality, Seldon is a top contender.</li> </ul> <ol> <li> <p>Ray \u21a9\u21a9</p> </li> <li> <p>Anyscale, Inc. \u21a9</p> </li> <li> <p>Ray | GitHub \u21a9</p> </li> <li> <p>Ray | Technology Radar | Thoughtworks \u21a9</p> </li> <li> <p>Join Ray Slack | Ray \u21a9</p> </li> <li> <p>Ray Summit 2024 \u21a9</p> </li> <li> <p>Comparing Ray Data to other systems | Ray Docs \u21a9</p> </li> <li> <p>Spark, Dask, and Ray: choosing the right framework \u21a9</p> </li> <li> <p>Offline Batch Inference: Comparing Ray, Apache Spark, and SageMaker \u21a9</p> </li> </ol>"},{"location":"side-projects/data2ml-ops/ray/ray-job/","title":"Submit a Ray Tune Job to Your Ray Cluster","text":"<p>In this guide, we'll walk through how to submit the Ray Tune script we created in the previous article to a Ray cluster. There are three main methods:</p> <ol> <li>Using the Ray Jobs API<sup>5</sup>, either<ul> <li>Ray Jobs CLI<sup>7</sup><sup>8</sup></li> <li>Ray Jobs Python SDK<sup>9</sup><sup>10</sup></li> </ul> </li> <li>Defining a <code>RayJob</code> custom resource and submitting it through the Kubernetes Operator.<sup>6</sup></li> <li>Executing the job interactively from the head node (not recommended).</li> </ol> <p>Each method has its ideal use case:</p> <p>When to Use Ray Jobs API</p> <ul> <li>Local Development and Testing: Perfect for quick iterations or debugging directly on a local or remote Ray cluster without dealing with Kubernetes complexity.</li> <li>Ad-Hoc or Short-Lived Jobs: Ideal for submitting one-off tasks via the API to an existing Ray cluster.</li> </ul> <p>When to Use <code>RayJob</code> on Kubernetes</p> <ul> <li>Automated Cluster Lifecycle Management: Ideal when you want Kubernetes to automatically spin up and tear down Ray clusters for each job.</li> <li>Leverage Kubernetes-Native Features: Useful when integrating with scheduling policies, resource quotas, monitoring tools, or other native Kubernetes features.</li> </ul> <p>In this guide, we'll use Ray Jobs CLI for submitting our Ray Tune job to the cluster.</p>"},{"location":"side-projects/data2ml-ops/ray/ray-job/#port-forward","title":"Port Forward","text":"<p>Since our Ray Cluster wasn't exposed via a LoadBalancer or NodePort, we'll use port forwarding to access the Ray Dashboard (which runs on port <code>8265</code> by default):</p> <pre><code>kubectl get service raycluster-kuberay-head-svc -n kuberay\n</code></pre> <pre><code>NAME                          TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                                         AGE\nraycluster-kuberay-head-svc   ClusterIP   None         &lt;none&gt;        10001/TCP,8265/TCP,6379/TCP,8080/TCP,8000/TCP   13m\n</code></pre> <pre><code>kubectl port-forward service/raycluster-kuberay-head-svc 8265:8265 -n kuberay &gt; /dev/null &amp;\n</code></pre> <pre><code>[1] 56915\n</code></pre> <p>We can now access the Ray Cluster dashboard at <code>http://127.0.0.1:8265</code>.</p> <p></p>"},{"location":"side-projects/data2ml-ops/ray/ray-job/#prepare-runtime-environment","title":"Prepare Runtime Environment","text":"<p>Before launching the Ray Tune job, we need to make sure that each trial's worker process has the correct runtime environment\u2014such as the working directory and required dependencies. We'll define this using a <code>runtime-env.yaml</code> and a <code>requirements.txt</code>, and pass them along during job submission.</p> runtime-env.yaml<pre><code>working_dir: .\npip: requirements.txt\n</code></pre> requirements.txt<pre><code>ray[tune,data,client]==2.46.0\nmlflow==2.22.0\noptuna==4.3.0\nxgboost==3.0.0\nfeast[gcp,redis]==0.48.0\npsycopg[binary]==3.2.7\nimbalanced-learn==0.13.0\nprotobuf==5.29.4\n</code></pre> <p>The runtime environment can be specififed via<sup>1</sup></p> <ul> <li><code>ray job submit --runtime-env=...</code>: the runtime environments are applied to both the driver process (entrypoint script) and the worker processes (all the tasks and actors created from the drive process)</li> <li><code>ray.init(rutime_env=...)</code> : the runtime environments are applied to the workers processes (all the tasks and actors), but not the driver process (entrypoint script).</li> </ul> <p>The runtime environment can include one or more fields below:<sup>2</sup><sup>3</sup></p> <ul> <li><code>working_dir</code></li> <li><code>py_modules</code></li> <li><code>py_executable</code></li> <li><code>excludes</code></li> <li><code>pip</code></li> <li><code>uv</code></li> <li><code>conda</code></li> <li><code>env_vars</code></li> <li><code>image_uri</code></li> <li><code>config</code></li> </ul> <p>Runtime environments and Docker can work hand in hand or independently, depending on your needs. For example, you might rely on a container image in the Cluster Launcher to manage large or static dependencies, while runtime environments are better suited for dynamic, job-specific configurations. When combined, the runtime environment extends the container image, inheriting its packages, files, and environment variables to provide a seamless and flexible setup.<sup>4</sup></p>"},{"location":"side-projects/data2ml-ops/ray/ray-job/#submit-a-ray-job-to-the-ray-cluster","title":"Submit a Ray Job to the Ray Cluster","text":"<p>With the dashboard accessible and runtime environment prepared, you can now submit the Ray Tune job to the cluster:</p> ray-job-submit-command.txt<pre><code>ray job submit \\\n  --address http://localhost:8265 \\\n  --runtime-env runtime-env.yaml \\\n  -- python training.py\n</code></pre> <pre><code>Job submission server address: http://localhost:8265\n2025-05-16 02:19:52,245 INFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_927761c1b60fb91e.zip.\n2025-05-16 02:19:52,245 INFO packaging.py:576 -- Creating a file package for local module '.'.\n\n-------------------------------------------------------\nJob 'raysubmit_8uVJaNE7m2cvM9uZ' submitted successfully\n-------------------------------------------------------\n\nNext steps\n  Query the logs of the job:\n    ray job logs raysubmit_8uVJaNE7m2cvM9uZ\n  Query the status of the job:\n    ray job status raysubmit_8uVJaNE7m2cvM9uZ\n  Request the job to be stopped:\n    ray job stop raysubmit_8uVJaNE7m2cvM9uZ\n\nTailing logs until the job exits (disable with --no-wait):\n2025-05-15 11:19:52,389 INFO job_manager.py:531 -- Runtime env is setting up.\n[I 2025-05-15 11:21:54,810] A new study created in memory with name: optuna\n2025-05-15 11:21:56,460 INFO worker.py:1554 -- Using address 10.244.0.99:6379 set in the environment variable RAY_ADDRESS\n2025-05-15 11:21:56,465 INFO worker.py:1694 -- Connecting to existing Ray cluster at address: 10.244.0.99:6379...\n2025-05-15 11:21:56,485 INFO worker.py:1888 -- Connected to Ray cluster.\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Configuration for experiment     fraud_detection   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Search algorithm                 SearchGenerator   \u2502\n\u2502 Scheduler                        FIFOScheduler     \u2502\n\u2502 Number of trials                 100               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\nView detailed results here: ray/fraud_detection\nTo visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2025-05-15_11-18-04_381196_1/artifacts/2025-05-15_11-22-03/fraud_detection/driver_artifacts`\n\n...\n...\n...\n...\n...\n\n------------------------------------------\nJob 'raysubmit_8uVJaNE7m2cvM9uZ' succeeded\n------------------------------------------\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/ray-job/#see-results","title":"See Results","text":"<p>Once the job finishes, you can verify the results in the Ray Dashboard:</p> <pre><code>http://127.0.0.1:8265\n</code></pre> <p></p> <p>You can also check the experiment in the MLflow UI. The parent-child run setup should be correctly reflected:</p> <p></p> <p>The hyperparameter tuning job completed successfully and produced a versioned MLflow model:</p> <p></p> <p>Since our MLflow instance is integrated with MinIO, the trained model is also available there. You can now use this model for deployment:</p> <p></p> <ol> <li> <p>Runtime Environment Specified by Both Job and Driver \u21a9</p> </li> <li> <p>Runtime Environment API Reference \u21a9</p> </li> <li> <p><code>ray.runtime_env.RuntimeEnv</code> \u21a9</p> </li> <li> <p>What is the relationship between runtime environments and Docker? \u21a9</p> </li> <li> <p>Ray Jobs API \u21a9</p> </li> <li> <p>RayJob Quickstart \u21a9</p> </li> <li> <p>Quickstart using the Ray Jobs CLI \u21a9</p> </li> <li> <p>Ray Jobs CLI API Reference \u21a9</p> </li> <li> <p>Ray Jobs Python SDK \u21a9</p> </li> <li> <p>Ray Jobs Python SDK API Reference \u21a9</p> </li> </ol>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/","title":"Integrate Ray Tune with Optuna, Imblearn, MLflow and MinIO","text":"<p>This guide walks you through how to use Ray Tune for hyperparameter tuning in a fraud detection model. The workflow includes:</p> <ol> <li>Loading training data from MinIO</li> <li>Defining a search space with Optuna, using over-sampling and down-sampling techniques like <code>AllKNN</code> and <code>SMOTE</code> specifically in <code>imblearn</code> packages to handle class imbalance.</li> <li>Training an XGBoost binary classifier with boosters like <code>gbtree</code>, <code>gblinear</code>, and <code>dart</code>, and tuning hyperparameters such as lambda, alpha, and eta.</li> <li>Logging metrics including accuracy, precision, recall, F1, and ROC AUC to both Ray Tune and MLflow.</li> <li>Manually configuring MLflow to support parent-child runs, instead of using the default <code>MLflowLoggerCallback</code> and <code>setup_mlflow</code></li> <li>Retraining and saving the best model with the optimal hyperparameters after tuning.</li> </ol> <p>Here is a full training transcipt.</p> Full Training Script training.py<pre><code># Import packages\nimport time\nfrom datetime import datetime, timedelta\nfrom pprint import pprint\nfrom typing import Any, Dict, Optional\n\nimport pandas as pd\nimport pyarrow.fs\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.under_sampling import AllKNN\nfrom sklearn.metrics import (accuracy_score, average_precision_score, f1_score,\n                             log_loss, precision_score, recall_score,\n                             roc_auc_score)\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\n\nimport mlflow\nimport ray\nfrom feast import FeatureStore\nfrom ray import tune\nfrom ray.tune.search import ConcurrencyLimiter\nfrom ray.tune.search.optuna import OptunaSearch\n\n\nfs = pyarrow.fs.S3FileSystem(\n    access_key=\"minio_user\",\n    secret_key=\"minio_password\",\n    scheme=\"http\",\n    endpoint_override=\"minio-api.minio.svc.cluster.local:9000\"\n)\n\n# Get Training Data\nwith fs.open_input_file(\"ray/training_data.csv\") as f:\n    data = pd.read_csv(f)\n\n# Alternative 1: Ray Data\n# ds = ray.data.read_csv(\n#     \"s3://ray/training_data.csv\",\n#     filesystem=fs\n# )\n\n# Alternative 2: Feast\n# now = datetime.now()\n# two_days_ago = datetime.now() - timedelta(days=2)\n# store = FeatureStore('.')\n# fs_fraud_detection_v1 = store.get_feature_service('fraud_detection_v1')\n# data = store.get_historical_features(\n#     entity_df=f\"\"\"\n#     select \n#         src_account as entity_id,\n#         timestamp as event_timestamp,\n#         is_fraud\n#     from\n#         feast-oss.fraud_tutorial.transactions\n#     where\n#         timestamp between timestamp('{two_days_ago.isoformat()}') \n#         and timestamp('{now.isoformat()}')\"\"\",\n#     features=fs_fraud_detection_v1,\n#     full_feature_names=False\n# ).to_df()\n\n# Configure Ray Tune\ndef space(trial) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Define-by-run function to construct a conditional search space.\n    Ensure no actual computation takes place here.\n\n    Args:\n        trial: Optuna Trial object\n\n    Returns:\n        Dict containing constant parameters or None\n    \"\"\"\n    # Resampler\n    resampler = trial.suggest_categorical(\"resampler\", [\"allknn\", \"smote\", \"passthrough\"])\n\n    # Booster\n    booster = trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"])\n    lmbd = trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True)\n    alpha = trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True)\n    if booster in [\"gbtree\", \"dart\"]:\n        max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n        eta = trial.suggest_float(\"eta\", 1e-3, 0.3, log=True)\n        gamma = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n        grow_policy = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n    # Constants\n    return {\n        \"objective\": \"binary:logistic\",\n        \"random_state\": 1025\n    }\n\ndef training_function(\n    config, data,\n    run_id, mlflow_tracking_uri, experiment_name\n):\n    # Set up mlflow \n    mlflow.set_tracking_uri(mlflow_tracking_uri)\n    mlflow.set_experiment(experiment_name=experiment_name)\n\n    # Split data\n    X = data[[\n        'has_fraud_7d',\n        'num_transactions_7d',\n        'credit_score',\n        'account_age_days',\n        'has_2fa_installed'\n    ]]\n    y = data['is_fraud']\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, stratify=y,\n        random_state=config[\"random_state\"]\n    )\n\n    # Define the resampler\n    if config[\"resampler\"] == \"allknn\":\n        resampler = AllKNN()\n    elif config[\"resampler\"] == \"smote\":\n        resampler = SMOTE()\n    else:\n        resampler = \"passthrough\"\n\n    # Define the classifier\n    new_config = {k: v for k, v in config.items() if k != \"resampler\"}\n    classifier = XGBClassifier(**new_config)\n\n    # Combine the resampler and classifier together\n    model = Pipeline(steps=[\n        (\"resampler\", resampler),\n        (\"classifier\", classifier)\n    ])\n\n    # Train the model\n    with mlflow.start_run(run_id=run_id):\n        with mlflow.start_run(\n            run_name=f\"{config['resampler']}-{config['booster']}-{config['lambda']:.2f}-{config['alpha']:.2f}\",\n            nested=True\n        ):\n            model.fit(X_train, y_train)\n            # Evaluate the model\n            y_prob = model.predict_proba(X_test)\n            y_prob = y_prob[:, 1]\n            y_pred = (y_prob &gt; 0.5).astype(int)\n            metrics = {\n                \"accuracy\": accuracy_score(y_test, y_pred),\n                \"precision\": precision_score(y_test, y_pred),\n                \"recall\": recall_score(y_test, y_pred),\n                \"f1\": f1_score(y_test, y_pred),\n                \"roc_auc\": roc_auc_score(y_test, y_prob),\n                \"log_loss\": log_loss(y_test, y_prob),\n                \"average_precision\": average_precision_score(y_test, y_prob)\n            }\n            # Log the metrics and hyperparameters\n            mlflow.log_params(config)\n            mlflow.log_metrics(metrics)\n            tune.report(metrics)\n\nsearch_alg = OptunaSearch(space=space, metric=\"f1\", mode=\"max\")\nsearch_alg = ConcurrencyLimiter(search_alg, max_concurrent=4)\ntune_config = tune.TuneConfig(\n    search_alg=search_alg,\n    num_samples=100,\n)\n\nEXPERIMENT_NAME = 'fraud_detection'\nRUN_NAME = 'first'\nTRACKING_URI = \"http://tracking-server.mlflow.svc.cluster.local:5000\"\nmlflow.set_tracking_uri(TRACKING_URI)\nif mlflow.get_experiment_by_name(EXPERIMENT_NAME) == None:\n    mlflow.create_experiment(EXPERIMENT_NAME)\nmlflow.set_experiment(EXPERIMENT_NAME)\nrun_config = tune.RunConfig(\n    name=EXPERIMENT_NAME,\n    storage_path=\"ray/\",\n    storage_filesystem=fs\n)\n\n# Run Ray Tune\nray.init()\nwith mlflow.start_run(run_name=RUN_NAME, nested=True) as run:\n    tuner = tune.Tuner(\n        tune.with_parameters(\n            training_function,\n            data=data,\n            run_id=run.info.run_id,\n            mlflow_tracking_uri=TRACKING_URI,\n            experiment_name=EXPERIMENT_NAME\n        ),\n        tune_config=tune_config,\n        run_config=run_config\n    )\n    results = tuner.fit()\n\n    # Retrain the model with the hyperparameters with best result\n    config = results.get_best_result(metric='f1', mode='max').config\n\n    # 1. Split data\n    X = data[[\n        'has_fraud_7d',\n        'num_transactions_7d',\n        'credit_score',\n        'account_age_days',\n        'has_2fa_installed'\n    ]]\n    y = data['is_fraud']\n\n    # 2. Define resampler\n    if config[\"resampler\"] == \"allknn\":\n        resampler = AllKNN()\n    elif config[\"resampler\"] == \"smote\":\n        resampler = SMOTE()\n    else:\n        resampler = \"passthrough\"\n\n    # 3. Define classifier\n    new_config = {k: v for k, v in config.items() if k != \"resampler\"}\n    classifier = XGBClassifier(**new_config)\n\n    # 4. Combine the resampler and classifier together\n    model = Pipeline(steps=[\n        (\"resampler\", resampler),\n        (\"classifier\", classifier)\n    ])\n\n    # 5. Train and evaluate the model\n    model.fit(X, y)\n    y_prob = model.predict_proba(X)\n    y_prob = y_prob[:, 1]\n    y_pred = (y_prob &gt; 0.5).astype(int)\n    metrics = {\n        \"accuracy\": accuracy_score(y, y_pred),\n        \"precision\": precision_score(y, y_pred),\n        \"recall\": recall_score(y, y_pred),\n        \"f1\": f1_score(y, y_pred),\n        \"roc_auc\": roc_auc_score(y, y_prob),\n        \"log_loss\": log_loss(y, y_prob),\n        \"average_precision\": average_precision_score(y, y_prob)\n    }\n\n    # 6. Log the hyperparameters, metrics and the model\n    mlflow.log_params(config)\n    mlflow.sklearn.log_model(\n        sk_model=model,\n        artifact_path=\"model\",\n        input_example=X.iloc[[0]],\n        metadata={\"version\": f\"{EXPERIMENT_NAME}_v1\"}\n    )\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#import-packages","title":"Import Packages","text":"<p>First, let\u2019s import the required packages.</p> training.py<pre><code># Import packages\nimport time\nfrom datetime import datetime, timedelta\nfrom pprint import pprint\nfrom typing import Any, Dict, Optional\n\nimport pandas as pd\nimport pyarrow.fs\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.under_sampling import AllKNN\nfrom sklearn.metrics import (accuracy_score, average_precision_score, f1_score,\n                             log_loss, precision_score, recall_score,\n                             roc_auc_score)\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\n\nimport mlflow\nimport ray\nfrom feast import FeatureStore\nfrom ray import tune\nfrom ray.tune.search import ConcurrencyLimiter\nfrom ray.tune.search.optuna import OptunaSearch\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#minio-integration","title":"MinIO Integration","text":"<p>We\u2019ll use <code>pyarrow.fs.S3FileSystem</code> to interact with MinIO deployed on Kubernetes. There are two main tasks here.</p> <ol> <li>Load training data stored in MinIO.</li> <li>Save Ray Tune metadata (like checkpoints and logs) back to MinIO during the tuning process.</li> </ol> <p>Here's how we configure the connection to MinIO using <code>S3FileSystem</code>, including the access key, secret key, and endpoint.</p> training.py<pre><code>fs = pyarrow.fs.S3FileSystem(\n    access_key=\"minio_user\",\n    secret_key=\"minio_password\",\n    scheme=\"http\",\n    endpoint_override=\"minio-api.minio.svc.cluster.local:9000\"\n)\n</code></pre> <p>These values should match what you specified when deploying MinIO on Kubernetes. For more details, refer to the configuration section below or revisit this article.</p> Info minio.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio\n  namespace: minio\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n        - name: minio\n          image: minio/minio:RELEASE.2025-04-22T22-12-26Z\n          args:\n            - server\n            - /data\n            - --console-address\n            - :9001\n          env:\n            - name: MINIO_ROOT_USER\n              value: minio_user\n            - name: MINIO_ROOT_PASSWORD\n              value: minio_password\n          ports:\n            - containerPort: 9000\n              protocol: TCP\n            - containerPort: 9001\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /minio/health/live\n              port: 9000\n            initialDelaySeconds: 30\n            periodSeconds: 20\n            timeoutSeconds: 15\n            failureThreshold: 6\n          readinessProbe:\n            httpGet:\n              path: /minio/health/ready\n              port: 9000\n            initialDelaySeconds: 15\n            periodSeconds: 10\n            timeoutSeconds: 10\n            failureThreshold: 3\n          volumeMounts:\n            - name: storage\n              mountPath: /data\n      volumes:\n        - name: storage\n          hostPath:\n            path: /home/docker/data/minio\n            type: DirectoryOrCreate\n      restartPolicy: Always\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: minio-console\n  namespace: minio\nspec:\n  selector:\n    app: minio\n  type: NodePort\n  ports:\n    - name: console\n      port: 9001\n      targetPort: 9001\n      nodePort: 30901\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: minio-api\n  namespace: minio\nspec:\n  selector:\n    app: minio\n  type: ClusterIP\n  ports:\n    - name: api\n      port: 9000\n      targetPort: 9000\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: minio-create-bucket\n  namespace: minio\nspec:\n  backoffLimit: 6\n  completions: 1\n  template:\n    metadata:\n      labels:\n        job: minio-create-bucket\n    spec:\n      initContainers:\n        - name: wait-for-minio\n          image: busybox\n          command:\n            - sh\n            - -c\n            - |\n              until nc -z minio-api.minio.svc.cluster.local 9000; do\n                echo \"Waiting for MinIO...\"\n                sleep 2\n              done\n              echo \"MinIO is ready!\"\n      containers:\n        - name: minio-create-buckets\n          image: minio/mc\n          command:\n            - sh\n            - -c\n            - |\n              mc alias set minio http://minio-api.minio.svc.cluster.local:9000 minio_user minio_password &amp;&amp;\n              for bucket in mlflow dbt sqlmesh ray; do\n                if ! mc ls minio/$bucket &gt;/dev/null 2&gt;&amp;1; then\n                  echo \"Creating bucket: $bucket\"\n                  mc mb minio/$bucket\n                  echo \"Bucket created: $bucket\"\n                else\n                  echo \"Bucket already exists: $bucket\"\n                fi\n              done\n      restartPolicy: OnFailure\n      terminationGracePeriodSeconds: 30\n</code></pre> <p>For other custom storage configuration, see here<sup>1</sup> for more.</p>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#get-the-training-data","title":"Get the Training Data","text":"<p>Since this is a demo project with a small dataset that fits into memory, we\u2019ll use Pandas to read the CSV file directly through the configured filesystem.</p> <p>If the dataset were larger or didn't fit in memory, we would use Ray Data instead. In the future, this could also integrate with Feast Offline Feature Server<sup>2</sup> for more advanced feature management.</p> training.py<pre><code># Get Training Data\nwith fs.open_input_file(\"ray/training_data.csv\") as f:\n    data = pd.read_csv(f)\n\n# Alternative 1: Ray Data\n# ds = ray.data.read_csv(\n#     \"s3://ray/training_data.csv\",\n#     filesystem=fs\n# )\n\n# Alternative 2: Feast\n# now = datetime.now()\n# two_days_ago = datetime.now() - timedelta(days=2)\n# store = FeatureStore('.')\n# fs_fraud_detection_v1 = store.get_feature_service('fraud_detection_v1')\n# data = store.get_historical_features(\n#     entity_df=f\"\"\"\n#     select \n#         src_account as entity_id,\n#         timestamp as event_timestamp,\n#         is_fraud\n#     from\n#         feast-oss.fraud_tutorial.transactions\n#     where\n#         timestamp between timestamp('{two_days_ago.isoformat()}') \n#         and timestamp('{now.isoformat()}')\"\"\",\n#     features=fs_fraud_detection_v1,\n#     full_feature_names=False\n# ).to_df()\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#define-the-runconfig","title":"Define the RunConfig","text":"<p>Next, we configure where Ray Tune stores its metadata by setting the <code>storage_path</code> and <code>storage_filesystem</code> fields in <code>tune.RunConfig()</code>.</p> training.py<pre><code>run_config = tune.RunConfig(\n    name=EXPERIMENT_NAME,\n    storage_path=\"ray/\",\n    storage_filesystem=fs\n)\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#optuna-integration-tuneconfig","title":"Optuna Integration (TuneConfig)","text":"<p>Ray Tune supports <code>OptunaSearch</code><sup>3</sup>, which we\u2019ll use to define the hyperparameter search strategy. A common way to define the search space is by passing a dictionary directly via the <code>param_space</code> argument in <code>tune.Tuner()</code>.</p> <pre><code>tuner = tune.Tuner(\n    objective,\n    tune_config=tune.TuneConfig(\n        metric=\"mean_loss\",\n        mode=\"min\",\n        search_alg=OptunaSearch(),\n        num_samples=1000,\n    ),\n    param_space={\n        \"steps\": 100,\n        \"width\": tune.uniform(0, 20),\n        \"height\": tune.uniform(-100, 100),\n        \"activation\": tune.choice([\"relu\", \"tanh\"]),        \n    },\n)\nresults = tuner.fit()\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#define-the-search-space","title":"Define the Search Space","text":"<p>Sometimes, we want a more flexible search space\u2014especially one with conditional logic. In such cases, we can pass a define-by-run function to <code>OptunaSearch()</code>, which dynamically defines the search space at runtime.<sup>3</sup></p> <p>This function, typically called <code>space</code>, takes a <code>trial</code> object as input. We use <code>trial.suggest_*()</code> methods from Optuna, along with conditionals and loops, to construct the space.<sup>4</sup></p> <p>This setup is helpful for handling more complex scenarios\u2014such as including or excluding hyperparameters based on earlier choices.</p> training.py<pre><code>def space(trial) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Define-by-run function to construct a conditional search space.\n    Ensure no actual computation takes place here.\n\n    Args:\n        trial: Optuna Trial object\n\n    Returns:\n        Dict containing constant parameters or None\n    \"\"\"\n    # Resampler\n    resampler = trial.suggest_categorical(\"resampler\", [\"allknn\", \"smote\", \"passthrough\"])\n\n    # Booster\n    booster = trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"])\n    lmbd = trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True)\n    alpha = trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True)\n    if booster in [\"gbtree\", \"dart\"]:\n        max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n        eta = trial.suggest_float(\"eta\", 1e-3, 0.3, log=True)\n        gamma = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n        grow_policy = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n    # Constants\n    return {\n        \"objective\": \"binary:logistic\",\n        \"random_state\": 1025\n    }\n</code></pre> <p>This <code>space()</code> function defines a conditional hyperparameter search space using Optuna's define-by-run API. Instead of declaring all parameters upfront, the search space is built dynamically as the <code>trial</code> runs. The function suggests different values for categorical and numerical hyperparameters, such as the <code>resampler</code> method (<code>allknn</code><sup>5</sup>, <code>smote</code><sup>6</sup>, or <code>passthrough</code>)<sup>5</sup> and the <code>booster</code> type (<code>gbtree</code>, <code>gblinear</code>, or <code>dart</code>). Based on the chosen booster, additional parameters like <code>max_depth</code>, <code>eta</code>, and <code>grow_policy</code> are conditionally added.</p> <p>Importantly, no actual model training or heavy computation is done inside this function\u2014it only defines the search space structure.<sup>3</sup> The function returns a dictionary of constant parameters (like the learning <code>objective</code> and <code>random_state</code>) to be merged later with the sampled hyperparameters. This design keeps the search logic modular and clean, separating the definition of search space from the training logic.</p>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#define-the-search-algorithm","title":"Define the Search Algorithm","text":"<p>Now we configure the search algorithm using Optuna. We pass our <code>space()</code> function into <code>OptunaSearch</code>, specifying that we want to maximize the F1 score. To avoid exhausting system resources, we wrap it in a <code>ConcurrencyLimiter</code> that restricts parallel trials to 4. Finally, the <code>TuneConfig</code> object ties everything together, specifying the search algorithm and the total number of trials (<code>num_samples=100</code>) to explore.</p> training.py<pre><code>search_alg = OptunaSearch(space=space, metric=\"f1\", mode=\"max\")\nsearch_alg = ConcurrencyLimiter(search_alg, max_concurrent=4)\ntune_config = tune.TuneConfig(\n    search_alg=search_alg,\n    num_samples=100,\n)\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#mlflow-integration","title":"MLflow Integration","text":"<p>Ray offers built-in integration with MLflow through <code>MLflowLoggerCallback</code><sup>7</sup> and <code>setup_mlflow</code><sup>7</sup>. These are convenient options, but they don't support parent-child runs<sup>8</sup>, which are essential for organizing experiments hierarchically. I've tried Databricks approach<sup>9</sup> for setting up parent-child runs but it didn't work.</p> <p>Thankfully, it's not difficult to manually integrate MLflow. So instead of using the built-in methods, we manually set up MLflow tracking inside the script. This integration spans multiple parts of the pipeline:</p> <ol> <li>Set up the tracking URI and experiment in the driver process.</li> <li>Start a parent run in the driver.</li> <li>Set up and log to MLflow from within each trial (i.e., in the worker process).</li> <li>After all trials finish, retrain the best model and log it under the parent run.</li> </ol>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#set-up-the-tracking-uri-and-the-experiment-in-the-driver-process","title":"Set up the Tracking URI and the Experiment in the Driver Process","text":"<p>We begin by setting the experiment name, the run name for this tuning session, and the address of the MLflow tracking server running inside the Kubernetes cluster.</p> training.py<pre><code>EXPERIMENT_NAME = 'fraud_detection'\nRUN_NAME = 'first'\nTRACKING_URI = \"http://tracking-server.mlflow.svc.cluster.local:5000\"\n</code></pre> training.py<pre><code>mlflow.set_tracking_uri(TRACKING_URI)\nif mlflow.get_experiment_by_name(EXPERIMENT_NAME) == None:\n    mlflow.create_experiment(EXPERIMENT_NAME)\nmlflow.set_experiment(EXPERIMENT_NAME)\n</code></pre> <p>These values should match what you specified when deploying MLflow on Kubernetes. For more details, refer to the configuration section below or revisit this article.</p> Info tracking-server.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: {{ .Values.trackingServer.name }}\n  namespace: {{ .Release.Namespace }}\nspec:\n  type: NodePort\n  selector:\n    app: {{ .Values.trackingServer.name }}\n  ports:\n    - port: {{ .Values.trackingServer.port }}\n      targetPort: {{ .Values.trackingServer.port }}\n      nodePort: 30500\n</code></pre> values.yaml<pre><code>trackingServer:\n  name: tracking-server\n  host: 0.0.0.0\n  port: 5000\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#start-the-parent-run-in-the-driver-process","title":"Start the Parent Run in the Driver Process","text":"<p>When we launch <code>Tuner.fit()</code>, we also start an MLflow parent run inside the Ray driver process. Since each trial runs in a worker process, it won\u2019t automatically inherit the MLflow context. So we need to explicitly pass the MLflow tracking URI, experiment name, and parent run ID into each worker so they can log their results correctly under the parent run.</p> training.py<pre><code># Run Ray Tune\nray.init()\nwith mlflow.start_run(run_name=RUN_NAME, nested=True) as run:\n    tuner = tune.Tuner(\n        tune.with_parameters(\n            training_function,\n            data=data,\n            run_id=run.info.run_id,\n            mlflow_tracking_uri=TRACKING_URI,\n            experiment_name=EXPERIMENT_NAME\n        ),\n        tune_config=tune_config,\n        run_config=run_config\n    )\n    results = tuner.fit()\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#integrate-with-mlflow-in-the-worker-process","title":"Integrate with MLflow in the Worker Process","text":"<p>Each trial starts by configuring MLflow to point to the correct tracking server and parent run. Inside the trial, we begin a nested (child) run under the parent run. After training, we log hyperparameters and evaluation metrics, which will be associated with this specific trial.</p> training.py<pre><code>def training_function(\n    config, data,\n    run_id, mlflow_tracking_uri, experiment_name\n):\n    # Set up mlflow \n    mlflow.set_tracking_uri(mlflow_tracking_uri)\n    mlflow.set_experiment(experiment_name=experiment_name)\n\n    # Split data\n    X = data[[\n        'has_fraud_7d',\n        'num_transactions_7d',\n        'credit_score',\n        'account_age_days',\n        'has_2fa_installed'\n    ]]\n    y = data['is_fraud']\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, stratify=y,\n        random_state=config[\"random_state\"]\n    )\n\n    # Define the resampler\n    if config[\"resampler\"] == \"allknn\":\n        resampler = AllKNN()\n    elif config[\"resampler\"] == \"smote\":\n        resampler = SMOTE()\n    else:\n        resampler = \"passthrough\"\n\n    # Define the classifier\n    new_config = {k: v for k, v in config.items() if k != \"resampler\"}\n    classifier = XGBClassifier(**new_config)\n\n    # Combine the resampler and classifier together\n    model = Pipeline(steps=[\n        (\"resampler\", resampler),\n        (\"classifier\", classifier)\n    ])\n\n    # Train the model\n    with mlflow.start_run(run_id=run_id):\n        with mlflow.start_run(\n            run_name=f\"{config['resampler']}-{config['booster']}-{config['lambda']:.2f}-{config['alpha']:.2f}\",\n            nested=True\n        ):\n            model.fit(X_train, y_train)\n            # Evaluate the model\n            y_prob = model.predict_proba(X_test)\n            y_prob = y_prob[:, 1]\n            y_pred = (y_prob &gt; 0.5).astype(int)\n            metrics = {\n                \"accuracy\": accuracy_score(y_test, y_pred),\n                \"precision\": precision_score(y_test, y_pred),\n                \"recall\": recall_score(y_test, y_pred),\n                \"f1\": f1_score(y_test, y_pred),\n                \"roc_auc\": roc_auc_score(y_test, y_prob),\n                \"log_loss\": log_loss(y_test, y_prob),\n                \"average_precision\": average_precision_score(y_test, y_prob)\n            }\n            # Log the metrics and hyperparameters\n            mlflow.log_params(config)\n            mlflow.log_metrics(metrics)\n            tune.report(metrics)\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#retrain-and-save-the-model-with-best-params-in-the-driver-process","title":"Retrain and Save the Model with Best Params in the Driver Process","text":"<p>Once all trials finish, we return to the driver process, where we access the <code>ResultGrid</code>. This object contains all trial results. We then select the best set of hyperparameters (e.g., the one with the highest F1 score), retrain the model with those parameters, and log the final model to MLflow under the original parent run.</p> training.py<pre><code>    # Retrain the model with the hyperparameters with best result\n    config = results.get_best_result(metric='f1', mode='max').config\n\n    # 1. Split data\n    X = data[[\n        'has_fraud_7d',\n        'num_transactions_7d',\n        'credit_score',\n        'account_age_days',\n        'has_2fa_installed'\n    ]]\n    y = data['is_fraud']\n\n    # 2. Define resampler\n    if config[\"resampler\"] == \"allknn\":\n        resampler = AllKNN()\n    elif config[\"resampler\"] == \"smote\":\n        resampler = SMOTE()\n    else:\n        resampler = \"passthrough\"\n\n    # 3. Define classifier\n    new_config = {k: v for k, v in config.items() if k != \"resampler\"}\n    classifier = XGBClassifier(**new_config)\n\n    # 4. Combine the resampler and classifier together\n    model = Pipeline(steps=[\n        (\"resampler\", resampler),\n        (\"classifier\", classifier)\n    ])\n\n    # 5. Train and evaluate the model\n    model.fit(X, y)\n    y_prob = model.predict_proba(X)\n    y_prob = y_prob[:, 1]\n    y_pred = (y_prob &gt; 0.5).astype(int)\n    metrics = {\n        \"accuracy\": accuracy_score(y, y_pred),\n        \"precision\": precision_score(y, y_pred),\n        \"recall\": recall_score(y, y_pred),\n        \"f1\": f1_score(y, y_pred),\n        \"roc_auc\": roc_auc_score(y, y_prob),\n        \"log_loss\": log_loss(y, y_prob),\n        \"average_precision\": average_precision_score(y, y_prob)\n    }\n\n    # 6. Log the hyperparameters, metrics and the model\n    mlflow.log_params(config)\n    mlflow.sklearn.log_model(\n        sk_model=model,\n        artifact_path=\"model\",\n        input_example=X.iloc[[0]],\n        metadata={\"version\": f\"{EXPERIMENT_NAME}_v1\"}\n    )\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#training-function-trainable","title":"Training Function (Trainable)","text":"<p>This is the training logic executed inside each worker process. Here's the typical workflow:</p> training.py<pre><code>def training_function(\n    config, data,\n    run_id, mlflow_tracking_uri, experiment_name\n):\n    # Set up mlflow \n    mlflow.set_tracking_uri(mlflow_tracking_uri)\n    mlflow.set_experiment(experiment_name=experiment_name)\n\n    # Split data\n    X = data[[\n        'has_fraud_7d',\n        'num_transactions_7d',\n        'credit_score',\n        'account_age_days',\n        'has_2fa_installed'\n    ]]\n    y = data['is_fraud']\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, stratify=y,\n        random_state=config[\"random_state\"]\n    )\n\n    # Define the resampler\n    if config[\"resampler\"] == \"allknn\":\n        resampler = AllKNN()\n    elif config[\"resampler\"] == \"smote\":\n        resampler = SMOTE()\n    else:\n        resampler = \"passthrough\"\n\n    # Define the classifier\n    new_config = {k: v for k, v in config.items() if k != \"resampler\"}\n    classifier = XGBClassifier(**new_config)\n\n    # Combine the resampler and classifier together\n    model = Pipeline(steps=[\n        (\"resampler\", resampler),\n        (\"classifier\", classifier)\n    ])\n\n    # Train the model\n    with mlflow.start_run(run_id=run_id):\n        with mlflow.start_run(\n            run_name=f\"{config['resampler']}-{config['booster']}-{config['lambda']:.2f}-{config['alpha']:.2f}\",\n            nested=True\n        ):\n            model.fit(X_train, y_train)\n            # Evaluate the model\n            y_prob = model.predict_proba(X_test)\n            y_prob = y_prob[:, 1]\n            y_pred = (y_prob &gt; 0.5).astype(int)\n            metrics = {\n                \"accuracy\": accuracy_score(y_test, y_pred),\n                \"precision\": precision_score(y_test, y_pred),\n                \"recall\": recall_score(y_test, y_pred),\n                \"f1\": f1_score(y_test, y_pred),\n                \"roc_auc\": roc_auc_score(y_test, y_prob),\n                \"log_loss\": log_loss(y_test, y_prob),\n                \"average_precision\": average_precision_score(y_test, y_prob)\n            }\n            # Log the metrics and hyperparameters\n            mlflow.log_params(config)\n            mlflow.log_metrics(metrics)\n            tune.report(metrics)\n</code></pre> <ol> <li>Retrieve hyperparameters and the dataset (from <code>config</code> and <code>data</code>).</li> <li>Split the dataset into training and validation sets.</li> <li>Set up a pipeline with the selected resampling method and booster.</li> <li>Train the model and log evaluation metrics and hyperparameters.</li> </ol>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#run-ray-tune","title":"Run Ray Tune","text":""},{"location":"side-projects/data2ml-ops/ray/ray-tune/#run-the-hyperparameter-optimization","title":"Run the Hyperparameter Optimization","text":"<p>With the data ready, search space and Optuna strategy defined, and MLflow properly configured, we\u2019re all set to launch Ray Tune via <code>tune.Tuner()</code>.</p> training.py<pre><code># Run Ray Tune\nray.init()\nwith mlflow.start_run(run_name=RUN_NAME, nested=True) as run:\n    tuner = tune.Tuner(\n        tune.with_parameters(\n            training_function,\n            data=data,\n            run_id=run.info.run_id,\n            mlflow_tracking_uri=TRACKING_URI,\n            experiment_name=EXPERIMENT_NAME\n        ),\n        tune_config=tune_config,\n        run_config=run_config\n    )\n    results = tuner.fit()\n</code></pre>"},{"location":"side-projects/data2ml-ops/ray/ray-tune/#retrain-and-save-the-model-with-the-best-hyperparameters","title":"Retrain and Save the Model with the Best Hyperparameters","text":"<p>After <code>tune.fit()</code> completes and all trials are evaluated, we move on to retraining the best model and logging it\u2014just as explained in the previous section.</p> <p>Once everything is in place, the next step is to submit the script to a Ray cluster. There are several ways to do that, and we\u2019ll cover them in the next article.</p> <ol> <li> <p>Configuring Persistent Storage | Ray Docs \u21a9</p> </li> <li> <p>Feast Offline Feature Server | Feast Docs \u21a9</p> </li> <li> <p>Running Tune experiments with Optuna | Ray Docs \u21a9\u21a9\u21a9</p> </li> <li> <p>Pythonic Search Space | Optuna Docs \u21a9</p> </li> <li> <p>AllKNN | imbalanced-learn Docs \u21a9\u21a9</p> </li> <li> <p>SMOTE | imbalanced-learn Docs \u21a9</p> </li> <li> <p>Using MLflow with Tune | Ray Docs \u21a9\u21a9</p> </li> <li> <p>Understanding Parent and Child Runs in MLflow | MLflow Docs \u21a9</p> </li> <li> <p>Integrate MLflow and Ray | Databricks Docs \u21a9</p> </li> </ol>"},{"location":"side-projects/data2ml-ops/ray/what-why-when/","title":"What, Why, When","text":""},{"location":"side-projects/data2ml-ops/ray/what-why-when/#what-is-ray","title":"What is Ray?","text":"<p>Ray is an open-source framework designed to scale Python and AI workloads efficiently. It provides a unified API that allows developers to run Python applications seamlessly\u2014from a laptop to a large distributed cluster.</p> <p>Built with flexibility in mind, Ray includes a suite of libraries tailored for common machine learning tasks such as data loading and transformation (Ray Data), distributed training (Ray Train), hyperparameter tuning (Ray Tune), and scalable model serving (Ray Serve). These libraries are modular and interoperable, making it easier to build, scale, and manage end-to-end ML pipelines.</p> <p>A good introduction to Ray:</p>"},{"location":"side-projects/data2ml-ops/ray/what-why-when/#why-ray","title":"Why Ray?","text":"<p>For data engineers, Ray offers Python-native distributed processing capabilities that make it easy to scale ETL and data workflows. With Ray Data, large-scale transformations and ingestion tasks can be executed efficiently without relying on heavier frameworks, improving overall pipeline performance.</p> <p>For data analysts, Ray enables parallel computation on large datasets using familiar tools like Pandas. This accelerates data preparation and reduces wait times, making the analysis process faster and more productive\u2014even when working with messy or high-volume data.</p> <p>For data scientists, Ray's Train and Tune modules support scalable model training and hyperparameter optimization. It integrates seamlessly with popular ML frameworks, enabling rapid experimentation and efficient model tuning across different environments.</p> <p>For machine learning engineers, Ray provides an end-to-end solution from training to deployment. With Ray Serve, models can be deployed and scaled with minimal overhead, while supporting flexible resource allocation and production-grade performance across varied use cases.</p>"},{"location":"side-projects/data2ml-ops/ray/what-why-when/#when-to-use-ray","title":"When to Use Ray?","text":""},{"location":"side-projects/data2ml-ops/ray/what-why-when/#scaling-python-workloads-without-rewriting-code","title":"Scaling Python Workloads Without Rewriting Code","text":"<p>When you need to scale Python applications\u2014such as data preprocessing, simulations, or backtesting\u2014Ray enables parallel execution across multiple cores or nodes with minimal code changes. Its Python-native API and dynamic task scheduling make it ideal for workloads that require fine-grained parallelism or involve dynamic task graphs<sup>1</sup>. This allows developers to scale their applications efficiently without the complexity of traditional distributed systems.</p>"},{"location":"side-projects/data2ml-ops/ray/what-why-when/#accelerating-machine-learning-workflows","title":"Accelerating Machine Learning Workflows","text":"<p>For machine learning tasks like distributed training and hyperparameter tuning, Ray's libraries\u2014such as Ray Train and Ray Tune\u2014provide scalable solutions that integrate seamlessly with popular ML frameworks like PyTorch and TensorFlow. This enables faster experimentation and model optimization by leveraging distributed computing resources, reducing the time from development to deployment.</p>"},{"location":"side-projects/data2ml-ops/ray/what-why-when/#deploying-scalable-and-responsive-ml-services","title":"Deploying Scalable and Responsive ML Services","text":"<p>When deploying machine learning models that require low-latency inference at scale, Ray Serve offers a flexible and production-ready serving layer. It supports dynamic model loading, autoscaling, and request batching, making it suitable for online prediction services and real-time ML applications. This ensures that ML services can handle varying loads efficiently while maintaining high performance.</p> <ol> <li> <p>Ray: Your Gateway to Scalable AI and Machine Learning Applications \u21a9</p> </li> <li> <p>Getting Started | Ray Docs \u21a9</p> </li> </ol>"},{"location":"side-projects/dcard-hw/","title":"2020 Dcard Data Engineering Intern","text":"<p>Dcard is a popular social media platform in Taiwan, especially among college students and young adults. It was launched in 2011 as a university-only online forum, similar in spirit to how Facebook started within universities.</p> <p>This project is a pre-interview assignment for the 2020 Dcard Data Engineer Internship Program.</p> <p>On Dcard's app and website, there is an important section called \"Trending Posts,\" where users can find the hottest discussion topics on the platform. As data enthusiasts, we are also curious about which posts have the potential to become trending. If we consider this factor in our recommendations, we might help users discover great posts faster. Therefore, in this assignment, we aim to predict whether a post has the potential to appear in the \"Trending Posts\" section based on some data.</p> <p>To simplify the problem, we define a trending post as one that receives at least 1000 likes within 36 hours of being posted. During testing, we will calculate whether a post's like count exceeds 1000 within 36 hours to determine the ground truth or prediction benchmark.</p> <p>Abstract</p> <pre><code>$ tree\n.\n\u251c\u2500\u2500 requirements.txt: A list of required Python packages and their versions.\n\u251c\u2500\u2500 preprocessing.py: A shared utility script for database connections, preprocessing, and other common functions.\n\u251c\u2500\u2500 training.py: A utility script for training the model.\n\u251c\u2500\u2500 predict.py: A utility script for making predictions.\n\u251c\u2500\u2500 outputs\n\u2502   \u251c\u2500\u2500 best_model.h5: The best model obtained after training.\n\u2502   \u251c\u2500\u2500 cv_results.csv: Cross-validation results.\n\u2502   \u2514\u2500\u2500 output.csv: Prediction results for the public testing dataset.\n\u2514\u2500\u2500 eda_evaluation.ipynb: A Jupyter notebook used for generating visualizations.\n</code></pre> <p>The training dataset includes articles spanning from April 1, 2019, to the end of October 2019, covering approximately seven months. The dataset contains around 793,000 articles, of which about 2.32% (approximately 18,000 articles) are classified as popular. Through exploratory data analysis, we observed high correlations among variables. Additionally, the timing of article publication significantly influences the proportion of popular articles and the total number of likes within the first 36 hours of posting.</p> <p>We decided to use a \"binary classification model without considering sequential information\" as our primary approach, focusing on handling imbalanced datasets, tree-based ensemble models, and subsequent discussions. The training process was divided into three main stages:</p> <ol> <li>Resampling</li> <li>Feature Transformation</li> <li>Classification</li> </ol> <p>After experimentation, we opted to omit the \"Feature Transformation\" stage. In total, 108 combinations were tested using <code>GridSearchCV</code> with <code>cv=3</code> to find the optimal configuration.</p> <p>Using the f1-score as the evaluation metric, the best-performing model was an <code>AdaBoostClassifier</code> without any resampling. This model consisted of 100 decision trees, each limited to a depth of 2. The average f1-score from cross-validation was 0.56, while the f1-score on the public test set was 0.53. Key findings from the experiments include:</p> <ul> <li>Different resampling strategies significantly impact the f1-score.</li> <li>Resampling strategies can effectively identify genuinely popular articles. However, this comes at the cost of reduced trust in the model's predictions of popular articles.</li> <li>Under both \"SMOTE resampling\" and \"no resampling\" scenarios, the choice of classifier did not lead to substantial changes in the f1-score.</li> <li>The choice of classifier had a relatively minor impact on the f1-score.</li> </ul> <p>Finally, we discussed several potential future directions, including exploring other resampling techniques, alternative evaluation metrics, and incorporating sequential information.</p>"},{"location":"side-projects/dcard-hw/#training-dataset","title":"Training Dataset","text":"<p>The training dataset covers posts from April 1, 2019, to the end of October 2019, approximately 7 months. It contains around 794,000 posts, of which about 2.32% (approximately 18,000 posts) are trending.</p> <pre><code>posts_train                Contains 793,751 records and 3 columns\npost_shared_train          Contains 304,260 records and 3 columns\npost_comment_created_train Contains 2,372,228 records and 3 columns\npost_liked_train           Contains 3,395,903 records and 3 columns\npost_collected_train       Contains 1,235,126 records and 3 columns\n</code></pre> <p>Table: <code>posts_train</code></p> column_name data_type description <code>post_key</code> string Unique identifier of the post <code>created_at_hour</code> datetime The hour when the post was created <code>like_count_36_hour</code> integer Number of likes the post received within 36 hours (only in train table) <p>Table: <code>post_shared_train</code></p> column_name data_type description <code>post_key</code> string Unique identifier of the post <code>created_at_hour</code> datetime The hour of the sharing activity <code>count</code> integer Number of shares the post received in that hour <p>Table: <code>post_comment_created_train</code></p> column_name data_type description <code>post_key</code> string Unique identifier of the post <code>created_at_hour</code> datetime The hour of the comment activity <code>count</code> integer Number of comments the post received in that hour <p>Table: <code>post_liked_train</code></p> column_name data_type description <code>post_key</code> string Unique identifier of the post <code>created_at_hour</code> datetime The hour of the like activity <code>count</code> integer Number of likes the post received in that hour <p>Table: <code>post_collected_train</code></p> column_name data_type description <code>post_key</code> string Unique identifier of the post <code>created_at_hour</code> datetime The hour of the collection activity <code>count</code> integer Number of times the post was bookmarked in that hour"},{"location":"side-projects/dcard-hw/#testing-dataset","title":"Testing Dataset","text":"<pre><code>posts_test                 Contains 225,986 records and 3 columns\npost_shared_test           Contains 83,376 records and 3 columns\npost_comment_created_test  Contains 607,251 records and 3 columns\npost_liked_test            Contains 908,910 records and 3 columns\npost_collected_test        Contains 275,073 records and 3 columns\n</code></pre>"},{"location":"side-projects/dcard-hw/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>For offline evaluation, only the first 10 hours of data for each post will be used as input for prediction. The primary evaluation metric is the F1-score.</p>"},{"location":"side-projects/dcard-hw/#submission-requirements","title":"Submission Requirements","text":"<p>Upon completing the assignment, you must submit at least the following four files. Failure to include any of these will be considered incomplete.</p> <ol> <li><code>Report.pdf</code><ul> <li>Instructions on how to use your code</li> <li>Methods and rationale</li> <li>Evaluation results on the provided testing data</li> <li>Experimental observations</li> </ul> </li> <li><code>train.py</code></li> <li><code>predict.py</code></li> <li><code>requirements.txt</code> or Pipfile</li> <li>(Optional) If your prediction requires a model file, please include it (we will not train it for you) and explain how to use it in Report.pdf.</li> </ol> <p>We have some requirements for the program structure to facilitate testing:</p> <ul> <li> <p>Training</p> <ul> <li>The outermost layer should be wrapped in train.py.</li> <li>The program should be executable as <code>python train.py {database_host} {model_filepath}</code>.</li> <li>Example: <code>python train.py localhost:8080 ./model.h5</code></li> </ul> </li> <li> <p>Prediction</p> <ul> <li>The program should be executable as <code>python predict.py {database_host} {model_filepath} {output_filepath}</code>.</li> <li>Specify where your model_filepath is located.</li> <li>Example: <code>python predict.py localhost:8080 ./model.h5 ./sample_output.csv</code></li> <li>Your program must achieve the following during prediction:<ul> <li>Read data from the database. The data format will match the tables described in the next section. For evaluation, we will use our own test data.</li> <li>Use another database's xxx_test tables as the test set during actual testing. Your predict.py should use these tables as input.</li> <li>Output a CSV file with two columns as shown below, including a header (refer to the provided sample_output.csv):<ul> <li>post_key: string type</li> <li>is_trending: bool type</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"side-projects/dcard-hw/#usage-instructions","title":"Usage Instructions","text":"<p>Environment:</p> <ul> <li>Operating System: Ubuntu 18.04 LTS Desktop</li> <li>Python version: Python 3.6.8</li> <li>Required Python packages and their versions are listed in <code>requirements.txt</code>.</li> </ul>"},{"location":"side-projects/dcard-hw/#trainingpy","title":"<code>training.py</code>","text":"<p>The usage of <code>training.py</code> is as follows:</p> <pre><code>usage: training.py [-h] -u USERNAME -p PASSWORD --host HOSTNAME\n                   [--port PORTNUMBER] [--protocol PROTOCOL]\n                   DATABASE OUTPUT_PATH\n</code></pre> <p>At a minimum, you must provide five parameters: \"username,\" \"password,\" \"host IP address,\" \"database name,\" and \"output path.\" To train on the training set, use the following command:</p> <pre><code>python training.py -u \"USERNAME\"\\\n                   -p \"PASSWORD\"\\\n                   --host \"HOSTNAME\"\\\n                   \"DATABASE\"\\\n                   \"OUTPUT_PATH\"\n</code></pre> <p>By default, the program connects to a PostgreSQL database on port 5432. If needed, you can use the <code>--protocol</code> and <code>--port</code> options to connect to other databases, such as MySQL:</p> Note <pre><code>python training.py -u \"USERNAME\"\\\n                -p \"PASSWORD\"\\\n                --host \"HOSTNAME\"\\\n                --port \"3306\"\\\n                --protocol \"mysql\"\\\n                \"DATABASE\"\\\n                \"OUTPUT_PATH\"\n</code></pre> <p>Danger</p> <p>After training, the program generates two files: \"best model\" and \"cross-validation results.\" The default filenames are <code>best_model.h5</code> and <code>cv_results.csv</code> (these cannot be changed). Therefore, when specifying <code>OUTPUT_PATH</code>, only the folder name is required.</p> <p>For more details, use the <code>-h</code> or <code>--help</code> options:</p> Note <pre><code>$ python training.py -h\nusage: training.py [-h] -u USERNAME -p PASSWORD --host HOSTNAME\n                [--port PORTNUMBER] [--protocol PROTOCOL]\n                DATABASE OUTPUT_PATH\n\npositional arguments:\nDATABASE             (Required) Database to use when connecting to server.\nOUTPUT_PATH          (Required) Best prediction model and cross validation\n                    results outputs file path.\n\noptional arguments:\n-h, --help           show this help message and exit\n-u USERNAME          (Required) User for login if not current user.\n-p PASSWORD          (Required) Password to use when connecting to server.\n--host HOSTNAME      (Required) Host address to connect.\n--port PORTNUMBER    Port number to use for connection (default: 5432)\n--protocol PROTOCOL  Protocol to connect. (default: postgres)\n</code></pre>"},{"location":"side-projects/dcard-hw/#predictpy","title":"<code>predict.py</code>","text":"<p>The usage of <code>predict.py</code> is as follows:</p> <pre><code>usage: predict.py [-h] -u USERNAME -p PASSWORD --host HOSTNAME\n                  [--port PORTNUMBER] [--protocol PROTOCOL] [-n]\n                  DATABASE MODEL_NAME OUTPUT_PATH\n</code></pre> <p>Similar to <code>training.py</code>, you must provide five parameters, with an additional parameter for the \"model path\" used to predict trending posts. To predict on the public test set, use the following command:</p> <pre><code>python predict.py -u \"USERNAME\"\\\n                  -p \"PASSWORD\"\\\n                  --host \"HOSTNAME\"\\\n                  \"DATABASE\"\\\n                  \"MODELNAME\"\\\n                  \"OUTPUT_PATH\"\n</code></pre> <p>After execution, the program outputs <code>output.csv</code> (filename cannot be changed) to the specified folder. Note that the <code>MODEL_NAME</code> option must include the model file name, not the folder path.</p> <p>As mentioned in the \"Assignment Supplementary Notes and Corrections\" email, the <code>posts_test</code> table in the private test set does not include the <code>like_count_36_hour</code> column. Therefore, you must use the <code>-n</code> option to indicate that this column is absent. To predict on the private test set, use the following command:</p> <pre><code>python predict.py -u \"USERNAME\"\\\n                  -p \"PASSWORD\"\\\n                  --host \"HOSTNAME\"\\\n                  -n\\\n                  \"DATABASE\"\\\n                  \"MODELNAME\"\\\n                  \"OUTPUT_PATH\"\n</code></pre> <p>If needed, you can also use the <code>--port</code> and <code>--protocol</code> options to connect to other databases.</p> <p>For more details, use the <code>-h</code> or <code>--help</code> options:</p> Note <pre><code>$ python predict.py -h\nusage: predict.py [-h] -u USERNAME -p PASSWORD --host HOSTNAME\n                [--port PORTNUMBER] [--protocol PROTOCOL] [-n]\n                DATABASE MODEL_NAME OUTPUT_PATH\n\npositional arguments:\nDATABASE             (Required) Database to use when connecting to server.\nMODEL_NAME           (Required) Prediction model name. If it is not in the\n                    current directory, please specify where it is.\nOUTPUT_PATH          (Required) File path of predicted results.\n\noptional arguments:\n-h, --help           show this help message and exit\n-u USERNAME          (Required) User for login if not current user.\n-p PASSWORD          (Required) Password to use when connecting to server.\n--host HOSTNAME      (Required) Host address to connect.\n--port PORTNUMBER    Port number to use for connection (default: 5432)\n--protocol PROTOCOL  Protocol to connect. (default: postgres)\n-n                   No like_count_36_hour column when the option is given.\n</code></pre>"},{"location":"side-projects/dcard-hw/eda_evaluation/","title":"\u532f\u5165\u76f8\u95dc\u5957\u4ef6","text":"Table of Contents <ul><li>1\u00a0\u00a0\u532f\u5165\u76f8\u95dc\u5957\u4ef6</li><li>2\u00a0\u00a0\u4e8b\u524d\u6e96\u5099</li><li>3\u00a0\u00a0EDA</li><li>4\u00a0\u00a0Evaluation<ul><li>4.1\u00a0\u00a0Resampler</li><li>4.2\u00a0\u00a0Resampler + Classifier</li><li>4.3\u00a0\u00a0Classifier</li><li>4.4\u00a0\u00a0Classifier + n_estimator</li><li>4.5\u00a0\u00a0<code>AdaBoostClassifier</code> + <code>max_depth</code></li><li>4.6\u00a0\u00a0<code>GradientBoostingClassifier</code>, <code>XGBClassifier</code> + <code>learning_rate</code></li><li>4.7\u00a0\u00a0Best Model<ul><li>4.7.1\u00a0\u00a0f1-score</li><li>4.7.2\u00a0\u00a0balanced accuracy</li></ul></li></ul></li></ul> In\u00a0[1]: Copied! <pre># Import built-in packages\nfrom math import isnan\nfrom functools import reduce\n\n# Import 3-rd party packages\nimport sqlalchemy\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom plotnine import *\n</pre> # Import built-in packages from math import isnan from functools import reduce  # Import 3-rd party packages import sqlalchemy import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from plotnine import * In\u00a0[2]: Copied! <pre>def print_info(info, width=61, fillchar='='):\n    \"\"\"\n    \u5370\u51fa\u683c\u5f0f\u5316\u7684\u8cc7\u8a0a\n    \"\"\"\n    temp_width = width - (width-len(info))//2\n    print(info.rjust(temp_width, fillchar).ljust(width, fillchar))\n</pre> def print_info(info, width=61, fillchar='='):     \"\"\"     \u5370\u51fa\u683c\u5f0f\u5316\u7684\u8cc7\u8a0a     \"\"\"     temp_width = width - (width-len(info))//2     print(info.rjust(temp_width, fillchar).ljust(width, fillchar)) In\u00a0[3]: Copied! <pre>def get_connector(user, host, database, password=None, port='5432', protocol='postgres'):\n    \"\"\"\n    \u53d6\u5f97\u9023\u7dda\u5f15\u64ce\uff0c\u9810\u8a2d\u70ba\u9023\u7dda\u81f3 PostgreSQL\uff0c\u57e0\u865f\u9810\u8a2d\u70ba 5432\u3002\n    \"\"\"\n    print_info(\"GETTING CONNECTOR START!\")\n    user_info = f'{user}:{password}' if password else user\n    url = f'{protocol}://{user_info}@{host}:{port}/{database}'\n    engine = sqlalchemy.create_engine(url, client_encoding='utf-8')\n    print_info(\"DONE!\")\n    return engine\n</pre> def get_connector(user, host, database, password=None, port='5432', protocol='postgres'):     \"\"\"     \u53d6\u5f97\u9023\u7dda\u5f15\u64ce\uff0c\u9810\u8a2d\u70ba\u9023\u7dda\u81f3 PostgreSQL\uff0c\u57e0\u865f\u9810\u8a2d\u70ba 5432\u3002     \"\"\"     print_info(\"GETTING CONNECTOR START!\")     user_info = f'{user}:{password}' if password else user     url = f'{protocol}://{user_info}@{host}:{port}/{database}'     engine = sqlalchemy.create_engine(url, client_encoding='utf-8')     print_info(\"DONE!\")     return engine In\u00a0[4]: Copied! <pre>def get_tables(engine, table_names):\n    \"\"\"\n    \u4f9d\u7167 `tables_names` \u7684\u9806\u5e8f\uff0c\u53d6\u5f97 tables\uff0c\u4e26\u4f9d\u5e8f\u5132\u5b58\u65bc `list` \u7576\u4e2d\uff0c\u56de\u50b3\u578b\u614b\u70ba `list`\uff0c\u6bcf\u500b element \u70ba `DataFrame`\u3002\n    \"\"\"\n    print_info(\"GETTING TABLES START!\")\n    rslt = []\n    for tn in table_names:\n        query = f'SELECT * FROM {tn}'\n        exec(f'{tn} = pd.read_sql(query, engine)')\n        # exec(f\"{tn} = pd.read_csv('{tn}.csv', encoding='utf8')\") # from current working directory\n        print(\n            f'{format(tn, \"26s\")} \u7e3d\u5171\u6709 {eval(f\"{tn}.shape[0]\"):9,} \u7b46\u8cc7\u6599\u548c {eval(f\"{tn}.shape[1]\")} \u500b\u6b04\u4f4d')\n        exec(f'rslt.append({tn})')\n    print_info(\"DONE!\")\n    return rslt\n</pre> def get_tables(engine, table_names):     \"\"\"     \u4f9d\u7167 `tables_names` \u7684\u9806\u5e8f\uff0c\u53d6\u5f97 tables\uff0c\u4e26\u4f9d\u5e8f\u5132\u5b58\u65bc `list` \u7576\u4e2d\uff0c\u56de\u50b3\u578b\u614b\u70ba `list`\uff0c\u6bcf\u500b element \u70ba `DataFrame`\u3002     \"\"\"     print_info(\"GETTING TABLES START!\")     rslt = []     for tn in table_names:         query = f'SELECT * FROM {tn}'         exec(f'{tn} = pd.read_sql(query, engine)')         # exec(f\"{tn} = pd.read_csv('{tn}.csv', encoding='utf8')\") # from current working directory         print(             f'{format(tn, \"26s\")} \u7e3d\u5171\u6709 {eval(f\"{tn}.shape[0]\"):9,} \u7b46\u8cc7\u6599\u548c {eval(f\"{tn}.shape[1]\")} \u500b\u6b04\u4f4d')         exec(f'rslt.append({tn})')     print_info(\"DONE!\")     return rslt In\u00a0[5]: Copied! <pre>def merge_tables(tables, table_names, how):\n    \"\"\"\n    \u5408\u4f75\u6240\u6709 tables\uff0c\u56de\u50b3\u578b\u614b\u70ba `DataFrame`\u3002\n    \"\"\"\n    print_info(\"MERGING TABLES START!\")\n    # \u5206\u5225\u8655\u7406 post_{shared, comment_created, liked, collected}_{train, test} \u56db\u500b tables\n    # groupby \u6bcf\u7bc7\u6587\u7ae0\uff0c\u5c07\u524d\u5341\u5c0f\u6642\u7684\u5206\u4eab\u6578\u3001\u8a55\u8ad6\u6578\u3001\u611b\u5fc3\u6578\u3001\u6536\u85cf\u6578\u52a0\u7e3d\u8d77\u4f86\n    for idx, (table, tn) in enumerate(zip(tables, table_names)):\n        if len(tn.split('_'))==2: continue                  # for handling posts_{train, test} table\n        col_name = f\"{tn.split('_')[1]}_count\"              # tn.split('_')[1] is either {shared, comment, liked, collected}\n        mapper = {'count': col_name}\n        exec(f\"tables[{idx}] = table.groupby(['post_key'], as_index=False).sum().rename(columns=mapper)\")\n    # \u5c07 tables \u5408\u4f75\u8d77\u4f86\u4e26\u56de\u50b3\u3002\n    total_df = reduce(lambda left, right: pd.merge(left, right, on=['post_key'], how=how), tables)\n    print_info(\"DONE!\")\n    return total_df\n</pre> def merge_tables(tables, table_names, how):     \"\"\"     \u5408\u4f75\u6240\u6709 tables\uff0c\u56de\u50b3\u578b\u614b\u70ba `DataFrame`\u3002     \"\"\"     print_info(\"MERGING TABLES START!\")     # \u5206\u5225\u8655\u7406 post_{shared, comment_created, liked, collected}_{train, test} \u56db\u500b tables     # groupby \u6bcf\u7bc7\u6587\u7ae0\uff0c\u5c07\u524d\u5341\u5c0f\u6642\u7684\u5206\u4eab\u6578\u3001\u8a55\u8ad6\u6578\u3001\u611b\u5fc3\u6578\u3001\u6536\u85cf\u6578\u52a0\u7e3d\u8d77\u4f86     for idx, (table, tn) in enumerate(zip(tables, table_names)):         if len(tn.split('_'))==2: continue                  # for handling posts_{train, test} table         col_name = f\"{tn.split('_')[1]}_count\"              # tn.split('_')[1] is either {shared, comment, liked, collected}         mapper = {'count': col_name}         exec(f\"tables[{idx}] = table.groupby(['post_key'], as_index=False).sum().rename(columns=mapper)\")     # \u5c07 tables \u5408\u4f75\u8d77\u4f86\u4e26\u56de\u50b3\u3002     total_df = reduce(lambda left, right: pd.merge(left, right, on=['post_key'], how=how), tables)     print_info(\"DONE!\")     return total_df In\u00a0[6]: Copied! <pre>def preprocess_total_df(total_df):\n    \"\"\"\n    \u9810\u8655\u7406\u525b\u5408\u4f75\u597d\u7684 total_df \u4ee5\u7b26\u5408\u5f8c\u7e8c\u5efa\u6a21\u9700\u6c42\uff0c\u56de\u50b3\u578b\u614b\u70ba `DataFrame`\u3002\n    \"\"\"\n    print_info(\"PREPROCESSING TOTAL_DF START!\")\n    total_df.set_index('post_key', inplace=True)                                    # post_key \u6b04\u4f4d\u8a2d\u70ba\u7d22\u5f15\n    total_df['created_at_hour'] = pd.to_datetime(total_df['created_at_hour'])       # \u5c07 created_at_hour \u6b04\u4f4d\u8f49\u63db\u6210 datetime \u578b\u614b\n    total_df['weekday'] = total_df['created_at_hour'].dt.dayofweek                  # \u64f7\u53d6\u51fa\u767c\u6587\u7684 weekday\n    total_df['hour'] = total_df['created_at_hour'].dt.hour                          # \u64f7\u53d6\u51fa\u767c\u6587\u7684 hour\n    total_df.fillna(0, inplace=True)                                                # NaN \u503c\u88dc 0\n    total_df['is_trending'] = 0+(total_df['like_count_36_hour']&gt;=1000)              # \u8f49\u63db\u6210 is_trending \u985e\u5225\u6b04\u4f4d\n    total_df = total_df.drop(['created_at_hour'], axis=1)                           # drop \u6389\u4e0d\u5fc5\u8981\u7684\u6b04\u4f4d\n    # \u5c07\u8a08\u6b21\u6b04\u4f4d\u8f49\u63db\u6210 int \u578b\u614b\n    col_names = ['shared_count', 'comment_count', 'liked_count', 'collected_count']\n    for cn in col_names:\n        total_df[cn] = total_df[cn].astype(dtype='int')\n    print_info(\"DONE!\")\n    return total_df\n</pre> def preprocess_total_df(total_df):     \"\"\"     \u9810\u8655\u7406\u525b\u5408\u4f75\u597d\u7684 total_df \u4ee5\u7b26\u5408\u5f8c\u7e8c\u5efa\u6a21\u9700\u6c42\uff0c\u56de\u50b3\u578b\u614b\u70ba `DataFrame`\u3002     \"\"\"     print_info(\"PREPROCESSING TOTAL_DF START!\")     total_df.set_index('post_key', inplace=True)                                    # post_key \u6b04\u4f4d\u8a2d\u70ba\u7d22\u5f15     total_df['created_at_hour'] = pd.to_datetime(total_df['created_at_hour'])       # \u5c07 created_at_hour \u6b04\u4f4d\u8f49\u63db\u6210 datetime \u578b\u614b     total_df['weekday'] = total_df['created_at_hour'].dt.dayofweek                  # \u64f7\u53d6\u51fa\u767c\u6587\u7684 weekday     total_df['hour'] = total_df['created_at_hour'].dt.hour                          # \u64f7\u53d6\u51fa\u767c\u6587\u7684 hour     total_df.fillna(0, inplace=True)                                                # NaN \u503c\u88dc 0     total_df['is_trending'] = 0+(total_df['like_count_36_hour']&gt;=1000)              # \u8f49\u63db\u6210 is_trending \u985e\u5225\u6b04\u4f4d     total_df = total_df.drop(['created_at_hour'], axis=1)                           # drop \u6389\u4e0d\u5fc5\u8981\u7684\u6b04\u4f4d     # \u5c07\u8a08\u6b21\u6b04\u4f4d\u8f49\u63db\u6210 int \u578b\u614b     col_names = ['shared_count', 'comment_count', 'liked_count', 'collected_count']     for cn in col_names:         total_df[cn] = total_df[cn].astype(dtype='int')     print_info(\"DONE!\")     return total_df In\u00a0[7]: Copied! <pre># Get engine\nengine = get_connector(\n    user=\"candidate\",\n    password=\"dcard-data-intern-2020\",\n    host=\"35.187.144.113\",\n    database=\"intern_task\"\n)\n# Get tables from db\ntable_names_train = ['posts_train', 'post_shared_train', \n                     'post_comment_created_train', 'post_liked_train', 'post_collected_train']\ntables_train = get_tables(engine, table_names_train)\n# Merge tables\ntotal_df_train = merge_tables(tables_train, table_names_train, how='left')\n# Preprocess total_df\ntotal_df_train = preprocess_total_df(total_df_train)\n\nengine.dispose()\n</pre> # Get engine engine = get_connector(     user=\"candidate\",     password=\"dcard-data-intern-2020\",     host=\"35.187.144.113\",     database=\"intern_task\" ) # Get tables from db table_names_train = ['posts_train', 'post_shared_train',                       'post_comment_created_train', 'post_liked_train', 'post_collected_train'] tables_train = get_tables(engine, table_names_train) # Merge tables total_df_train = merge_tables(tables_train, table_names_train, how='left') # Preprocess total_df total_df_train = preprocess_total_df(total_df_train)  engine.dispose() <pre>===================GETTING CONNECTOR START!==================\n============================DONE!============================\n====================GETTING TABLES START!====================\nposts_train                \u7e3d\u5171\u6709   793,751 \u7b46\u8cc7\u6599\u548c 3 \u500b\u6b04\u4f4d\npost_shared_train          \u7e3d\u5171\u6709   304,260 \u7b46\u8cc7\u6599\u548c 3 \u500b\u6b04\u4f4d\npost_comment_created_train \u7e3d\u5171\u6709 2,372,228 \u7b46\u8cc7\u6599\u548c 3 \u500b\u6b04\u4f4d\npost_liked_train           \u7e3d\u5171\u6709 3,395,903 \u7b46\u8cc7\u6599\u548c 3 \u500b\u6b04\u4f4d\npost_collected_train       \u7e3d\u5171\u6709 1,235,126 \u7b46\u8cc7\u6599\u548c 3 \u500b\u6b04\u4f4d\n============================DONE!============================\n====================MERGING TABLES START!====================\n============================DONE!============================\n================PREPROCESSING TOTAL_DF START!================\n============================DONE!============================\n</pre> In\u00a0[8]: Copied! <pre>cv_results = pd.read_csv('./outputs/cv_results.csv')\n</pre> cv_results = pd.read_csv('./outputs/cv_results.csv') In\u00a0[9]: Copied! <pre>temp = total_df_train.drop(columns=['weekday', 'hour', 'is_trending'])\nsns.heatmap(temp.corr(), cmap='YlGnBu')\n</pre> temp = total_df_train.drop(columns=['weekday', 'hour', 'is_trending']) sns.heatmap(temp.corr(), cmap='YlGnBu') Out[9]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x23755717ba8&gt;</pre> In\u00a0[10]: Copied! <pre>mapper = dict(zip([0, 1, 2, 3, 4, 5, 6], ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']))\n</pre> mapper = dict(zip([0, 1, 2, 3, 4, 5, 6], ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])) In\u00a0[11]: Copied! <pre># \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\u7684\u767c\u6587\u6578\nnum_articles_heatmap_df = total_df_train.groupby(['weekday', 'hour']).size().reset_index().rename(columns={0:'count'})\nnum_articles_heatmap_df = num_articles_heatmap_df.pivot(index='weekday', columns='hour', values='count')\nnum_articles_heatmap_df = num_articles_heatmap_df.rename(mapper=mapper, axis=0)\nnum_articles_heatmap_df = num_articles_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday', \n                                                           'Thursday', 'Friday', 'Saturday', 'Sunday'])\nplt.figure(figsize=(20, 5))\nplt.title(f'Number of Articles by Day of Week / Hour of Day')\nsns.heatmap(num_articles_heatmap_df, cmap='OrRd', cbar=False)\n</pre> # \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\u7684\u767c\u6587\u6578 num_articles_heatmap_df = total_df_train.groupby(['weekday', 'hour']).size().reset_index().rename(columns={0:'count'}) num_articles_heatmap_df = num_articles_heatmap_df.pivot(index='weekday', columns='hour', values='count') num_articles_heatmap_df = num_articles_heatmap_df.rename(mapper=mapper, axis=0) num_articles_heatmap_df = num_articles_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday',                                                             'Thursday', 'Friday', 'Saturday', 'Sunday']) plt.figure(figsize=(20, 5)) plt.title(f'Number of Articles by Day of Week / Hour of Day') sns.heatmap(num_articles_heatmap_df, cmap='OrRd', cbar=False) Out[11]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x237490fb518&gt;</pre> In\u00a0[12]: Copied! <pre># \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\u7684\u71b1\u9580\u6587\u7ae0\u6bd4\u4f8b\nnum_pops_heatmap_df = total_df_train.groupby(['weekday', 'hour'])['is_trending'].sum().reset_index()\nnum_pops_heatmap_df = num_pops_heatmap_df.pivot(index='weekday', columns='hour', values='is_trending')\nnum_pops_heatmap_df = num_pops_heatmap_df.rename(mapper=mapper, axis=0)\nnum_pops_heatmap_df = num_pops_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday', \n                                                   'Thursday', 'Friday', 'Saturday', 'Sunday'])\npct_pops_heatmap_df = num_pops_heatmap_df/num_articles_heatmap_df\nplt.figure(figsize=(20, 5))\nplt.title(f'Percentage of Popular Articles by Day of Week / Hour of Day ')\nsns.heatmap(pct_pops_heatmap_df, cmap='Blues', cbar=False)\n</pre> # \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\u7684\u71b1\u9580\u6587\u7ae0\u6bd4\u4f8b num_pops_heatmap_df = total_df_train.groupby(['weekday', 'hour'])['is_trending'].sum().reset_index() num_pops_heatmap_df = num_pops_heatmap_df.pivot(index='weekday', columns='hour', values='is_trending') num_pops_heatmap_df = num_pops_heatmap_df.rename(mapper=mapper, axis=0) num_pops_heatmap_df = num_pops_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday',                                                     'Thursday', 'Friday', 'Saturday', 'Sunday']) pct_pops_heatmap_df = num_pops_heatmap_df/num_articles_heatmap_df plt.figure(figsize=(20, 5)) plt.title(f'Percentage of Popular Articles by Day of Week / Hour of Day ') sns.heatmap(pct_pops_heatmap_df, cmap='Blues', cbar=False) Out[12]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2375e9e5cc0&gt;</pre> In\u00a0[13]: Copied! <pre># \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\uff0c\u524d 10 \u5c0f\u6642\u611b\u5fc3\u5e73\u5747\u6578\nnum_likes_10_heatmap_df = total_df_train.groupby(['weekday', 'hour'])['liked_count'].mean().reset_index()\nnum_likes_10_heatmap_df = num_likes_10_heatmap_df.pivot(index='weekday', columns='hour', values='liked_count')\nnum_likes_10_heatmap_df = num_likes_10_heatmap_df.rename(mapper=mapper, axis=0)\nnum_likes_10_heatmap_df = num_likes_10_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday', \n                                                           'Thursday', 'Friday', 'Saturday', 'Sunday'])\nplt.figure(figsize=(20, 5))\nplt.title(f'Average Likes within 10 hours by Day of Week / Hour of Day')\nsns.heatmap(num_likes_10_heatmap_df, cmap='Purples', cbar=False)\n</pre> # \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\uff0c\u524d 10 \u5c0f\u6642\u611b\u5fc3\u5e73\u5747\u6578 num_likes_10_heatmap_df = total_df_train.groupby(['weekday', 'hour'])['liked_count'].mean().reset_index() num_likes_10_heatmap_df = num_likes_10_heatmap_df.pivot(index='weekday', columns='hour', values='liked_count') num_likes_10_heatmap_df = num_likes_10_heatmap_df.rename(mapper=mapper, axis=0) num_likes_10_heatmap_df = num_likes_10_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday',                                                             'Thursday', 'Friday', 'Saturday', 'Sunday']) plt.figure(figsize=(20, 5)) plt.title(f'Average Likes within 10 hours by Day of Week / Hour of Day') sns.heatmap(num_likes_10_heatmap_df, cmap='Purples', cbar=False) Out[13]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x237625a2ef0&gt;</pre> In\u00a0[14]: Copied! <pre># \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\uff0c\u524d 36 \u5c0f\u6642\u611b\u5fc3\u5e73\u5747\u6578\nnum_likes_36_heatmap_df = total_df_train.groupby(['weekday', 'hour'])['like_count_36_hour'].mean().reset_index()\nnum_likes_36_heatmap_df = num_likes_36_heatmap_df.pivot(index='weekday', columns='hour', values='like_count_36_hour')\nnum_likes_36_heatmap_df = num_likes_36_heatmap_df.rename(mapper=mapper, axis=0)\nnum_likes_36_heatmap_df = num_likes_36_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday', \n                                                           'Thursday', 'Friday', 'Saturday', 'Sunday'])\nplt.figure(figsize=(20, 5))\nplt.title(f'Average Likes within 36 hours by Day of Week / Hour of Day ')\nsns.heatmap(num_likes_36_heatmap_df, cmap='YlGn', cbar=False)\n</pre> # \u89c0\u5bdf\u4e0d\u540c\u6642\u6bb5\u4e0b\uff0c\u524d 36 \u5c0f\u6642\u611b\u5fc3\u5e73\u5747\u6578 num_likes_36_heatmap_df = total_df_train.groupby(['weekday', 'hour'])['like_count_36_hour'].mean().reset_index() num_likes_36_heatmap_df = num_likes_36_heatmap_df.pivot(index='weekday', columns='hour', values='like_count_36_hour') num_likes_36_heatmap_df = num_likes_36_heatmap_df.rename(mapper=mapper, axis=0) num_likes_36_heatmap_df = num_likes_36_heatmap_df.reindex(['Monday', 'Tuesday', 'Wednesday',                                                             'Thursday', 'Friday', 'Saturday', 'Sunday']) plt.figure(figsize=(20, 5)) plt.title(f'Average Likes within 36 hours by Day of Week / Hour of Day ') sns.heatmap(num_likes_36_heatmap_df, cmap='YlGn', cbar=False) Out[14]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2376129c400&gt;</pre> In\u00a0[15]: Copied! <pre># \u53bb\u9664\u7528\u4e0d\u5230\u7684\u6b04\u4f4d\u8cc7\u8a0a\ncv_results = cv_results.drop(columns=[col for col in cv_results.columns if 'split' in col])\n# \u5c07\u6b04\u4f4d\u5167\u5bb9\u5316\u7c21\ndef transform(elem, mapper):\n    if type(elem)==float and isnan(elem):\n        return elem\n    for sub_str in mapper:\n        if sub_str in elem:\n            return mapper[sub_str]\n    return elem\n# resampler\nmapper = {\n    'SMOTE': 'SMOTE',\n    'NearMiss': 'NearMiss'\n}\ncv_results['param_resampler'] = cv_results['param_resampler'].apply(transform, args=(mapper,))\n# classifier\nmapper = {\n    'AdaBoostClassifier': 'AdaBoostClassifier',\n    'XGBClassifier': 'XGBClassifier',\n    'GradientBoostingClassifier': 'GradientBoostingClassifier'\n}\ncv_results['param_classifier'] = cv_results['param_classifier'].apply(transform, args=(mapper,))\n# classifier__base_estimator\nmapper = {\n    'max_depth=1': 'DecisionTreeClassifier(max_depth=1)',\n    'max_depth=2': 'DecisionTreeClassifier(max_depth=2)',\n    'max_depth=3': 'DecisionTreeClassifier(max_depth=3)'\n}\ncv_results['param_classifier__base_estimator'] = cv_results['param_classifier__base_estimator'].apply(transform, args=(mapper,))\n</pre> # \u53bb\u9664\u7528\u4e0d\u5230\u7684\u6b04\u4f4d\u8cc7\u8a0a cv_results = cv_results.drop(columns=[col for col in cv_results.columns if 'split' in col]) # \u5c07\u6b04\u4f4d\u5167\u5bb9\u5316\u7c21 def transform(elem, mapper):     if type(elem)==float and isnan(elem):         return elem     for sub_str in mapper:         if sub_str in elem:             return mapper[sub_str]     return elem # resampler mapper = {     'SMOTE': 'SMOTE',     'NearMiss': 'NearMiss' } cv_results['param_resampler'] = cv_results['param_resampler'].apply(transform, args=(mapper,)) # classifier mapper = {     'AdaBoostClassifier': 'AdaBoostClassifier',     'XGBClassifier': 'XGBClassifier',     'GradientBoostingClassifier': 'GradientBoostingClassifier' } cv_results['param_classifier'] = cv_results['param_classifier'].apply(transform, args=(mapper,)) # classifier__base_estimator mapper = {     'max_depth=1': 'DecisionTreeClassifier(max_depth=1)',     'max_depth=2': 'DecisionTreeClassifier(max_depth=2)',     'max_depth=3': 'DecisionTreeClassifier(max_depth=3)' } cv_results['param_classifier__base_estimator'] = cv_results['param_classifier__base_estimator'].apply(transform, args=(mapper,)) In\u00a0[16]: Copied! <pre>temp = cv_results.groupby(['param_resampler'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean()\ntemp.reset_index(inplace=True)\n\n(ggplot(temp, aes(x='param_resampler', y='mean_test_f1_score', fill='param_resampler'))\n + geom_bar(stat=\"identity\")\n + ylim(0,1)\n + scale_fill_brewer('qualitative', 2)\n + geom_text(aes(label='mean_test_f1_score'), format_string='{:.2f}')\n + ggtitle(f'Average F1 Score by Resampler')\n + labs(fill=f'Resampler')\n + xlab('Resampler')\n + ylab(f'Average F1 score'))\n</pre> temp = cv_results.groupby(['param_resampler'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean() temp.reset_index(inplace=True)  (ggplot(temp, aes(x='param_resampler', y='mean_test_f1_score', fill='param_resampler'))  + geom_bar(stat=\"identity\")  + ylim(0,1)  + scale_fill_brewer('qualitative', 2)  + geom_text(aes(label='mean_test_f1_score'), format_string='{:.2f}')  + ggtitle(f'Average F1 Score by Resampler')  + labs(fill=f'Resampler')  + xlab('Resampler')  + ylab(f'Average F1 score')) Out[16]: <pre>&lt;ggplot: (-9223371884558871573)&gt;</pre> In\u00a0[17]: Copied! <pre>(ggplot(temp, aes(x='param_resampler', y='mean_test_recall', fill='param_resampler'))\n + geom_bar(stat=\"identity\")\n + ylim(0,1)\n + scale_fill_brewer('qualitative', 2)\n + geom_text(aes(label='mean_test_recall'), format_string='{:.2f}')\n + ggtitle(f'Average Recall by Resampler')\n + labs(fill=f'Resampler')\n + xlab('Resampler')\n + ylab(f'Average Recall'))\n</pre> (ggplot(temp, aes(x='param_resampler', y='mean_test_recall', fill='param_resampler'))  + geom_bar(stat=\"identity\")  + ylim(0,1)  + scale_fill_brewer('qualitative', 2)  + geom_text(aes(label='mean_test_recall'), format_string='{:.2f}')  + ggtitle(f'Average Recall by Resampler')  + labs(fill=f'Resampler')  + xlab('Resampler')  + ylab(f'Average Recall')) Out[17]: <pre>&lt;ggplot: (-9223371884558718762)&gt;</pre> In\u00a0[18]: Copied! <pre>(ggplot(temp, aes(x='param_resampler', y='mean_test_precision', fill='param_resampler'))\n + geom_bar(stat=\"identity\")\n + ylim(0,1)\n + scale_fill_brewer('qualitative', 2)\n + geom_text(aes(label='mean_test_precision'), format_string='{:.2f}')\n + ggtitle(f'Average Precision by Resampler')\n + labs(fill=f'Resampler')\n + xlab('Resampler')\n + ylab(f'Average Precision'))\n</pre> (ggplot(temp, aes(x='param_resampler', y='mean_test_precision', fill='param_resampler'))  + geom_bar(stat=\"identity\")  + ylim(0,1)  + scale_fill_brewer('qualitative', 2)  + geom_text(aes(label='mean_test_precision'), format_string='{:.2f}')  + ggtitle(f'Average Precision by Resampler')  + labs(fill=f'Resampler')  + xlab('Resampler')  + ylab(f'Average Precision')) Out[18]: <pre>&lt;ggplot: (152294750826)&gt;</pre> In\u00a0[19]: Copied! <pre>(ggplot(temp, aes(x='param_resampler', y='mean_test_balanced_accuracy', fill='param_resampler'))\n + geom_bar(stat=\"identity\")\n + ylim(0,1)\n + scale_fill_brewer('qualitative', 2)\n + geom_text(aes(label='mean_test_balanced_accuracy'), format_string='{:.2f}')\n + ggtitle(f'Average Balanced Accuracy by Resampler')\n + labs(fill=f'Resampler')\n + xlab('Resampler')\n + ylab(f'Average Balanced Accuracy'))\n</pre> (ggplot(temp, aes(x='param_resampler', y='mean_test_balanced_accuracy', fill='param_resampler'))  + geom_bar(stat=\"identity\")  + ylim(0,1)  + scale_fill_brewer('qualitative', 2)  + geom_text(aes(label='mean_test_balanced_accuracy'), format_string='{:.2f}')  + ggtitle(f'Average Balanced Accuracy by Resampler')  + labs(fill=f'Resampler')  + xlab('Resampler')  + ylab(f'Average Balanced Accuracy')) Out[19]: <pre>&lt;ggplot: (-9223371884547166951)&gt;</pre> In\u00a0[20]: Copied! <pre>temp = cv_results.groupby(['param_resampler', 'param_classifier'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean()\ntemp.reset_index(inplace=True)\n\n(ggplot(temp, aes(x='param_resampler', y='mean_test_f1_score', fill='param_classifier'))\n + geom_bar(position='dodge', stat=\"identity\")\n + ylim(0,1)\n + geom_text(aes(label='mean_test_f1_score'), position=position_dodge(width=0.9), format_string='{:.2f}')\n + ggtitle(f'Average F1 Score by Resampler and Classifier')\n + labs(fill=f'Classifier')\n + xlab('Resampler')\n + ylab(f'Average F1 score'))\n</pre> temp = cv_results.groupby(['param_resampler', 'param_classifier'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean() temp.reset_index(inplace=True)  (ggplot(temp, aes(x='param_resampler', y='mean_test_f1_score', fill='param_classifier'))  + geom_bar(position='dodge', stat=\"identity\")  + ylim(0,1)  + geom_text(aes(label='mean_test_f1_score'), position=position_dodge(width=0.9), format_string='{:.2f}')  + ggtitle(f'Average F1 Score by Resampler and Classifier')  + labs(fill=f'Classifier')  + xlab('Resampler')  + ylab(f'Average F1 score')) Out[20]: <pre>&lt;ggplot: (-9223371884550063342)&gt;</pre> In\u00a0[21]: Copied! <pre>temp = cv_results.groupby(['param_classifier'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean()\ntemp.reset_index(inplace=True)\n\n(ggplot(temp, aes(x='param_classifier', y='mean_test_f1_score', fill='param_classifier'))\n + geom_bar(stat=\"identity\")\n + ylim(0,1)\n + geom_text(aes(label='mean_test_f1_score'), format_string='{:.2f}')\n + ggtitle('Average F1 Score by Classifier')\n + labs(fill='Classifier')\n + xlab('Classifier')\n + ylab(f'Average F1 score'))\n</pre> temp = cv_results.groupby(['param_classifier'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean() temp.reset_index(inplace=True)  (ggplot(temp, aes(x='param_classifier', y='mean_test_f1_score', fill='param_classifier'))  + geom_bar(stat=\"identity\")  + ylim(0,1)  + geom_text(aes(label='mean_test_f1_score'), format_string='{:.2f}')  + ggtitle('Average F1 Score by Classifier')  + labs(fill='Classifier')  + xlab('Classifier')  + ylab(f'Average F1 score')) Out[21]: <pre>&lt;ggplot: (-9223371884545924818)&gt;</pre> In\u00a0[22]: Copied! <pre>temp = cv_results.groupby(['param_classifier', 'param_classifier__n_estimators'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean()\ntemp.reset_index(inplace=True)\n\n(ggplot(temp, aes(x='param_classifier__n_estimators', y='mean_test_f1_score', color='param_classifier'))\n + geom_line()\n + geom_point()\n + ylim(0,1)\n + ggtitle('Average F1 Score by Classifier and Number of Estimators')\n + labs(color='Classifier')\n + xlab('Number of Estimators')\n + ylab('Average F1 score'))\n</pre> temp = cv_results.groupby(['param_classifier', 'param_classifier__n_estimators'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean() temp.reset_index(inplace=True)  (ggplot(temp, aes(x='param_classifier__n_estimators', y='mean_test_f1_score', color='param_classifier'))  + geom_line()  + geom_point()  + ylim(0,1)  + ggtitle('Average F1 Score by Classifier and Number of Estimators')  + labs(color='Classifier')  + xlab('Number of Estimators')  + ylab('Average F1 score')) Out[22]: <pre>&lt;ggplot: (-9223371884546480871)&gt;</pre> In\u00a0[23]: Copied! <pre>cv_results[cv_results['param_resampler']=='passthrough'].groupby(['param_classifier', 'param_classifier__base_estimator'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean()\n</pre> cv_results[cv_results['param_resampler']=='passthrough'].groupby(['param_classifier', 'param_classifier__base_estimator'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean() Out[23]: mean_test_precision mean_test_recall mean_test_specificity mean_test_f1_score mean_test_balanced_accuracy param_classifier param_classifier__base_estimator AdaBoostClassifier DecisionTreeClassifier(max_depth=1) 0.738579 0.436288 0.996339 0.548524 0.716314 DecisionTreeClassifier(max_depth=2) 0.759336 0.443006 0.996670 0.559510 0.719838 DecisionTreeClassifier(max_depth=3) 0.755862 0.441223 0.996619 0.557159 0.718921 In\u00a0[24]: Copied! <pre>cv_results[cv_results['param_resampler']=='passthrough'].groupby(['param_classifier', 'param_classifier__learning_rate'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean()\n</pre> cv_results[cv_results['param_resampler']=='passthrough'].groupby(['param_classifier', 'param_classifier__learning_rate'])['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy'].mean() Out[24]: mean_test_precision mean_test_recall mean_test_specificity mean_test_f1_score mean_test_balanced_accuracy param_classifier param_classifier__learning_rate GradientBoostingClassifier 0.025 0.790585 0.395179 0.997518 0.526909 0.696348 0.050 0.780465 0.423388 0.997177 0.548966 0.710282 0.100 0.778204 0.434859 0.997062 0.557939 0.715961 XGBClassifier 0.025 0.754734 0.404196 0.996884 0.526422 0.700540 0.050 0.776283 0.406060 0.997226 0.533204 0.701643 0.100 0.783911 0.419787 0.997256 0.546763 0.708522 In\u00a0[25]: Copied! <pre>print(cv_results[cv_results['rank_test_f1_score']==1]['params'].values[0])\n</pre> print(cv_results[cv_results['rank_test_f1_score']==1]['params'].values[0]) <pre>{'classifier': AdaBoostClassifier(algorithm='SAMME.R',\n                   base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n                                                         class_weight=None,\n                                                         criterion='gini',\n                                                         max_depth=2,\n                                                         max_features=None,\n                                                         max_leaf_nodes=None,\n                                                         min_impurity_decrease=0.0,\n                                                         min_impurity_split=None,\n                                                         min_samples_leaf=1,\n                                                         min_samples_split=2,\n                                                         min_weight_fraction_leaf=0.0,\n                                                         presort='deprecated',\n                                                         random_state=None,\n                                                         splitter='best'),\n                   learning_rate=1.0, n_estimators=100, random_state=None), 'classifier__base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n                       max_depth=2, max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, presort='deprecated',\n                       random_state=None, splitter='best'), 'classifier__n_estimators': 100, 'resampler': 'passthrough'}\n</pre> In\u00a0[26]: Copied! <pre>temp = cv_results[cv_results['rank_test_f1_score']==1]\ntemp[['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy']]\n</pre> temp = cv_results[cv_results['rank_test_f1_score']==1] temp[['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy']] Out[26]: mean_test_precision mean_test_recall mean_test_specificity mean_test_f1_score mean_test_balanced_accuracy 15 0.759668 0.44419 0.996667 0.560527 0.720429 In\u00a0[27]: Copied! <pre>print(cv_results[cv_results['rank_test_balanced_accuracy']==1]['params'].values[0])\n</pre> print(cv_results[cv_results['rank_test_balanced_accuracy']==1]['params'].values[0]) <pre>{'classifier': GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n                           learning_rate=0.1, loss='deviance', max_depth=3,\n                           max_features=None, max_leaf_nodes=None,\n                           min_impurity_decrease=0.0, min_impurity_split=None,\n                           min_samples_leaf=1, min_samples_split=2,\n                           min_weight_fraction_leaf=0.0, n_estimators=100,\n                           n_iter_no_change=None, presort='deprecated',\n                           random_state=None, subsample=1.0, tol=0.0001,\n                           validation_fraction=0.1, verbose=0,\n                           warm_start=False), 'classifier__learning_rate': 0.025, 'classifier__n_estimators': 120, 'resampler': SMOTE(k_neighbors=5, n_jobs=None, random_state=None, sampling_strategy='auto')}\n</pre> In\u00a0[28]: Copied! <pre>temp = cv_results[cv_results['rank_test_balanced_accuracy']==1]\ntemp[['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy']]\n</pre> temp = cv_results[cv_results['rank_test_balanced_accuracy']==1] temp[['mean_test_precision', 'mean_test_recall', 'mean_test_specificity', 'mean_test_f1_score', 'mean_test_balanced_accuracy']] Out[28]: mean_test_precision mean_test_recall mean_test_specificity mean_test_f1_score mean_test_balanced_accuracy 46 0.199325 0.958312 0.908746 0.330003 0.933529"},{"location":"side-projects/dcard-hw/eda_evaluation/","title":"\u532f\u5165\u76f8\u95dc\u5957\u4ef6\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/","title":"\u4e8b\u524d\u6e96\u5099\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#eda","title":"EDA\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#evaluation","title":"Evaluation\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#resampler","title":"Resampler\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#resampler-classifier","title":"Resampler + Classifier\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#classifier","title":"Classifier\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#classifier-n_estimator","title":"Classifier + n_estimator\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#adaboostclassifier-max_depth","title":"<code>AdaBoostClassifier</code> + <code>max_depth</code>\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#gradientboostingclassifier-xgbclassifier-learning_rate","title":"<code>GradientBoostingClassifier</code>, <code>XGBClassifier</code> + <code>learning_rate</code>\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#best-model","title":"Best Model\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#f1-score","title":"f1-score\u00b6","text":""},{"location":"side-projects/dcard-hw/eda_evaluation/#balanced-accuracy","title":"balanced accuracy\u00b6","text":""},{"location":"side-projects/dcard-hw/docs/1-eda/","title":"Exploratory Data Analysis (EDA)","text":"<p>The dataset is divided into training and testing sets. To avoid data leakage, only the training set is analyzed during EDA, leaving the testing set aside.</p> <p>When we first receive a dataset, the initial step is to examine its details, including the number of records and columns in each table. Below is the dataset information as of the update on 2020/04/13:</p> <pre><code>posts_train                Contains 793,751 records and 3 columns\npost_shared_train          Contains 304,260 records and 3 columns\npost_comment_created_train Contains 2,372,228 records and 3 columns\npost_liked_train           Contains 3,395,903 records and 3 columns\npost_collected_train       Contains 1,235,126 records and 3 columns\n</code></pre> <p>The training set covers posts from April 1, 2019, to the end of October 2019, spanning approximately seven months with around 793,000 posts. The goal is to build a predictive model that uses 10-hour post metrics (e.g., shares, comments, likes, and saves) to predict whether a post will receive 1,000 likes within 36 hours, classifying it as a \"popular post.\"</p> <p>Approximately 2.32% of the training posts are popular, equating to about 18,000 posts. This imbalance in the dataset necessitates techniques like over/undersampling during preprocessing and alternative evaluation metrics during model assessment.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#problem-definition","title":"Problem Definition","text":"<p>The task can be approached in four ways, based on \"whether sequence information is considered\" and \"whether the problem is framed as regression or binary classification\":</p> Regression Binary Classification With Sequence Info RNNs (e.g., GRU), traditional time series models (e.g., ARMA, ARIMA) Same as left Without Sequence Info Poisson regression, SVM, tree-based models, etc. Logistic regression, SVM, tree-based models, etc. <p>For simplicity and time constraints, we focus on \"without sequence info\" and \"binary classification,\" aggregating 10-hour metrics and building a binary classification model to predict popular posts. The focus will be on handling imbalanced data, tree-based models, and subsequent discussions.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#relationships-between-variables","title":"Relationships Between Variables","text":"<p>We simplify the dataset to include total shares, comments, likes, and saves within 10 hours and use a heatmap to observe their relationships with the total likes within 36 hours:</p> <p></p> <p>Info</p> <p>Key observations from the heatmap:</p> <ul> <li>Total likes within 36 hours moderately correlate with total likes within 10 hours (.58), shares (.36), and saves (.36), but weakly with comments (.17).</li> <li>Total likes within 10 hours moderately correlate with shares (.63) and saves (.61).</li> <li>Shares and saves within 10 hours moderately correlate (.48).</li> </ul> <p>In simple terms, posts with more likes within 10 hours tend to have more shares and saves. However, the strongest predictor of total likes within 36 hours is the likes within 10 hours. Comments show little correlation with total likes.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#heatmaps-of-key-metrics","title":"Heatmaps of Key Metrics","text":"<p>Danger</p> <p>To protect Dcard's proprietary information, color bars (<code>cbar=False</code>) are omitted, showing only relative relationships.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#total-posts-by-time","title":"Total Posts by Time","text":"<p>We examine whether the number of posts varies across different time periods:</p> <p></p> <p>The x-axis represents 24 hours, and the y-axis represents days of the week.</p> <p>Info</p> <p>Observations:</p> <ul> <li>Posts are concentrated during midday, afternoon, and evening (12:00\u201318:00), with weekdays slightly higher than weekends.</li> <li>The second-highest posting period is weekday mornings (05:00\u201312:00).</li> <li>Posts are relatively fewer during evenings (18:00\u201301:00) on both weekdays and weekends.</li> </ul> <p>These trends are reasonable, as students primarily post during the day. The relatively high number of early morning posts might be due to companies posting content before students wake up.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#popular-post-proportion-by-time","title":"Popular Post Proportion by Time","text":"<p>Next, we analyze whether certain time periods have a higher proportion of popular posts:</p> <p></p> <p>Info</p> <p>Observations:</p> <ul> <li>Posts during late-night and early-morning hours on weekends have a higher likelihood of being popular, likely due to increased user activity during these times.</li> <li>The heatmap confirms that the proportion of popular posts varies by time.</li> </ul>"},{"location":"side-projects/dcard-hw/docs/1-eda/#average-likes-within-10-hours-by-time","title":"Average Likes Within 10 Hours by Time","text":"<p>We then examine the average likes within 10 hours for posts made at different times:</p> <p></p> <p>Info</p> <p>Observations:</p> <ul> <li>Posts made between 21:00\u201311:00 generally receive more likes within 10 hours.</li> <li>Posts made between 11:00\u201321:00, especially during late afternoon and dinner hours, receive fewer likes on average.</li> </ul> <p>This difference might be because students are less active during late afternoon and dinner hours but more active during the evening. Early morning posts are also visible to students the next day.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#average-likes-within-36-hours-by-time","title":"Average Likes Within 36 Hours by Time","text":"<p>Finally, we analyze the average likes within 36 hours for posts made at different times:</p> <p></p> <p>The trends are consistent with the 10-hour analysis and are not elaborated further.</p>"},{"location":"side-projects/dcard-hw/docs/1-eda/#summary","title":"Summary","text":"<p>Info</p> <p>Key takeaways:</p> <ul> <li>Variables are generally highly correlated. Polynomial transformations (<code>PolynomialFeatures</code>) may not yield significant improvements during feature engineering.</li> <li>Posting time significantly impacts the proportion of popular posts and the number of likes, and this information should be incorporated into the model.</li> </ul>"},{"location":"side-projects/dcard-hw/docs/2-training/","title":"Feature Engineering","text":"<p>After conducting exploratory data analysis (EDA), we gained a deeper understanding of the training dataset. Before diving into feature engineering, we first organize the training dataset into the following format:</p> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 793751 entries, 0 to 793750\nData columns (total 8 columns):\npost_key           793751 non-null object\nshared_count       793751 non-null int64\ncomment_count      793751 non-null int64\nlike_count         793751 non-null int64\ncollected_count    793751 non-null int64\nweekday            793751 non-null int64\nhour               793751 non-null int64\nis_trending        793751 non-null int64\ndtypes: bool(1), int64(6), object(1)\nmemory usage: 43.1+ MB\n</code></pre> <p>In this section, we will discuss the techniques and models used throughout the data pipeline, including over/undersampling, polynomial transformations, one-hot encoding, and tree-based models.</p> <p>Info</p> <p>The training process can be divided into three main stages:</p> <ol> <li>Resampling</li> <li>Column Transformation</li> <li>Classification</li> </ol> <p>These stages can be represented as the following <code>Pipeline</code> object:</p> <pre><code>cachedir = mkdtemp()\npipe = Pipeline(steps=[('resampler', 'passthrough'),\n                       # ('columntransformer', 'passthrough'),\n                       ('classifier', 'passthrough')],\n                memory=cachedir)\n</code></pre> <p>For each stage, we experiment with two to three different approaches and several hyperparameter settings to identify the optimal combination.</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#handling-imbalanced-datasets-stage-1","title":"Handling Imbalanced Datasets (STAGE 1)","text":"<p>In a binary classification problem, an imbalanced dataset refers to a scenario where the target variable (\\(y\\)) is predominantly of one class (majority) with only a small proportion belonging to the other class (minority). </p> <p>Training a model on such a dataset without addressing the imbalance often results in a biased model that predicts most samples as the majority class, ignoring valuable information from the minority class.</p> <p>A potential solution is resampling, which can be categorized into oversampling and undersampling:</p> <ul> <li>Oversampling: Increases the proportion of minority samples in the dataset.</li> <li>Undersampling: Reduces the proportion of majority samples in the dataset.</li> </ul> <p>Both methods help the model pay more attention to minority samples during training. The simplest approach is random sampling, where majority samples are removed, or minority samples are duplicated.</p> <p>The <code>imblearn</code> library provides implementations for various resampling techniques, including <code>RandomOverSampler</code> and <code>RandomUnderSampler</code>. Additionally, we utilize <code>SMOTE</code> and <code>NearMiss</code>. Below is a brief overview:</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#smote","title":"SMOTE","text":"<p>SMOTE (Synthetic Minority Oversampling Technique) is an oversampling method that synthesizes new minority samples between existing ones, increasing the proportion of the minority class. The following diagram illustrates this:</p> <p> Image Source</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#nearmiss","title":"NearMiss","text":"<p>NearMiss is an undersampling method with three versions. We focus on NearMiss-1, which calculates the average distance of all majority samples to their \\(k\\) nearest minority neighbors and removes the majority samples closest to the minority samples until the class ratio is 1:1. The following diagram illustrates this:</p> <p> Image Source</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#polynomial-transformation-and-one-hot-encoding-stage-2","title":"Polynomial Transformation and One-hot Encoding (STAGE 2)","text":"<p>Next, we use <code>sklearn</code>'s <code>PolynomialFeatures</code> and <code>OneHotEncoder</code> to transform specific features:</p> <ul> <li>For <code>shared_count</code>, <code>comment_count</code>, <code>liked_count</code>, and <code>collected_count</code>, we apply second-degree polynomial transformations to capture non-linear relationships and interactions between features.</li> <li>For the <code>weekday</code> feature, we convert integer values (<code>0</code> - <code>6</code>, representing Monday to Sunday) into one-hot encoded vectors, e.g., <code>[1, 0, 0, 0, 0, 0, 0]</code> for Monday.</li> </ul>"},{"location":"side-projects/dcard-hw/docs/2-training/#tree-based-ensemble-models-stage-3","title":"Tree-based Ensemble Models (STAGE 3)","text":"<p>For classification, we primarily use tree-based ensemble models, including <code>AdaBoostClassifier</code>, <code>GradientBoostingClassifier</code>, and <code>XGBClassifier</code>. These models are chosen for several reasons:</p> <ul> <li>They are invariant to monotonic transformations of features, reducing the need for extensive feature engineering.</li> <li>They offer high interpretability, making it easier to understand feature importance.</li> <li>They perform well on large and complex datasets and are often top performers in Kaggle competitions (e.g., <code>XGBoost</code>, <code>LightGBM</code>, <code>CatBoost</code>).</li> </ul> <p>Ensemble learning can be categorized into Bagging (bootstrap aggregating) and Boosting.</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#bagging","title":"Bagging","text":"<p>The most well-known Bagging application is Random Forest, which builds multiple decision trees using bootstrap sampling and random feature selection. Each tree learns a subset of features, and their predictions are aggregated for the final result.</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#boosting","title":"Boosting","text":""},{"location":"side-projects/dcard-hw/docs/2-training/#adaptive-boosting","title":"Adaptive Boosting","text":"<p>Adaptive Boosting (AdaBoost) sequentially builds \\(T\\) weak learners \\(h_t(x)\\), with each model focusing on samples misclassified by the previous one. Each model is assigned a weight \\(\\alpha_t\\) based on its performance:</p> <ul> <li>Higher weights indicate better performance.</li> <li>Lower weights indicate worse performance.</li> </ul> <p>The final model \\(H(x)\\) aggregates the predictions of all \\(T\\) weak learners. For more details, refer to this note.</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#gradient-boosting","title":"Gradient Boosting","text":"<p>Gradient Boosting builds \\(T\\) models \\(h_t(x)\\) sequentially, where each model predicts the gradient (pseudo-residuals) of the previous model's errors. The final model \\(H(x)\\) is the sum of all previous models. For mathematical derivations, refer to this note.</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#extreme-gradient-boosting","title":"Extreme Gradient Boosting","text":"<p>XGBoost is an optimized implementation of Gradient Boosting with enhancements like weighted quantile sketch, parallel learning, and cache-aware access. For more details, refer to this paper.</p>"},{"location":"side-projects/dcard-hw/docs/2-training/#hyperparameter-optimization","title":"Hyperparameter Optimization","text":"<p>After understanding the techniques used in each stage, we define the possible methods and hyperparameter combinations for each stage as follows:</p> <p>Resampler - <code>passthrough</code>: No resampling. - <code>NearMiss</code>: Default parameters. - <code>SMOTE</code>: Default parameters.</p> <p>Column Transformer - <code>passthrough</code>: No feature transformation. - <code>col_trans</code>: Apply polynomial transformations and one-hot encoding.</p> <p>Classifier - <code>AdaBoostClassifier</code>: Default parameters with tree depth limited to <code>[1, 2, 3]</code>. - <code>GradientBoostingClassifier</code>, <code>XGBClassifier</code>: Default parameters with learning rates <code>[0.025, 0.05, 0.1]</code>.</p> <p>For all classifiers, we set the number of decision trees to <code>[90, 100, 110, 120]</code> and tune additional hyperparameters.</p> <p>Initially, there are 216 combinations to test, which is too many given time constraints. Experiments show that models with feature transformations generally perform worse, likely due to high feature correlations identified during EDA. As a result, we omit the \"Feature Transformation\" stage, reducing the combinations to 108. We use <code>GridSearchCV</code> with <code>cv=3</code> to find the best combination.</p> <p>The parameter grid is defined as follows:</p> <pre><code># poly_cols = ['shared_count', 'comment_count', 'liked_count', 'collected_count']\n# col_trans = make_column_transformer((OneHotEncoder(dtype='int'), ['weekday']),\n#                                     (PolynomialFeatures(include_bias=False), poly_cols),\n#                                     remainder='passthrough')\nparam_grid_ada = {\n    'resampler': ['passthrough', SMOTE(), NearMiss()],\n    # 'columntransformer': ['passthrough', col_trans],\n    'classifier': [AdaBoostClassifier()],\n    'classifier__n_estimators': [90, 100, 110, 120],\n    'classifier__base_estimator': [DecisionTreeClassifier(max_depth=1), \n                                   DecisionTreeClassifier(max_depth=2),\n                                   DecisionTreeClassifier(max_depth=3)]\n}\nparam_grid_gb = {\n    'resampler': ['passthrough', SMOTE(), NearMiss()],\n    # 'columntransformer': ['passthrough', col_trans],\n    'classifier': [GradientBoostingClassifier(), XGBClassifier()],\n    'classifier__n_estimators': [90, 100, 110, 120],\n    'classifier__learning_rate': [0.025, 0.05, 0.1]\n}\nparam_grid = [param_grid_ada, param_grid_gb]\n</code></pre>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/","title":"Results and Discussion","text":""},{"location":"side-projects/dcard-hw/docs/3-evaluation/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>Before discussing the results, let us revisit some commonly used metrics for binary classification, explained using this assignment as an example:</p> Actual\uff3cPredicted Negative Positive Negative \\(\\color{red}{\\text{TN}}\\) \\(\\color{blue}{\\text{FP}}\\) Positive \\(\\color{green}{\\text{FN}}\\) \\(\\color{orange}{\\text{TP}}\\) <p>\\(\\text{Precision}\\): Measures the proportion of articles predicted as popular that are actually popular. Higher values indicate greater trust in the model's predictions for popular articles. Formula: $$ \\text{Precision} = \\frac{\\color{orange}{\\text{TP}}}{\\color{blue}{\\text{FP}} + \\color{orange}{\\text{TP}}} $$</p> <p>\\(\\text{Recall}\\): Measures the proportion of actual popular articles that are correctly predicted by the model. Also known as True Positive Rate (TPR) or Sensitivity. Higher values indicate the model's ability to capture actual popular articles. Formula: $$ \\text{Recall} = \\dfrac{\\color{orange}{\\text{TP}}}{\\color{green}{\\text{FN}} + \\color{orange}{\\text{TP}}} $$ </p> <p>\\(\\text{Specificity}\\): Measures the proportion of actual non-popular articles that are correctly predicted by the model. Also known as True Negative Rate (TNR). Higher values indicate the model's ability to capture actual non-popular articles. Formula: $$ \\text{Specificity} = \\dfrac{\\color{red}{\\text{TN}}}{\\color{red}{\\text{TN}}+\\color{blue}{\\text{FP}}} $$</p> <p>\\(\\text{F1-score}\\): A harmonic mean of \\(\\text{Precision}\\) and \\(\\text{Recall}\\), ranging from \\(0\\) to \\(1\\), with higher values being better. Formula: $$ \\text{F1-score} = \\dfrac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$</p> <p>\\(\\text{Balanced Acc.}\\): A combined metric of \\(\\text{TPR}\\) and \\(\\text{TNR}\\), ranging from \\(0\\) to \\(1\\), with higher values being better. Formula: $$ \\text{Balanced Acc.} = \\dfrac{\\text{TNR} + \\text{TPR}}{2} $$</p> <p>When using <code>GridSearchCV</code> to find the best parameter combination, we record these five metrics and select the best combination based on the f1-score. Example code: <pre><code>scoring = {\n    'precision': 'precision',\n    'recall': 'recall',\n    'specificity': make_scorer(specificity_score),\n    'balanced_accuracy': 'balanced_accuracy',\n    'f1_score': 'f1',\n}\ngrid_search = GridSearchCV(pipe, param_grid=param_grid, scoring=scoring, refit='f1_score', \n                           n_jobs=-1, cv=3, return_train_score=True)\n</code></pre></p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#experimental-results","title":"Experimental Results","text":"<p>Info</p> <p>The best model is <code>AdaBoostClassifier</code> without any resampling, consisting of 100 decision trees with a maximum depth of 2. The average f1-score during cross-validation is 0.56, while the f1-score on the public test set is 0.53. Detailed prediction information is as follows:</p> <p></p> Note <pre><code>===================GETTING CONNECTOR START!==================\n============================DONE!============================\n====================GETTING TABLES START!====================\nposts_test                 Total:   225,986 rows, 3 columns\npost_shared_test           Total:    83,376 rows, 3 columns\npost_comment_created_test  Total:   607,251 rows, 3 columns\npost_liked_test            Total:   908,910 rows, 3 columns\npost_collected_test        Total:   275,073 rows, 3 columns\n============================DONE!============================\n====================MERGING TABLES START!====================\n============================DONE!============================\n================PREPROCESSING TOTAL_DF START!================\n============================DONE!============================\n==================PREDICTING TESTSET START!==================\nf1-score     = 0.53\nbalanced acc = 0.70\n\n            precision    recall  f1-score   support\n\n        0       0.99      1.00      0.99    221479\n        1       0.75      0.40      0.53      4507\n\n    accuracy                           0.99    225986\nmacro avg       0.87      0.70      0.76    225986\nweighted avg       0.98      0.99      0.98    225986\n\n============================DONE!============================\n</code></pre> <p>Now, let us analyze the experimental results. (All figures below are based on cross-validation results, not the entire training set or public test set.)</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#resampler","title":"Resampler","text":"<p>First, let us examine how different resampling strategies affect the f1-score: </p> <p>Info</p> <p>Different resampling strategies indeed affect the f1-score:</p> <ul> <li>NearMiss (undersampling) has the lowest f1-score, likely due to excessive removal of non-popular articles, losing too much majority class information.</li> <li>SMOTE (oversampling) achieves a moderate f1-score.</li> <li>No resampling achieves the highest f1-score.</li> </ul> <p>Next, we investigate how these resampling strategies impact precision and recall:</p> <p></p> <p>Info</p> <ul> <li>NearMiss and SMOTE significantly increase the model's focus on the minority class, resulting in excellent recall scores of 0.91 and 0.95, respectively. However, this comes at the cost of precision, which drops to 0.07 and 0.20, respectively.</li> <li>In other words, resampling strategies can capture actual popular articles but reduce the trustworthiness of the predicted popular articles.</li> </ul> <p>We further explore whether resampling strategies interact with different classifiers to influence the f1-score:</p> <p></p> <p>Info</p> <ul> <li>Under \"SMOTE\" and \"No Resampling\" strategies, different classifiers do not significantly affect the f1-score.</li> <li>However, under the NearMiss strategy, <code>XGBClassifier</code> achieves the highest f1-score (0.18), while <code>AdaBoostClassifier</code> has the lowest (0.07).<ul> <li><code>AdaBoostClassifier</code> performs poorly because it relies on weak classifiers, which struggle with limited majority class information.</li> <li><code>XGBClassifier</code> outperforms <code>GradientBoostingClassifier</code> due to its optimized GBDT implementation.</li> </ul> </li> </ul>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#classifier","title":"Classifier","text":"<p>Next, let us examine how different classifiers affect the f1-score:</p> <p></p> <p>Info</p> <ul> <li>Different classifiers have minimal impact on the f1-score. On average, <code>XGBClassifier</code> achieves the highest score (0.35), primarily due to its performance under the NearMiss strategy.</li> </ul> <p>Finally, we analyze whether the number of internal classifiers in ensemble models affects the f1-score:</p> <p></p> <p>Clearly, the number of classifiers has little impact. Similarly, the tree depth for <code>AdaBoostClassifier</code> and the learning rate for the other two models also have minimal impact on the f1-score (figures omitted).</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#future-directions","title":"Future Directions","text":"<p>The experimental results are summarized above. Due to time constraints, additional attempts were not included. Potential future directions are outlined below:</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#explore-other-resampling-techniques","title":"Explore Other Resampling Techniques","text":"<p>Resampling techniques can increase the model's focus on the minority class. Although the experimental results were not ideal, we can continue fine-tuning hyperparameters or exploring other resampling techniques. Refer to the \"Over-sampling\" and \"Under-sampling\" sections of the <code>imblearn</code> User Guide for potential directions.</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#consider-other-evaluation-metrics","title":"Consider Other Evaluation Metrics","text":"<p>The assignment requires using f1-score as the evaluation metric. However, if we use balanced accuracy instead, the best model would be a <code>GradientBoostingClassifier</code> trained with SMOTE, consisting of 120 classifiers and a learning rate of 0.1, achieving a balanced accuracy of 0.93.</p> <p>The impact of different resampling strategies on balanced accuracy is shown below:</p> <p></p> <p>Info</p> <p>SMOTE achieves the highest balanced accuracy. If the goal is to preliminarily identify potentially popular articles for subsequent workflows, balanced accuracy might be a better evaluation metric.</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#explore-other-feature-transformations-and-classifiers","title":"Explore Other Feature Transformations and Classifiers","text":"<p>The experiment only considered tree-based ensemble models, which require minimal feature transformation. However, we could explore logistic regression, support vector machines, Poisson regression, etc., combined with effective feature transformations. For example, converting <code>weekday</code> and <code>hour</code> into circular coordinates (refer to this post) could improve model performance.</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#incorporate-sequential-information","title":"Incorporate Sequential Information","text":"<p>The experiment ignored the \"time trends\" of shares, comments, likes, and collections within 10 hours of posting. One potential direction is to use recurrent neural networks (RNN, LSTM, GRU, etc.) to capture these trends and nonlinear relationships between variables.</p> <p>A simple approach is to combine the four count variables into a 4-dimensional vector (e.g., <code>[4, 23, 17, 0]</code> for 4 shares, 23 comments, etc.), with a sequence length of 10. Each article's sequential information would then be a <code>(10, 4)</code> matrix, which can be fed into the model for training.</p> <p>For details on LSTM models, refer to my notes.</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#explore-other-hyperparameter-optimization-methods","title":"Explore Other Hyperparameter Optimization Methods","text":"<p>The experiment used <code>GridSearchCV</code> for hyperparameter optimization. However, <code>RandomizedSearchCV</code> might be a better choice for optimizing a large number of hyperparameter combinations. Refer to this 2012 JMLR paper for details.</p> <p>Additionally, consider Bayesian optimization implementations provided by <code>optuna</code> or <code>hyperopt</code>. Watch this video for details, and compare the two libraries in this article.</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#incorporate-text-data-and-user-behavior","title":"Incorporate Text Data and User Behavior","text":"<p>The assignment does not include text data or user behavior. Since the ultimate goal is to \"recommend articles more accurately to users,\" consider incorporating Latent Dirichlet Allocation (LDA) topic modeling to enrich article topic information. Refer to my presentation for details on LDA.</p> <p>Additionally, combining user behavior data could enable more refined personalized text recommendations. Refer to this video and this paper for details.</p>"},{"location":"side-projects/dcard-hw/docs/3-evaluation/#tags-dcard","title":"tags: <code>dcard</code>","text":""},{"location":"side-projects/dcard-hw/docs/4-resampling/","title":"Resampling","text":""},{"location":"side-projects/dcard-hw/docs/4-resampling/#over-sampling","title":"Over Sampling","text":""},{"location":"side-projects/dcard-hw/docs/4-resampling/#under-sampling","title":"Under Sampling","text":""},{"location":"side-projects/leetcode/","title":"Problem Lists","text":"<ul> <li>15 Patterns</li> <li>Blind 75</li> <li>Grind 75</li> <li>SQL 50</li> <li>Advanced SQL 50</li> </ul>"},{"location":"side-projects/leetcode/15-patterns/","title":"LeetCode 15 Patterns","text":"<p>Two Pointers</p> <ul> <li> 1. Two Sum</li> <li> 167. Two Sum II - Input Array Is Sorted</li> <li> 15. 3Sum</li> <li> 11. Container With Most Water</li> </ul> <p>Overlapping Intervals</p> <ul> <li> 56. Merge Intervals</li> <li> 57. Insert Interval</li> <li> 435. Non-overlapping Intervals</li> </ul> <p>Sliding Window</p> <ul> <li> 643. Maximum Average Subarray I</li> <li> 3. Longest Substring Without Repeating Characters</li> <li> 76. Minimum Window Substring</li> </ul> <p>Prefix Sum</p> <ul> <li> 303. Range Sum Query - Immutable</li> <li> 525. Contiguous Array</li> <li> 560. Subarray Sum Equals K</li> </ul> <p>Fast and Slow Pointers</p> <ul> <li> 141. Linked List Cycle</li> <li> 202. Happy Number</li> <li> 287. Find the Duplicate Number</li> </ul> <p>Linked List In-place Reversal</p> <ul> <li> 206. Reverse Linked List</li> <li> 92. Reverse Linked List II</li> <li> 24. Swap Nodes in Pairs</li> </ul> <p>Monotonic Stack</p> <ul> <li> 496. Next Greater Element I</li> <li> 739. Daily Temperatures</li> <li> 84. Largest Rectangle in Histogram</li> </ul> <p>Top K Elements</p> <ul> <li> 215. Kth Largest Element in an Array</li> <li> 347. Top K Frequent Elements</li> <li> 373. Find K Pairs with Smallest Sums</li> </ul> <p>Dynamic Programming</p> <ul> <li> 70. Climbing Stairs</li> <li> 322. Coin Change</li> <li> 300. Longest Increasing Subsequence</li> <li> 312. Burst Balloons</li> <li> 416. Partition Equal Subset Sum</li> <li> 1143. Longest Common Subsequence</li> </ul> <p>Backtracking</p> <ul> <li> 46. Permutations</li> <li> 78. Subsets</li> <li> 51. N-Queens</li> </ul> <p>Binary Search</p> <ul> <li> 374. Guess Number Higher or Lower</li> <li> 2300. Successful Pairs of Spells and Potions</li> <li> 162. Find Peak Element</li> <li> 875. Koko Eating Bananas</li> </ul> <p>Modified Binary Search</p> <ul> <li> 33. Search in Rotated Sorted Array</li> <li> 153. Find Minimum in Rotated Sorted Array</li> <li> 240. Search a 2D Matrix II</li> </ul> <p>Binary Tree Traversal</p> <ul> <li> 257. Binary Tree Paths</li> <li> 230. Kth Smallest Element in a BST</li> <li> 124. Binary Tree Maximum Path Sum</li> <li> 107. Binary Tree Level Order Traversal II</li> </ul> <p>DFS</p> <ul> <li> 133. Clone Graph</li> <li> 113. Path Sum II</li> <li> 210. Course Schedule II</li> </ul> <p>BFS</p> <ul> <li> 102. Binary Tree Level Order Traversal</li> <li> 994. Rotting Oranges</li> <li> 127. Word Ladder</li> </ul> <p>Matrix Traversal</p> <ul> <li> 733. Flood Fill</li> <li> 200. Number of Islands</li> <li> 130. Surrounded Regions</li> </ul>"},{"location":"side-projects/leetcode/blind-75/","title":"Blind 75","text":"<p>https://neetcode.io/practice?tab=blind75</p>"},{"location":"side-projects/leetcode/grind-75/","title":"Grind 75","text":"<p>Grind 75 questions</p>"},{"location":"side-projects/leetcode/sql-50-advanced/","title":"Advanced SQL 50","text":"<p>Advanced Topics: Window Function and CTE</p> <ul> <li> 1767. Find The Subtasks That Did Not Execute</li> <li> 1225. Report Contiguous Dates</li> </ul>"},{"location":"side-projects/leetcode/sql-50/","title":"SQL 50","text":""},{"location":"side-projects/leetcode/template-sql/","title":"Question","text":""},{"location":"side-projects/leetcode/template-sql/#summary","title":"Summary","text":""},{"location":"side-projects/leetcode/template-sql/#implementation","title":"Implementation","text":"PostgreSQLPandas"},{"location":"side-projects/leetcode/template/","title":"Question","text":""},{"location":"side-projects/leetcode/template/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Brute Force Lorem ipsum dolor Lorem ipsum dolor"},{"location":"side-projects/leetcode/template/#implementation","title":"Implementation","text":"PythonJavaScriptGo"},{"location":"side-projects/leetcode/1-two-sum/","title":"1. Two Sum","text":""},{"location":"side-projects/leetcode/1-two-sum/#summary","title":"Summary","text":"Solution Approach Explanation Time Complexity Space Complexity Brute Force (Nested Loop) Check every possible pair of numbers to see if they add up to the target. This approach is straightforward but slow. \\(O(n^2)\\): We need to check all pairs, which requires a nested loop. \\(O(1)\\): Constant space as we don't need extra storage. Two-pointer technique (Sorted Array) Sort the array and use two pointers (one at the start, one at the end). Move pointers based on the sum comparison to target. \\(O(n\\log n)\\): Sorting takes \\(O(n \\log n)\\), and a linear scan is \\(O(n)\\). \\(O(1)\\): Only pointers, no extra data structures needed. Hash Map (One-pass) Traverse the array while storing each number in a hash map. For each number, check if the complement (target - number) is already in the map. \\(O(n)\\): Single pass through the array to check and store elements. \\(O(n)\\): Extra space for storing up to <code>n</code> elements in the hash map."},{"location":"side-projects/leetcode/1-two-sum/#implementation","title":"Implementation","text":"Python <pre><code># Reminders\n# 1. exactly one solution\n# 2. may not use the same element twice\n# 3. return indices\n# 4. input: may be some duplicate values\n# 5. return the answer in any order.\n\n# Naive approach\n# Nested for-loop\n# - outer loop: go through the array from the first element to the second last.\n# - inner loop: go through the array from the element that is next to the element that the outer loop focus on to the last\n# time complexity: O(n^2)\n# space complexity: O(1)\n\n\n# Hashmap approach\n# time complexity: O(n)\n# space complexity: O(n)\n# Explanation:\n# 1. go through the array, store the complement and the index of it for each number in dict\n# 2. if the number is in the dict, then we found the answer\n\n\n# d = {2:0}\n# target = 9\n# complement = 2\n#           &gt;\n# nums = [2,7,11,15]\n\nclass Solution:\n    def twoSum(self, nums: List[int], target: int) -&gt; List[int]:\n        num_to_idx = {}\n        for i, num in enumerate(nums):\n            complement = target - num\n            if complement in num_to_idx:\n                return [num_to_idx[complement], i]\n            num_to_idx[num] = i\n        return []\n</code></pre>"},{"location":"side-projects/leetcode/1004-max-consecutive-ones-iii/","title":"1004. Max Consecutive Ones Iii","text":""},{"location":"side-projects/leetcode/102-binary-tree-level-order-traversal/","title":"102. Binary Tree Level Order Traversal","text":""},{"location":"side-projects/leetcode/102-binary-tree-level-order-traversal/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity BFS with Queue Use a queue to traverse nodes level by level. For each level, process all current nodes in queue, add their children for next level, and collect values. \\(O(n)\\) \\(O(w)\\) where \\(w\\) is max width"},{"location":"side-projects/leetcode/102-binary-tree-level-order-traversal/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code># Definition for a binary tree node.\n# class TreeNode:\n#     def __init__(self, val=0, left=None, right=None):\n#         self.val = val\n#         self.left = left\n#         self.right = right\nfrom collections import deque\nclass Solution:\n    def levelOrder(self, root: Optional[TreeNode]) -&gt; List[List[int]]:\n        result = []\n\n        if root == None:\n            return result\n\n        queue = deque()\n        queue.append(root)\n        while queue:\n            nodes = []\n            for _ in range(len(queue)):\n                n = queue.popleft()\n                if n.left:\n                    queue.append(n.left)\n                if n.right:\n                    queue.append(n.right)\n                nodes.append(n.val)\n            result.append(nodes)\n\n        return result\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"side-projects/leetcode/104-maximum-depth-of-binary-tree/","title":"104. Maximum Depth Of Binary Tree","text":""},{"location":"side-projects/leetcode/107-binary-tree-level-order-traversal-ii/","title":"107. Binary Tree Level Order Traversal II","text":"<p>Video</p>"},{"location":"side-projects/leetcode/1071-greatest-common-divisor-of-strings/","title":"1071. Greatest Common Divisor Of Strings","text":""},{"location":"side-projects/leetcode/11-container-with-most-water/","title":"11. Container With Most Water","text":""},{"location":"side-projects/leetcode/11-container-with-most-water/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Brute Force (Nested Loop) Calculate the area for every possible pair of lines using nested loops. Keep track of the maximum area found. This approach is straightforward but inefficient. \\(O(n\u00b2)\\): Double loop to check all pairs. \\(O(1)\\): Constant space for variables only. Two-pointer technique Use two pointers, one at each end of the array, to represent the lines. Calculate the area and move the pointer with the shorter line inward to potentially find a larger area. Continue until the pointers meet. This approach is efficient and leverages the idea that a shorter line limits the maximum area. \\(O(n)\\): Single pass as we move pointers inward. \\(O(1)\\): Constant space with only two pointers."},{"location":"side-projects/leetcode/11-container-with-most-water/#implementation","title":"Implementation","text":"Python <pre><code>class Solution:\n    def maxArea(self, height: List[int]) -&gt; int:\n        l = 0\n        r = len(height) - 1\n        max_area = 0\n        while l &lt; r:\n            curr_area = (r - l) * min(height[l], height[r])\n            max_area = max(max_area, curr_area)\n\n            if height[l] &lt; height[r]:\n                l += 1\n            else:\n                r -= 1\n        return max_area\n</code></pre>"},{"location":"side-projects/leetcode/113-path-sum-ii/","title":"113. Path Sum II","text":""},{"location":"side-projects/leetcode/1137-n-th-tribonacci-number/","title":"1137. N Th Tribonacci Number","text":""},{"location":"side-projects/leetcode/1143-longest-common-subsequence/","title":"1143. Longest Common Subsequence","text":""},{"location":"side-projects/leetcode/1161-maximum-level-sum-of-a-binary-tree/","title":"1161. Maximum Level Sum Of A Binary Tree","text":""},{"location":"side-projects/leetcode/1207-unique-number-of-occurrences/","title":"1207. Unique Number Of Occurrences","text":""},{"location":"side-projects/leetcode/1225-report-contiguous-dates/","title":"1225. Report Contiguous Dates","text":""},{"location":"side-projects/leetcode/1225-report-contiguous-dates/#summary","title":"Summary","text":"<p>The solution uses a clever grouping technique to identify contiguous date periods:</p> <ol> <li>Union: Combine failed and succeeded dates into a single dataset with period state labels</li> <li>Rank: Assign sequential ranks to dates within each period state (failed/succeeded)</li> <li>Group: Calculate <code>date - rank * interval '1 day'</code> - this produces the same value for all contiguous dates</li> <li>Aggregate: Group by the calculated value and find min/max dates for each contiguous period</li> </ol> <p>The key insight is that subtracting the rank from each date creates a constant identifier for contiguous sequences, making it easy to group and extract date ranges.</p>"},{"location":"side-projects/leetcode/1225-report-contiguous-dates/#implementation","title":"Implementation","text":"PostgreSQLPandas <pre><code>-- Write your PostgreSQL query statement below\nwith cte as (\n    select\n      fail_date as date,\n      'failed' as period_state\n    from failed\n    union\n    select\n      success_date as date,\n      'succeeded' as period_state\n    from succeeded\n),\ncte2 as (\n  select\n    date,\n    period_state,\n    rank() over (partition by period_state order by date asc) as rank\n  from cte\n  where date between '2019-01-01' and '2019-12-31'\n),\ncte3 as (\n  select \n    *,\n    date - rank * interval '1 day' AS grp\n  from cte2\n)\n\nselect\n  period_state,\n  min(date) as start_date,\n  max(date) as end_date\nfrom cte3\ngroup by grp, period_state\norder by start_date\n</code></pre> <pre><code>\n</code></pre>"},{"location":"side-projects/leetcode/124-binary-tree-maximum-path-sum/","title":"124. Binary Tree Maximum Path Sum","text":""},{"location":"side-projects/leetcode/124-binary-tree-maximum-path-sum/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Brute Force Lorem ipsum dolor Lorem ipsum dolor"},{"location":"side-projects/leetcode/124-binary-tree-maximum-path-sum/#implementation","title":"Implementation","text":"PythonJavaScriptGo"},{"location":"side-projects/leetcode/1268-search-suggestions-system/","title":"1268. Search Suggestions System","text":""},{"location":"side-projects/leetcode/127-word-ladder/","title":"127. Word Ladder","text":"<p>NeetCode</p>"},{"location":"side-projects/leetcode/13-path-sum-ii/","title":"13. Path Sum II","text":"<p>Video</p>"},{"location":"side-projects/leetcode/130-surrounded-regions/","title":"130. Surrounded Regions","text":"<p>NeetCode</p>"},{"location":"side-projects/leetcode/1318-minimum-flips-to-make-a-or-b-equal-to-c/","title":"1318. Minimum Flips To Make A Or B Equal To C","text":""},{"location":"side-projects/leetcode/133-clone-graph/","title":"133. Clone Graph","text":""},{"location":"side-projects/leetcode/133-clone-graph/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity DFS with HashMap Use DFS to traverse the graph while maintaining a hashmap to store cloned nodes. For each node, create a clone and recursively clone all neighbors, using the hashmap to avoid cycles and duplicate work. \\(O(V + E)\\) \\(O(V)\\)"},{"location":"side-projects/leetcode/133-clone-graph/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code>\"\"\"\n# Definition for a Node.\nclass Node:\n    def __init__(self, val = 0, neighbors = None):\n        self.val = val\n        self.neighbors = neighbors if neighbors is not None else []\n\"\"\"\n\nfrom typing import Optional\nclass Solution:\n    def cloneGraph(self, node: Optional['Node']) -&gt; Optional['Node']:\n        old2new = {}\n\n        def clone(old: Node) -&gt; Node:\n            \"\"\"\n            dfs-cloning approach\n            \"\"\"\n            if old in old2new:\n                return old2new[old]\n\n            new = Node(old.val)\n            old2new[old] = new\n            for nb in old.neighbors:\n                new.neighbors.append(\n                    clone(nb)\n                )\n\n            return new\n\n        return clone(node) if node else None\n</code></pre> <pre><code>\n</code></pre> <p><pre><code>\n</code></pre> ```</p>"},{"location":"side-projects/leetcode/136-single-number/","title":"136. Single Number","text":""},{"location":"side-projects/leetcode/1372-longest-zigzag-path-in-a-binary-tree/","title":"1372. Longest Zigzag Path In A Binary Tree","text":""},{"location":"side-projects/leetcode/141-linked-list-cycle/","title":"141. Linked List Cycle","text":""},{"location":"side-projects/leetcode/141-linked-list-cycle/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Hash Set Store visited nodes in a set. If we encounter a node already in the set, there's a cycle. \\(O(n)\\) \\(O(n)\\) Two Pointers (Floyd's Cycle Detection) Use slow and fast pointers. Slow moves 1 step, fast moves 2 steps. If they meet, there's a cycle. \\(O(n)\\) \\(O(1)\\)"},{"location":"side-projects/leetcode/141-linked-list-cycle/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code># Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, x):\n#         self.val = x\n#         self.next = None\n\nclass Solution:\n    def hasCycle(self, head: Optional[ListNode]) -&gt; bool:\n        slow, fast = head, head\n        while fast and fast.next:\n            slow = slow.next\n            fast = fast.next.next\n            if slow == fast:\n                return True\n        return False\n</code></pre> <pre><code>\n</code></pre> <p><pre><code>\n</code></pre> ```</p>"},{"location":"side-projects/leetcode/1431-kids-with-the-greatest-number-of-candies/","title":"1431. Kids With The Greatest Number Of Candies","text":""},{"location":"side-projects/leetcode/1448-count-good-nodes-in-binary-tree/","title":"1448. Count Good Nodes In Binary Tree","text":""},{"location":"side-projects/leetcode/1456-maximum-vowels-in-a-substring-of-given-length/","title":"1456. Maximum Vowels In A Substring Of Given Length","text":""},{"location":"side-projects/leetcode/1466-reorder-routes-to-make-all-paths-lead-to-the-city-zero/","title":"1466. Reorder Routes To Make All Paths Lead To The City Zero","text":""},{"location":"side-projects/leetcode/1493-longest-subarray-of-1s-after-deleting-one-element/","title":"1493. Longest Subarray Of 1s After Deleting One Element","text":""},{"location":"side-projects/leetcode/15-3sum/","title":"15. 3Sum","text":""},{"location":"side-projects/leetcode/15-3sum/#summary","title":"Summary","text":"Solution Approach Explanation Time Complexity Space Complexity Brute Force Iterate through all triplets and check if their sum is zero. This approach is straightforward but inefficient for large arrays. \\(O(n^3)\\) - three nested loops iterate through the array \\(O(1)\\) - no extra space used beyond input storage Two Pointers Sort the array first, then for each element, use two pointers from both ends to find pairs that sum to the target. Skip duplicates to avoid redundant triplets. \\(O(n^2)\\) - outer loop iterates \\(n\\) times, inner two-pointer search takes \\(O(n)\\) \\(O(1)\\) - only uses constant extra space for pointers and variables HashSet without Sorting Convert 3Sum to 2Sum for each element. Use sets for deduplication and avoid processing duplicate first elements. No array modification required. \\(O(n^2)\\) - outer loop iterates \\(n\\) times, inner 2Sum takes \\(O(n)\\) \\(O(n)\\) - uses sets for storing seen elements and results"},{"location":"side-projects/leetcode/15-3sum/#implementation","title":"Implementation","text":"Python <pre><code># HashSet without Sorting\ndef two_sum(nums: List[int], target: int, result: List[List[int]]) -&gt; List[List[int]]:\n    seen = set()\n    for j in range(len(nums)):\n        complement = target - nums[j]\n        if complement in seen:\n            t = tuple(sorted(\n                [-target, nums[j], complement]\n            ))\n            result.add(t)\n        seen.add(nums[j])\n\nclass Solution:\n    def threeSum(self, nums: List[int]) -&gt; List[List[int]]:\n        seen = set()\n        result = set()\n        for i in range(len(nums)-2):\n            target = -nums[i]\n            if nums[i] not in seen:\n                two_sum(\n                    nums=nums[i+1:],\n                    target=target,\n                    result=result\n                )\n                seen.add(nums[i])\n        return list(result)\n</code></pre>"},{"location":"side-projects/leetcode/151-reverse-words-in-a-string/","title":"151. Reverse Words In A String","text":""},{"location":"side-projects/leetcode/153-find-minimum-in-rotated-sorted-array/","title":"153. Find Minimum in Rotated Sorted Array","text":""},{"location":"side-projects/leetcode/153-find-minimum-in-rotated-sorted-array/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Binary Search Use binary search to find the minimum element. First check if array is not rotated (<code>nums[0] &lt; nums[-1]</code>). Otherwise, find the element where both left and right neighbors are larger. Compare middle element with the last element to determine which half contains the minimum. \\(O(\\log n)\\) \\(O(1)\\)"},{"location":"side-projects/leetcode/153-find-minimum-in-rotated-sorted-array/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code>class Solution:\n    def findMin(self, nums: List[int]) -&gt; int:\n        l, r = 0, len(nums)-1\n\n        # Rotated len(nums) times\n        if nums[l] &lt; nums[r]:\n            return nums[l]\n\n        while l &lt;= r:\n            m = (l + r) // 2\n\n            left_larger_than_curr  = (m == 0           or nums[m-1] &gt; nums[m])\n            right_larger_than_curr = (m == len(nums)-1 or nums[m+1] &gt; nums[m])\n            if left_larger_than_curr and right_larger_than_curr:\n                return nums[m]\n\n            if nums[m] &gt; nums[-1]: # left sorted side\n                l = m + 1\n            else:\n                r = m - 1\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"side-projects/leetcode/162-find-peak-element/","title":"162. Find Peak Element","text":""},{"location":"side-projects/leetcode/1657-determine-if-two-strings-are-close/","title":"1657. Determine If Two Strings Are Close","text":""},{"location":"side-projects/leetcode/167-two-sum-ii-input-array-is-sorted/","title":"167. Two Sum II - Input Array Is Sorted","text":""},{"location":"side-projects/leetcode/167-two-sum-ii-input-array-is-sorted/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Naive Approach (Nested Loop) Use two nested loops to try every possible pair of numbers. For each pair, check if their sum matches the target. Return the indices if a match is found. O(n\u00b2): The outer loop iterates over each element, and the inner loop checks all elements after it, resulting in quadratic time. O(1): Constant space since no extra data structures are used. Two-pointer technique Use two pointers on the sorted array, one starting at the beginning and the other at the end. Check the sum and adjust the pointers accordingly to find the solution in one pass. O(n): Linear scan as the two pointers converge towards the solution. O(1): Only pointers are used, with no extra space."},{"location":"side-projects/leetcode/167-two-sum-ii-input-array-is-sorted/#implementation","title":"Implementation","text":"Python <pre><code>class Solution:\n    def twoSum(self, numbers: List[int], target: int) -&gt; List[int]:\n        l = 0\n        r = len(numbers) - 1\n        while l &lt; r:\n            curr_sum = numbers[l] + numbers[r]\n            if curr_sum &gt; target:\n                r -= 1\n            elif curr_sum &lt; target:\n                l += 1\n            else:\n                return [l+1, r+1]\n        return []\n</code></pre>"},{"location":"side-projects/leetcode/1679-maximum-number-of-k-sum-pairs/","title":"1679. Maximum Number Of K Sum Pairs","text":""},{"location":"side-projects/leetcode/17-letter-combinations-of-a-phone-number/","title":"17. Letter Combinations Of A Phone Number","text":""},{"location":"side-projects/leetcode/1732-find-the-highest-altitude/","title":"1732. Find The Highest Altitude","text":""},{"location":"side-projects/leetcode/1767-find-the-subtasks-that-did-not-execute/","title":"1767. Find The Subtasks That Did Not Execute","text":""},{"location":"side-projects/leetcode/1767-find-the-subtasks-that-did-not-execute/#summary","title":"Summary","text":"<p>The solution uses a recursive Common Table Expression (CTE) to generate all possible subtasks for each task, then filters out the ones that were executed.</p> <p>The recursive CTE works in two parts:</p> <ol> <li>Base case: Start with the original tasks table, which contains each task and its maximum subtask_id</li> <li>Recursive case: For each task, generate all subtasks by decrementing the subtask_id until it reaches 1</li> </ol> <p>This creates a complete set of all possible (task_id, subtask_id) pairs. Finally, we use a NOT IN clause to exclude the subtasks that appear in the executed table, leaving only the subtasks that did not execute.</p> <p>See Work with recursive CTEs in BigQuery for more details on recursive CTEs.</p>"},{"location":"side-projects/leetcode/1767-find-the-subtasks-that-did-not-execute/#implementation","title":"Implementation","text":"PostgreSQLPandas <pre><code>with recursive task_subtasks (task_id, subtask_id) as (\n    select\n        *\n    from tasks\n\n    union\n\n    select\n        task_id,\n        subtask_id - 1\n    from task_subtasks\n    where subtask_id &gt; 1\n)\n\nselect\n    task_id,\n    subtask_id\nfrom task_subtasks\nwhere (task_id, subtask_id) not in (\n    select\n        *\n    from executed\n)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"side-projects/leetcode/1768-merge-strings-alternately/","title":"1768. Merge Strings Alternately","text":""},{"location":"side-projects/leetcode/1926-nearest-exit-from-entrance-in-maze/","title":"1926. Nearest Exit From Entrance In Maze","text":""},{"location":"side-projects/leetcode/198-house-robber/","title":"198. House Robber","text":""},{"location":"side-projects/leetcode/199-binary-tree-right-side-view/","title":"199. Binary Tree Right Side View","text":""},{"location":"side-projects/leetcode/200-number-of-islands/","title":"200. Number of Islands","text":"<p>NeetCode</p>"},{"location":"side-projects/leetcode/202-happy-number/","title":"202. Happy Number","text":""},{"location":"side-projects/leetcode/202-happy-number/#summary","title":"Summary","text":"Solution Approach Explanation Time Complexity Space Complexity Brute Force with Set Each iteration computes sum of squares in \\(O(\\log n)\\) time; number of iterations is constant as numbers quickly reduce to a small cycle or 1, so overall \\(O(\\log n)\\) time and \\(O(1)\\) space. \\(O(\\log n)\\) \\(O(1)\\) Floyd's Cycle Detection Use slow and fast pointers to detect cycles without extra space. \\(O(\\log n)\\) \\(O(1)\\)"},{"location":"side-projects/leetcode/202-happy-number/#implementation","title":"Implementation","text":"PythonJavaScriptGo Brute Force with Set<pre><code>class Solution:\n    def isHappy(self, n: int) -&gt; bool:\n        seen = set()\n\n        while n not in seen:\n            seen.add(n)\n            n = self.getNext(n)\n\n        return n == 1\n\n    def getNext(self, n: int) -&gt; int:\n        nextNumber = 0\n        while n != 0:\n            nextNumber += (n % 10) ** 2\n            n //= 10\n        return nextNumber\n</code></pre> Fast Slow Pointers<pre><code>class Solution:\n    def isHappy(self, n: int) -&gt; bool:\n        slow = n\n        fast = self.getNext(n)\n\n        while slow != fast:\n            slow = self.getNext(slow)\n            fast = self.getNext(self.getNext(fast))\n\n        return fast == 1\n\n    def getNext(self, n: int) -&gt; int:\n        nextNumber = 0\n        while n != 0:\n            nextNumber += (n % 10) ** 2\n            n //= 10\n        return nextNumber\n</code></pre> <pre><code>\n</code></pre> <p><pre><code>\n</code></pre> ```</p>"},{"location":"side-projects/leetcode/206-reverse-linked-list/","title":"206. Reverse Linked List","text":""},{"location":"side-projects/leetcode/206-reverse-linked-list/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Iterative Use three pointers (prev, curr, next) to reverse links while traversing \\(O(n)\\) \\(O(1)\\) Recursive Recursively reverse from tail, then fix current node's links \\(O(n)\\) \\(O(n)\\)"},{"location":"side-projects/leetcode/206-reverse-linked-list/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code>class Solution:\n    def reverseList(self, head: Optional[ListNode]) -&gt; Optional[ListNode]:\n\n        prev = None\n        curr = head\n\n        while curr != None:\n            nxt = curr.next\n\n            curr.next = prev\n\n            prev = curr\n            curr = nxt\n\n        return prev\n</code></pre> <pre><code>class Solution:\n    def reverseList(self, head: Optional[ListNode]) -&gt; Optional[ListNode]:\n\n        if head == None:\n            return None\n\n        new_head = head\n        if head.next != None:\n            new_head = self.reverseList(head.next)\n            head.next.next = head\n        head.next = None\n        return new_head\n</code></pre> <pre><code>\n</code></pre> <p><pre><code>\n</code></pre> ```</p>"},{"location":"side-projects/leetcode/208-implement-trie-prefix-tree/","title":"208. Implement Trie Prefix Tree","text":""},{"location":"side-projects/leetcode/2095-delete-the-middle-node-of-a-linked-list/","title":"2095. Delete The Middle Node Of A Linked List","text":""},{"location":"side-projects/leetcode/210-course-schedule-ii/","title":"210. Course Schedule II","text":"<p>NeetCode</p>"},{"location":"side-projects/leetcode/2130-maximum-twin-sum-of-a-linked-list/","title":"2130. Maximum Twin Sum Of A Linked List","text":""},{"location":"side-projects/leetcode/215-kth-largest-element-in-an-array/","title":"215. Kth Largest Element In An Array","text":""},{"location":"side-projects/leetcode/215-kth-largest-element-in-an-array/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Brute Force Heap Quick Select"},{"location":"side-projects/leetcode/215-kth-largest-element-in-an-array/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code>class Solution:\n    def findKthLargest(self, nums: List[int], k: int) -&gt; int:\n        # O(N logN)\n        # nums.sort()\n        # return nums[len(nums) - k]\n\n        k = len(nums)-k\n        def quick_select(l: int, r: int):\n            pivot_value = nums[r]\n            p = l\n\n            # put all the value that is less than or equal to the pivot value to the left\n            for i in range(l, r):\n                curr_value = nums[i]\n                if curr_value &lt;= pivot_value:\n                    nums[p], nums[i] = nums[i], nums[p]\n                    p += 1\n            # put the pivot value to the pivot location\n            nums[p], nums[r] = nums[r], nums[p]\n\n            # Case 1: the index that you are looking for is on the right the pivot value\n            if k &gt; p:\n                return quick_select(p+1, r)\n            # Case 2: the index that you are looking for is equal to the pivot value\n            elif k == p:\n                return nums[p]\n            # Case 3: the index that you are looking for is on the left the pivot value\n            else:\n                return quick_select(l, p-1)\n\n        return quick_select(0, len(nums)-1)\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"side-projects/leetcode/216-combination-sum-iii/","title":"216. Combination Sum Iii","text":""},{"location":"side-projects/leetcode/2215-find-the-difference-of-two-arrays/","title":"2215. Find The Difference Of Two Arrays","text":""},{"location":"side-projects/leetcode/230-kth-smallest-element-in-a-bst/","title":"230. Kth Smallest Element in a BST","text":""},{"location":"side-projects/leetcode/230-kth-smallest-element-in-a-bst/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Recursive In-Order Use DFS with in-order traversal to visit nodes in sorted order. Count visited nodes and return value when counter equals \\(k\\). \\(O(H + k)\\) \\(O(H)\\) Iterative In-Order Use stack to simulate in-order traversal iteratively. Process left subtree, visit node, then right subtree until kth element found. \\(O(H + k)\\) \\(O(H)\\) <p>Time Complexity Explanation: \\(O(H + k)\\) where \\(H\\) is tree height. We traverse at most \\(H\\) nodes to reach leftmost node, then visit \\(k\\) nodes in sorted order.</p> <p>Space Complexity Explanation: \\(O(H)\\) where \\(H\\) is tree height. Recursive approach uses call stack of depth \\(H\\). Iterative approach uses explicit stack that can grow up to \\(H\\) nodes in worst case (skewed tree).</p>"},{"location":"side-projects/leetcode/230-kth-smallest-element-in-a-bst/#implementation","title":"Implementation","text":"PythonJavaScriptGo Recursive<pre><code>class Solution:\n    def kthSmallest(self, root: Optional[TreeNode], k: int) -&gt; int:\n        ans = float(\"inf\")\n        counter = 0\n        def dfs(node: TreeNode, k: int) -&gt; None:\n            nonlocal ans, counter\n\n            if node.left:\n                dfs(node.left, k)\n\n            counter += 1\n            if counter == k:\n                ans = node.val\n                return\n\n            if node.right:\n                dfs(node.right, k)\n\n        dfs(root, k)\n        return ans\n</code></pre> Iterative<pre><code>class Solution:\n    def kthSmallest(self, root: Optional[TreeNode], k: int) -&gt; int:\n        counter = 0\n        stack = []\n        current_node = root\n\n        while current_node or stack:\n            while current_node:\n                stack.append(current_node)\n                current_node = current_node.left\n\n            current_node = stack.pop()\n            counter += 1\n            if counter == k:\n                return current_node.val\n            current_node = current_node.right\n        return \n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"side-projects/leetcode/2300-successful-pairs-of-spells-and-potions/","title":"2300. Successful Pairs Of Spells And Potions","text":""},{"location":"side-projects/leetcode/2300-successful-pairs-of-spells-and-potions/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Brute Force For each spell, check every potion to see if their product meets the success threshold. Count valid pairs by iterating through all spell-potion combinations. \\(O(n \\cdot m)\\) - Check each of \\(n\\) spells against each of \\(m\\) potions \\(O(1)\\) - Only store counters and result array Binary Search Sort potions array, then for each spell find the minimum potion strength needed. Use binary search to find the first valid potion, then count remaining potions. \\(O(m \\log m + n \\log m)\\) (Sort potions once, then binary search for each spell) \\(O(1)\\) (Constant extra space excluding output array)"},{"location":"side-projects/leetcode/2300-successful-pairs-of-spells-and-potions/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code>class Solution:\n    def successfulPairs(self, spells: List[int], potions: List[int], success: int) -&gt; List[int]:\n        potions.sort() # O(m log m)\n        result = []\n        for s in spells: # O(n log m)\n            l = 0\n            r = len(potions) - 1\n\n            # idx = float(\"inf\")\n            idx = len(potions)\n\n            while l &lt;= r:\n                m = (l + r) // 2\n                if s * potions[m] &gt;= success:\n                    r = m - 1\n                    idx = m\n                else:\n                    l = m + 1\n\n            # if idx == float(\"inf\"):\n            #     result.append(0)\n            # else:\n            #     result.append(len(potions)-idx)\n            result.append(len(potions)-idx)\n        return result\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"side-projects/leetcode/2336-smallest-number-in-infinite-set/","title":"2336. Smallest Number In Infinite Set","text":""},{"location":"side-projects/leetcode/2352-equal-row-and-column-pairs/","title":"2352. Equal Row And Column Pairs","text":""},{"location":"side-projects/leetcode/236-lowest-common-ancestor-of-a-binary-tree/","title":"236. Lowest Common Ancestor Of A Binary Tree","text":""},{"location":"side-projects/leetcode/238-product-of-array-except-self/","title":"238. Product Of Array Except Self","text":""},{"location":"side-projects/leetcode/2390-removing-stars-from-a-string/","title":"2390. Removing Stars From A String","text":""},{"location":"side-projects/leetcode/24-swap-nodes-in-pairs/","title":"24. Swap Nodes in Pairs","text":""},{"location":"side-projects/leetcode/24-swap-nodes-in-pairs/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Iterative Pointer Manipulation Use a dummy head node and maintain three pointers (prev, curr, next) to iteratively swap adjacent pairs. For each pair, redirect pointers to swap positions. Continue until no more pairs exist. \\(O(n)\\) \\(O(1)\\) <p>Example</p> <pre><code>   [0 -&gt; 1 -&gt; 2 -&gt; 3 -&gt; 4]\n-&gt; [0 -&gt; 2 -&gt; 1 -&gt; 3 -&gt; 4]\n-&gt; [0 -&gt; 2 -&gt; 1 -&gt; 4 -&gt; 3]\n</code></pre>"},{"location":"side-projects/leetcode/24-swap-nodes-in-pairs/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code># Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, val=0, next=None):\n#         self.val = val\n#         self.next = next\nclass Solution:\n    def swapPairs(self, head: Optional[ListNode]) -&gt; Optional[ListNode]:\n        preHead = ListNode(None, head)\n\n        prev = preHead\n        curr = head\n\n        while curr and curr.next:\n            nxt = curr.next\n            aftNxt = curr.next.next\n\n            prev.next = nxt\n            curr.next = aftNxt\n            nxt.next = curr\n\n            prev = curr\n            curr = aftNxt\n\n        return preHead.next\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"side-projects/leetcode/240-search-a-2d-matrix-ii/","title":"240. Search a 2D Matrix II","text":"<p>Video</p>"},{"location":"side-projects/leetcode/2462-total-cost-to-hire-k-workers/","title":"2462. Total Cost To Hire K Workers","text":""},{"location":"side-projects/leetcode/2542-maximum-subsequence-score/","title":"2542. Maximum Subsequence Score","text":""},{"location":"side-projects/leetcode/257-binary-tree-paths/","title":"257. Binary Tree Paths","text":""},{"location":"side-projects/leetcode/257-binary-tree-paths/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Recursive DFS Traverse tree recursively, building path by adding nodes and backtracking. When reaching leaf, add complete path to result. Uses implicit call stack. \\(O(N)\\) \\(O(N)\\) Iterative DFS Use explicit stack to store <code>(node, path_string)</code> pairs. Build path strings as we traverse. When reaching leaf, add path to result. No backtracking needed. \\(O(N)\\) \\(O(N)\\)"},{"location":"side-projects/leetcode/257-binary-tree-paths/#implementation","title":"Implementation","text":"PythonJavaScriptGo Recursive DFS<pre><code>class Solution:\n    def binaryTreePaths(self, root: Optional[TreeNode]) -&gt; List[str]:\n\n        result: List[str] = []\n        path: List[str] = []\n\n        def dfs(node: TreeNode, path: List[str]) -&gt; None:\n            path.append(str(node.val))\n\n            if node.left == None and node.right == None:\n                result.append(\"-&gt;\".join(path))\n\n            if node.left:\n                dfs(node.left, path)\n\n            if node.right:\n                dfs(node.right, path)\n\n            path.pop()\n\n        dfs(root, path)\n        return result\n</code></pre> Iterative DFS<pre><code>class Solution:\n    def binaryTreePaths(self, root: Optional[TreeNode]) -&gt; List[str]:\n        result: List[str] = []\n\n        stack: List[Tuple[TreeNode, str]] = []\n        stack.append(\n            (root, str(root.val))\n        )\n        while stack:\n            node, path = stack.pop()\n            if node.left == None and node.right == None:\n                result.append(path)\n            if node.left:\n                stack.append(\n                    (node.left, path + \"-&gt;\" + str(node.left.val))\n                )\n            if node.right:\n                stack.append(\n                    (node.right, path + \"-&gt;\" + str(node.right.val))\n                )\n        return result\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"side-projects/leetcode/283-move-zeroes/","title":"283. Move Zeroes","text":""},{"location":"side-projects/leetcode/287-find-the-duplicate-number/","title":"287. Find the Duplicate Number","text":""},{"location":"side-projects/leetcode/287-find-the-duplicate-number/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Fast-Slow Pointers Treat array as linked list where <code>nums[i]</code> points to index <code>nums[i]</code>. Use Floyd's cycle detection to find where cycle begins (the duplicate). \\(O(n)\\) \\(O(1)\\) Binary Search Binary search on value range <code>[1, n-1]</code>. For each mid value, count <code>numbers &lt;= mid</code>. If <code>count &gt; mid</code>, duplicate is in left half, else right half. \\(O(n \\log n)\\) \\(O(1)\\)"},{"location":"side-projects/leetcode/287-find-the-duplicate-number/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code>class Solution:\n    def findDuplicate(self, nums: List[int]) -&gt; int:\n        def getNext(idx: int) -&gt; int:\n            return nums[idx]\n\n        slow, fast = 0, 0\n\n        while True:\n            slow = getNext(slow)\n            fast = getNext(getNext(fast))\n            if slow == fast:\n                break\n\n        slow2 = 0\n        while True:\n            slow = getNext(slow)\n            slow2 = getNext(slow2)\n            if slow == slow2:\n                break\n\n        return slow\n</code></pre> <pre><code>class Solution:\n    def findDuplicate(self, nums: List[int]) -&gt; int:\n\n        low = 1\n        high = len(nums) - 1\n\n        while low &lt;= high:\n            target = (low + high) // 2\n\n            counts = 0\n            for num in nums:\n                if num &lt;= target:\n                    counts += 1\n\n            if counts &lt;= target:\n                low = target + 1\n            else:\n                duplicate = target\n                high = target - 1\n\n        return duplicate\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"side-projects/leetcode/3-longest-substring-without-repeating-characters/","title":"3. Longest Substring Without Repeating Characters","text":""},{"location":"side-projects/leetcode/3-longest-substring-without-repeating-characters/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Brute Force Check all possible substrings and verify each one has unique characters using nested loops. \\(O(n^2)\\) \\(O(min(m,n))\\) Sliding Window (Two Pointers) Use left and right pointers with a set to track characters. When duplicate found, shrink window from left until duplicate removed. \\(O(n)\\) \\(O(min(m,n))\\) Sliding Window (Length Tracking) Similar approach but tracks substring length directly instead of using two pointers. Removes characters from the beginning when duplicates found. \\(O(n)\\) \\(O(min(m,n))\\)"},{"location":"side-projects/leetcode/3-longest-substring-without-repeating-characters/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code>class Solution:\n    def lengthOfLongestSubstring(self, s: str) -&gt; int:\n        max_length = 0\n        chars = set()\n        left = 0\n        for right in range(len(s)):\n            while s[right] in chars:\n                chars.remove(s[left])\n                left += 1\n            chars.add(s[right])\n            max_length = max(\n                max_length,\n                len(s[left:right+1])\n            )\n        return max_length\n</code></pre> <pre><code>class Solution:\n    def lengthOfLongestSubstring(self, s: str) -&gt; int:\n        max_length = 0\n        char_set = set()\n        len_substr = 0\n        for i in range(len(s)):\n            while s[i] in char_set:\n                char_set.remove(s[i-len_substr])\n                len_substr -= 1\n            char_set.add(s[i])\n            len_substr += 1\n            max_length = max(\n                max_length,\n                len_substr\n            )\n        return max_length\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"side-projects/leetcode/300-longest-increasing-subsequence/","title":"300. Longest Increasing Subsequence","text":""},{"location":"side-projects/leetcode/300-longest-increasing-subsequence/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Brute Force Generate all possible subsequences of the array (using recursion or bitmasking). For each subsequence, check if it is strictly increasing by comparing each pair of adjacent elements. Track and return the maximum length among all valid increasing subsequences. \\(O(2^n \\cdot n)\\) There are \\(2^n\\) possible subsequences for an array of length \\(n\\). For each subsequence, checking if it is increasing takes up to \\(O(n)\\) time. So, total time is \\(O(2^n \\cdot n)\\). \\(O(n)\\) The recursion stack or temporary storage for a subsequence can take up to \\(O(n)\\) space. DP (Bottom-Up) Use a DP array where each entry \\(dp[i]\\) stores the length of the longest increasing subsequence starting at index \\(i\\). Iterate backwards, and for each \\(i\\), check all \\(j &gt; i\\); if \\(nums[j] &gt; nums[i]\\), update \\(dp[i]\\). The answer is the maximum value in \\(dp\\). \\(O(n^2)\\) For each index, we may check all following indices, leading to \\(O(n^2)\\) total operations. \\(O(n)\\) The DP array uses \\(O(n)\\) space. Lorem ipsum dolor"},{"location":"side-projects/leetcode/300-longest-increasing-subsequence/#implementation","title":"Implementation","text":"PythonJavaScriptGo"},{"location":"side-projects/leetcode/303-range-sum-query-immutable/","title":"303. Range Sum Query - Immutable","text":""},{"location":"side-projects/leetcode/303-range-sum-query-immutable/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Brute Force For each query, iterate through the range <code>[left, right]</code> and sum all elements. Simple but inefficient for multiple queries. Init: \\(O(1)\\), Query: \\(O(n)\\) \\(O(1)\\) Prefix Sum (Optimal) Precompute cumulative sums in constructor. Each query becomes \\(O(1)\\) by subtracting prefix sums. Init: \\(O(n)\\), Query: \\(O(1)\\) \\(O(n)\\)"},{"location":"side-projects/leetcode/303-range-sum-query-immutable/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code>class NumArray:\n    def __init__(self, nums: List[int]):\n        self._prefix_sums = [0] * (len(nums)+1)\n        for i in range(len(nums)):\n            self._prefix_sums[i+1] = self._prefix_sums[i] + nums[i]\n        print(self._prefix_sums)\n\n    def sumRange(self, left: int, right: int) -&gt; int:\n        return self._prefix_sums[right+1] - self._prefix_sums[left]\n\n# Your NumArray object will be instantiated and called as such:\n# obj = NumArray(nums)\n# param_1 = obj.sumRange(left,right)\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"side-projects/leetcode/312-burst-balloons/","title":"312. Burst Balloons","text":""},{"location":"side-projects/leetcode/322-coin-change/","title":"322. Coin Change","text":""},{"location":"side-projects/leetcode/322-coin-change/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Dynamic Programming (Bottom-up) Build solution from amount 0 to target amount. For each amount, try all coins and take minimum coins needed. Use <code>dp[i]</code> to store minimum coins for amount <code>i</code>. O(amount \u00d7 coins) O(amount)"},{"location":"side-projects/leetcode/322-coin-change/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code>class Solution:\n    def coinChange(self, coins: List[int], amount: int) -&gt; int:\n        dp = [amount+1] * (amount+1)\n        dp[0] = 0\n        for a in range(1, amount+1):\n            for c in coins:\n                if a &gt;= c:\n                    dp[a] = min(\n                        dp[a],\n                        dp[a-c] + 1\n                    )\n\n        if dp[amount] == amount+1:\n            return -1\n        else:\n            return dp[amount]\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"side-projects/leetcode/328-odd-even-linked-list/","title":"328. Odd Even Linked List","text":""},{"location":"side-projects/leetcode/33-search-in-rotated-sorted-array/","title":"33. Search In Rotated Sorted Array","text":""},{"location":"side-projects/leetcode/33-search-in-rotated-sorted-array/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Modified Binary Search Use binary search by comparing middle element with the last element to determine which half is sorted. Then check if <code>target</code> lies in the sorted half to decide search direction. \\(O(\\log n)\\) \\(O(1)\\)"},{"location":"side-projects/leetcode/33-search-in-rotated-sorted-array/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code>class Solution:\n    def search(self, nums: List[int], target: int) -&gt; int:\n        l = 0\n        r = len(nums)-1\n\n        while l &lt;= r:\n            m = (l + r) // 2\n\n            if nums[m] == target:\n                return m\n\n            # left sorted side\n            if nums[m] &gt;= nums[-1]:\n                if nums[0] &lt;= target &lt;= nums[m]:\n                    r = m - 1\n                else:\n                    l = m + 1\n            # right sorted side\n            else:\n                if nums[m] &lt;= target &lt;= nums[-1]:\n                    l = m + 1\n                else:\n                    r = m - 1\n\n        return -1\n</code></pre> <pre><code>\n</code></pre> <p><pre><code>\n</code></pre> ```</p>"},{"location":"side-projects/leetcode/334-increasing-triplet-subsequence/","title":"334. Increasing Triplet Subsequence","text":""},{"location":"side-projects/leetcode/338-counting-bits/","title":"338. Counting Bits","text":""},{"location":"side-projects/leetcode/345-reverse-vowels-of-a-string/","title":"345. Reverse Vowels Of A String","text":""},{"location":"side-projects/leetcode/347-top-k-frequent-elements/","title":"347. Top K Frequent Elements","text":""},{"location":"side-projects/leetcode/373-find-k-pairs-with-smallest-sums/","title":"373. Find K Pairs with Smallest Sums","text":""},{"location":"side-projects/leetcode/374-guess-number-higher-or-lower/","title":"374. Guess Number Higher Or Lower","text":""},{"location":"side-projects/leetcode/374-guess-number-higher-or-lower/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Brute Force Start from 1, incrementally guess each number up to <code>n</code> until the correct number is found \\(O(n)\\) \\(O(1)\\) Binary Search Repeatedly guess the middle of the current range and narrow the range based on feedback from the guess API \\(O(\\log n)\\) \\(O(1)\\)"},{"location":"side-projects/leetcode/374-guess-number-higher-or-lower/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code># The guess API is already defined for you.\n# @param num, your guess\n# @return -1 if num is higher than the picked number\n#          1 if num is lower than the picked number\n#          otherwise return 0\n# def guess(num: int) -&gt; int:\nclass Solution:\n    def guessNumber(self, n: int) -&gt; int:\n        l = 1\n        r = n\n\n        while True:\n            m = (l + r) // 2\n\n            res = guess(m)\n            if res == -1:\n                r = m - 1\n            elif res == 1:\n                l = m + 1\n            else:\n                return m\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"side-projects/leetcode/392-is-subsequence/","title":"392. Is Subsequence","text":""},{"location":"side-projects/leetcode/394-decode-string/","title":"394. Decode String","text":""},{"location":"side-projects/leetcode/399-evaluate-division/","title":"399. Evaluate Division","text":""},{"location":"side-projects/leetcode/416-partition-equal-subset-sum/","title":"416. Partition Equal Subset Sum","text":""},{"location":"side-projects/leetcode/435-non-overlapping-intervals/","title":"435. Non Overlapping Intervals","text":""},{"location":"side-projects/leetcode/435-non-overlapping-intervals/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Brute Force Generate all possible combinations of intervals to remove. For each combination, check if remaining intervals are non-overlapping. Return minimum removals needed. \\(O(2^n)\\) \\(O(n)\\) Greedy (Implemented) Sort intervals by start time. Iterate through sorted intervals, when overlap detected, remove the interval with larger end time (keep shorter one). Count total removals. \\(O(n log n)\\) \\(O(1)\\)"},{"location":"side-projects/leetcode/435-non-overlapping-intervals/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code>class Solution:\n    def eraseOverlapIntervals(self, intervals: List[List[int]]) -&gt; int:\n        intervals.sort(key = lambda i: i[0])\n\n        counter = 0\n\n        prev_end = intervals[0][1]\n        for curr_start, curr_end in intervals[1:]:\n            if curr_start &lt; prev_end:\n                counter += 1\n                # leave the shorter interval\n                prev_end = min(\n                    prev_end, curr_end\n                )\n            else:\n                prev_end = curr_end\n\n        return counter\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"side-projects/leetcode/437-path-sum-iii/","title":"437. Path Sum Iii","text":""},{"location":"side-projects/leetcode/443-string-compression/","title":"443. String Compression","text":""},{"location":"side-projects/leetcode/450-delete-node-in-a-bst/","title":"450. Delete Node In A Bst","text":""},{"location":"side-projects/leetcode/452-minimum-number-of-arrows-to-burst-balloons/","title":"452. Minimum Number Of Arrows To Burst Balloons","text":""},{"location":"side-projects/leetcode/46-permutations/","title":"46. Permutations","text":""},{"location":"side-projects/leetcode/46-permutations/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Brute Force Generate all possible arrangements by trying every element at each position. For each position, iterate through all remaining elements and recursively build permutations. \\(O(n! \\cdot n)\\) \\(O(n! \\cdot n)\\) Recursive (Backtracking) Recursively generate permutations by taking the first element and inserting it at all positions in permutations of remaining elements. Base case returns when only one element remains. \\(O(n!)\\) \\(O(n!)\\) Iterative (Deque) Use a deque to iteratively build permutations. For each number, insert it at all possible positions in existing permutations. Start with empty permutation and expand by adding each number at every valid position. \\(O(n!)\\) \\(O(n!)\\)"},{"location":"side-projects/leetcode/46-permutations/#implementation","title":"Implementation","text":"PythonJavaScriptGo Iterative<pre><code>from collections import deque\nclass Solution:\n    def permute(self, nums: List[int]) -&gt; List[List[int]]:\n        result = deque([[]])\n        for num in nums:\n            for _ in range(len(result)):\n                perm = result.popleft()\n                for i in range(len(perm)+1):\n                    result.append(\n                        perm[:i] + [num] + perm[i:]\n                    )\n        return list(result)\n</code></pre> Recursive<pre><code>class Solution:\n    def permute(self, nums: List[int]) -&gt; List[List[int]]:\n        if len(nums) == 1:\n            return [nums]\n\n        result = []\n        perms = self.permute(nums[1:])\n        for perm in perms:\n            for i in range(len(perm)+1):\n                result.append(\n                    perm[:i] + [nums[0]] + perm[i:]\n                )\n        return result\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"side-projects/leetcode/496-next-greater-element-i/","title":"496. Next Greater Element I","text":""},{"location":"side-projects/leetcode/496-next-greater-element-i/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Brute Force Create a mapping of <code>nums1</code> elements to indices. For each element in <code>nums2</code>, if it exists in <code>nums1</code>, scan rightward to find the next greater element using nested loops. \\(O(n \u00d7 m)\\) \\(O(n)\\) Stack Use a monotonic decreasing stack to track elements awaiting their next greater element. When a larger element is found, pop from stack and update results for all smaller elements. \\(O(n + m)\\) \\(O(n)\\)"},{"location":"side-projects/leetcode/496-next-greater-element-i/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code>class Solution:\n    def nextGreaterElement(self, nums1: List[int], nums2: List[int]) -&gt; List[int]:\n        n2i = {\n            num: i\n            for i, num in enumerate(nums1)\n        }\n        result = [-1] * len(nums1)\n        for i in range(len(nums2)):\n            if nums2[i] not in n2i:\n                continue\n            for j in range(i+1, len(nums2)):\n                if nums2[j] &gt; nums2[i]:\n                    idx = n2i[nums2[i]]\n                    result[idx] = nums2[j]\n                    break\n        return result\n</code></pre> <pre><code>class Solution:\n    def nextGreaterElement(self, nums1: List[int], nums2: List[int]) -&gt; List[int]:\n        n2i = {\n            num: i\n            for i, num in enumerate(nums1)\n        }\n        result = [-1] * len(nums1)\n        stack = []\n        for i in range(len(nums2)):\n            current = nums2[i]\n            while len(stack) != 0 and current &gt; stack[-1]:\n                idx = n2i[stack.pop()]\n                result[idx] = current\n            if current in n2i:\n                stack.append(current)\n        return result\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"side-projects/leetcode/51-n-queens/","title":"51. N-Queens","text":""},{"location":"side-projects/leetcode/525-contiguous-array/","title":"525. Contiguous Array","text":""},{"location":"side-projects/leetcode/525-contiguous-array/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Brute Force For every subarray, count 0s and 1s; if equal, update max length. \\(O(n^2)\\) \\(O(1)\\) Hash Map (Optimal) Use a running count (increment for 1, decrement for 0) and store first occurrence of each count in a hash map. If the same count is seen again, the subarray between indices has equal 0s and 1s. \\(O(n)\\) \\(O(n)\\)"},{"location":"side-projects/leetcode/525-contiguous-array/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code>class Solution:\n    def findMaxLength(self, nums: List[int]) -&gt; int:\n        result = 0\n        num_zeros = 0\n        num_ones = 0\n\n        prefix_diff = {}\n        for i, n in enumerate(nums):\n            num_zeros += 1 if n == 0 else 0\n            num_ones += 1 if n == 1 else 0\n\n            diff = num_ones - num_zeros\n            if diff == 0:\n                result = num_zeros + num_ones\n            else:\n                if diff not in prefix_diff:\n                    prefix_diff[diff] = i\n                else:\n                    j = prefix_diff[diff]\n                    result = max(result, i-j)\n        return result\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"side-projects/leetcode/547-number-of-provinces/","title":"547. Number Of Provinces","text":""},{"location":"side-projects/leetcode/56-merge-intervals/","title":"56. Merge Intervals","text":""},{"location":"side-projects/leetcode/56-merge-intervals/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Sort and Merge Sort intervals by start time. Iterate through sorted intervals, comparing each with the last merged interval. If they overlap, merge by updating the end time to the maximum. Otherwise, add the current interval to results. \\(O(n log n)\\) \\(O(1)\\) or \\(O(n)\\)"},{"location":"side-projects/leetcode/56-merge-intervals/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code>class Solution:\n    def merge(self, intervals: List[List[int]]) -&gt; List[List[int]]:\n        intervals.sort(key = lambda i: i[0])\n\n        output = [intervals[0]]\n        for i in range(1, len(intervals)):\n            prev_start, prev_end = output[-1]\n            curr_start, curr_end = intervals[i]\n\n            if curr_start &lt;= prev_end: # overlapping\n                # merge\n                output[-1][1] = max(prev_end, curr_end)\n            else:\n                output.append([curr_start, curr_end])\n\n        return output\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"side-projects/leetcode/560-subarray-sum-equals-k/","title":"560. Subarray Sum Equals K","text":""},{"location":"side-projects/leetcode/560-subarray-sum-equals-k/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Brute Force Check all possible subarrays and calculate their sums. \\(O(n^2)\\): Two nested loops \\(O(1)\\): No extra space used Prefix Sum + Hashmap Use a hashmap to store prefix sums and their counts. Check for complement in each step. \\(O(n)\\): Single iteration \\(O(n)\\): Hashmap stores prefix sums"},{"location":"side-projects/leetcode/560-subarray-sum-equals-k/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code>class Solution:\n    def subarraySum(self, nums: List[int], k: int) -&gt; int:\n        # key:value = prefix_sum: count\n        prefix_sums = {0: 1}\n        counts = 0\n\n        curr_sum = 0\n        for i, num in enumerate(nums):\n            curr_sum += num\n            complement = curr_sum - k\n            # Add up \n            if complement in prefix_sums:\n                counts += prefix_sums[complement]\n            # Update prefix_sums\n            if curr_sum in prefix_sums:\n                prefix_sums[curr_sum] += 1\n            else:\n                prefix_sums[curr_sum] = 1\n        return counts\n</code></pre> <pre><code>\n</code></pre> <p><pre><code>\n</code></pre> ```</p>"},{"location":"side-projects/leetcode/57-insert-interval/","title":"57. Insert Interval","text":""},{"location":"side-projects/leetcode/57-insert-interval/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Linear Scan Iterate through intervals. Handle 3 cases: (1) new interval is completely before current - add new interval and remaining intervals; (2) new interval is completely after current - add current interval to result; (3) overlapping intervals - merge by taking min start and max end, continue merging until no overlap. O(n) O(n)"},{"location":"side-projects/leetcode/57-insert-interval/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code>class Solution:\n    def insert(self, intervals: List[List[int]], newInterval: List[int]) -&gt; List[List[int]]:\n        output = []\n\n        for i, interval in enumerate(intervals):\n            curr_start, curr_end = interval\n            new_start, new_end = newInterval\n            # Case 1: new interval is all the way to the left\n            if new_end &lt; curr_start:\n                output.append(newInterval)\n                output += intervals[i:]\n                return output\n            # Case 2: new interval is all the way to the right\n            elif curr_end &lt; new_start:\n                output.append(interval)\n            # Case 3: overlapping\n            else:\n                newInterval = [\n                    min(new_start, curr_start),\n                    max(new_end, curr_end)\n                ]\n\n        output.append(newInterval)\n\n        return output\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"side-projects/leetcode/605-can-place-flowers/","title":"605. Can Place Flowers","text":""},{"location":"side-projects/leetcode/62-unique-paths/","title":"62. Unique Paths","text":""},{"location":"side-projects/leetcode/643-maximum-average-subarray-i/","title":"643. Maximum Average Subarray I","text":""},{"location":"side-projects/leetcode/643-maximum-average-subarray-i/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Brute Force Check all possible subarrays of length k and calculate their averages \\(O(n*k)\\) \\(O(1)\\) Sliding Window Maintain a window sum and slide it across the array, updating sum efficiently \\(O(n)\\) \\(O(1)\\)"},{"location":"side-projects/leetcode/643-maximum-average-subarray-i/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code>class Solution:\n    def findMaxAverage(self, nums: List[int], k: int) -&gt; float:\n        curr_sums = 0\n        max_sums = -float(\"inf\")\n        for i in range(len(nums)):\n            if i+1 &lt; k:\n                curr_sums += nums[i]\n            elif i+1 == k:\n                curr_sums += nums[i]\n                max_sums = max(\n                    max_sums,\n                    curr_sums\n                )\n            else:\n                curr_sums += nums[i]\n                curr_sums -= nums[i-k]\n                max_sums = max(\n                    max_sums,\n                    curr_sums\n                )\n        return max_sums / k\n</code></pre> <pre><code>\n</code></pre> <p><pre><code>\n</code></pre> ```</p>"},{"location":"side-projects/leetcode/649-dota2-senate/","title":"649. Dota2 Senate","text":""},{"location":"side-projects/leetcode/70-climbing-stairs/","title":"70. Climbing Stairs","text":""},{"location":"side-projects/leetcode/70-climbing-stairs/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Dynamic Programming Count ways to reach each step by adding ways from the previous two steps. Like Fibonacci - each number is sum of previous two. \\(O(n)\\) \\(O(n)\\)"},{"location":"side-projects/leetcode/70-climbing-stairs/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code>class Solution:\n    def climbStairs(self, n: int) -&gt; int:\n        dp = []\n        for i in range(n):\n            if i==0:\n                dp.append(1)\n            elif i==1:\n                dp.append(2)\n            else:\n                dp.append(dp[i-2] + dp[i-1])\n        return dp[-1]\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"side-projects/leetcode/700-search-in-a-binary-search-tree/","title":"700. Search In A Binary Search Tree","text":""},{"location":"side-projects/leetcode/714-best-time-to-buy-and-sell-stock-with-transaction-fee/","title":"714. Best Time To Buy And Sell Stock With Transaction Fee","text":""},{"location":"side-projects/leetcode/72-edit-distance/","title":"72. Edit Distance","text":""},{"location":"side-projects/leetcode/724-find-pivot-index/","title":"724. Find Pivot Index","text":""},{"location":"side-projects/leetcode/733-flood-fill/","title":"733. Flood Fill","text":""},{"location":"side-projects/leetcode/733-flood-fill/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity BFS (Breadth-First Search) Use a queue to explore connected pixels level by level. Start from the given pixel, add it to <code>queue</code> and <code>visited</code> set. For each pixel, change its color and explore all 4 adjacent pixels that have the original color and haven't been visited. Continue until queue is empty. \\(O(m \\times n)\\) \\(O(m \\times n)\\)"},{"location":"side-projects/leetcode/733-flood-fill/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code>from collections import deque\nclass Solution:\n    def floodFill(self, image: List[List[int]], sr: int, sc: int, color: int) -&gt; List[List[int]]:\n        originalColor = image[sr][sc]\n        if originalColor == color:\n            return image\n\n        m, n = len(image), len(image[0])\n\n        delta = ((0, 1), (1, 0), (-1, 0), (0, -1))\n\n        queue = deque([(sr, sc)])\n        seen: Set[Tuple[int, int]] = set()\n        seen.add((sr, sc))\n        while queue:\n            r, c = queue.popleft()\n            image[r][c] = color\n            for dr, dc in delta:\n                row = r + dr\n                col = c + dc\n                if (0 &lt;= row &lt;= m-1) and (0 &lt;= col &lt;= n-1):\n                    if image[row][col] == originalColor:\n                        if (row, col) not in seen:\n                            queue.append((row, col))\n                            seen.add((row, col))\n        return image\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"side-projects/leetcode/735-asteroid-collision/","title":"735. Asteroid Collision","text":""},{"location":"side-projects/leetcode/739-daily-temperatures/","title":"739. Daily Temperatures","text":""},{"location":"side-projects/leetcode/739-daily-temperatures/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Brute Force For each day, iterate through all subsequent days to find the first day with higher temperature \\(O(n^2)\\) \\(O(1)\\) Monotonic Stack Use a decreasing stack to store (index, temperature) pairs. When a warmer day is found, pop all cooler days from stack and calculate their wait times \\(O(n)\\) \\(O(n)\\)"},{"location":"side-projects/leetcode/739-daily-temperatures/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code>class Solution:\n    def dailyTemperatures(self, temperatures: List[int]) -&gt; List[int]:\n        days = [0] * len(temperatures)\n        stack: list[tuple[int, int]] = [] # list of pairs = (idx, temp)\n\n        for curr_day, curr_temp in enumerate(temperatures):\n\n            while len(stack) != 0 and stack[-1][1] &lt; curr_temp:\n                target_day, _ = stack.pop()\n                days[target_day] = curr_day - target_day\n\n            stack.append((curr_day, curr_temp))\n\n        return days\n</code></pre> <pre><code>\n</code></pre> <p><pre><code>\n</code></pre> ```</p>"},{"location":"side-projects/leetcode/746-min-cost-climbing-stairs/","title":"746. Min Cost Climbing Stairs","text":""},{"location":"side-projects/leetcode/76-minimum-window-substring/","title":"76. Minimum Window Substring","text":""},{"location":"side-projects/leetcode/76-minimum-window-substring/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Brute Force Generate all \\(O(n^2)\\) substrings of <code>s</code>. For each substring, count character frequencies and compare with <code>t</code>'s requirements. Track the shortest valid substring. \\(O(n^2 \\times m)\\) \\(O(m)\\) Sliding Window Use two pointers to maintain a dynamic window. Expand right pointer to include characters until window contains all chars from <code>t</code>. Then contract left pointer to minimize window size while maintaining validity. \\(O(n + m)\\) \\(O(m)\\)"},{"location":"side-projects/leetcode/76-minimum-window-substring/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code>from collections import defaultdict\nclass Solution:\n    def minWindow(self, s: str, t: str) -&gt; str:\n        result = \"\"\n        result_len = float(\"inf\")\n\n        if t == \"\":\n            return result\n\n        need_map = defaultdict(int)\n        need_len = len(t)\n        for char in t:\n            need_map[char] += 1\n\n        have_map = defaultdict(int)\n        have_len= 0\n\n\n        l = 0\n        for r in range(len(s)):\n            r_char = s[r]\n            have_map[r_char] += 1\n            if r_char in need_map and have_map[r_char] &lt;= need_map[r_char]:\n                have_len += 1\n\n            while have_len == need_len:\n                candidate = s[l: r+1]\n                if len(candidate) &lt; result_len:\n                    result = candidate\n                    result_len = len(candidate)\n\n                l_char = s[l]\n                have_map[l_char] -= 1\n                if l_char in need_map and have_map[l_char] &lt; need_map[l_char]:\n                    have_len -= 1\n                l += 1\n        return result\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"side-projects/leetcode/78-subsets/","title":"78. Subsets","text":""},{"location":"side-projects/leetcode/78-subsets/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Iterative (Expanding Subsets) Start with empty subset. For each number, create new subsets by copying existing ones and adding the current number. Doubles subset count with each iteration. \\(O(n \\times 2^n)\\) \\(O(2^n)\\) for output Recursive (Backtracking) Build subsets by making binary choices: include or exclude each element. Recursively explore both paths, adding current subset to result when reaching base case. \\(O(n \\times 2^n)\\) \\(O(2^n)\\) for output"},{"location":"side-projects/leetcode/78-subsets/#implementation","title":"Implementation","text":"PythonJavaScriptGo iterative<pre><code>class Solution:\n    def subsets(self, nums: List[int]) -&gt; List[List[int]]:\n\n        res = [[]]\n        for n in nums:\n            for _ in range(len(res)):\n                subset = res.pop(0)\n\n                # not include the current number \"n\"\n                res.append(subset.copy())\n\n                # include the current number \"n\"\n                subset.append(n)\n                res.append(\n                    subset\n                )\n\n        return res\n</code></pre> recursive<pre><code>class Solution:\n    def subsets(self, nums: List[int]) -&gt; List[List[int]]:\n        res = []\n\n        subset = []\n\n        def dfs(i) -&gt; None:\n            if i &gt;= len(nums):\n                res.append(subset.copy())\n                print(subset)\n                return\n\n            subset.append(nums[i])\n            dfs(i+1)\n\n            subset.pop()\n            dfs(i+1)\n\n        dfs(0)\n        return res\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"side-projects/leetcode/790-domino-and-tromino-tiling/","title":"790. Domino And Tromino Tiling","text":""},{"location":"side-projects/leetcode/84-largest-rectangle-in-histogram/","title":"84. Largest Rectangle in Histogram","text":""},{"location":"side-projects/leetcode/84-largest-rectangle-in-histogram/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Stack-based (Monotonic Stack) Use a stack to track indices and heights. When current height is smaller than stack top, pop elements and calculate area using the popped height. The width is determined by the distance between current index and the updated index from popped elements. This ensures we find the maximum rectangle for each height efficiently by maintaining increasing heights in the stack. \\(O(n)\\) \\(O(n)\\)"},{"location":"side-projects/leetcode/84-largest-rectangle-in-histogram/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code>class Solution:\n    def largestRectangleArea(self, heights: List[int]) -&gt; int:\n        stack: List[Tuple[int, int]] = []  # [(idx, height)]\n\n        max_h = -float(\"inf\")\n        for curr_i, curr_h in enumerate(heights):\n\n            updated_i = curr_i\n\n            while stack and curr_h &lt; stack[-1][1]:\n                prev_i, prev_h = stack.pop()\n                updated_i = prev_i\n                w = curr_i - prev_i\n                max_h = max(max_h, prev_h * w)\n\n            stack.append((updated_i, curr_h))\n\n\n        for i, h in stack:\n            w = len(heights) - i\n            max_h = max(\n                max_h,\n                h * w\n            )\n\n        return max_h\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"side-projects/leetcode/841-keys-and-rooms/","title":"841. Keys And Rooms","text":""},{"location":"side-projects/leetcode/872-leaf-similar-trees/","title":"872. Leaf Similar Trees","text":""},{"location":"side-projects/leetcode/875-koko-eating-bananas/","title":"875. Koko Eating Bananas","text":""},{"location":"side-projects/leetcode/901-online-stock-span/","title":"901. Online Stock Span","text":""},{"location":"side-projects/leetcode/92-reverse-linked-list-ii/","title":"92. Reverse Linked List II","text":""},{"location":"side-projects/leetcode/92-reverse-linked-list-ii/#summary","title":"Summary","text":"Solution Approach Explanation (1-minute) Time Complexity Space Complexity Three-Stage Reversal Navigate to position before <code>left</code>, reverse sublist from <code>left</code> to <code>right</code> using standard reversal, then reconnect the reversed portion \\(O(n)\\) \\(O(1)\\)"},{"location":"side-projects/leetcode/92-reverse-linked-list-ii/#implementation","title":"Implementation","text":"PythonJavaScriptGo <pre><code># Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, val=0, next=None):\n#         self.val = val\n#         self.next = next\nclass Solution:\n    def reverseBetween(self, head: Optional[ListNode], left: int, right: int) -&gt; Optional[ListNode]:\n        pre_head = ListNode(None, head)\n        # Stage 1\n        prev = pre_head\n        curr = head\n        for _ in range(left-1):\n            prev = curr\n            curr = curr.next\n        pre_left = prev\n        # Stage 2\n        prev = None\n        for _ in range(right-left+1):\n            nxt = curr.next\n\n            curr.next = prev\n\n            prev = curr\n            curr = nxt\n        # Stage 3\n        pre_left.next.next = curr\n        pre_left.next = prev\n\n        return pre_head.next\n</code></pre> <pre><code>\n</code></pre> <p><pre><code>\n</code></pre> ```</p>"},{"location":"side-projects/leetcode/933-number-of-recent-calls/","title":"933. Number Of Recent Calls","text":""},{"location":"side-projects/leetcode/994-rotting-oranges/","title":"994. Rotting Oranges","text":"<p>NeetCode</p>"},{"location":"side-projects/restful-apis-with-flask/","title":"RESTful APIs with Flask","text":"<p>This repository documents my journey of learning how to build RESTful APIs using Flask. It includes step-by-step implementations of various concepts, from basic API design principles to advanced features like authentication, database integration, deployment, and third-party integrations. </p> <p>The content is based on two Udemy courses: \"REST APIs with Flask and Python\" and \"Advanced REST APIs with Flask and Python\". Each section highlights key topics, tools, and techniques, making it a comprehensive resource for anyone looking to learn Flask for API development.</p> <p></p> <p>Certificate</p> <p></p> <p>Certificate</p>"},{"location":"side-projects/restful-apis-with-flask/#about-this-project","title":"About This Project","text":"<p>The goal of this project is to: - Learn and experiment with Flask for building RESTful APIs. - Understand best practices for API design and implementation. - Explore integrations with databases, authentication, and other web technologies.</p>"},{"location":"side-projects/restful-apis-with-flask/#features","title":"Features","text":"<ul> <li>RESTful API Design: Follows REST principles for clean and scalable APIs.</li> <li>Flask Framework: Built using Flask for simplicity and flexibility.</li> <li>Database Integration: Includes examples of working with databases like SQLite or SQLAlchemy.</li> <li>Authentication: Demonstrates how to secure APIs with authentication mechanisms.</li> <li>Error Handling: Implements robust error handling for better user experience.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/#how-to-use-this-repository","title":"How to Use This Repository","text":"<p>Feel free to browse the code, read the documentation, and run the examples. If you're new to Flask or REST APIs, this project can serve as a learning resource.</p>"},{"location":"side-projects/restful-apis-with-flask/#getting-started","title":"Getting Started","text":"<ol> <li> <p>Clone the repository:     <pre><code>gh repo clone your-username/rest-apis-with-flask\ncd rest-apis-with-flask\n</code></pre></p> </li> <li> <p>Install dependencies:     <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Run the application:     <pre><code>python app.py\n</code></pre></p> </li> </ol> <p>Thank you for visiting, and I hope you find this project helpful!</p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/","title":"\u5c07 Flask \u61c9\u7528\u7a0b\u5f0f\u90e8\u7f72\u5728 Ubuntu 16.04 Server","text":"<p>\u9996\u5148\uff0c\u6211\u5011\u5fc5\u9808\u8a3b\u518a DigitalOcean \u5e33\u865f\u4e26\u5728\u4e0a\u9762\u79df\u7528\u4e00\u500b\u865b\u64ec\u4e3b\u6a5f\uff0c\u7248\u672c\u70ba Ubuntu 16.04 Server\u3002\u5982\u4f55\u4ee5 SSH \u9023\u7dda\u4e0d\u662f\u8ab2\u7a0b\u91cd\u9ede\uff0c\u56e0\u6b64\u6211\u5011\u5728\u6b64\u5148\u7565\u904e\u3002\u9023\u7dda\u5f8c\u7dca\u63a5\u8457\u9032\u884c\u5e7e\u9805\u4e8b\u524d\u6e96\u5099\uff1a</p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#_1","title":"\u66f4\u65b0\u5009\u5eab\u6e05\u55ae","text":"<pre><code># apt-get update\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#os","title":"\u5728 OS \u4e0a\u5275\u5efa\u65b0\u4f7f\u7528\u8005","text":"<pre><code># adduser &lt;username&gt;\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#super-user","title":"\u7d66\u4e88 super user \u6b0a\u9650","text":"<p>\u9032\u5165 <code>/etc/sudoers</code> \u6a94\u6848\uff1a <pre><code># visudo\n</code></pre></p> <p>\u5728 \"User privilege specification\" \u4e0b\u65b9\u66ff\u65b0\u4f7f\u7528\u8005\u52a0\u5165 super user \u7684\u6b0a\u9650\uff1a <pre><code>&lt;username&gt; ALL=(ALL:ALL) ALL\n</code></pre></p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#1-postresql","title":"1. \u5b89\u88dd\u4e26\u8a2d\u5b9a PostreSQL \u8cc7\u6599\u5eab","text":""},{"location":"side-projects/restful-apis-with-flask/basics/section9/#postgresql","title":"\u5b89\u88dd PostgreSQL","text":"<pre><code># apt-get install postgresql postgresql-contrib\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#postgres","title":"\u5207\u63db\u6210 <code>postgres</code> \u4f7f\u7528\u8005","text":"<pre><code># sudo -i -u postgres\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#postgresql_1","title":"\u66ff\u65b0\u4f7f\u7528\u8005\uff0c\u5275\u5efa PostgreSQL \u7576\u4e2d\u7684\u5e33\u865f\u548c\u8cc7\u6599\u5eab","text":"<pre><code>$ createuser &lt;username&gt; -P\n$ createdb &lt;username&gt;\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#postgresql_2","title":"\u5f37\u5236\u4ee5\u5bc6\u78bc\u767b\u5165 PostgreSQL","text":"<p>\u9032\u5165 <code>pg_hba.conf</code> \u914d\u7f6e\u6a94\uff1a <pre><code>$ nano /etc/postgresql/10/main/pg_hba.conf\n</code></pre></p> <p>\u5c07 <pre><code>local all all peer\n</code></pre> \u6539\u70ba <pre><code>local all all md5\n</code></pre></p> <p>\u5b8c\u6210\u5f8c\u8df3\u51fa\u914d\u7f6e\u6a94\uff0c\u63a5\u8457\u6539\u70ba\u65b0\u4f7f\u7528\u8005\u4f86\u64cd\u4f5c\u3002</p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#2-nginx","title":"2. \u5b89\u88dd\u4e26\u8a2d\u5b9a Nginx \u4f3a\u670d\u5668","text":""},{"location":"side-projects/restful-apis-with-flask/basics/section9/#nginx","title":"\u5b89\u88dd Nginx","text":"<pre><code>$ sudo apt-get install nginx\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#nginx-ssh","title":"\u958b\u555f\u9632\u706b\u7246\u4e26\u5141\u8a31 <code>nginx</code> \u548c <code>ssh</code>","text":"<pre><code>$ sudo ufw enable\n$ sudo ufw allow 'Nginx HTTP'\n$ sudo ufw allow ssh\n</code></pre>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#flask-nginx","title":"\u66ff\u6211\u5011\u7684 Flask \u61c9\u7528\u7a0b\u5f0f\u52a0\u5165 Nginx \u914d\u7f6e\u6a94","text":"<p>\u5275\u5efa\u4e26\u9032\u5165 <code>items-rest.conf</code> \u914d\u7f6e\u6a94\uff1a <pre><code>$ sudo nano /etc/nginx/sites-available/items-rest.conf\n</code></pre></p> <p>\u8f38\u5165\u4ee5\u4e0b\u5167\u5bb9\uff1a <pre><code>server {\n    listen 80;\n    real_ip_header X-Forwarded-For;\n    set_real_ip_from 127.0.0.1;\n    server_name localhost;\n\n    location / {\n        include uwsgi_params;\n        uwsgi_pass unix:/var/www/html/items-rest/socket.sock;\n        uwsgi_modifier1 30;\n    }\n\n    error_page 404 /404.html;\n    location = 404.html {\n        root /usr/share/nginx/html;\n    }\n\n    error_page 500 502  503 504 50x.html;\n    location = /50x.html {\n        root /usr/share/nginx/html;\n    }\n}\n</code></pre></p> <p>\u5b8c\u6210\u5f8c\u8df3\u51fa\u914d\u7f6e\u6a94\uff0c\u63a5\u8457\u5efa\u7acb soft link\uff0c\u555f\u7528\u914d\u7f6e\uff1a <pre><code>$ sudo ln -s /etc/nginx/sites-available/items-rest.conf /etc/nginx/sites-enabled/\n</code></pre></p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#3-flask","title":"3. \u8a2d\u5b9a Flask \u61c9\u7528\u7a0b\u5f0f\u6240\u9700\u57f7\u884c\u74b0\u5883","text":""},{"location":"side-projects/restful-apis-with-flask/basics/section9/#_2","title":"\u5275\u5efa\u5c08\u6848\u76ee\u9304\u4e26\u7d66\u4e88\u9069\u7576\u6b0a\u9650","text":"<pre><code>$ sudo mkdir /var/www/html/items-rest\n$ sudo chown &lt;username&gt;:&lt;username&gt; /var/www/html/items-rest\n</code></pre> <p>\u5b8c\u6210\u5f8c\u9032\u5165\u8a72\u76ee\u9304\uff1a <pre><code>$ cd /var/www/html/items-rest\n</code></pre></p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#_3","title":"\u8a2d\u5b9a\u5c08\u6848\u6240\u9700\u74b0\u5883","text":"<p>\u4e0b\u8f09\u5c08\u6848\u5167\u5bb9\u4e26\u5275\u5efa\u65e5\u8a8c\u6a94\u76ee\u9304\uff1a <pre><code>$ git clone https://github.com/schoolofcode-me/stores-rest-api.git .\n$ mkdir log\n</code></pre></p> <p>\u5efa\u7acb\u865b\u64ec\u74b0\u5883\u4e26\u5b89\u88dd\u6240\u9700\u5957\u4ef6\uff1a <pre><code>$ sudo apt-get install python-pip python3-dev libpq-dev\n$ pip install virtualenv\n$ virtualenv venv --python=python3.6\n$ source venv/bin/activate\n(venv)$ pip install -r requirements.txt\n</code></pre></p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#4-uwsgi","title":"4. \u8a2d\u5b9a uWSGI \u4f3a\u670d\u5668","text":""},{"location":"side-projects/restful-apis-with-flask/basics/section9/#uwsgi_items_restservice-ubuntu","title":"\u5275\u5efa <code>uwsgi_items_rest.service</code> Ubuntu \u670d\u52d9","text":"<p>\u5275\u5efa\u4e26\u9032\u5165 <code>uwsgi_items_rest.service</code> \u6a94\uff1a <pre><code>$ sudo nano /etc/systemd/system/uwsgi_items_rest.service\n</code></pre></p> <p>\u8f38\u5165\u4ee5\u4e0b\u5167\u5bb9\uff1a <pre><code>[Unit]\nDescription=uWSGI items rest\n\n[Service]\nEnvironment=DATABASE_URL=postgres://jose:1234@localhost:5432/jose\nExecStart=/var/www/html/items-rest/venv/bin/uwsgi --master --emperor /var/www/html/items-rest/uwsgi.ini --die-on-term --uid jose --gid jose --logto /var/www/html/items-rest/log/emperor.log\nRestart=always\nKillSignal=SIGQUIT\nType=notify\nNotifyAccess=all\n\n[Install]\nWantedBy=multi-user.target\n</code></pre></p> <p>\u5b8c\u6210\u5f8c\u8df3\u51fa\u6a94\u6848\u3002</p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#uwsgiini","title":"\u4fee\u6539 <code>uwsgi.ini</code> \u914d\u7f6e\u6a94","text":"<p>\u9032\u5165\u5c08\u6848\u5167\u7684 <code>uwsgi.ini</code> \u914d\u7f6e\u6a94\uff1a <pre><code>$ nano uwsgi.ini\n</code></pre></p> <p>\u8f38\u5165\u4ee5\u4e0b\u5167\u5bb9\uff1a <pre><code>[uwsgi]\nbase = /var/www/html/items-rest\napp = run\nmodule = %(app)\n\nhome = %(base)/venv\npythonpath = %(base)\n\nsocket = %(base)/socket.sock\n\nchmod-socket = 777\n\nprocesses = 8\n\nthreads = 8\n\nharakiri = 15\n\ncallable = app\n\nlogto = /var/www/html/items-rest/log/%n.log\n</code></pre></p> <p>\u5b8c\u6210\u5f8c\u8df3\u51fa\u914d\u7f6e\u6a94\u3002</p>"},{"location":"side-projects/restful-apis-with-flask/basics/section9/#5-flask","title":"5. \u555f\u52d5 Flask \u61c9\u7528\u7a0b\u5f0f","text":"<p>\u522a\u9664 Nginx \u9810\u8a2d\u914d\u7f6e\u6a94\uff0c\u907f\u514d\u8b80\u53d6\u932f\u8aa4\u7684\u914d\u7f6e\u6a94\uff0c\u63a5\u8457 reload \u4e26 restart\uff1a <pre><code>$ sudo rm /etc/nginx/sites-enabled/default\n$ sudo systemctl reload nginx \n$ sudo systemctl restart nginx\n</code></pre></p> <p>\u555f\u52d5 <code>uwsgi_items_rest</code> \u670d\u52d9\uff1a <pre><code>$ sudo systemctl start uwsgi_items_rest\n</code></pre></p> <p>\u5b8c\u6210\uff01</p>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/","title":"Advanced","text":""},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-1-course-preparations","title":"Section 1: Course Preparations","text":"<p>Preparations for the course:</p> <ul> <li>Simplified authentication mechanism.</li> <li>Added type hinting.</li> <li>Unified code style.</li> <li>Changed all <code>Resource</code> methods to class methods (using <code>@classmethod</code>).</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-2-marshmallow-integration","title":"Section 2: Marshmallow Integration","text":"<p>Introducing <code>marshmallow</code>, <code>flask-marshmallow</code>, and <code>marshmallow-sqlalchemy</code>:</p> <ul> <li>Simplified request parsing, <code>Model</code> object creation, and JSON responses by defining <code>Schema</code> for each <code>Resource</code>.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-3-email-verification","title":"Section 3: Email Verification","text":"<ul> <li>Implemented user email verification process (using Mailgun).</li> <li>Used <code>.env</code> files to store sensitive data.</li> <li>Returned <code>.html</code> files in <code>Flask-RESTful</code> using <code>make_response()</code> and <code>render_template()</code>.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-4-optimized-email-verification","title":"Section 4: Optimized Email Verification","text":"<p>Optimized the email verification process:</p> <ul> <li>Added expiration for verification and resend functionality.</li> <li>Refactored project structure by treating <code>confirmation</code> as a resource.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-6-secure-configuration-and-file-uploads","title":"Section 6: Secure Configuration and File Uploads","text":"<ul> <li>Configured the application more securely (using <code>from_object()</code> and <code>from_envvar()</code>).</li> <li>Learned the relationships between <code>WSGI</code>, <code>uwsgi</code>, <code>uWSGI</code>, and <code>Werkzeug</code>.</li> <li>Introduced <code>Flask-Uploads</code> for handling file uploads, downloads, and deletions (using <code>UploadSet</code>, <code>FileStorage</code>).</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-7-database-version-control","title":"Section 7: Database Version Control","text":"<ul> <li>Introduced <code>Flask-Migrate</code> for database version control, including adding, deleting, and modifying details.</li> <li>Common commands include <code>flask db init</code>, <code>flask db upgrade</code>, <code>flask db downgrade</code>.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-8-oauth-integration","title":"Section 8: OAuth Integration","text":"<ul> <li>Learned OAuth third-party login flow (e.g., GitHub), including authentication, authorization, and obtaining <code>access_token</code>.</li> <li>Introduced <code>Flask-OAuthlib</code>.</li> <li>Used Flask's <code>g</code> to store <code>access_token</code>.</li> <li>Allowed third-party login users to set passwords.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/advanced/#section-9-payment-integration","title":"Section 9: Payment Integration","text":"<ul> <li>Integrated <code>Stripe</code> for third-party payment processing.</li> <li>Added an \"Order\" resource and implemented many-to-many relationships using <code>Flask-SQLAlchemy</code>.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/","title":"Basics","text":""},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-3-introduction-to-flask","title":"Section 3: Introduction to Flask","text":"<ul> <li>Introduction to the Flask web framework, using decorators to set up application routes.</li> <li>Understanding common HTTP request methods: GET, POST, PUT, DELETE.</li> <li>Understanding common HTTP status codes: 200, 201, 202, 401, 404.</li> <li>Understanding RESTful API design principles focusing on \"resources\" and statelessness.</li> <li>Implementing a RESTful API server application.</li> <li>Testing APIs using the Postman application.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-4-flask-restful-and-jwt","title":"Section 4: Flask-RESTful and JWT","text":"<ul> <li>Implementing RESTful API server applications using <code>Flask-RESTful</code>.</li> <li>Implementing JSON Web Token (JWT) authentication using <code>Flask-JWT</code>.</li> <li>Parsing user input JSON data using <code>RequestParser</code>.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-5-database-integration-with-sqlite","title":"Section 5: Database Integration with SQLite","text":"<ul> <li>Introducing <code>sqlite3</code> to store user and item information in a database.</li> <li>Implementing user registration functionality.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-6-database-integration-with-sqlalchemy","title":"Section 6: Database Integration with SQLAlchemy","text":"<ul> <li>Introducing <code>Flask-SQLAlchemy</code> to interact with the database using ORM.</li> <li>Adding store information with a one-to-many relationship to items.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-7-deploying-to-heroku","title":"Section 7: Deploying to Heroku","text":"<p>Deploying the Flask application to Heroku and using Heroku's PostgreSQL. Steps:</p> <ol> <li>Modify the project locally (e.g., add <code>Procfile</code>, <code>runtime.txt</code>, <code>uwsgi.ini</code>), then <code>commit</code> and <code>push</code> to the specified GitHub repo.</li> <li>Register on Heroku, create an application, connect it to the GitHub repo, and add the <code>heroku/python</code> buildpack and <code>Heroku Postgres</code> add-on.</li> <li>Install the Heroku CLI locally (see here) and log in using <code>heroku login</code>.</li> <li>Add a Heroku remote using <code>heroku git:remote -a &lt;app-name&gt;</code>.</li> <li>Deploy the project by pushing the <code>basics/section8</code> subdirectory to Heroku using <code>git subtree push --prefix basics/section8 heroku master</code>.</li> </ol> <p>Testing: Access here to retrieve all stores and their items in the database, returned in JSON format.</p>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-8-deploying-to-digitalocean","title":"Section 8: Deploying to DigitalOcean","text":"<p>Deploying the Flask application to a DigitalOcean Droplet. Steps:</p> <ol> <li>Register on DigitalOcean, create a Droplet with Ubuntu 16.04, set up SSH, and connect using PuTTY.</li> <li>Create a new user on the operating system.</li> <li>Install and configure PostgreSQL, including creating a new user and database with appropriate permissions.</li> <li>Install and configure the Nginx server, including firewall settings, error pages, and uwsgi parameters.</li> <li>Set up a Python virtual environment, install required packages, and clone the project from GitHub.</li> <li>Configure an Ubuntu service to run the uwsgi server, including log directories, processes, and threads.</li> </ol> <p>Testing: Access here (created on 2020/05/30) to retrieve all stores and their items in the database, returned in JSON format.</p>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-9-domain-and-https","title":"Section 9: Domain and HTTPS","text":"<p>Book</p> <ul> <li>Registering a domain and configuring DNS servers.</li> <li>Obtaining an SSL certificate for HTTPS communication and configuring Nginx.</li> </ul>"},{"location":"side-projects/restful-apis-with-flask/docs/basics/#section-11-advanced-jwt-features","title":"Section 11: Advanced JWT Features","text":"<p>Introducing <code>Flask-JWT-Extended</code>:</p> <ul> <li>Implementing token-refreshing to improve user experience by avoiding frequent logins while requiring re-login for critical actions for security (using <code>@jwt_refresh_token_required</code>, <code>create_refresh_token()</code>, <code>create_access_token()</code>).</li> <li>Responding with appropriate data based on user roles (visitor, user, admin) using <code>@jwt.user_claims_loader</code>, <code>@jwt_optional</code>, <code>get_jwt_claims()</code>.</li> <li>Returning specific error messages for token-related issues using <code>@jwt.expired_token_loader</code>, <code>@jwt.invalid_token_loader</code>, <code>@jwt.needs_fresh_token_loader</code>.</li> <li>Implementing a logout mechanism using a blacklist (with <code>@jwt.token_in_blacklist_loader</code>, <code>get_raw_jwt()</code>).</li> </ul> <p>Book</p>"},{"location":"side-projects/retail-lakehouse/","title":"Retail Lakehouse with Debezium, Kafka, Iceberg, and Trino","text":"","tags":["Debizium","Apache Kafka","Apache Iceberg","Apache Spark","Trino"]},{"location":"side-projects/retail-lakehouse/#highlights","title":"\ud83d\udca1 Highlights","text":"Debezium Kafka Source ConnectorKafka ClusterIceberg Kafka Sink ConnectorIceberg Data LakehouseTrino <p>Highlights</p> <ul> <li> Implemented real-time, event-driven data pipelines by capturing MySQL database change events (CDC) with Debezium and streaming them into Kafka, enabling downstream analytics.</li> <li> Designed a non-intrusive CDC architecture leveraging MySQL/PostgreSQL binary logs, requiring no changes to source systems while ensuring exactly-once delivery and high fault tolerance via Kafka Connect.</li> <li> Improved system resilience and observability through Debezium's offset tracking and recovery features, enabling resumable pipelines and reliable data integration across distributed environments.</li> </ul> <p>Highlights</p> <ul> <li> Provisioned a fault-tolerant Kafka cluster on Kubernetes using the Strimzi Operator, enabling declarative configuration and seamless lifecycle management</li> <li> Enabled KRaft (Kafka Raft Metadata mode) with a dual-role cluster, removing dependency on ZooKeeper and simplifying cluster architecture</li> <li> Designed for high availability by replicating Kafka topics and internal state across multiple brokers using replication factor and in-sync replicas (ISR).</li> </ul> <p>Highlights</p> <ul> <li> Ensured centralized commit coordination for Apache Iceberg via the Kafka Sink Connector, enabling consistent and atomic writes across distributed systems.</li> <li> Achieved exactly-once delivery semantics between Kafka and Iceberg tables, minimizing data duplication and ensuring data integrity.</li> <li> Utilized <code>DebeziumTransform</code> SMT to adapt Debezium CDC messages for compatibility with Iceberg's CDC feature, supporting real-time change propagation.</li> <li> Enabled automatic table creation and schema evolution, simplifying integration and reducing operational overhead when ingesting data into Iceberg tables.</li> </ul> <p>Highlights</p> <ul> <li> Adopted Apache Iceberg to bring ACID-compliant transactions and schema evolution to the data lake architecture.</li> <li> Managed Iceberg tables using AWS Glue Data Catalog as the catalog layer and Amazon S3 as the storage layer.</li> <li> Enabled data debugging and auditability through Iceberg's time travel and snapshot rollback features.</li> <li> Implemented branching and tagging (WAP) to support isolated writes, data validation, and safe promotion in production workflows</li> </ul> <p>Highlights</p> <ul> <li> Integrated Trino to enable federated SQL queries across Apache Iceberg (S3) and external systems like BigQuery, improving analytical agility.</li> <li> Simplified data access across multiple data sources without data duplication, enabling ad-hoc analytics and reporting from a unified SQL interface.</li> <li> Integrated Google OAuth 2.0 with Trino to enable token-based authentication, improving platform auditability and user accountability.</li> </ul>","tags":["Debizium","Apache Kafka","Apache Iceberg","Apache Spark","Trino"]},{"location":"side-projects/retail-lakehouse/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<p>Architecture Overview</p> <p></p> <p>Architecture Overview (Observability Engineering)</p>","tags":["Debizium","Apache Kafka","Apache Iceberg","Apache Spark","Trino"]},{"location":"side-projects/retail-lakehouse/#whats-inside","title":"\ud83d\uddc2\ufe0f What's Inside?","text":"<p>First, clone the repository:</p> <pre><code>mkdir -p ~/Projects\ncd ~/Projects\ngit clone git@github.com:kuanchoulai10/retail-lakehouse.git\n</code></pre> <p>The project structure looks like this:</p> <pre><code>.\n\u251c\u2500\u2500 kafka-cluster\n\u251c\u2500\u2500 mysql\n\u251c\u2500\u2500 kafka-debezium-mysql-connector\n\u251c\u2500\u2500 kafka-iceberg-connector\n\u251c\u2500\u2500 trino\n\u251c\u2500\u2500 spark\n\u2514\u2500\u2500 prometheus-grafana\n</code></pre>","tags":["Debizium","Apache Kafka","Apache Iceberg","Apache Spark","Trino"]},{"location":"side-projects/retail-lakehouse/#deployment-steps","title":"\ud83d\udcd1 Deployment Steps","text":"<p>The basic deployment path includes the following steps:</p> <p>Deployment Steps</p> <ul> <li> Deploy a Kafka Cluster via the Strimzi Operator</li> <li> Deploy a MySQL Database</li> <li> Deploy a Debezium Kafka Source Connector</li> <li> Deploy an Iceberg Kafka Sink Connector</li> <li> Deploy a Trino Cluster</li> <li> Deploy a Spark Cluster (WIP)</li> <li> Deploy Prometheus and Grafana (WIP)</li> </ul>","tags":["Debizium","Apache Kafka","Apache Iceberg","Apache Spark","Trino"]},{"location":"side-projects/retail-lakehouse/prerequisites/","title":"Prerequisites","text":"<p>For this project, I use a Mac mini (2024) with the following specifications:</p> <ul> <li>Apple M4 chip</li> <li>10-core CPU</li> <li>10-core GPU</li> <li>16-core Neural Engine</li> <li>32GB RAM</li> <li>512GB SSD</li> </ul>"},{"location":"side-projects/retail-lakehouse/prerequisites/#installing-required-tools-via-homebrew","title":"Installing Required Tools via Homebrew","text":"<p>Make sure you have Homebrew installed. Then install the required tools:</p> <pre><code>brew install colima, docker, kubectl, minikube, helm, openjdk@21 jsonnet jsonnet-bundler\n</code></pre>"},{"location":"side-projects/retail-lakehouse/prerequisites/#setting-up-a-local-kubernetes-cluster","title":"Setting Up a Local Kubernetes Cluster","text":"<p>Start colima with more resources:</p> <pre><code>colima start \\\n    --cpu 9 \\\n    --memory 24 \\\n    --disk 120 \\\n    --runtime docker \\\n    --profile data\n</code></pre> <p>This will create a local VM with docker runtime. After installing colima, make sure <code>docker</code> uses the colima context (<code>docker context ls</code>).</p> <p>Then start minikube with more resources:</p> <pre><code>minikube start \\\n  --profile retail-lakehouse \\\n  --nodes 3 \\\n  --cpus 3 \\\n  --memory 8G \\\n  --disk-size 40G \\\n  --driver docker \\\n  --container-runtime docker \\\n  --kubernetes-version v1.30.2 \\\n  --addons registry \\\n  --insecure-registry \"10.0.0.0/24\" \\\n  --delete-on-failure\n</code></pre> <p>This will create a 3-node Kubernetes cluster (3 CPUs, 8GB RAM, 40GB disk each) with a local container registry addon enabled. The <code>--insecure-registry</code> flag allows pushing images to the local registry.</p> <p> /// K8s Cluster Environment ///</p>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-debezium-mysql-connector/","title":"Debezium MySQL Kafka Connector Deployment","text":"<p>Architecture Overview</p> <p>Make sure you have deployed a Kafka cluster and a MySQL database first.</p> <p>Deployment Steps</p> <ul> <li> Deploy a Kafka Cluster via the Strimzi Operator</li> <li> Deploy a MySQL Database</li> <li> Deploy a Debezium Kafka Source Connector</li> <li> Deploy an Iceberg Kafka Sink Connector</li> </ul> <p>After the Kafka cluster and MySQL database are up and running, you can deploy the Debezium MySQL Kafka Connector by running the following commands:</p> <pre><code>cd ~/Projects/retail-lakehouse/kafka-debezium-mysql-connector\nbash install.sh\n</code></pre> install.sh <pre><code>#!/bin/bash\n\nset -e\n\n# Deploy Debezium Connect Cluster\nkubectl apply -f debezium-secret.yaml\nkubectl apply -f debezium-role.yaml\nkubectl apply -f debezium-role-binding.yaml\nkubectl apply -f debezium-connect-cluster.yaml -n kafka-cdc\nsleep 5\nkubectl logs pod/debezium-connect-cluster-connect-build -n kafka-cdc\nsleep 5\nkubectl wait --for=condition=terminated pod -l app.kubernetes.io/name=kafka-connect-build -n kafka-cdc --timeout=1200s\n\nkubectl apply -f debezium-connector.yaml -n kafka-cdc\nkubectl get kafkaconnector -n kafka-cdc\n</code></pre> <p>This script will deploy the necessary secrets and roles in order to create a Debezium Connect Cluster and a Debezium Source Connector on that cluster.</p> <p>If you don't like my script and want to do it step by step manually, please continue reading. This article will walk you through how to deploy a Debezium MySQL Kafka Connector on Kubernetes step by step, explaining each part along the way.</p>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-debezium-mysql-connector/#create-secret-and-role-for-accessing-the-database","title":"Create Secret and Role for Accessing the Database","text":"<pre><code>kubectl apply -f debezium-secret.yaml\nkubectl apply -f debezium-role.yaml\nkubectl apply -f debezium-role-binding.yaml\n</code></pre> Result <pre><code>secret/debezium-secret created\nrole.rbac.authorization.k8s.io/debezium-role created\nrolebinding.rbac.authorization.k8s.io/debezium-role-binding created\n</code></pre> <p>YAML Files</p> debezium-secret.yaml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: debezium-secret\n  namespace: kafka-cdc\ntype: Opaque\ndata:\n  username: ZGViZXppdW0=\n  password: ZGJ6\n</code></pre> debezium-role.yaml<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: debezium-role\n  namespace: kafka-cdc\nrules:\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  resourceNames: [\"debezium-secret\"]\n  verbs: [\"get\"]\n</code></pre> debezium-role-binding.yaml<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: debezium-role-binding\n  namespace: kafka-cdc\nsubjects:\n- kind: ServiceAccount\n  name: debezium-connect-cluster-connect\n  namespace: kafka-cdc\nroleRef:\n  kind: Role\n  name: debezium-role\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-debezium-mysql-connector/#create-a-debezium-kafka-connect-cluster","title":"Create a Debezium Kafka Connect Cluster","text":"<p>To deploy a Debezium MySQL connector, you need to deploy a Kafka Connect cluster with the required connector plug-in(s), before instantiating the actual connector itself.</p> <p>As the first step, a container image for Kafka Connect with the plug-in has to be created.</p> <p>Strimzi also can be used for building and pushing the required container image for us. In fact, both tasks can be merged together and instructions for building the container image can be provided directly within the <code>KafkaConnect</code> object specification:</p> Prerequisite <p>Make sure you have enabled the local registry and set up <code>insecure-registry</code> in your Minikube cluster. If you have followed the instructions in the Prerequisites section, you should have already done this step. If not, you can enable it now by running the following command:</p> <pre><code>minikube addons enable registry -p retail-lakehouse\n</code></pre> Result <pre><code>\ud83d\udca1  registry is an addon maintained by minikube. For any concerns contact minikube on GitHub.\nYou can view the list of minikube maintainers at: https://github.com/kubernetes/minikube/blob/master/OWNERS\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                      \u2502\n\u2502    Registry addon with docker driver uses port 49609 please use that instead of default port 5000    \u2502\n\u2502                                                                                                      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\ud83d\udcd8  For more information see: https://minikube.sigs.k8s.io/docs/drivers/docker\n    \u25aa Using image docker.io/registry:2.8.3\n    \u25aa Using image gcr.io/k8s-minikube/kube-registry-proxy:0.0.6\n\ud83d\udd0e  Verifying registry addon...\n\ud83c\udf1f  The 'registry' addon is enabled\n</code></pre> <p>You can check the IP address of the local registry by running the following command:</p> <pre><code>kubectl -n kube-system get svc registry -o jsonpath='{.spec.clusterIP}'\n</code></pre> Result <pre><code>10.109.40.28\n</code></pre> debezium-connect-cluster.yaml <pre><code>apiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaConnect\nmetadata:\n  name: debezium-connect-cluster\n  namespace: kafka-cdc\n  annotations:\n    strimzi.io/use-connector-resources: \"true\"\nspec:\n  version: 4.0.0\n  replicas: 1\n  bootstrapServers: kafka-cluster-kafka-bootstrap:9092\n  config:\n    config.providers: secrets\n    config.providers.secrets.class: io.strimzi.kafka.KubernetesSecretConfigProvider\n    group.id: debezium-connect-cluster\n    exactly.once.support: required\n    offset.storage.topic: debezium-connect-cluster-offsets\n    config.storage.topic: debezium-connect-cluster-configs\n    status.storage.topic: debezium-connect-cluster-status\n    # -1 means it will use the default replication factor configured in the broker\n    offset.storage.replication.factor: -1\n    config.storage.replication.factor: -1\n    status.storage.replication.factor: -1\n  build:\n    output:\n      # https://strimzi.io/docs/operators/latest/configuring.html#type-DockerOutput-reference\n      type: docker\n      image: 10.109.40.28/debezium-mysql-connector:latest\n    plugins:\n      - name: debezium-mysql-connector\n        artifacts:\n          - type: tgz\n            url: https://repo1.maven.org/maven2/io/debezium/debezium-connector-mysql/3.1.0.Final/debezium-connector-mysql-3.1.0.Final-plugin.tar.gz\n</code></pre> <pre><code>kubectl apply -f debezium-connect-cluster.yaml -n kafka-cdc\n</code></pre> Result <pre><code>kafkaconnect.kafka.strimzi.io/debezium-connect-cluster created\n</code></pre> <p>Check the current resources in the <code>kafka-cdc</code> namespace:</p> <pre><code>kubectl get all -n kafka-cdc\n</code></pre> Result <pre><code>NAME                                                    READY   STATUS    RESTARTS   AGE\npod/kafka-cluster-dual-role-0                           1/1     Running   0          66m\npod/kafka-cluster-dual-role-1                           1/1     Running   0          66m\npod/kafka-cluster-dual-role-2                           1/1     Running   0          66m\npod/kafka-cluster-entity-operator-5b998f6cbf-c8hdf      2/2     Running   0          65m\npod/debezium-connect-cluster-connect-build              1/1     Running   0          49s\npod/mysql-6b84fd947d-9g9lt                              1/1     Running   0          60m\n\nNAME                                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                        AGE\nservice/kafka-cluster-kafka-bootstrap      ClusterIP   10.105.50.103   &lt;none&gt;        9091/TCP,9092/TCP,9093/TCP                     66m\nservice/kafka-cluster-kafka-brokers        ClusterIP   None            &lt;none&gt;        9090/TCP,9091/TCP,8443/TCP,9092/TCP,9093/TCP   66m\nservice/mysql                              ClusterIP   None            &lt;none&gt;        3306/TCP                                       60m\n\nNAME                                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/kafka-cluster-entity-operator      1/1     1            1           65m\ndeployment.apps/mysql                              1/1     1            1           60m\n\nNAME                                                          DESIRED   CURRENT   READY   AGE\nreplicaset.apps/kafka-cluster-entity-operator-5b998f6cbf      1         1         1       65m\nreplicaset.apps/mysql-6b84fd947d                              1         1         1       60m\n</code></pre> <p>After a while, when the build is complete, you should see the <code>debezium-connect-cluster-connect-build</code> pod disappear and a new pod named <code>debezium-connect-cluster-connect-0</code> appear:</p> <pre><code>NAME                                                    READY   STATUS    RESTARTS   AGE\npod/kafka-cluster-dual-role-0                           1/1     Running   0          100m\npod/kafka-cluster-dual-role-1                           1/1     Running   0          100m\npod/kafka-cluster-dual-role-2                           1/1     Running   0          100m\npod/kafka-cluster-entity-operator-5b998f6cbf-c8hdf      2/2     Running   0          99m\npod/debezium-connect-cluster-connect-0                  1/1     Running   0          30m\npod/mysql-6b84fd947d-9g9lt                              1/1     Running   0          94m\n\nNAME                                           TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                        AGE\nservice/kafka-cluster-kafka-bootstrap          ClusterIP   10.105.50.103    &lt;none&gt;        9091/TCP,9092/TCP,9093/TCP                     100m\nservice/kafka-cluster-kafka-brokers            ClusterIP   None             &lt;none&gt;        9090/TCP,9091/TCP,8443/TCP,9092/TCP,9093/TCP   100m\nservice/debezium-connect-cluster-connect       ClusterIP   None             &lt;none&gt;        8083/TCP                                       30m\nservice/debezium-connect-cluster-connect-api   ClusterIP   10.100.229.177   &lt;none&gt;        8083/TCP                                       30m\nservice/mysql                                  ClusterIP   None             &lt;none&gt;        3306/TCP                                       94m\n\nNAME                                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/kafka-cluster-entity-operator      1/1     1            1           99m\ndeployment.apps/mysql                              1/1     1            1           94m\n\nNAME                                                          DESIRED   CURRENT   READY   AGE\nreplicaset.apps/kafka-cluster-entity-operator-5b998f6cbf      1         1         1       99m\nreplicaset.apps/mysql-6b84fd947d                              1         1         1       94m\n</code></pre> Warning <p>If the build fails, you can check the logs of the build pod to see what went wrong. You can also describe the pod to get more information about its status.</p> <pre><code>kubectl logs pod/debezium-connect-cluster-build -n kafka-cdc\nkubectl describe pod/debezium-connect-cluster-build -n kafka-cdc\n</code></pre> <p>If the build success, but the pod is not running successfully, you can check the logs of the running pod to see what went wrong. You can also describe the pod to get more information about its status.</p> <pre><code>kubectl logs pod/debezium-connect-cluster-connect-0 -n kafka-cdc\nkubectl describe pod/debezium-connect-cluster-connect-0 -n kafka-cdc\n</code></pre> <p>You can also check the Minikube configuration to see if the local registry is set up correctly.</p> <pre><code>cat ~/.minikube/profiles/retail-lakehouse/config.json | jq .\n</code></pre> <p>If there is any issue with pulling the image from the local registry, you can check the local registry to see if the image is there.</p> <pre><code>kubectl port-forward --namespace kube-system service/registry 5000:80\n</code></pre> <p>Create another terminal window, then run the following commands to check the local registry:</p> <pre><code>curl http://localhost:5000/v2/_catalog\ncurl http://localhost:5000/v2/debezium-mysql-connector/tags/list\n</code></pre>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-debezium-mysql-connector/#create-a-debezium-source-connector","title":"Create a Debezium Source Connector","text":"<pre><code>kubectl apply -f debezium-connector.yaml -n kafka-cdc\n</code></pre> Result <pre><code>kafkaconnector.kafka.strimzi.io/debezium-connector created\n</code></pre> debezium-connector.yaml <pre><code>apiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaConnector\nmetadata:\n  name: debezium-connector\n  labels:\n    strimzi.io/cluster: debezium-connect-cluster\nspec:\n  class: io.debezium.connector.mysql.MySqlConnector\n  tasksMax: 1\n  config:\n    tasks.max: 1\n    database.hostname: mysql\n    database.port: 3306\n    database.user: ${secrets:kafka-cdc/debezium-secret:username}\n    database.password: ${secrets:kafka-cdc/debezium-secret:password}\n    database.server.id: 184054\n    topic.prefix: mysql\n    database.include.list: inventory\n    schema.history.internal.kafka.bootstrap.servers: kafka-cluster-kafka-bootstrap:9092\n    schema.history.internal.kafka.topic: schema-changes.inventory\n    exactly.once.source.support: enabled\n</code></pre> <ul> <li><code>database.include.list: inventory</code>: Specifies the database to capture changes from.</li> <li><code>topic.prefix: mysql</code>: Sets the prefix for Kafka topics where change events will be published.</li> <li><code>schema.history.internal.kafka.topic: schema-changes.inventory</code>: Specifies the Kafka topic to store schema history.</li> <li><code>exactly.once.source.support: enabled</code>: Enables exactly-once delivery semantics for the source connector.</li> </ul> <pre><code>kubectl get kafkaconnector debezium-connector -n kafka-cdc\n</code></pre> Result <pre><code>NAME                 CLUSTER                    CONNECTOR CLASS                              MAX TASKS   READY\ndebezium-connector   debezium-connect-cluster   io.debezium.connector.mysql.MySqlConnector   1           True\n</code></pre>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-debezium-mysql-connector/#verify-the-cdc-pipeline","title":"Verify the CDC Pipeline","text":"<p>In order to make sure the CDC pipeline is working correctly, we check 2 things:</p> <p>Checklist</p> <ul> <li> In the Kafka cluster, we should see topics created by the Debezium MySQL Connector, such as <code>mysql.inventory.customers</code>.</li> <li> When we update a record in the <code>customers</code> table in the MySQL database, we should see a corresponding message appear in the <code>mysql.inventory.customers</code> topic.</li> </ul>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-debezium-mysql-connector/#list-topics","title":"List Topics","text":"<p>List all topics in the Kafka cluster:</p> <pre><code>kubectl run kafka-topics-cli \\\n  -n kafka-cdc \\\n  -it --rm \\\n  --image=quay.io/strimzi/kafka:0.46.1-kafka-4.0.0 \\\n  --restart=Never -- \\\n  bin/kafka-topics.sh \\\n    --bootstrap-server kafka-cluster-kafka-bootstrap:9092 \\\n    --list\n</code></pre> Result <pre><code>__consumer_offsets\ndebezium-cluster-configs\ndebezium-cluster-offsets\ndebezium-cluster-status\nmysql\nmysql.inventory.addresses\nmysql.inventory.customers\nmysql.inventory.geom\nmysql.inventory.orders\nmysql.inventory.products\nmysql.inventory.products_on_hand\nschema-changes.inventory\n</code></pre>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-debezium-mysql-connector/#watch-changes","title":"Watch Changes","text":"<p>Watch the <code>mysql.inventory.customers</code> topic for changes:</p> <pre><code>kubectl run kafka-cli \\\n  -n kafka-cdc \\\n  -it --rm \\\n  --image=quay.io/strimzi/kafka:0.46.1-kafka-4.0.0 \\\n  --restart=Never -- \\\n  bin/kafka-console-consumer.sh \\\n    --bootstrap-server kafka-cluster-kafka-bootstrap:9092 \\\n    --topic mysql.inventory.customers \\\n    --partition 0\n    --offset -10\n    --max-messages 10\n</code></pre> <p>Create another terminal window, then enter the MySQL pod and update a record in the <code>customers</code> table. Specifically, we will change the <code>first_name</code> of the customer with <code>id=1001</code> from <code>Sally</code> to <code>Sally Marie</code>:</p> <pre><code>kubectl exec -n kafka-cdc -it mysql-6b84fd947d-9g9lt -- mysql -uroot -pdebezium\n</code></pre> <pre><code>sql&gt; use inventory;\nsql&gt; update customers set first_name=\"Sally Marie\" where id=1001;\n</code></pre> <p>Go back to the first terminal window where you are watching the <code>mysql.inventory.customers</code> topic. You should see a new message appear that reflects the change you just made in the MySQL database.</p> Result <pre><code>{\n\"schema\": {\n    \"type\": \"struct\",\n    \"fields\": [\n    {\n        \"type\": \"struct\",\n        \"fields\": [\n        {\n            \"type\": \"int32\",\n            \"optional\": false,\n            \"field\": \"id\"\n        },\n        {\n            \"type\": \"string\",\n            \"optional\": false,\n            \"field\": \"first_name\"\n        },\n        {\n            \"type\": \"string\",\n            \"optional\": false,\n            \"field\": \"last_name\"\n        },\n        {\n            \"type\": \"string\",\n            \"optional\": false,\n            \"field\": \"email\"\n        }\n        ],\n        \"optional\": true,\n        \"name\": \"mysql.inventory.customers.Value\",\n        \"field\": \"before\"\n    },\n    {\n        \"type\": \"struct\",\n        \"fields\": [\n        {\n            \"type\": \"int32\",\n            \"optional\": false,\n            \"field\": \"id\"\n        },\n        {\n            \"type\": \"string\",\n            \"optional\": false,\n            \"field\": \"first_name\"\n        },\n        {\n            \"type\": \"string\",\n            \"optional\": false,\n            \"field\": \"last_name\"\n        },\n        {\n            \"type\": \"string\",\n            \"optional\": false,\n            \"field\": \"email\"\n        }\n        ],\n        \"optional\": true,\n        \"name\": \"mysql.inventory.customers.Value\",\n        \"field\": \"after\"\n    },\n    {\n        \"type\": \"struct\",\n        \"fields\": [\n        {\n            \"type\": \"string\",\n            \"optional\": false,\n            \"field\": \"version\"\n        },\n        {\n            \"type\": \"string\",\n            \"optional\": false,\n            \"field\": \"connector\"\n        },\n        {\n            \"type\": \"string\",\n            \"optional\": false,\n            \"field\": \"name\"\n        },\n        {\n            \"type\": \"int64\",\n            \"optional\": false,\n            \"field\": \"ts_ms\"\n        },\n        {\n            \"type\": \"string\",\n            \"optional\": true,\n            \"name\": \"io.debezium.data.Enum\",\n            \"version\": 1,\n            \"parameters\": {\n            \"allowed\": \"true,first,first_in_data_collection,last_in_data_collection,last,false,incremental\"\n            },\n            \"default\": \"false\",\n            \"field\": \"snapshot\"\n        },\n        {\n            \"type\": \"string\",\n            \"optional\": false,\n            \"field\": \"db\"\n        },\n        {\n            \"type\": \"string\",\n            \"optional\": true,\n            \"field\": \"sequence\"\n        },\n        {\n            \"type\": \"int64\",\n            \"optional\": true,\n            \"field\": \"ts_us\"\n        },\n        {\n            \"type\": \"int64\",\n            \"optional\": true,\n            \"field\": \"ts_ns\"\n        },\n        {\n            \"type\": \"string\",\n            \"optional\": true,\n            \"field\": \"table\"\n        },\n        {\n            \"type\": \"int64\",\n            \"optional\": false,\n            \"field\": \"server_id\"\n        },\n        {\n            \"type\": \"string\",\n            \"optional\": true,\n            \"field\": \"gtid\"\n        },\n        {\n            \"type\": \"string\",\n            \"optional\": false,\n            \"field\": \"file\"\n        },\n        {\n            \"type\": \"int64\",\n            \"optional\": false,\n            \"field\": \"pos\"\n        },\n        {\n            \"type\": \"int32\",\n            \"optional\": false,\n            \"field\": \"row\"\n        },\n        {\n            \"type\": \"int64\",\n            \"optional\": true,\n            \"field\": \"thread\"\n        },\n        {\n            \"type\": \"string\",\n            \"optional\": true,\n            \"field\": \"query\"\n        }\n        ],\n        \"optional\": false,\n        \"name\": \"io.debezium.connector.mysql.Source\",\n        \"version\": 1,\n        \"field\": \"source\"\n    },\n    {\n        \"type\": \"struct\",\n        \"fields\": [\n        {\n            \"type\": \"string\",\n            \"optional\": false,\n            \"field\": \"id\"\n        },\n        {\n            \"type\": \"int64\",\n            \"optional\": false,\n            \"field\": \"total_order\"\n        },\n        {\n            \"type\": \"int64\",\n            \"optional\": false,\n            \"field\": \"data_collection_order\"\n        }\n        ],\n        \"optional\": true,\n        \"name\": \"event.block\",\n        \"version\": 1,\n        \"field\": \"transaction\"\n    },\n    {\n        \"type\": \"string\",\n        \"optional\": false,\n        \"field\": \"op\"\n    },\n    {\n        \"type\": \"int64\",\n        \"optional\": true,\n        \"field\": \"ts_ms\"\n    },\n    {\n        \"type\": \"int64\",\n        \"optional\": true,\n        \"field\": \"ts_us\"\n    },\n    {\n        \"type\": \"int64\",\n        \"optional\": true,\n        \"field\": \"ts_ns\"\n    }\n    ],\n    \"optional\": false,\n    \"name\": \"mysql.inventory.customers.Envelope\",\n    \"version\": 2\n},\n\"payload\": {\n    \"before\": {\n    \"id\": 1001,\n    \"first_name\": \"Sally\",\n    \"last_name\": \"Thomas\",\n    \"email\": \"sally.thomas@acme.com\"\n    },\n    \"after\": {\n    \"id\": 1001,\n    \"first_name\": \"Sally Marie\",\n    \"last_name\": \"Thomas\",\n    \"email\": \"sally.thomas@acme.com\"\n    },\n    \"source\": {\n    \"version\": \"3.1.0.Final\",\n    \"connector\": \"mysql\",\n    \"name\": \"mysql\",\n    \"ts_ms\": 1751201044000,\n    \"snapshot\": \"false\",\n    \"db\": \"inventory\",\n    \"sequence\": null,\n    \"ts_us\": 1751201044000000,\n    \"ts_ns\": 1751201044000000000,\n    \"table\": \"customers\",\n    \"server_id\": 1,\n    \"gtid\": null,\n    \"file\": \"binlog.000002\",\n    \"pos\": 401,\n    \"row\": 0,\n    \"thread\": 14,\n    \"query\": null\n    },\n    \"transaction\": null,\n    \"op\": \"u\",\n    \"ts_ms\": 1751201044907,\n    \"ts_us\": 1751201044907793,\n    \"ts_ns\": 1751201044907793970\n}\n}\n</code></pre>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-debezium-mysql-connector/#references","title":"References","text":"<ul> <li>Deploying Debezium on Kubernetes | Debezium Documentation</li> <li>Deploying and Managing | Strimzi Documentation</li> <li>Using the Iceberg framework in AWS Glue | AWS</li> <li>Iceberg Kafka Connector | Iceberg</li> </ul>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-iceberg-connector/","title":"Kafka Cluster Deployment","text":"<p>Architecture Overview</p>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-iceberg-connector/#build-the-iceberg-connector-zip-archive","title":"Build the Iceberg Connector ZIP Archive","text":"<p>Because the Iceberg Kafka Connect connector is not provided by Iceberg, you need to build the connector ZIP archive first. You can do this by cloning the Iceberg repository and checking out the desired version, then running the build command.</p> <pre><code>git clone https://github.com/apache/iceberg.git\ngit checkout apache-iceberg-1.9.1\n\n./gradlew -x test -x integrationTest clean build\n</code></pre> Result <pre><code>Welcome to Gradle 8.13!\n\nHere are the highlights of this release:\n- Daemon JVM auto-provisioning\n- Enhancements for Scala plugin and JUnit testing\n- Improvements for build authors and plugin developers\n\nFor more details see https://docs.gradle.org/8.13/release-notes.html\n\nStarting a Gradle Daemon (subsequent builds will be faster)\nConfiguration on demand is an incubating feature.\n\n&gt; Task :iceberg-aws:validateS3SignerSpec\nValidating spec /Users/kcl/projects/iceberg/aws/src/main/resources/s3-signer-open-api.yaml\n\n&gt; Task :iceberg-open-api:validateRESTCatalogSpec\nValidating spec /Users/kcl/projects/iceberg/open-api/rest-catalog-open-api.yaml\nSpec is valid.\n\n&gt; Task :iceberg-aws:validateS3SignerSpec\nSpec is valid.\n\n&gt; Task :iceberg-spark:iceberg-spark-3.5_2.12:scalastyleMainCheck\nProcessed 7 file(s)\nFound 0 errors\nFound 0 warnings\nFinished in 4727 ms\n\n&gt; Task :iceberg-bundled-guava:shadowJar\nMANIFEST.MF will be copied to 'META-INF/MANIFEST.MF', overwriting MANIFEST.MF, which has already been copied there.\nfile '/Users/kcl/projects/iceberg/bundled-guava/build/classes/java/main/org/apache/iceberg/GuavaClasses.class' will be copied to 'org/apache/iceberg/GuavaClasses.class', overwriting file '/Users/kcl/projects/iceberg/bundled-guava/build/classes/java/main/org/apache/iceberg/GuavaClasses.class', which has already been copied there.\nfile '/Users/kcl/.gradle/caches/modules-2/files-2.1/com.google.guava/guava/33.4.7-jre/c1f6ad95476208ef852f92919e7a9e22abd83a98/guava-33.4.7-jre.jar' will be copied to 'guava-33.4.7-jre.jar', overwriting file '/Users/kcl/.gradle/caches/modules-2/files-2.1/com.google.guava/guava/33.4.7-jre/c1f6ad95476208ef852f92919e7a9e22abd83a98/guava-33.4.7-jre.jar', which has already been copied there.\nfile '/Users/kcl/.gradle/caches/modules-2/files-2.1/com.google.guava/failureaccess/1.0.3/aeaffd00d57023a2c947393ed251f0354f0985fc/failureaccess-1.0.3.jar' will be copied to 'failureaccess-1.0.3.jar', overwriting file '/Users/kcl/.gradle/caches/modules-2/files-2.1/com.google.guava/failureaccess/1.0.3/aeaffd00d57023a2c947393ed251f0354f0985fc/failureaccess-1.0.3.jar', which has already been copied there.\nfile '/Users/kcl/.gradle/caches/modules-2/files-2.1/com.google.guava/listenablefuture/9999.0-empty-to-avoid-conflict-with-guava/b421526c5f297295adef1c886e5246c39d4ac629/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar' will be copied to 'listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar', overwriting file '/Users/kcl/.gradle/caches/modules-2/files-2.1/com.google.guava/listenablefuture/9999.0-empty-to-avoid-conflict-with-guava/b421526c5f297295adef1c886e5246c39d4ac629/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar', which has already been copied there.\nfile '/Users/kcl/.gradle/caches/modules-2/files-2.1/org.jspecify/jspecify/1.0.0/7425a601c1c7ec76645a78d22b8c6a627edee507/jspecify-1.0.0.jar' will be copied to 'jspecify-1.0.0.jar', overwriting file '/Users/kcl/.gradle/caches/modules-2/files-2.1/org.jspecify/jspecify/1.0.0/7425a601c1c7ec76645a78d22b8c6a627edee507/jspecify-1.0.0.jar', which has already been copied there.\nfile '/Users/kcl/projects/iceberg/bundled-guava/LICENSE' will be copied to 'LICENSE', overwriting file '/Users/kcl/projects/iceberg/bundled-guava/LICENSE', which has already been copied there.\nfile '/Users/kcl/projects/iceberg/bundled-guava/NOTICE' will be copied to 'NOTICE', overwriting file '/Users/kcl/projects/iceberg/bundled-guava/NOTICE', which has already been copied there.\n\n&gt; Task :iceberg-common:compileJava\n/Users/kcl/projects/iceberg/common/src/main/java/org/apache/iceberg/common/DynConstructors.java:270: Note: [SafeLoggingPropagation] Safe logging annotations should be propagated to encapsulating elements to allow static analysis tooling to work with as much information as possible. This check can be auto-fixed using `./gradlew classes testClasses -PerrorProneApply=SafeLoggingPropagation`\nprivate static String formatProblems(Map&lt;String, Throwable&gt; problems) {\n                        ^\n    (see https://github.com/palantir/gradle-baseline#baseline-error-prone-checks)\nDid you mean '@Unsafe private static String formatProblems(Map&lt;String, Throwable&gt; problems) {'?\nNote: Some input files use or override a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\n\n&gt; Task :iceberg-api:compileJava\n/Users/kcl/projects/iceberg/api/src/main/java/org/apache/iceberg/transforms/Timestamps.java:47: warning: [ImmutableEnumChecker] enums should be immutable: 'Timestamps' has field 'apply' of type 'org.apache.iceberg.util.SerializableFunction&lt;java.lang.Long,java.lang.Integer&gt;', the declaration of type 'org.apache.iceberg.util.SerializableFunction&lt;java.lang.Long,java.lang.Integer&gt;' is not annotated with @com.google.errorprone.annotations.Immutable\nprivate final SerializableFunction&lt;Long, Integer&gt; apply;\n                                                    ^\n    (see https://errorprone.info/bugpattern/ImmutableEnumChecker)\n\n&gt; Task :iceberg-spark:iceberg-spark-extensions-3.5_2.12:scalastyleMainCheck\nProcessed 48 file(s)\nFound 0 errors\nFound 0 warnings\nFinished in 8938 ms\n\n&gt; Task :iceberg-api:compileJava\nNote: Some input files use or override a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\nNote: Some input files use unchecked or unsafe operations.\nNote: Recompile with -Xlint:unchecked for details.\n1 warning\n\n&gt; Task :iceberg-api:compileTestJava\nNote: Some input files use or override a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\nNote: Some input files use unchecked or unsafe operations.\nNote: Recompile with -Xlint:unchecked for details.\n\n&gt; Task :iceberg-api:testJar\nfile '/Users/kcl/projects/iceberg/build/iceberg-build.properties' will be copied to 'iceberg-build.properties', overwriting file '/Users/kcl/projects/iceberg/api/build/resources/test/iceberg-build.properties', which has already been copied there.\n\n&gt; Task :iceberg-core:compileJava\nNote: Some input files use or override a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\nNote: Some input files use unchecked or unsafe operations.\nNote: Recompile with -Xlint:unchecked for details.\n\n&gt; Task :iceberg-azure:compileJava\n/Users/kcl/projects/iceberg/azure/src/main/java/org/apache/iceberg/azure/adlsv2/ADLSFileIO.java:136: warning: [UnnecessaryParentheses] These grouping parentheses are unnecessary; it is unlikely the code will be misinterpreted without them\n        .ifPresent((provider -&gt; this.vendedAdlsCredentialProvider = provider));\n                ^\n    (see https://errorprone.info/bugpattern/UnnecessaryParentheses)\nDid you mean '.ifPresent( provider -&gt; this.vendedAdlsCredentialProvider = provider);'?\nNote: Some input files use or override a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\n1 warning\n\n&gt; Task :iceberg-aliyun:compileJava\nNote: Some input files use or override a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\n\n&gt; Task :iceberg-parquet:compileJava\nNote: Some input files use or override a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\nNote: Some input files use unchecked or unsafe operations.\nNote: Recompile with -Xlint:unchecked for details.\n\n&gt; Task :iceberg-aws:compileJava\nNote: Some input files use or override a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\nNote: Some input files use unchecked or unsafe operations.\nNote: Recompile with -Xlint:unchecked for details.\n\n&gt; Task :iceberg-data:compileJava\nNote: Some input files use or override a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\n\n&gt; Task :iceberg-arrow:compileJava\nNote: /Users/kcl/projects/iceberg/arrow/src/main/java/org/apache/iceberg/arrow/ArrowSchemaUtil.java uses or overrides a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\n\n&gt; Task :iceberg-core:compileTestJava\nNote: Some input files use or override a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\nNote: Some input files use unchecked or unsafe operations.\nNote: Recompile with -Xlint:unchecked for details.\n\n&gt; Task :iceberg-gcp:compileJava\nNote: Some input files use or override a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\n\n&gt; Task :iceberg-aws:compileTestJava\nNote: Some input files use or override a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\n\n&gt; Task :iceberg-aws:compileIntegrationJava\nNote: Some input files use or override a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\nNote: Some input files use unchecked or unsafe operations.\nNote: Recompile with -Xlint:unchecked for details.\n\n&gt; Task :iceberg-arrow:compileTestJava\nNote: /Users/kcl/projects/iceberg/arrow/src/test/java/org/apache/iceberg/arrow/vectorized/ArrowReaderTest.java uses or overrides a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\n\n&gt; Task :iceberg-data:compileTestJava\nNote: Some input files use or override a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\nNote: Some input files use unchecked or unsafe operations.\nNote: Recompile with -Xlint:unchecked for details.\n\n&gt; Task :iceberg-hive-metastore:compileTestJava\nNote: /Users/kcl/projects/iceberg/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCatalog.java uses or overrides a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\nNote: /Users/kcl/projects/iceberg/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCatalog.java uses unchecked or unsafe operations.\nNote: Recompile with -Xlint:unchecked for details.\n\n&gt; Task :iceberg-mr:compileJava\nNote: /Users/kcl/projects/iceberg/mr/src/main/java/org/apache/iceberg/mr/mapred/MapredIcebergInputFormat.java uses unchecked or unsafe operations.\nNote: Recompile with -Xlint:unchecked for details.\n\n&gt; Task :iceberg-nessie:compileJava\nNote: /Users/kcl/projects/iceberg/nessie/src/main/java/org/apache/iceberg/nessie/NessieIcebergClient.java uses or overrides a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\n\n&gt; Task :iceberg-parquet:javadoc\n/Users/kcl/projects/iceberg/parquet/src/main/java/org/apache/iceberg/data/parquet/GenericParquetWriter.java:64: warning: reference not accessible: org.apache.iceberg.data.parquet.BaseParquetWriter\n* @deprecated will be removed in 1.10.0; use {@link #createWriter(Types.StructType, MessageType)}\n                                                ^\n/Users/kcl/projects/iceberg/parquet/src/main/java/org/apache/iceberg/data/parquet/InternalWriter.java:67: warning: reference not accessible: org.apache.iceberg.data.parquet.BaseParquetWriter\n* @deprecated will be removed in 1.10.0; use {@link #createWriter(Types.StructType, MessageType)}\n                                                ^\n/Users/kcl/projects/iceberg/parquet/src/main/java/org/apache/iceberg/data/parquet/GenericParquetWriter.java:64: warning: reference not accessible: org.apache.iceberg.data.parquet.BaseParquetWriter\n* @deprecated will be removed in 1.10.0; use {@link #createWriter(Types.StructType, MessageType)}\n                                                ^\n/Users/kcl/projects/iceberg/parquet/src/main/java/org/apache/iceberg/data/parquet/InternalWriter.java:67: warning: reference not accessible: org.apache.iceberg.data.parquet.BaseParquetWriter\n* @deprecated will be removed in 1.10.0; use {@link #createWriter(Types.StructType, MessageType)}\n                                                ^\n4 warnings\n\n&gt; Task :iceberg-parquet:compileTestJava\nNote: Some input files use or override a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\n\n&gt; Task :iceberg-spark:iceberg-spark-3.5_2.12:compileScala\n[Warn] : javac: [options] system modules path not set in conjunction with -source 11\nCould not determine source for class org.apache.iceberg.spark.ImmutableParquetBatchReadConf\nCould not determine source for class org.apache.iceberg.spark.ImmutableOrcBatchReadConf$Builder\nCould not determine source for class org.apache.iceberg.spark.ImmutableOrcBatchReadConf\nCould not determine source for class org.apache.iceberg.spark.ImmutableParquetBatchReadConf$Builder\n\n&gt; Task :iceberg-kafka-connect:iceberg-kafka-connect:compileJava\nNote: /Users/kcl/projects/iceberg/kafka-connect/kafka-connect/src/main/java/org/apache/iceberg/connect/channel/CommitterImpl.java uses or overrides a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\n\n&gt; Task :iceberg-flink:iceberg-flink-1.20:compileJava\nNote: Some input files use or override a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\nNote: Some input files use unchecked or unsafe operations.\nNote: Recompile with -Xlint:unchecked for details.\n\n&gt; Task :iceberg-spark:iceberg-spark-3.5_2.12:compileTestJava\nNote: Some input files use or override a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\nNote: Some input files use unchecked or unsafe operations.\nNote: Recompile with -Xlint:unchecked for details.\n\n&gt; Task :iceberg-spark:iceberg-spark-3.5_2.12:compileJmhJava\nNote: Some input files use or override a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\n\n&gt; Task :iceberg-spark:iceberg-spark-extensions-3.5_2.12:compileTestJava\nNote: /Users/kcl/projects/iceberg/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewritePositionDeleteFiles.java uses or overrides a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\nNote: Some input files use unchecked or unsafe operations.\nNote: Recompile with -Xlint:unchecked for details.\n\n&gt; Task :iceberg-flink:iceberg-flink-1.20:compileTestJava\nNote: Some input files use or override a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\nNote: Some input files use unchecked or unsafe operations.\nNote: Recompile with -Xlint:unchecked for details.\n\n[Incubating] Problems report is available at: file:///Users/kcl/projects/iceberg/build/reports/problems/problems-report.html\n\nDeprecated Gradle features were used in this build, making it incompatible with Gradle 9.0.\n\nYou can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins.\n\nFor more on this, please refer to https://docs.gradle.org/8.13/userguide/command_line_interface.html#sec:command_line_warnings in the Gradle documentation\n\nBUILD SUCCESSFUL in 27m 45s\n454 actionable tasks: 443 executed, 5 from cache, 6 up-to-date\n</code></pre> <p>The ZIP archive will be found under <code>./kafka-connect/kafka-connect-runtime/build/distributions</code>. There is one distribution that bundles the Hive Metastore client and related dependencies, and one that does not.</p> <p>Upload the ZIP archive to a publicly accessible S3 bucket so that the Iceberg Kafka Connect connector can be downloaded during the Kafka Connect cluster creation and image build process. Make note of the ZIP archive URL, as it will be required in subsequent steps.</p>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-iceberg-connector/#create-a-secret-for-connecting-to-aws","title":"Create a Secret for Connecting to AWS","text":"iceberg-secret.yaml <pre><code>kubectl apply -f iceberg-secret.yaml -n kafka-cdc\n</code></pre>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-iceberg-connector/#create-an-iceberg-kafka-connect-cluster","title":"Create an Iceberg Kafka Connect Cluster","text":"iceberg-connect-cluster.yaml <pre><code>kubectl apply -f iceberg-connect-cluster.yaml -n kafka-cdc\n</code></pre> <pre><code>kubectl get all -n kafka-cdc\n\nNAME                                                 READY   STATUS    RESTARTS   AGE\npod/debezium-connect-cluster-connect-0               1/1     Running   0          8m15s\npod/iceberg-connect-cluster-connect-0                1/1     Running   0          73s\npod/kafka-cluster-dual-role-0                        1/1     Running   0          14m\npod/kafka-cluster-dual-role-1                        1/1     Running   0          14m\npod/kafka-cluster-dual-role-2                        1/1     Running   0          14m\npod/kafka-cluster-entity-operator-598bb8df8b-q2d4x   2/2     Running   0          13m\npod/mysql-6b84fd947d-kpdk4                           1/1     Running   0          12m\n\nNAME                                           TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                        AGE\nservice/debezium-connect-cluster-connect       ClusterIP   None             &lt;none&gt;        8083/TCP                                       8m15s\nservice/debezium-connect-cluster-connect-api   ClusterIP   10.103.185.205   &lt;none&gt;        8083/TCP                                       8m15s\nservice/iceberg-connect-cluster-connect        ClusterIP   None             &lt;none&gt;        8083/TCP                                       73s\nservice/iceberg-connect-cluster-connect-api    ClusterIP   10.102.45.63     &lt;none&gt;        8083/TCP                                       73s\nservice/kafka-cluster-kafka-bootstrap          ClusterIP   10.105.98.134    &lt;none&gt;        9091/TCP,9092/TCP,9093/TCP                     14m\nservice/kafka-cluster-kafka-brokers            ClusterIP   None             &lt;none&gt;        9090/TCP,9091/TCP,8443/TCP,9092/TCP,9093/TCP   14m\nservice/mysql                                  ClusterIP   None             &lt;none&gt;        3306/TCP                                       12m\n\nNAME                                            READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/kafka-cluster-entity-operator   1/1     1            1           13m\ndeployment.apps/mysql                           1/1     1            1           12m\n\nNAME                                                       DESIRED   CURRENT   READY   AGE\nreplicaset.apps/kafka-cluster-entity-operator-598bb8df8b   1         1         1       13m\nreplicaset.apps/mysql-6b84fd947d                           1         1         1       12m\n</code></pre>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-iceberg-connector/#create-an-iceberg-sink-connector","title":"Create an Iceberg Sink Connector","text":"iceberg-connector.yaml <pre><code>kubectl apply -f iceberg-connector.yaml -n kafka-cdc\n</code></pre> <pre><code>kubectl get kafkaconnector -n kafka-cdc\n\nNAME                 CLUSTER                    CONNECTOR CLASS                                   MAX TASKS   READY\ndebezium-connector   debezium-connect-cluster   io.debezium.connector.mysql.MySqlConnector        1           True\niceberg-connector    iceberg-connect-cluster    org.apache.iceberg.connect.IcebergSinkConnector   1           True\n</code></pre>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-iceberg-connector/#verify-the-data-pipeline","title":"Verify the Data Pipeline","text":"<pre><code>kubectl exec -n kafka-cdc -it mysql-6b84fd947d-9g9lt -- mysql -uroot -pdebezium\n</code></pre> <pre><code>sql&gt; use inventory;\nsql&gt; INSERT INTO orders (order_date, purchaser, quantity, product_id)\nVALUES ('2016-03-01', 1004, 3, 108);\n</code></pre>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-iceberg-connector/#schema-evolution","title":"Schema Evolution","text":"<pre><code>mysql&gt; ALTER TABLE orders ADD COLUMN shipping_address TEXT DEFAULT NULL;\nQuery OK, 0 rows affected (0.30 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n</code></pre> <pre><code>mysql&gt; describe orders;\n+------------------+------+------+-----+---------+----------------+\n| Field            | Type | Null | Key | Default | Extra          |\n+------------------+------+------+-----+---------+----------------+\n| order_number     | int  | NO   | PRI | NULL    | auto_increment |\n| order_date       | date | NO   |     | NULL    |                |\n| purchaser        | int  | NO   | MUL | NULL    |                |\n| quantity         | int  | NO   |     | NULL    |                |\n| product_id       | int  | NO   | MUL | NULL    |                |\n| shipping_address | text | YES  |     | NULL    |                |\n+------------------+------+------+-----+---------+----------------+\n6 rows in set (0.05 sec)\n</code></pre> <pre><code>mysql&gt; select * from orders;\n+--------------+------------+-----------+----------+------------+------------------+\n| order_number | order_date | purchaser | quantity | product_id | shipping_address |\n+--------------+------------+-----------+----------+------------+------------------+\n|        10001 | 2016-01-16 |      1001 |        1 |        102 | NULL             |\n|        10002 | 2016-01-17 |      1002 |        2 |        105 | NULL             |\n|        10003 | 2016-02-19 |      1002 |        2 |        106 | NULL             |\n|        10004 | 2016-02-21 |      1003 |        1 |        107 | NULL             |\n|        10005 | 2016-03-01 |      1004 |        3 |        108 | NULL             |\n+--------------+------------+-----------+----------+------------+------------------+\n5 rows in set (0.00 sec)\n</code></pre> <pre><code>INSERT INTO orders (order_date, purchaser, quantity, product_id, shipping_address)\nVALUES ('2016-03-05', 1005, 2, 109, '123 Main Street, Taipei City');\nQuery OK, 1 row affected (0.04 sec)\n</code></pre> <pre><code>mysql&gt; select * from orders;\n+--------------+------------+-----------+----------+------------+------------------------------+\n| order_number | order_date | purchaser | quantity | product_id | shipping_address             |\n+--------------+------------+-----------+----------+------------+------------------------------+\n|        10001 | 2016-01-16 |      1001 |        1 |        102 | NULL                         |\n|        10002 | 2016-01-17 |      1002 |        2 |        105 | NULL                         |\n|        10003 | 2016-02-19 |      1002 |        2 |        106 | NULL                         |\n|        10004 | 2016-02-21 |      1003 |        1 |        107 | NULL                         |\n|        10005 | 2016-03-01 |      1004 |        3 |        108 | NULL                         |\n|        10006 | 2016-03-05 |      1005 |        2 |        109 | 123 Main Street, Taipei City |\n+--------------+------------+-----------+----------+------------+------------------------------+\n6 rows in set (0.02 sec)\n</code></pre>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-iceberg-connector/#references","title":"References","text":"<ul> <li>Deploying Debezium on Kubernetes | Debezium Documentation</li> <li>Deploying and Managing | Strimzi Documentation</li> <li>Using the Iceberg framework in AWS Glue | AWS</li> <li>Iceberg Kafka Connector | Iceberg</li> </ul>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-kafka-cluster/","title":"Kafka Cluster Deployment","text":"<p>Architecture Overview</p> <p>Deployment Steps</p> <ul> <li> Deploy a Kafka Cluster via the Strimzi Operator</li> <li> Deploy a MySQL Database</li> <li> Deploy a Debezium Kafka Source Connector</li> <li> Deploy an Iceberg Kafka Sink Connector</li> </ul> <p>Without further ado, let's jump straight to the commands:</p> <pre><code>cd ~/Projects/retail-lakehouse/kafka-cluster\nbash install.sh\n</code></pre> install.sh <pre><code>#!/bin/bash\n\nset -e\n\n# Deploy Strimzi Cluster Operator\nkubectl create namespace strimzi\nkubectl create namespace kafka-cdc\n\nhelm repo add strimzi https://strimzi.io/charts/\nhelm repo update\nhelm install \\\n  strimzi-cluster-operator \\\n  oci://quay.io/strimzi-helm/strimzi-kafka-operator \\\n  -f values.yaml \\\n  -n strimzi \\\n  --version 0.46.1\nsleep 5\nkubectl wait --for=condition=Ready pod -l name=strimzi-cluster-operator -n strimzi --timeout=1200s\n\n# Deploy Kafka cluster using Strimzi\nkubectl apply -f kafka-cluster.yaml -n kafka-cdc\nsleep 5\nkubectl wait --for=condition=Ready pod -l app.kubernetes.io/name=kafka -n kafka-cdc --timeout=1200s\nsleep 5\nkubectl wait --for=condition=Ready pod -l app.kubernetes.io/name=entity-operator -n kafka-cdc --timeout=1200s\n</code></pre> <p>This script will deploy a Kafka Cluster using the Strimzi Operator in the <code>kafka-cdc</code> namespace.</p> <p>If you don't like my script and want to do it step by step manually, please continue reading. This article will walk you through how to deploy a Kafka Cluster using the Strimzi Operator on Kubernetes step by step, explaining each part along the way.</p>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-kafka-cluster/#deploy-the-strimzi-cluster-operator","title":"Deploy the Strimzi Cluster Operator","text":"<p>Before we dive into deploying Kafka, let's talk about what Strimzi is and why we need it. </p> <p>Strimzi is a Kubernetes operator that makes running Apache Kafka on Kubernetes much easier. Think of it as your Kafka cluster manager and it handles all the complex setup, configuration, and maintenance tasks that would otherwise require manual intervention.</p> <p>Instead of manually creating Kafka pods, services, and configurations, Strimzi lets you define what you want in simple YAML files, and it takes care of the rest.</p> <p>You can deploy Strimzi on Kubernetes 1.25 and later using one of the following methods:</p> <ul> <li> Deployment files (YAML files)</li> <li> OperatorHub.io</li> <li> Helm chart</li> </ul> <p>In this guide, we'll use the Helm chart method because it's straightforward and allows for easy customization.</p> <p>First, we need to create two separate Kubernetes namespaces, one for the Strimzi operator and another for our Kafka cluster:</p> <pre><code>kubectl create namespace strimzi\nkubectl create namespace kafka-cdc\n</code></pre> <p>Next, we need to add the Strimzi Helm repository to our local Helm setup and install the Strimzi operator with our custom values:</p> <pre><code>helm repo add strimzi https://strimzi.io/charts/\nhelm repo update\nhelm install \\\n  strimzi-cluster-operator \\\n  oci://quay.io/strimzi-helm/strimzi-kafka-operator \\\n  -f ~/Projects/retail-lakehouse/kafka-cluster/values.yaml \\\n  -n strimzi \\\n  --version 0.46.1\n</code></pre> <p>Here's our customized <code>values.yaml</code> file and the most important settings is <code>watchNamespaces</code>, which tells Strimzi to specifically watch the <code>kafka-cdc</code> namespace where we'll deploy our Kafka cluster.</p> values.yaml <pre><code># Default values for strimzi-kafka-operator.\n\n# Default replicas for the cluster operator\nreplicas: 1\n\n# If you set `watchNamespaces` to the same value as ``.Release.Namespace` (e.g. `helm ... --namespace $NAMESPACE`),\n# the chart will fail because duplicate RoleBindings will be attempted to be created in the same namespace\nwatchNamespaces:\n  - kafka-cdc\nwatchAnyNamespace: false\n\ndefaultImageRegistry: quay.io\ndefaultImageRepository: strimzi\ndefaultImageTag: 0.46.1\n\nimage:\n  registry: \"\"\n  repository: \"\"\n  name: operator\n  tag: \"\"\n  # imagePullSecrets:\n  #   - name: secretname\nlogVolume: co-config-volume\nlogConfigMap: strimzi-cluster-operator\nlogConfiguration: \"\"\nlogLevel: ${env:STRIMZI_LOG_LEVEL:-INFO}\nfullReconciliationIntervalMs: 120000\noperationTimeoutMs: 300000\nkubernetesServiceDnsDomain: cluster.local\nfeatureGates: \"\"\ntmpDirSizeLimit: 1Mi\n\n# Example on how to configure extraEnvs\n# extraEnvs:\n#   - name: JAVA_OPTS\n#     value: \"-Xms256m -Xmx256m\"\n\nextraEnvs: []\n\ntolerations: []\naffinity: {}\nannotations: {}\nlabels: {}\nnodeSelector: {}\ndeploymentAnnotations: {}\npriorityClassName: \"\"\n\npodSecurityContext: {}\nsecurityContext: {}\nrbac:\n  create: yes\nserviceAccountCreate: yes\nserviceAccount: strimzi-cluster-operator\n\nleaderElection:\n  enable: true\n\n# https://kubernetes.io/docs/tasks/run-application/configure-pdb/\npodDisruptionBudget:\n  enabled: false\n  # The PDB definition only has two attributes to control the availability requirements: minAvailable or maxUnavailable (mutually exclusive).\n  # Field maxUnavailable tells how many pods can be down and minAvailable tells how many pods must be running in a cluster.\n\n  # The pdb template will check values according to below order\n  #\n  #  {{- if .Values.podDisruptionBudget.minAvailable }}\n  #     minAvailable: {{ .Values.podDisruptionBudget.minAvailable }}\n  #  {{- end  }}\n  #  {{- if .Values.podDisruptionBudget.maxUnavailable }}\n  #     maxUnavailable: {{ .Values.podDisruptionBudget.maxUnavailable }}\n  #  {{- end }}\n  #\n  # If both values are set, the template will use the first one and ignore the second one. currently by default minAvailable is set to 1\n  minAvailable: 1\n  maxUnavailable:\n\n# If you are using the grafana dashboard sidecar,\n# you can import some default dashboards here\ndashboards:\n  enabled: false\n  namespace: ~\n  label: grafana_dashboard # this is the default value from the grafana chart\n  labelValue: \"1\" # this is the default value from the grafana chart\n  annotations: {}\n  extraLabels: {}\n\n# Docker images that operator uses to provision various components of Strimzi. To use your own registry prefix the\n# repository name with your registry URL.\n# Ex) repository: registry.xyzcorp.com/strimzi/kafka\nkafka:\n  image:\n    registry: \"\"\n    repository: \"\"\n    name: kafka\n    tagPrefix: \"\"\nkafkaConnect:\n  image:\n    registry: \"\"\n    repository: \"\"\n    name: kafka\n    tagPrefix: \"\"\ntopicOperator:\n  image:\n    registry: \"\"\n    repository: \"\"\n    name: operator\n    tag: \"\"\nuserOperator:\n  image:\n    registry:\n    repository:\n    name: operator\n    tag: \"\"\nkafkaInit:\n  image:\n    registry: \"\"\n    repository: \"\"\n    name: operator\n    tag: \"\"\nkafkaBridge:\n  image:\n    registry: \"\"\n    repository:\n    name: kafka-bridge\n    tag: 0.32.0\nkafkaExporter:\n  image:\n    registry: \"\"\n    repository: \"\"\n    name: kafka\n    tagPrefix: \"\"\nkafkaMirrorMaker2:\n  image:\n    registry: \"\"\n    repository: \"\"\n    name: kafka\n    tagPrefix: \"\"\ncruiseControl:\n  image:\n    registry: \"\"\n    repository: \"\"\n    name: kafka\n    tagPrefix: \"\"\nkanikoExecutor:\n  image:\n    registry: \"\"\n    repository: \"\"\n    name: kaniko-executor\n    tag: \"\"\nmavenBuilder:\n  image:\n    registry: \"\"\n    repository: \"\"\n    name: maven-builder\n    tag: \"\"\nresources:\n  limits:\n    memory: 384Mi\n    cpu: 1000m\n  requests:\n    memory: 384Mi\n    cpu: 200m\nlivenessProbe:\n  initialDelaySeconds: 10\n  periodSeconds: 30\nreadinessProbe:\n  initialDelaySeconds: 10\n  periodSeconds: 30\n\ncreateGlobalResources: true\n# Create clusterroles that extend existing clusterroles to interact with strimzi crds\n# Ref: https://kubernetes.io/docs/reference/access-authn-authz/rbac/#aggregated-clusterroles\ncreateAggregateRoles: false\n# Override the exclude pattern for exclude some labels\nlabelsExclusionPattern: \"\"\n# Controls whether Strimzi generates network policy resources (By default true)\ngenerateNetworkPolicy: true\n# Override the value for Connect build timeout\nconnectBuildTimeoutMs: 300000\n# Controls whether Strimzi generates pod disruption budget resources (By default true)\ngeneratePodDisruptionBudget: true\n</code></pre> <p>If everything goes well, you'll see output like this:</p> Result <pre><code>Pulled: quay.io/strimzi-helm/strimzi-kafka-operator:0.46.1\nDigest: sha256:e87ea2a03985f5dd50fee1f8706f737fa1151b86dce5021b6c0798ac8b17e27f\nNAME: strimzi-cluster-operator\nLAST DEPLOYED: Sun Jun 29 17:25:49 2025\nNAMESPACE: strimzi\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nThank you for installing strimzi-kafka-operator-0.46.1\n\nTo create a Kafka cluster refer to the following documentation.\n\nhttps://strimzi.io/docs/operators/latest/deploying.html#deploying-cluster-operator-helm-chart-str\n</code></pre> <p>Great! The <code>STATUS: deployed</code> tells us everything went smoothly.</p> <p>Let's make sure our Strimzi operator is actually running:</p> <pre><code>helm ls -n strimzi\n</code></pre> Result <pre><code>NAME                        NAMESPACE   REVISION    UPDATED                                 STATUS      HART                            APP VERSION\nstrimzi-cluster-operator    strimzi     1           2025-06-29 17:25:49.773026 +0800 CST    deployed    trimzi-kafka-operator-0.46.1    0.46.1     \n</code></pre> <pre><code>kubectl get all -n strimzi\n</code></pre> Result <pre><code>NAME                                           READY   STATUS    RESTARTS   AGE\npod/strimzi-cluster-operator-74f577b78-s9n25   1/1     Running   0          108s\n\nNAME                                           READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/strimzi-cluster-operator       1/1     1            1           108s\n\nNAME                                                 DESIRED   CURRENT   READY   AGE\nreplicaset.apps/strimzi-cluster-operator-74f577b78   1         1         1       108s\n</code></pre> <p>Perfect! At this point, the Strimzi operator is running and watching for Kafka-related resources in the <code>kafka-cdc</code> namespace. It's like having a dedicated Kafka administrator ready to spring into action whenever we create Kafka clusters or related components.</p>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-kafka-cluster/#deploy-a-kafka-cluster","title":"Deploy a Kafka Cluster","text":"<p>Now that we have Strimzi operator running, let's deploy our actual Kafka cluster!</p> <pre><code>kubectl create -f kafka-cluster.yaml -n kafka-cdc\n</code></pre> <p>This command tells Kubernetes to create both the Kafka cluster and the node pool in our <code>kafka-cdc</code> namespace. You should see output confirming both resources were created:</p> Result <pre><code>kafka.kafka.strimzi.io/kafka-cluster created\nkafkanodepool.kafka.strimzi.io/dual-role created\n</code></pre> <p>Kafka clusters take a bit of time to start up - they need to elect controllers, establish consensus, and create internal topics. Let's check on the progress:</p> <pre><code>kubectl get all -n kafka-cdc\n</code></pre> <p>Initially, you might see pods in <code>Pending</code> or <code>ContainerCreating</code> status. After a minute or two, you should see something like this:</p> Result <pre><code>NAME                                                    READY   STATUS    RESTARTS   AGE\npod/kafka-cluster-dual-role-0                           1/1     Running   0          60s\npod/kafka-cluster-dual-role-1                           1/1     Running   0          60s\npod/kafka-cluster-dual-role-2                           1/1     Running   0          60s\npod/kafka-cluster-entity-operator-5b998f6cbf-c8hdf      2/2     Running   0          24s\n\nNAME                                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                        AGE\nservice/kafka-cluster-kafka-bootstrap      ClusterIP   10.105.50.103   &lt;none&gt;        9091/TCP,9092/TCP,9093/TCP                     61s\nservice/kafka-cluster-kafka-brokers        ClusterIP   None            &lt;none&gt;        9090/TCP,9091/TCP,8443/TCP,9092/TCP,9093/TCP   61s\n\nNAME                                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/kafka-cluster-entity-operator      1/1     1            1           24s\n\nNAME                                                          DESIRED   CURRENT   READY   AGE\nreplicaset.apps/kafka-cluster-entity-operator-5b998f6cbf      1         1         1       24s\n</code></pre> <p>Components of the Kafka Cluster:</p> <ul> <li>Three Kafka Pods: These are your dual-role nodes, numbered 0, 1, and 2</li> <li>Entity Operator: This Strimzi component manages Kafka topics and users for you</li> <li>Bootstrap Service: This is how clients discover and connect to your Kafka cluster</li> <li>Broker Service: This provides direct access to individual brokers when needed</li> </ul> <p>If you are interested in how the Kafka cluster is configured, you can continue reading the next section. If not, you are good to go to the next step: Deploy a MySQL Database.</p>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-kafka-cluster/#deep-dive-understanding-the-kafka-configuration","title":"Deep Dive: Understanding the Kafka Configuration","text":"<p>Let's break down what this configuration is telling Kubernetes to create for us.</p>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-kafka-cluster/#version","title":"Version","text":"kafka-cluster.yaml:cluster <pre><code>apiVersion: kafka.strimzi.io/v1beta2\nkind: Kafka\nmetadata:\n  name: kafka-cluster\n  namespace: kafka-cdc\n  annotations:\n    strimzi.io/node-pools: enabled\n    strimzi.io/kraft: enabled\nspec:\n  kafka:\n    version: 4.0.0\n    metadataVersion: 4.0-IV3\n    listeners:\n      - name: plain\n        port: 9092\n        type: internal\n        tls: false\n      - name: tls\n        port: 9093\n        type: internal\n        tls: true\n    config:\n      default.replication.factor: 3\n      offsets.topic.replication.factor: 3\n      transaction.state.log.replication.factor: 3\n      transaction.state.log.min.isr: 2\n      min.insync.replicas: 2\n    template:\n      pod:\n        securityContext:\n          runAsUser: 0\n          fsGroup: 0\n  entityOperator:\n    topicOperator: {}\n    userOperator: {}\n</code></pre> <p>We're using Kafka 4.0. The <code>spec.kafka.metadataVersion: 4.0-IV3</code> setting is important (the \"IV\" stands for \"Incompatible Version,\" which means this version has significant metadata structure changes that aren't backward compatible). We need to explicitly specify this to confirm we understand we're using the latest format.</p>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-kafka-cluster/#fault-tolerance-and-consistency","title":"Fault Tolerance and Consistency","text":"kafka-cluster.yaml:cluster <pre><code>apiVersion: kafka.strimzi.io/v1beta2\nkind: Kafka\nmetadata:\n  name: kafka-cluster\n  namespace: kafka-cdc\n  annotations:\n    strimzi.io/node-pools: enabled\n    strimzi.io/kraft: enabled\nspec:\n  kafka:\n    version: 4.0.0\n    metadataVersion: 4.0-IV3\n    listeners:\n      - name: plain\n        port: 9092\n        type: internal\n        tls: false\n      - name: tls\n        port: 9093\n        type: internal\n        tls: true\n    config:\n      default.replication.factor: 3\n      offsets.topic.replication.factor: 3\n      transaction.state.log.replication.factor: 3\n      transaction.state.log.min.isr: 2\n      min.insync.replicas: 2\n    template:\n      pod:\n        securityContext:\n          runAsUser: 0\n          fsGroup: 0\n  entityOperator:\n    topicOperator: {}\n    userOperator: {}\n</code></pre> <p>The configuration section ensures our cluster is production-ready with proper replication and consistency guarantees via settings under <code>spec.kafka.config</code>:</p> <ul> <li><code>default.replication.factor: 3</code></li> <li><code>offsets.topic.replication.factor: 3</code></li> <li><code>transaction.state.log.replication.factor: 3</code></li> </ul> <p>We're telling Kafka to keep <code>3</code> copies of everything, including your regular topics, consumer offset tracking, and transaction state. This means we can lose one broker and still have all our data.</p> <ul> <li><code>transaction.state.log.min.isr: 2</code></li> <li><code>min.insync.replicas: 2</code></li> </ul> <p>These configurations ensure that at least <code>2</code> replicas must acknowledge a write before it's considered successful. This prevents data loss even if a broker fails right after acknowledging a write.</p>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-kafka-cluster/#listeners-how-clients-connect","title":"Listeners: How Clients Connect","text":"kafka-cluster.yaml:cluster <pre><code>apiVersion: kafka.strimzi.io/v1beta2\nkind: Kafka\nmetadata:\n  name: kafka-cluster\n  namespace: kafka-cdc\n  annotations:\n    strimzi.io/node-pools: enabled\n    strimzi.io/kraft: enabled\nspec:\n  kafka:\n    version: 4.0.0\n    metadataVersion: 4.0-IV3\n    listeners:\n      - name: plain\n        port: 9092\n        type: internal\n        tls: false\n      - name: tls\n        port: 9093\n        type: internal\n        tls: true\n    config:\n      default.replication.factor: 3\n      offsets.topic.replication.factor: 3\n      transaction.state.log.replication.factor: 3\n      transaction.state.log.min.isr: 2\n      min.insync.replicas: 2\n    template:\n      pod:\n        securityContext:\n          runAsUser: 0\n          fsGroup: 0\n  entityOperator:\n    topicOperator: {}\n    userOperator: {}\n</code></pre> <p>Our configuration sets up two ways for applications to connect to Kafka:</p> Plain Listener (port <code>9092</code>) <p>This is your standard, unencrypted connection. Perfect for development environments, internal applications where network security is handled at other layers, and high-throughput scenarios where TLS overhead isn't desired.</p> TLS Listener (port <code>9093</code>) <p>This provides encrypted connections for, cross-namespace communication, production environments with security requirements, any scenario where data in transit needs protection.</p>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-kafka-cluster/#kraft-mode","title":"KRaft Mode","text":"kafka-cluster.yaml <pre><code>apiVersion: kafka.strimzi.io/v1beta2\nkind: Kafka\nmetadata:\n  name: kafka-cluster\n  namespace: kafka-cdc\n  annotations:\n    strimzi.io/node-pools: enabled\n    strimzi.io/kraft: enabled\nspec:\n  kafka:\n    version: 4.0.0\n    metadataVersion: 4.0-IV3\n    listeners:\n      - name: plain\n        port: 9092\n        type: internal\n        tls: false\n      - name: tls\n        port: 9093\n        type: internal\n        tls: true\n    config:\n      default.replication.factor: 3\n      offsets.topic.replication.factor: 3\n      transaction.state.log.replication.factor: 3\n      transaction.state.log.min.isr: 2\n      min.insync.replicas: 2\n    template:\n      pod:\n        securityContext:\n          runAsUser: 0\n          fsGroup: 0\n  entityOperator:\n    topicOperator: {}\n    userOperator: {}\n---\napiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaNodePool\nmetadata:\n  name: dual-role\n  namespace: kafka-cdc\n  labels:\n    strimzi.io/cluster: kafka-cluster\nspec:\n  replicas: 3\n  roles:\n    - controller\n    - broker\n  storage:\n    type: jbod\n    volumes:\n      - id: 1\n        type: persistent-claim\n        size: 100Gi\n        deleteClaim: false\n        kraftMetadata: shared\n</code></pre> <p>If you're coming from a traditional Kafka background, you might be expecting to see ZooKeeper configurations, but we're going to use Kafka's newer KRaft mode instead. Think of KRaft as Kafka's way of saying \"I don't need ZooKeeper anymore. I can manage my own metadata.\"</p> Understanding KRaft Mode <p>If you've worked with Kafka before, you might remember the pain of managing ZooKeeper alongside your Kafka clusters. KRaft mode eliminates that complexity entirely. Here's what makes it special:</p> <p>What KRaft Replaces:</p> <p>Instead of relying on an external ZooKeeper ensemble to store Kafka's metadata (like topic configurations, partition assignments, and cluster membership), Kafka brokers now handle this responsibility themselves using a consensus algorithm similar to Raft.</p> <p>Why This Matters:</p> <ul> <li>Simplified Operations: One less system to deploy, monitor, and troubleshoot</li> <li>Better Performance: No more network hops to ZooKeeper for metadata operations</li> <li>Improved Reliability: Fewer moving parts means fewer potential failure points</li> </ul> <p>To enable KRaft mode in Strimzi, we add two key annotations to our <code>Kafka</code> resource:</p> <ul> <li><code>strimzi.io/node-pools: enabled</code>: Tells Strimzi we want to use the newer node pool architecture</li> <li><code>strimzi.io/kraft: enabled</code>: Enables KRaft mode instead of ZooKeeper</li> </ul> <p>The configuration file we'll use defines not just a <code>Kafka</code> cluster, but also something called a <code>KafkaNodePool</code>. This is Strimzi's way of letting you organize your Kafka nodes into different groups with different roles and storage configurations. It lets you organize your Kafka nodes into logical groups.</p> <p>In our case, we're creating a <code>dual-role</code> node pool with 3 nodes where each node acts as both a Controller (managing metadata) and a Broker (handling client data traffic).</p> <p>Why choose dual-role nodes?</p> <ul> <li>Resource Efficient: Fewer total machines needed</li> <li>Simpler Architecture: No need to separate controller and broker concerns</li> <li>Still Highly Available: With 3 nodes, we can tolerate losing one node</li> </ul> <p>Each node gets a <code>100GB</code> persistent volume that stores both regular Kafka logs and KRaft metadata. The <code>spec.storage.volumes.kraftMetadata: shared</code> setting means both types of data live on the same disk, which is fine for most use cases.</p>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-kafka-cluster/#conclusion","title":"Conclusion","text":"<p>Congratulations! You now have a fully functional Kafka cluster running in KRaft mode.</p> <p>In the next section, we'll deploy a MySQL database that will serve as our data source for change data capture.</p>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-kafka-cluster/#references","title":"References","text":"<ul> <li>Deploying Debezium on Kubernetes | Debezium Documentation</li> <li>Deploying and Managing | Strimzi Documentation</li> <li>Using the Iceberg framework in AWS Glue | AWS</li> <li>Iceberg Kafka Connector | Iceberg</li> </ul>"},{"location":"side-projects/retail-lakehouse/cdc/deployment-mysql/","title":"MySQL Deployment","text":"<p>Architecture Overview</p> <p>Make sure you have deployed a Kafka cluster first.</p> <p>Deployment Steps</p> <ul> <li> Deploy a Kafka Cluster via the Strimzi Operator</li> <li> Deploy a MySQL Database</li> <li> Deploy a Debezium Kafka Source Connector</li> <li> Deploy an Iceberg Kafka Sink Connector</li> </ul> <p>After the Kafka cluster is up and running, you can deploy MySQL by running the following commands:</p> <pre><code>cd ~/Projects/retail-lakehouse/mysql\nbash /install.sh\n</code></pre> Result <pre><code>service/mysql created\ndeployment.apps/mysql created\n</code></pre> <p>The <code>install.sh</code> script in the <code>mysql</code> directory is as follows:</p> install.sh <pre><code>#!/bin/bash\n\nset -e\n\ncd ~/Projects/retail-lakehouse/mysql\nkubectl apply -f mysql.yaml -n kafka-cdc\nsleep 5\nkubectl wait --for=condition=Ready pod -l app=mysql -n kafka-cdc --timeout=1200s\n</code></pre> <p>You can check the <code>mysql.yaml</code> file in the <code>mysql</code> directory:</p> mysql.yaml <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: mysql\n  namespace: kafka-cdc\nspec:\n  ports:\n  - port: 3306\n  selector:\n    app: mysql\n  clusterIP: None\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - image: quay.io/debezium/example-mysql:3.2\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: debezium\n        - name: MYSQL_USER\n          value: mysqluser\n        - name: MYSQL_PASSWORD\n          value: mysqlpw\n        ports:\n        - containerPort: 3306\n          name: mysql\n</code></pre> <p>To verify that MySQL is running, you can use the following command:</p> <pre><code>kubectl get all -n kafka-cdc\n</code></pre> Result <pre><code>NAME                                                    READY   STATUS    RESTARTS   AGE\npod/kafka-cluster-dual-role-0                           1/1     Running   0          15m\npod/kafka-cluster-dual-role-1                           1/1     Running   0          15m\npod/kafka-cluster-dual-role-2                           1/1     Running   0          15m\npod/kafka-cluster-entity-operator-5b998f6cbf-c8hdf      2/2     Running   0          15m\npod/mysql-6b84fd947d-9g9lt                              1/1     Running   0          10m\n\nNAME                                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                        AGE\nservice/kafka-cluster-kafka-bootstrap      ClusterIP   10.105.50.103   &lt;none&gt;        9091/TCP,9092/TCP,9093/TCP                     15m\nservice/kafka-cluster-kafka-brokers        ClusterIP   None            &lt;none&gt;        9090/TCP,9091/TCP,8443/TCP,9092/TCP,9093/TCP   15m\nservice/mysql                              ClusterIP   None            &lt;none&gt;        3306/TCP                                       10m\n\nNAME                                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/kafka-cluster-entity-operator      1/1     1            1           15m\ndeployment.apps/mysql                              1/1     1            1           10m\n\nNAME                                                          DESIRED   CURRENT   READY   AGE\nreplicaset.apps/kafka-cluster-entity-operator-5b998f6cbf      1         1         1       15m\nreplicaset.apps/mysql-6b84fd947d                              1         1         1       10m\n</code></pre> <p>Perfect! You have successfully deployed MySQL. Next, you can proceed to Deploy the Debezium MySQL Source Connector.</p>"},{"location":"side-projects/retail-lakehouse/cdc/overview/","title":"Overview","text":"<p>Architecture Overview</p> <p>This project is divided into several parts, one of which is the Change Data Capture (CDC) Pipeline. The main function of the CDC Pipeline is to capture database change data in real time and deliver it into Apache Iceberg tables, enabling downstream analytics and queries.</p> <p>Specifically, we first deploy a Kafka Cluster to store real-time change data captured from the MySQL database. Next, we deploy a MySQL database to simulate the source of actual data write traffic. Then, we deploy the Debezium MySQL Source Connector on Kafka Connect to capture real-time change data from the MySQL database. Finally, we deploy the Iceberg Sink Connector on Kafka Connect to write the captured change data into Apache Iceberg tables.</p> <p>Deployment Steps</p> <ul> <li> Deploy a Kafka Cluster via the Strimzi Operator</li> <li> Deploy a MySQL Database</li> <li> Deploy a Debezium Kafka Source Connector</li> <li> Deploy an Iceberg Kafka Sink Connector</li> </ul>"},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/1-iceberg-getting-started/","title":"Getting Started","text":"<p>Reference: Docker, Spark, and Iceberg: The Fastest Way to Try Iceberg!</p> In\u00a0[1]: Copied! <pre>from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"Jupyter\").getOrCreate()\n\nspark\n</pre> from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"Jupyter\").getOrCreate()  spark <pre>25/06/20 09:57:25 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n</pre> Out[1]: <p>SparkSession - in-memory</p> <p>SparkContext</p> <p>Spark UI</p> Version <code>v3.5.5</code> Master <code>local[*]</code> AppName <code>PySparkShell</code> <p>To be able to rerun the notebook several times, let's drop the table if it exists to start fresh.</p> In\u00a0[2]: Copied! <pre>%%sql\n\nSHOW CATALOGS\n</pre> %%sql  SHOW CATALOGS Out[2]: catalog demo spark_catalog In\u00a0[3]: Copied! <pre>%%sql\n\nSHOW DATABASES\n</pre> %%sql  SHOW DATABASES Out[3]: namespace nyc In\u00a0[4]: Copied! <pre>%%sql\n\nCREATE DATABASE IF NOT EXISTS nyc\n</pre> %%sql  CREATE DATABASE IF NOT EXISTS nyc Out[4]: In\u00a0[5]: Copied! <pre>%%sql\n\nDROP TABLE IF EXISTS nyc.taxis\n</pre> %%sql  DROP TABLE IF EXISTS nyc.taxis Out[5]: In\u00a0[6]: Copied! <pre>df = spark.read.parquet(\"/home/iceberg/data/yellow_tripdata_2021-04.parquet\")\ndf.write.saveAsTable(\"nyc.taxis\")\n</pre> df = spark.read.parquet(\"/home/iceberg/data/yellow_tripdata_2021-04.parquet\") df.write.saveAsTable(\"nyc.taxis\") <pre>                                                                                </pre> In\u00a0[7]: Copied! <pre>%%sql\n\nDESCRIBE EXTENDED nyc.taxis\n</pre> %%sql  DESCRIBE EXTENDED nyc.taxis Out[7]: col_name data_type comment VendorID bigint None tpep_pickup_datetime timestamp_ntz None tpep_dropoff_datetime timestamp_ntz None passenger_count double None trip_distance double None RatecodeID double None store_and_fwd_flag string None PULocationID bigint None DOLocationID bigint None payment_type bigint None fare_amount double None extra double None mta_tax double None tip_amount double None tolls_amount double None improvement_surcharge double None total_amount double None congestion_surcharge double None airport_fee double None # Metadata Columns _spec_id int _partition struct&lt;&gt; _file string _pos bigint _deleted boolean # Detailed Table Information Name demo.nyc.taxis Type MANAGED Location s3://warehouse/nyc/taxis Provider iceberg Owner root Table Properties [created-at=2025-06-20T09:58:21.220162508Z,current-snapshot-id=4656165136069422702,format=iceberg/parquet,format-version=2,write.format.default=parquet,write.parquet.compression-codec=zstd] In\u00a0[8]: Copied! <pre>%%sql\n\nSELECT COUNT(*) as cnt\nFROM nyc.taxis\n</pre> %%sql  SELECT COUNT(*) as cnt FROM nyc.taxis Out[8]: cnt 2171187 In\u00a0[9]: Copied! <pre>%%sql\n\nALTER TABLE nyc.taxis RENAME COLUMN fare_amount TO fare\n</pre> %%sql  ALTER TABLE nyc.taxis RENAME COLUMN fare_amount TO fare Out[9]: In\u00a0[10]: Copied! <pre>%%sql\n\nALTER TABLE nyc.taxis RENAME COLUMN trip_distance TO distance\n</pre> %%sql  ALTER TABLE nyc.taxis RENAME COLUMN trip_distance TO distance Out[10]: In\u00a0[11]: Copied! <pre>%%sql\n\nALTER TABLE nyc.taxis ALTER COLUMN distance COMMENT 'The elapsed trip distance in miles reported by the taximeter.'\n</pre> %%sql  ALTER TABLE nyc.taxis ALTER COLUMN distance COMMENT 'The elapsed trip distance in miles reported by the taximeter.' Out[11]: In\u00a0[12]: Copied! <pre>%%sql\n\nALTER TABLE nyc.taxis ALTER COLUMN distance TYPE double;\n</pre> %%sql  ALTER TABLE nyc.taxis ALTER COLUMN distance TYPE double; Out[12]: In\u00a0[13]: Copied! <pre>%%sql\n\nALTER TABLE nyc.taxis ALTER COLUMN distance AFTER fare;\n</pre> %%sql  ALTER TABLE nyc.taxis ALTER COLUMN distance AFTER fare; Out[13]: In\u00a0[14]: Copied! <pre>%%sql\n\nALTER TABLE nyc.taxis\nADD COLUMN fare_per_distance_unit float AFTER distance\n</pre> %%sql  ALTER TABLE nyc.taxis ADD COLUMN fare_per_distance_unit float AFTER distance Out[14]: <p>Let's update the new <code>fare_per_distance_unit</code> to equal <code>fare</code> divided by <code>distance</code>.</p> In\u00a0[15]: Copied! <pre>%%sql\n\nUPDATE nyc.taxis\nSET fare_per_distance_unit = fare/distance\n</pre> %%sql  UPDATE nyc.taxis SET fare_per_distance_unit = fare/distance <pre>                                                                                </pre> Out[15]: In\u00a0[16]: Copied! <pre>%%sql\n\nSELECT\nVendorID\n,tpep_pickup_datetime\n,tpep_dropoff_datetime\n,fare\n,distance\n,fare_per_distance_unit\nFROM nyc.taxis\n</pre> %%sql  SELECT VendorID ,tpep_pickup_datetime ,tpep_dropoff_datetime ,fare ,distance ,fare_per_distance_unit FROM nyc.taxis Out[16]: VendorID tpep_pickup_datetime tpep_dropoff_datetime fare distance fare_per_distance_unit 1 2021-04-01 00:00:18 2021-04-01 00:21:54 25.5 8.4 3.0357143878936768 1 2021-04-01 00:42:37 2021-04-01 00:46:23 5.0 0.9 5.55555534362793 1 2021-04-01 00:57:56 2021-04-01 01:08:22 11.5 3.4 3.382352828979492 1 2021-04-01 00:01:58 2021-04-01 00:54:27 44.2 0.0 None 2 2021-04-01 00:24:55 2021-04-01 00:34:33 9.0 1.96 4.591836929321289 2 2021-04-01 00:19:16 2021-04-01 00:21:46 4.5 0.77 5.844155788421631 2 2021-04-01 00:25:11 2021-04-01 00:31:53 11.5 3.65 3.1506848335266113 1 2021-04-01 00:27:53 2021-04-01 00:47:03 26.5 8.9 2.9775280952453613 2 2021-04-01 00:24:24 2021-04-01 00:37:50 12.0 2.98 4.026845455169678 1 2021-04-01 00:19:18 2021-04-01 00:41:25 28.0 8.9 3.1460673809051514 2 2021-04-01 00:04:25 2021-04-01 00:29:22 23.5 7.48 3.1417112350463867 2 2021-04-01 00:03:07 2021-04-01 00:18:02 13.5 3.39 3.9823009967803955 2 2021-04-01 00:35:44 2021-04-01 00:51:06 14.0 3.51 3.9886040687561035 2 2021-04-01 00:52:32 2021-04-01 01:04:41 12.5 3.42 3.654970645904541 2 2021-04-01 00:28:05 2021-04-01 00:47:59 33.5 12.14 2.7594728469848633 1 2021-04-01 00:39:01 2021-04-01 00:57:39 32.0 11.8 2.711864471435547 2 2021-04-01 00:15:10 2021-04-01 00:22:46 7.5 1.44 5.208333492279053 2 2021-04-01 00:30:46 2021-04-01 00:39:52 8.5 1.65 5.151515007019043 2 2021-04-01 00:48:18 2021-04-01 01:06:50 25.0 8.16 3.063725471496582 1 2021-04-01 00:19:42 2021-04-01 00:33:25 21.5 7.4 2.9054055213928223 2 2021-04-01 00:14:42 2021-04-01 00:42:59 38.5 13.65 2.8205127716064453 2 2021-04-01 00:48:53 2021-04-01 01:02:10 12.5 3.14 3.980891704559326 2 2021-04-01 00:54:51 2021-04-01 01:01:47 8.0 1.89 4.232804298400879 1 2021-04-01 00:17:17 2021-04-01 00:43:38 42.5 15.5 2.7419354915618896 1 2021-04-01 00:24:04 2021-04-01 00:56:20 52.0 20.1 2.587064743041992 2 2021-04-01 00:31:12 2021-04-01 00:36:07 6.0 1.21 4.958677768707275 2 2021-04-01 00:36:46 2021-04-01 00:40:07 4.5 0.5 9.0 2 2021-04-01 00:27:29 2021-04-01 00:42:54 28.0 10.12 2.766798496246338 1 2021-04-01 00:30:59 2021-04-01 00:36:40 6.0 1.0 6.0 2 2021-04-01 00:10:17 2021-04-01 00:25:00 27.0 9.4 2.872340440750122 2 2021-04-01 00:12:31 2021-04-01 00:14:32 4.5 0.89 5.056180000305176 2 2021-04-01 00:22:25 2021-04-01 00:39:31 16.5 4.67 3.5331904888153076 1 2021-04-01 00:39:29 2021-04-01 01:02:44 52.0 17.2 3.0232558250427246 2 2021-04-01 00:36:24 2021-04-01 23:39:59 45.5 16.09 2.827843427658081 2 2021-04-01 00:01:40 2021-04-01 00:06:54 8.0 2.06 3.8834950923919678 2 2021-04-01 00:32:22 2021-04-01 00:42:47 11.0 3.24 3.395061731338501 2 2021-04-01 00:53:54 2021-04-01 00:58:37 7.5 1.93 3.8860104084014893 2 2021-04-01 00:14:08 2021-04-01 00:23:22 15.0 4.77 3.1446540355682373 1 2021-04-01 00:13:34 2021-04-01 00:29:53 24.0 8.3 2.891566276550293 1 2021-04-01 00:20:43 2021-04-01 00:52:07 52.0 18.9 2.7513227462768555 2 2021-04-01 00:46:15 2021-04-01 00:52:13 6.0 1.04 5.769230842590332 2 2021-04-01 00:29:43 2021-04-01 00:37:43 10.0 3.08 3.246753215789795 2 2021-04-01 00:33:29 2021-04-01 01:04:31 52.0 19.46 2.6721479892730713 1 2021-04-01 00:25:43 2021-04-01 00:58:50 85.0 26.3 3.2319390773773193 1 2021-04-01 00:03:42 2021-04-01 00:16:54 20.0 7.0 2.857142925262451 1 2021-04-01 00:32:51 2021-04-01 00:44:32 12.5 3.6 3.472222328186035 1 2021-04-01 00:59:07 2021-04-01 01:04:18 6.0 1.2 5.0 2 2021-03-31 23:57:46 2021-04-01 00:01:31 5.5 1.18 4.661016941070557 2 2021-04-01 00:34:30 2021-04-01 00:40:33 6.0 0.95 6.315789699554443 2 2021-04-01 00:48:51 2021-04-01 00:55:10 7.0 1.44 4.861111164093018 2 2021-04-01 00:19:34 2021-04-01 00:24:29 6.0 1.05 5.714285850524902 2 2021-04-01 00:43:38 2021-04-01 01:06:40 30.0 7.54 3.9787797927856445 2 2021-04-01 00:43:47 2021-04-01 01:19:20 51.5 18.99 2.711953639984131 1 2021-04-01 00:03:09 2021-04-01 00:09:11 8.5 2.4 3.5416667461395264 2 2021-04-01 00:12:20 2021-04-01 00:57:11 43.0 13.27 3.240391969680786 2 2021-03-31 23:56:44 2021-04-01 00:01:14 5.0 0.69 7.246376991271973 2 2021-04-01 00:04:04 2021-04-01 00:09:28 6.0 1.12 5.357142925262451 2 2021-04-01 00:17:47 2021-04-01 00:33:39 15.0 4.08 3.6764705181121826 2 2021-04-01 00:51:32 2021-04-01 01:09:05 21.0 6.34 3.312302827835083 1 2021-04-01 00:15:13 2021-04-01 00:36:09 28.2 0.0 None 2 2021-04-01 00:01:02 2021-04-01 00:07:46 7.0 1.69 4.142011642456055 2 2021-04-01 00:16:31 2021-04-01 00:20:07 5.0 0.86 5.813953399658203 2 2021-04-01 00:17:14 2021-04-01 00:43:30 23.0 6.72 3.422619104385376 2 2021-04-01 00:07:48 2021-04-01 00:33:07 34.5 12.37 2.789005756378174 2 2021-04-01 00:32:23 2021-04-01 00:51:58 26.0 9.11 2.85400652885437 2 2021-04-01 00:01:58 2021-04-01 00:08:13 6.5 1.33 4.887217998504639 2 2021-04-01 00:16:48 2021-04-01 00:23:45 9.0 2.66 3.3834586143493652 2 2021-04-01 00:08:52 2021-04-01 00:21:46 17.5 5.4 3.2407407760620117 2 2021-04-01 00:53:47 2021-04-01 01:14:13 19.0 5.47 3.473491668701172 2 2021-04-01 00:13:06 2021-04-01 00:29:40 16.5 4.59 3.594771146774292 2 2021-04-01 00:35:02 2021-04-01 01:01:27 33.0 11.19 2.949061632156372 2 2021-04-01 00:05:00 2021-04-01 00:17:40 12.0 3.09 3.8834950923919678 2 2021-04-01 00:54:19 2021-04-01 23:25:04 12.5 3.3 3.7878787517547607 2 2021-04-01 00:11:19 2021-04-01 00:12:55 3.5 0.44 7.954545497894287 2 2021-04-01 00:26:23 2021-04-01 00:31:14 5.5 1.03 5.339805603027344 2 2021-04-01 00:12:04 2021-04-01 00:17:26 7.5 1.78 4.2134833335876465 2 2021-04-01 00:29:03 2021-04-01 00:49:47 32.0 11.41 2.8045573234558105 1 2021-04-01 00:06:19 2021-04-01 00:19:18 11.5 2.8 4.107142925262451 1 2021-04-01 00:31:32 2021-04-01 00:36:12 6.5 1.6 4.0625 2 2021-04-01 00:10:29 2021-04-01 00:14:55 5.5 0.86 6.395349025726318 2 2021-04-01 00:36:45 2021-04-01 00:42:05 6.0 0.91 6.593406677246094 2 2021-04-01 00:29:42 2021-04-01 00:58:06 52.0 17.9 2.9050278663635254 1 2021-04-01 00:01:15 2021-04-01 00:15:05 17.0 5.3 3.207547187805176 2 2021-04-01 00:13:04 2021-04-01 00:21:48 12.0 3.5 3.4285714626312256 1 2021-04-01 00:06:13 2021-04-01 00:29:21 36.5 13.3 2.74436092376709 2 2021-04-01 00:04:50 2021-04-01 00:20:32 27.0 9.69 2.7863776683807373 1 2021-04-01 00:02:44 2021-04-01 00:21:21 18.0 5.4 3.3333332538604736 1 2021-04-01 00:44:08 2021-04-01 00:57:16 13.0 3.5 3.7142856121063232 2 2021-04-01 00:05:48 2021-04-01 00:25:39 21.5 6.59 3.262518882751465 2 2021-04-01 00:00:49 2021-04-01 00:12:50 12.0 3.0 4.0 1 2021-04-01 00:09:05 2021-04-01 00:33:26 34.0 12.0 2.8333332538604736 1 2021-04-01 00:02:07 2021-04-01 00:11:47 10.5 2.6 4.038461685180664 1 2021-04-01 00:41:49 2021-04-01 00:42:48 3.0 0.2 15.0 2 2021-04-01 00:03:37 2021-04-01 00:19:46 28.5 10.41 2.7377521991729736 1 2021-04-01 00:06:18 2021-04-01 00:13:35 6.5 0.9 7.222222328186035 1 2021-04-01 00:19:44 2021-04-01 00:24:55 5.5 1.1 5.0 2 2021-04-01 00:05:45 2021-04-01 00:15:21 -9.0 1.96 -4.591836929321289 2 2021-04-01 00:05:45 2021-04-01 00:15:21 9.0 1.96 4.591836929321289 2 2021-04-01 00:26:14 2021-04-01 00:30:22 5.0 0.85 5.882352828979492 2 2021-04-01 00:46:33 2021-04-01 00:58:41 15.5 4.47 3.4675614833831787 In\u00a0[17]: Copied! <pre>%%sql\n\nDELETE FROM nyc.taxis\nWHERE fare_per_distance_unit &gt; 4.0 OR distance &gt; 2.0\n</pre> %%sql  DELETE FROM nyc.taxis WHERE fare_per_distance_unit &gt; 4.0 OR distance &gt; 2.0 <pre>                                                                                </pre> Out[17]: <p>There are some fares that have a <code>null</code> for <code>fare_per_distance_unit</code> due to the distance being <code>0</code>. Let's remove those as well.</p> In\u00a0[18]: Copied! <pre>%%sql\n\nDELETE FROM nyc.taxis\nWHERE fare_per_distance_unit is null\n</pre> %%sql  DELETE FROM nyc.taxis WHERE fare_per_distance_unit is null Out[18]: In\u00a0[19]: Copied! <pre>%%sql\n\nSELECT\nVendorID\n,tpep_pickup_datetime\n,tpep_dropoff_datetime\n,fare\n,distance\n,fare_per_distance_unit\nFROM nyc.taxis\n</pre> %%sql  SELECT VendorID ,tpep_pickup_datetime ,tpep_dropoff_datetime ,fare ,distance ,fare_per_distance_unit FROM nyc.taxis Out[19]: VendorID tpep_pickup_datetime tpep_dropoff_datetime fare distance fare_per_distance_unit 2 2021-04-01 00:53:54 2021-04-01 00:58:37 7.5 1.93 3.8860104084014893 2 2021-04-01 00:05:45 2021-04-01 00:15:21 -9.0 1.96 -4.591836929321289 2 2021-04-01 00:16:50 2021-04-01 00:22:58 7.5 1.95 3.846153736114502 2 2021-04-01 00:22:33 2021-04-01 00:23:00 -2.5 0.03 -83.33333587646484 1 2021-04-01 00:48:37 2021-04-01 00:54:30 7.5 1.9 3.9473683834075928 2 2021-04-01 00:26:08 2021-04-01 00:29:36 6.5 1.79 3.6312849521636963 2 2021-04-01 00:10:58 2021-04-01 00:16:00 6.5 1.66 3.9156627655029297 2 2021-04-01 00:13:14 2021-04-01 00:16:13 -4.5 0.73 -6.164383411407471 1 2021-04-01 00:20:52 2021-04-01 00:27:13 8.0 2.0 4.0 2 2021-04-01 01:12:20 2021-04-01 01:17:19 7.0 1.92 3.6458332538604736 2 2021-04-01 01:13:21 2021-04-01 01:19:26 7.5 1.88 3.9893617630004883 2 2021-04-01 01:34:44 2021-04-01 01:37:56 -4.5 0.65 -6.92307710647583 2 2021-04-01 01:40:11 2021-04-01 01:40:44 -3.0 0.25 -12.0 1 2021-04-01 01:20:36 2021-04-01 01:24:39 6.5 1.7 3.8235294818878174 1 2021-04-01 02:00:45 2021-04-01 02:07:54 8.0 2.0 4.0 2 2021-04-01 02:20:02 2021-04-01 02:25:56 7.5 1.98 3.7878787517547607 1 2021-04-01 02:16:09 2021-04-01 02:19:05 0.0 1.8 0.0 1 2021-04-01 02:16:11 2021-04-01 02:23:16 8.0 2.0 4.0 2 2021-04-01 03:30:12 2021-04-01 03:34:12 6.5 1.76 3.6931817531585693 1 2021-04-01 03:12:19 2021-04-01 03:17:22 7.0 1.8 3.8888888359069824 2 2021-04-01 03:39:52 2021-04-01 03:54:10 -11.5 1.91 -6.020942211151123 2 2021-04-01 04:45:21 2021-04-01 04:50:11 6.5 1.7 3.8235294818878174 1 2021-04-01 04:56:59 2021-04-01 05:02:22 7.5 2.0 3.75 1 2021-04-01 04:44:55 2021-04-01 04:49:22 6.0 1.5 4.0 2 2021-04-01 05:59:37 2021-04-01 06:04:14 -5.5 0.79 -6.962025165557861 2 2021-04-01 05:37:22 2021-04-01 05:42:04 6.5 1.69 3.846153736114502 2 2021-04-01 05:26:28 2021-04-01 05:30:33 6.5 1.67 3.8922154903411865 1 2021-04-01 05:45:24 2021-04-01 05:50:38 7.5 1.9 3.9473683834075928 2 2021-04-01 05:35:16 2021-04-01 05:41:15 7.0 1.76 3.9772727489471436 2 2021-04-01 05:37:16 2021-04-01 05:42:23 7.0 1.76 3.9772727489471436 1 2021-04-01 05:46:30 2021-04-01 05:51:54 7.0 1.9 3.6842105388641357 1 2021-04-01 06:27:52 2021-04-01 06:32:25 7.0 1.8 3.8888888359069824 1 2021-04-01 06:23:54 2021-04-01 06:30:46 8.0 2.0 4.0 2 2021-04-01 06:21:02 2021-04-01 06:28:30 7.5 1.98 3.7878787517547607 2 2021-04-01 06:54:28 2021-04-01 07:00:00 7.0 1.82 3.846153736114502 2 2021-04-01 06:41:31 2021-04-01 06:46:39 6.5 1.67 3.8922154903411865 2 2021-04-01 06:32:13 2021-04-01 06:36:21 6.0 1.56 3.846153736114502 1 2021-04-01 06:24:30 2021-04-01 06:30:16 7.5 2.0 3.75 2 2021-04-01 06:45:04 2021-04-01 06:50:07 7.0 1.76 3.9772727489471436 1 2021-04-01 06:27:04 2021-04-01 06:31:10 6.0 1.5 4.0 1 2021-04-01 06:25:35 2021-04-01 06:30:46 7.5 2.0 3.75 1 2021-04-01 06:34:44 2021-04-01 06:39:05 6.5 1.7 3.8235294818878174 2 2021-04-01 06:58:56 2021-04-01 07:03:35 6.5 1.64 3.9634146690368652 2 2021-04-01 06:28:41 2021-04-01 06:33:51 -5.5 0.97 -5.670103073120117 2 2021-04-01 06:34:59 2021-04-01 06:39:25 7.0 1.88 3.7234041690826416 2 2021-04-01 06:42:04 2021-04-01 06:47:26 7.0 1.78 3.932584285736084 1 2021-04-01 06:32:45 2021-04-01 06:39:10 7.5 1.9 3.9473683834075928 2 2021-04-01 06:40:40 2021-04-01 06:46:36 7.5 1.91 3.926701545715332 2 2021-04-01 06:57:53 2021-04-01 06:58:16 -2.5 0.1 -25.0 2 2021-04-01 06:05:23 2021-04-01 06:09:28 6.5 1.66 3.9156627655029297 2 2021-04-01 06:52:38 2021-04-01 06:59:10 -6.0 0.72 -8.333333015441895 2 2021-04-01 07:28:40 2021-04-01 07:33:05 7.0 1.89 3.7037036418914795 2 2021-04-01 07:24:05 2021-04-01 07:28:37 6.5 1.75 3.7142856121063232 1 2021-04-01 07:39:03 2021-04-01 07:47:16 8.0 2.0 4.0 2 2021-04-01 07:44:04 2021-04-01 07:48:10 -3.5 0.01 -350.0 2 2021-04-01 07:16:17 2021-04-01 07:22:38 7.5 1.92 3.90625 2 2021-04-01 07:08:09 2021-04-01 07:14:02 7.0 1.75 4.0 1 2021-04-01 07:49:47 2021-04-01 07:55:45 7.0 1.8 3.8888888359069824 2 2021-04-01 07:35:24 2021-04-01 07:40:55 7.5 1.91 3.926701545715332 1 2021-04-01 07:17:43 2021-04-01 07:20:58 5.5 1.4 3.9285714626312256 2 2021-04-01 07:08:56 2021-04-01 07:13:09 6.0 1.53 3.9215686321258545 2 2021-04-01 07:00:44 2021-04-01 07:04:20 7.5 1.98 3.7878787517547607 2 2021-04-01 07:08:42 2021-04-01 07:12:32 7.0 1.82 3.846153736114502 2 2021-04-01 07:24:53 2021-04-01 07:26:46 -4.0 0.58 -6.896551609039307 2 2021-04-01 07:54:51 2021-04-01 08:02:32 8.0 2.0 4.0 2 2021-04-01 07:34:02 2021-04-01 07:39:00 7.0 1.75 4.0 2 2021-04-01 07:08:40 2021-04-01 07:15:09 7.5 1.93 3.8860104084014893 2 2021-04-01 07:41:03 2021-04-01 07:46:36 6.5 1.63 3.987730026245117 2 2021-04-01 07:33:52 2021-04-01 07:36:06 -4.5 0.94 -4.787233829498291 2 2021-04-01 07:20:30 2021-04-01 07:25:09 6.5 1.65 3.939393997192383 1 2021-04-01 07:01:27 2021-04-01 07:07:24 7.5 1.9 3.9473683834075928 2 2021-04-01 07:16:58 2021-04-01 07:17:13 -2.5 0.08 -31.25 2 2021-04-01 07:14:50 2021-04-01 07:19:35 6.5 1.66 3.9156627655029297 2 2021-04-01 07:45:48 2021-04-01 07:52:20 7.5 2.0 3.75 2 2021-04-01 07:00:10 2021-04-01 07:00:34 -2.5 0.07 -35.71428680419922 2 2021-04-01 07:04:34 2021-04-01 07:10:10 7.0 1.83 3.825136661529541 2 2021-04-01 07:23:20 2021-04-01 07:30:41 7.5 2.0 3.75 1 2021-04-01 07:12:14 2021-04-01 07:19:02 8.0 2.0 4.0 2 2021-04-01 07:04:35 2021-04-01 07:09:12 6.5 1.76 3.6931817531585693 1 2021-04-01 07:02:52 2021-04-01 07:08:27 7.5 1.9 3.9473683834075928 2 2021-04-01 07:27:23 2021-04-01 07:32:42 7.0 1.9 3.6842105388641357 2 2021-04-01 07:52:14 2021-04-01 07:57:29 -6.0 1.06 -5.660377502441406 1 2021-04-01 07:40:41 2021-04-01 07:46:59 7.5 2.0 3.75 2 2021-04-01 07:24:54 2021-04-01 07:30:04 -5.5 0.85 -6.470588207244873 2 2021-04-01 07:37:05 2021-04-01 07:43:31 -6.0 0.58 -10.344827651977539 2 2021-04-01 07:51:34 2021-04-01 07:51:50 -2.5 0.01 -250.0 2 2021-04-01 07:00:43 2021-04-01 07:05:50 7.0 1.82 3.846153736114502 1 2021-04-01 07:13:30 2021-04-01 07:13:38 2.5 0.9 2.777777671813965 1 2021-04-01 08:49:08 2021-04-01 08:53:13 6.0 1.5 4.0 2 2021-04-01 08:35:32 2021-04-01 08:40:39 7.5 1.94 3.8659794330596924 1 2021-04-01 08:13:54 2021-04-01 08:21:50 8.0 2.0 4.0 2 2021-04-01 08:31:29 2021-04-01 08:36:29 7.0 1.8 3.8888888359069824 2 2021-04-01 08:29:21 2021-04-01 08:30:04 -3.0 0.32 -9.375 2 2021-04-01 08:33:38 2021-04-01 08:34:43 -3.0 0.08 -37.5 2 2021-04-01 08:01:47 2021-04-01 08:07:02 -5.5 0.98 -5.612245082855225 2 2021-04-01 08:07:43 2021-04-01 08:08:05 -2.5 0.08 -31.25 2 2021-04-01 08:29:08 2021-04-01 08:32:44 5.5 1.39 3.956834554672241 1 2021-04-01 08:59:57 2021-04-01 09:07:01 7.5 1.9 3.9473683834075928 2 2021-04-01 08:55:46 2021-04-01 09:02:17 -6.0 0.78 -7.692307472229004 2 2021-04-01 08:16:46 2021-04-01 08:21:00 6.5 1.67 3.8922154903411865 In\u00a0[20]: Copied! <pre>%%sql\n\nSELECT COUNT(*) as cnt\nFROM nyc.taxis\n</pre> %%sql  SELECT COUNT(*) as cnt FROM nyc.taxis Out[20]: cnt 17703 In\u00a0[23]: Copied! <pre>%%sql\n\nALTER TABLE nyc.taxis\nADD PARTITION FIELD VendorID\n</pre> %%sql  ALTER TABLE nyc.taxis ADD PARTITION FIELD VendorID Out[23]: In\u00a0[26]: Copied! <pre>%%sql\n\nDESCRIBE TABLE EXTENDED nyc.taxis;\n</pre> %%sql  DESCRIBE TABLE EXTENDED nyc.taxis; Out[26]: col_name data_type comment VendorID bigint None tpep_pickup_datetime timestamp_ntz None tpep_dropoff_datetime timestamp_ntz None passenger_count double None RatecodeID double None store_and_fwd_flag string None PULocationID bigint None DOLocationID bigint None payment_type bigint None fare double None distance double The elapsed trip distance in miles reported by the taximeter. fare_per_distance_unit float None extra double None mta_tax double None tip_amount double None tolls_amount double None improvement_surcharge double None total_amount double None congestion_surcharge double None airport_fee double None # Partition Information # col_name data_type comment VendorID bigint None # Metadata Columns _spec_id int _partition struct&lt;VendorID:bigint&gt; _file string _pos bigint _deleted boolean # Detailed Table Information Name demo.nyc.taxis Type MANAGED Location s3://warehouse/nyc/taxis Provider iceberg Owner root Table Properties [created-at=2025-06-20T09:58:21.220162508Z,current-snapshot-id=1280087261393946112,format=iceberg/parquet,format-version=2,write.format.default=parquet,write.parquet.compression-codec=zstd] In\u00a0[27]: Copied! <pre>%%sql\n\nSELECT snapshot_id, manifest_list\nFROM nyc.taxis.snapshots\n</pre> %%sql  SELECT snapshot_id, manifest_list FROM nyc.taxis.snapshots Out[27]: snapshot_id manifest_list 4656165136069422702 s3://warehouse/nyc/taxis/metadata/snap-4656165136069422702-1-b07eea67-1b9f-4231-acb2-b6eed7ccd24c.avro 1284970284820445596 s3://warehouse/nyc/taxis/metadata/snap-1284970284820445596-1-0e8016e0-364d-488b-8f05-d763561d72df.avro 3983088334768464117 s3://warehouse/nyc/taxis/metadata/snap-3983088334768464117-1-13bb707e-e812-4b18-937e-55c4639723e0.avro 1280087261393946112 s3://warehouse/nyc/taxis/metadata/snap-1280087261393946112-1-319556dc-0c2f-4929-8104-7758597e94e8.avro <p>The <code>files</code> table contains loads of information on data files, including column level statistics such as null counts, lower bounds, and upper bounds.</p> In\u00a0[28]: Copied! <pre>%%sql\n\nSELECT file_path, file_format, record_count, null_value_counts, lower_bounds, upper_bounds\nFROM nyc.taxis.files\n</pre> %%sql  SELECT file_path, file_format, record_count, null_value_counts, lower_bounds, upper_bounds FROM nyc.taxis.files Out[28]: file_path file_format record_count null_value_counts lower_bounds upper_bounds s3://warehouse/nyc/taxis/data/00000-21-5b6b2036-89db-4c14-9bec-932bd1dda5f5-0-00001.parquet PARQUET 17703 {1: 0, 2: 0, 3: 0, 4: 299, 5: 0, 6: 299, 7: 299, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 299, 19: 299, 20: 0} {1: bytearray(b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00'), 2: bytearray(b'@\\x88-\\xfe\\xdd\\xbe\\x05\\x00'), 3: bytearray(b'@\\x98\\x82 \\xde\\xbe\\x05\\x00'), 4: bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'), 5: bytearray(b'{\\x14\\xaeG\\xe1z\\x84?'), 6: bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?'), 7: bytearray(b'N'), 8: bytearray(b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00'), 9: bytearray(b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00'), 10: bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'), 11: bytearray(b'\\x00\\x00\\x00\\x00\\x00 l\\xc0'), 12: bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x12\\xc0'), 13: bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\xe0\\xbf'), 14: bytearray(b'\\x85\\xebQ\\xb8\\x1e\\xd5t\\xc0'), 15: bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\xc0'), 16: bytearray(b'333333\\xd3\\xbf'), 17: bytearray(b'R\\xb8\\x1e\\x85\\xebYu\\xc0'), 18: bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\xc0'), 19: bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\xf4\\xbf'), 20: bytearray(b'\\x00\\x00\\xfa\\xc5')} {1: bytearray(b'\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00'), 2: bytearray(b'@\\xa72h9\\xc1\\x05\\x00'), 3: bytearray(b'\\xc0X\\xfclC\\xc1\\x05\\x00'), 4: bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x18@'), 5: bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00@'), 6: bytearray(b'\\x00\\x00\\x00\\x00\\x00\\xc0X@'), 7: bytearray(b'Y'), 8: bytearray(b'\\t\\x01\\x00\\x00\\x00\\x00\\x00\\x00'), 9: bytearray(b'\\t\\x01\\x00\\x00\\x00\\x00\\x00\\x00'), 10: bytearray(b'\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00'), 11: bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00 @'), 12: bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x16@'), 13: bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\xe0?'), 14: bytearray(b'\\x00\\x00\\x00\\x00\\x00@U@'), 15: bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x80;@'), 16: bytearray(b'333333\\xd3?'), 17: bytearray(b'33333SU@'), 18: bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x04@'), 19: bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\xf4?'), 20: bytearray(b'\\x00\\x00\\x80@')} In\u00a0[29]: Copied! <pre>%%sql\n\nSELECT *\nFROM nyc.taxis.history\n</pre> %%sql  SELECT * FROM nyc.taxis.history Out[29]: made_current_at snapshot_id parent_id is_current_ancestor 2025-06-20 09:58:28.093000 4656165136069422702 None True 2025-06-20 10:00:39.150000 1284970284820445596 4656165136069422702 True 2025-06-20 10:01:37.382000 3983088334768464117 1284970284820445596 True 2025-06-20 10:01:48.206000 1280087261393946112 3983088334768464117 True <p>You can time-travel by altering the <code>current-snapshot-id</code> property of the table to reference any snapshot in the table's history. Let's revert the table to it's original state by traveling to the very first snapshot ID.</p> In\u00a0[30]: Copied! <pre>%%sql --var df\n\nSELECT *\nFROM nyc.taxis.history\n</pre> %%sql --var df  SELECT * FROM nyc.taxis.history Out[30]: made_current_at snapshot_id parent_id is_current_ancestor 2025-06-20 09:58:28.093000 4656165136069422702 None True 2025-06-20 10:00:39.150000 1284970284820445596 4656165136069422702 True 2025-06-20 10:01:37.382000 3983088334768464117 1284970284820445596 True 2025-06-20 10:01:48.206000 1280087261393946112 3983088334768464117 True In\u00a0[31]: Copied! <pre>df.head()\n</pre> df.head() Out[31]: <pre>Row(made_current_at=datetime.datetime(2025, 6, 20, 9, 58, 28, 93000), snapshot_id=4656165136069422702, parent_id=None, is_current_ancestor=True)</pre> In\u00a0[32]: Copied! <pre>type(df)\n</pre> type(df) Out[32]: <pre>pyspark.sql.dataframe.DataFrame</pre> In\u00a0[33]: Copied! <pre>original_snapshot = df.head().snapshot_id\nspark.sql(f\"CALL system.rollback_to_snapshot('nyc.taxis', {original_snapshot})\")\noriginal_snapshot\n</pre> original_snapshot = df.head().snapshot_id spark.sql(f\"CALL system.rollback_to_snapshot('nyc.taxis', {original_snapshot})\") original_snapshot Out[33]: <pre>4656165136069422702</pre> In\u00a0[34]: Copied! <pre>%%sql\n\nSELECT\nVendorID\n,tpep_pickup_datetime\n,tpep_dropoff_datetime\n,fare\n,distance\n,fare_per_distance_unit\nFROM nyc.taxis\n</pre> %%sql  SELECT VendorID ,tpep_pickup_datetime ,tpep_dropoff_datetime ,fare ,distance ,fare_per_distance_unit FROM nyc.taxis Out[34]: VendorID tpep_pickup_datetime tpep_dropoff_datetime fare distance fare_per_distance_unit 1 2021-04-01 00:00:18 2021-04-01 00:21:54 25.5 8.4 None 1 2021-04-01 00:42:37 2021-04-01 00:46:23 5.0 0.9 None 1 2021-04-01 00:57:56 2021-04-01 01:08:22 11.5 3.4 None 1 2021-04-01 00:01:58 2021-04-01 00:54:27 44.2 0.0 None 2 2021-04-01 00:24:55 2021-04-01 00:34:33 9.0 1.96 None 2 2021-04-01 00:19:16 2021-04-01 00:21:46 4.5 0.77 None 2 2021-04-01 00:25:11 2021-04-01 00:31:53 11.5 3.65 None 1 2021-04-01 00:27:53 2021-04-01 00:47:03 26.5 8.9 None 2 2021-04-01 00:24:24 2021-04-01 00:37:50 12.0 2.98 None 1 2021-04-01 00:19:18 2021-04-01 00:41:25 28.0 8.9 None 2 2021-04-01 00:04:25 2021-04-01 00:29:22 23.5 7.48 None 2 2021-04-01 00:03:07 2021-04-01 00:18:02 13.5 3.39 None 2 2021-04-01 00:35:44 2021-04-01 00:51:06 14.0 3.51 None 2 2021-04-01 00:52:32 2021-04-01 01:04:41 12.5 3.42 None 2 2021-04-01 00:28:05 2021-04-01 00:47:59 33.5 12.14 None 1 2021-04-01 00:39:01 2021-04-01 00:57:39 32.0 11.8 None 2 2021-04-01 00:15:10 2021-04-01 00:22:46 7.5 1.44 None 2 2021-04-01 00:30:46 2021-04-01 00:39:52 8.5 1.65 None 2 2021-04-01 00:48:18 2021-04-01 01:06:50 25.0 8.16 None 1 2021-04-01 00:19:42 2021-04-01 00:33:25 21.5 7.4 None 2 2021-04-01 00:14:42 2021-04-01 00:42:59 38.5 13.65 None 2 2021-04-01 00:48:53 2021-04-01 01:02:10 12.5 3.14 None 2 2021-04-01 00:54:51 2021-04-01 01:01:47 8.0 1.89 None 1 2021-04-01 00:17:17 2021-04-01 00:43:38 42.5 15.5 None 1 2021-04-01 00:24:04 2021-04-01 00:56:20 52.0 20.1 None 2 2021-04-01 00:31:12 2021-04-01 00:36:07 6.0 1.21 None 2 2021-04-01 00:36:46 2021-04-01 00:40:07 4.5 0.5 None 2 2021-04-01 00:27:29 2021-04-01 00:42:54 28.0 10.12 None 1 2021-04-01 00:30:59 2021-04-01 00:36:40 6.0 1.0 None 2 2021-04-01 00:10:17 2021-04-01 00:25:00 27.0 9.4 None 2 2021-04-01 00:12:31 2021-04-01 00:14:32 4.5 0.89 None 2 2021-04-01 00:22:25 2021-04-01 00:39:31 16.5 4.67 None 1 2021-04-01 00:39:29 2021-04-01 01:02:44 52.0 17.2 None 2 2021-04-01 00:36:24 2021-04-01 23:39:59 45.5 16.09 None 2 2021-04-01 00:01:40 2021-04-01 00:06:54 8.0 2.06 None 2 2021-04-01 00:32:22 2021-04-01 00:42:47 11.0 3.24 None 2 2021-04-01 00:53:54 2021-04-01 00:58:37 7.5 1.93 None 2 2021-04-01 00:14:08 2021-04-01 00:23:22 15.0 4.77 None 1 2021-04-01 00:13:34 2021-04-01 00:29:53 24.0 8.3 None 1 2021-04-01 00:20:43 2021-04-01 00:52:07 52.0 18.9 None 2 2021-04-01 00:46:15 2021-04-01 00:52:13 6.0 1.04 None 2 2021-04-01 00:29:43 2021-04-01 00:37:43 10.0 3.08 None 2 2021-04-01 00:33:29 2021-04-01 01:04:31 52.0 19.46 None 1 2021-04-01 00:25:43 2021-04-01 00:58:50 85.0 26.3 None 1 2021-04-01 00:03:42 2021-04-01 00:16:54 20.0 7.0 None 1 2021-04-01 00:32:51 2021-04-01 00:44:32 12.5 3.6 None 1 2021-04-01 00:59:07 2021-04-01 01:04:18 6.0 1.2 None 2 2021-03-31 23:57:46 2021-04-01 00:01:31 5.5 1.18 None 2 2021-04-01 00:34:30 2021-04-01 00:40:33 6.0 0.95 None 2 2021-04-01 00:48:51 2021-04-01 00:55:10 7.0 1.44 None 2 2021-04-01 00:19:34 2021-04-01 00:24:29 6.0 1.05 None 2 2021-04-01 00:43:38 2021-04-01 01:06:40 30.0 7.54 None 2 2021-04-01 00:43:47 2021-04-01 01:19:20 51.5 18.99 None 1 2021-04-01 00:03:09 2021-04-01 00:09:11 8.5 2.4 None 2 2021-04-01 00:12:20 2021-04-01 00:57:11 43.0 13.27 None 2 2021-03-31 23:56:44 2021-04-01 00:01:14 5.0 0.69 None 2 2021-04-01 00:04:04 2021-04-01 00:09:28 6.0 1.12 None 2 2021-04-01 00:17:47 2021-04-01 00:33:39 15.0 4.08 None 2 2021-04-01 00:51:32 2021-04-01 01:09:05 21.0 6.34 None 1 2021-04-01 00:15:13 2021-04-01 00:36:09 28.2 0.0 None 2 2021-04-01 00:01:02 2021-04-01 00:07:46 7.0 1.69 None 2 2021-04-01 00:16:31 2021-04-01 00:20:07 5.0 0.86 None 2 2021-04-01 00:17:14 2021-04-01 00:43:30 23.0 6.72 None 2 2021-04-01 00:07:48 2021-04-01 00:33:07 34.5 12.37 None 2 2021-04-01 00:32:23 2021-04-01 00:51:58 26.0 9.11 None 2 2021-04-01 00:01:58 2021-04-01 00:08:13 6.5 1.33 None 2 2021-04-01 00:16:48 2021-04-01 00:23:45 9.0 2.66 None 2 2021-04-01 00:08:52 2021-04-01 00:21:46 17.5 5.4 None 2 2021-04-01 00:53:47 2021-04-01 01:14:13 19.0 5.47 None 2 2021-04-01 00:13:06 2021-04-01 00:29:40 16.5 4.59 None 2 2021-04-01 00:35:02 2021-04-01 01:01:27 33.0 11.19 None 2 2021-04-01 00:05:00 2021-04-01 00:17:40 12.0 3.09 None 2 2021-04-01 00:54:19 2021-04-01 23:25:04 12.5 3.3 None 2 2021-04-01 00:11:19 2021-04-01 00:12:55 3.5 0.44 None 2 2021-04-01 00:26:23 2021-04-01 00:31:14 5.5 1.03 None 2 2021-04-01 00:12:04 2021-04-01 00:17:26 7.5 1.78 None 2 2021-04-01 00:29:03 2021-04-01 00:49:47 32.0 11.41 None 1 2021-04-01 00:06:19 2021-04-01 00:19:18 11.5 2.8 None 1 2021-04-01 00:31:32 2021-04-01 00:36:12 6.5 1.6 None 2 2021-04-01 00:10:29 2021-04-01 00:14:55 5.5 0.86 None 2 2021-04-01 00:36:45 2021-04-01 00:42:05 6.0 0.91 None 2 2021-04-01 00:29:42 2021-04-01 00:58:06 52.0 17.9 None 1 2021-04-01 00:01:15 2021-04-01 00:15:05 17.0 5.3 None 2 2021-04-01 00:13:04 2021-04-01 00:21:48 12.0 3.5 None 1 2021-04-01 00:06:13 2021-04-01 00:29:21 36.5 13.3 None 2 2021-04-01 00:04:50 2021-04-01 00:20:32 27.0 9.69 None 1 2021-04-01 00:02:44 2021-04-01 00:21:21 18.0 5.4 None 1 2021-04-01 00:44:08 2021-04-01 00:57:16 13.0 3.5 None 2 2021-04-01 00:05:48 2021-04-01 00:25:39 21.5 6.59 None 2 2021-04-01 00:00:49 2021-04-01 00:12:50 12.0 3.0 None 1 2021-04-01 00:09:05 2021-04-01 00:33:26 34.0 12.0 None 1 2021-04-01 00:02:07 2021-04-01 00:11:47 10.5 2.6 None 1 2021-04-01 00:41:49 2021-04-01 00:42:48 3.0 0.2 None 2 2021-04-01 00:03:37 2021-04-01 00:19:46 28.5 10.41 None 1 2021-04-01 00:06:18 2021-04-01 00:13:35 6.5 0.9 None 1 2021-04-01 00:19:44 2021-04-01 00:24:55 5.5 1.1 None 2 2021-04-01 00:05:45 2021-04-01 00:15:21 -9.0 1.96 None 2 2021-04-01 00:05:45 2021-04-01 00:15:21 9.0 1.96 None 2 2021-04-01 00:26:14 2021-04-01 00:30:22 5.0 0.85 None 2 2021-04-01 00:46:33 2021-04-01 00:58:41 15.5 4.47 None <p>Another look at the history table shows that the original state of the table has been added as a new entry with the original snapshot ID.</p> In\u00a0[35]: Copied! <pre>%%sql\n\nSELECT *\nFROM nyc.taxis.history\n</pre> %%sql  SELECT * FROM nyc.taxis.history Out[35]: made_current_at snapshot_id parent_id is_current_ancestor 2025-06-20 09:58:28.093000 4656165136069422702 None True 2025-06-20 10:00:39.150000 1284970284820445596 4656165136069422702 False 2025-06-20 10:01:37.382000 3983088334768464117 1284970284820445596 False 2025-06-20 10:01:48.206000 1280087261393946112 3983088334768464117 False 2025-06-20 10:17:46.994000 4656165136069422702 None True In\u00a0[36]: Copied! <pre>%%sql\n\nSELECT COUNT(*) as cnt\nFROM nyc.taxis\n</pre> %%sql  SELECT COUNT(*) as cnt FROM nyc.taxis Out[36]: cnt 2171187"},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/1-iceberg-getting-started/#getting-started","title":"Getting Started\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/1-iceberg-getting-started/#load-one-month-of-nyc-taxilimousine-trip-data","title":"Load One Month of NYC Taxi/Limousine Trip Data\u00b6","text":"<p>For this notebook, we will use the New York City Taxi and Limousine Commision Trip Record Data that's available on the AWS Open Data Registry. This contains data of trips taken by taxis and for-hire vehicles in New York City. We'll save this into an iceberg table called <code>taxis</code>.</p>"},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/1-iceberg-getting-started/#schema-evolution","title":"Schema Evolution\u00b6","text":"<p>Adding, dropping, renaming, or altering columns is easy and safe in Iceberg. In this example, we'll rename <code>fare_amount</code> to <code>fare</code> and <code>trip_distance</code> to <code>distance</code>. We'll also add a float column <code>fare_per_distance_unit</code> immediately after <code>distance</code>.</p>"},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/1-iceberg-getting-started/#expressive-sql-for-row-level-changes","title":"Expressive SQL for Row Level Changes\u00b6","text":"<p>With Iceberg tables, <code>DELETE</code> queries can be used to perform row-level deletes. This is as simple as providing the table name and a <code>WHERE</code> predicate. If the filter matches an entire partition of the table, Iceberg will intelligently perform a metadata-only operation where it simply deletes the metadata for that partition.</p> <p>Let's perform a row-level delete for all rows that have a <code>fare_per_distance_unit</code> greater than 4 or a <code>distance</code> greater than 2. This should leave us with relatively short trips that have a relatively high fare per distance traveled.</p>"},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/1-iceberg-getting-started/#partitioning","title":"Partitioning\u00b6","text":"<p>A table\u2019s partitioning can be updated in place and applied only to newly written data. Query plans are then split, using the old partition scheme for data written before the partition scheme was changed, and using the new partition scheme for data written after. People querying the table don\u2019t even have to be aware of this split. Simple predicates in WHERE clauses are automatically converted to partition filters that prune out files with no matches. This is what\u2019s referred to in Iceberg as Hidden Partitioning.</p>"},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/1-iceberg-getting-started/#metadata-tables","title":"Metadata Tables\u00b6","text":"<p>Iceberg tables contain very rich metadata that can be easily queried. For example, you can retrieve the manifest list for any snapshot, simply by querying the table's <code>snapshots</code> table.</p>"},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/1-iceberg-getting-started/#time-travel","title":"Time Travel\u00b6","text":"<p>The history table lists all snapshots and which parent snapshot they derive from. The <code>is_current_ancestor</code> flag let's you know if a snapshot is part of the linear history of the current snapshot of the table.</p>"},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/2-iceberg-integrated-audits-demo/","title":"Integrated Audits Demo","text":"<p>Reference: Integrated Audits: Streamlined Data Observability with Apache Iceberg</p> In\u00a0[1]: Copied! <pre>from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"Jupyter\").getOrCreate()\n\nspark\n</pre> from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"Jupyter\").getOrCreate()  spark <pre>25/06/21 04:34:30 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n</pre> Out[1]: <p>SparkSession - in-memory</p> <p>SparkContext</p> <p>Spark UI</p> Version <code>v3.5.5</code> Master <code>local[*]</code> AppName <code>PySparkShell</code> <p>To be able to rerun the notebook several times, let's drop the <code>permits</code> table if it exists to start fresh.</p> In\u00a0[2]: Copied! <pre>%%sql\n\nCREATE DATABASE IF NOT EXISTS nyc\n</pre> %%sql  CREATE DATABASE IF NOT EXISTS nyc Out[2]: In\u00a0[3]: Copied! <pre>%%sql\n\nDROP TABLE IF EXISTS nyc.permits\n</pre> %%sql  DROP TABLE IF EXISTS nyc.permits Out[3]: <p>For this demo, we will use the New York City Film Permits dataset available as part of the NYC Open Data initiative. We're using a locally saved copy of a 1000 record sample, but feel free to download the entire dataset to use in this notebook!</p> <p>We'll save the sample dataset into an iceberg table called <code>permits</code>.</p> In\u00a0[4]: Copied! <pre>df = spark.read.option(\"inferSchema\",\"true\").option(\"multiline\",\"true\").json(\"/home/iceberg/data/nyc_film_permits.json\")\ndf.write.saveAsTable(\"nyc.permits\")\n</pre> df = spark.read.option(\"inferSchema\",\"true\").option(\"multiline\",\"true\").json(\"/home/iceberg/data/nyc_film_permits.json\") df.write.saveAsTable(\"nyc.permits\") <pre>                                                                                </pre> <p>Taking a quick peek at the data, you can see that there are a number of permits for different boroughs in New York.</p> In\u00a0[5]: Copied! <pre>spark.read \\\n    .format(\"iceberg\") \\\n    .load(\"nyc.permits\") \\\n    .groupBy(\"borough\") \\\n    .count() \\\n    .show()\n</pre> spark.read \\     .format(\"iceberg\") \\     .load(\"nyc.permits\") \\     .groupBy(\"borough\") \\     .count() \\     .show() <pre>+-------------+-----+\n|      borough|count|\n+-------------+-----+\n|       Queens|   96|\n|     Brooklyn|  378|\n|Staten Island|    1|\n|    Manhattan|  518|\n|        Bronx|    7|\n+-------------+-----+\n\n</pre> <p>An integrated audit session is a single cadence of:</p> <ol> <li>Staging changes to a table</li> <li>Auditing the staged changes</li> <li>Committing the changes (optional)</li> </ol> <p>Each of these sessions must be represented with an ID. You can use any convention that makes sense in your environment but in this demo we'll simply use a UUID.</p> In\u00a0[6]: Copied! <pre>import uuid\nia_session_id = uuid.uuid4().hex\nia_session_id\n</pre> import uuid ia_session_id = uuid.uuid4().hex ia_session_id Out[6]: <pre>'92d9d9eb680740468822d1ea99068c8a'</pre> <p>Tables by default are not configured to allow integrated audits, therefore the first step is enabling this by setting the <code>write.wap.enabled</code> table metadata property to <code>true</code></p> In\u00a0[7]: Copied! <pre>%%sql\n\nALTER TABLE nyc.permits\nSET TBLPROPERTIES (\n    'write.wap.enabled'='true'\n)\n</pre> %%sql  ALTER TABLE nyc.permits SET TBLPROPERTIES (     'write.wap.enabled'='true' ) Out[7]: <p>Next, the <code>spark.wap.id</code> property of your Spark session configuration must be set to the integrated audit session ID.</p> In\u00a0[8]: Copied! <pre>spark.conf.set('spark.wap.id', ia_session_id)\n</pre> spark.conf.set('spark.wap.id', ia_session_id) <p>With a <code>spark.wap.id</code> value set, you can now safely write directly to the permits table--don't worry, these changes will only be staged, not committed!</p> <p>To stage the changes, you simply write directly to the <code>permits</code> table. This is awesome in situations where you're working with a large and complex data ingestion pipeline. Instead of including hard-coded logic in your pipeline to switch between a sort of \"audit-mode\" as opposed to \"production-mode\", with integrated audits you simple run your production code!</p> <p>For this demo, let's use a simple query that deletes all records for film permits in the manhattan borough.</p> In\u00a0[9]: Copied! <pre>%%sql\n\nDELETE FROM nyc.permits\nWHERE borough='Manhattan'\n</pre> %%sql  DELETE FROM nyc.permits WHERE borough='Manhattan' Out[9]: <p>As described, even though the query was executed against the production table, these changes are only staged and not committed since we are within an integrated audit session. Let's confirm this by verifying that a count by borough still includes the Manhattan records.</p> In\u00a0[10]: Copied! <pre>%%sql\n\nSELECT borough, count(*) permit_cnt\nFROM nyc.permits\nGROUP BY borough\n</pre> %%sql  SELECT borough, count(*) permit_cnt FROM nyc.permits GROUP BY borough Out[10]: borough permit_cnt Queens 96 Brooklyn 378 Staten Island 1 Manhattan 518 Bronx 7 <p>Once the changes for this session are staged, you can perform all of your audits to validate the data. The first step is to retrieve the snapshot ID generated by the changes and tagged with this integrated audit session ID.</p> In\u00a0[14]: Copied! <pre>%%sql\n\nselect\n  *\nfrom nyc.permits.history\n</pre> %%sql  select   * from nyc.permits.history Out[14]: made_current_at snapshot_id parent_id is_current_ancestor 2025-06-21 04:36:21.362000 2044767572491260316 None True In\u00a0[11]: Copied! <pre>query = f\"\"\"\nSELECT\n  snapshot_id\nFROM nyc.permits.snapshots\nWHERE summary['wap.id'] = '{ia_session_id}'\n\"\"\"\n\nia_session_snapshot = spark.sql(query).head().snapshot_id\n</pre> query = f\"\"\" SELECT   snapshot_id FROM nyc.permits.snapshots WHERE summary['wap.id'] = '{ia_session_id}' \"\"\"  ia_session_snapshot = spark.sql(query).head().snapshot_id In\u00a0[12]: Copied! <pre>ia_session_snapshot\n</pre> ia_session_snapshot Out[12]: <pre>2689398741546157644</pre> <p>This snapshot includes the staged (but not commited) changes to your production table. Once you have this snapshot ID, you can use Iceberg's Time Travel feature to query it!</p> In\u00a0[13]: Copied! <pre>spark.read \\\n    .option(\"snapshot-id\", ia_session_snapshot) \\\n    .format(\"iceberg\") \\\n    .load(\"nyc.permits\") \\\n    .groupBy(\"borough\") \\\n    .count() \\\n    .show()\n</pre> spark.read \\     .option(\"snapshot-id\", ia_session_snapshot) \\     .format(\"iceberg\") \\     .load(\"nyc.permits\") \\     .groupBy(\"borough\") \\     .count() \\     .show() <pre>+-------------+-----+\n|      borough|count|\n+-------------+-----+\n|       Queens|   96|\n|     Brooklyn|  378|\n|Staten Island|    1|\n|        Bronx|    7|\n+-------------+-----+\n\n</pre> <p>At this point, you can use any auditing tool or technique to validate your changes. For this demo, we'll do a simple audit that confirms that the only remaining boroughs are Queens, Brooklyn, Bronx, and Staten Island. If either borough is missing or any additional boroughs are found, we'll raise an exception.</p> In\u00a0[15]: Copied! <pre>expected_boroughs = {\"Queens\", \"Brooklyn\", \"Bronx\", \"Staten Island\"}\ndistinct_boroughs = spark.read \\\n    .option(\"snapshot-id\", ia_session_snapshot) \\\n    .format(\"iceberg\") \\\n    .load(\"nyc.permits\") \\\n    .select(\"borough\") \\\n    .distinct() \\\n    .toLocalIterator()\nboroughs = {row[0] for row in distinct_boroughs}\n</pre> expected_boroughs = {\"Queens\", \"Brooklyn\", \"Bronx\", \"Staten Island\"} distinct_boroughs = spark.read \\     .option(\"snapshot-id\", ia_session_snapshot) \\     .format(\"iceberg\") \\     .load(\"nyc.permits\") \\     .select(\"borough\") \\     .distinct() \\     .toLocalIterator() boroughs = {row[0] for row in distinct_boroughs} In\u00a0[16]: Copied! <pre># Since `boroughs` and `required_boroughs` are both sets (array of distinct items),\n# we can confirm that they match by checking that the lengths of the sets are equal\n# to eachother as well as to the union of both sets.\nif len(boroughs) != len(expected_boroughs) != len(set.union(boroughs, expected_boroughs)):\n    raise ValueError(f\"Audit failed, borough set does not match expected boroughs: {boroughs} != {expected_boroughs}\")\n</pre> # Since `boroughs` and `required_boroughs` are both sets (array of distinct items), # we can confirm that they match by checking that the lengths of the sets are equal # to eachother as well as to the union of both sets. if len(boroughs) != len(expected_boroughs) != len(set.union(boroughs, expected_boroughs)):     raise ValueError(f\"Audit failed, borough set does not match expected boroughs: {boroughs} != {expected_boroughs}\") <p>If the above check does not fail, we can go ahead and commit our staged data to publish our changes!</p> <p>After the audits are completed, publishing the data is as simple as running a <code>cherrypick_snapshot</code> stored procedure.</p> In\u00a0[17]: Copied! <pre>publish_query = f\"CALL system.cherrypick_snapshot('nyc.permits', {ia_session_snapshot})\"\n%sql $publish_query\n</pre> publish_query = f\"CALL system.cherrypick_snapshot('nyc.permits', {ia_session_snapshot})\" %sql $publish_query Out[17]: source_snapshot_id current_snapshot_id 2689398741546157644 2689398741546157644 <p>That's it! Publishing the changes from this integrated audit session is a simple metadata-only operation that instantly makes the changes live for all downstream consumers querying the <code>permits</code> table! Query results will now include the commit that removed all Manhattan records.</p> In\u00a0[18]: Copied! <pre>spark.read \\\n    .format(\"iceberg\") \\\n    .load(\"nyc.permits\") \\\n    .groupBy(\"borough\") \\\n    .count() \\\n    .show()\n</pre> spark.read \\     .format(\"iceberg\") \\     .load(\"nyc.permits\") \\     .groupBy(\"borough\") \\     .count() \\     .show() <pre>+-------------+-----+\n|      borough|count|\n+-------------+-----+\n|       Queens|   96|\n|     Brooklyn|  378|\n|Staten Island|    1|\n|        Bronx|    7|\n+-------------+-----+\n\n</pre> In\u00a0[19]: Copied! <pre>%%sql\n\nselect\n  *\nfrom nyc.permits.history\n</pre> %%sql  select   * from nyc.permits.history Out[19]: made_current_at snapshot_id parent_id is_current_ancestor 2025-06-21 04:36:21.362000 2044767572491260316 None True 2025-06-21 05:08:54.301000 2689398741546157644 2044767572491260316 True <p>What about when your audits fail? What happens to the snapshots generated? How about the data and metadata files?</p> <p>One of the best parts of Iceberg's integrated audits is that the cleanup of \"staged-yet-not-committed-data\" is part of the normal snapshot cleanup process of a typical Iceberg warehouse. To be more specific, let's say a daily snapshot expiration is performed on the data warehouse (using the expire_snapshots procedure) and all snapshots older than 7 days are expired. That means once your staged snapshot reaches 7 days in age, it will be expired.</p> <p>Additionally, since the changes were never committed, the underlying data files for the snapshot will be removed since they're not referenced by any other snapshots in the linear history of the table.</p> <p>Let's see this in action. First, start a new integrated audit session and stage a commit by inserting a single record.</p> In\u00a0[20]: Copied! <pre>ia_session_id = uuid.uuid4().hex\nia_session_id\n</pre> ia_session_id = uuid.uuid4().hex ia_session_id Out[20]: <pre>'1782105f3a254b4d9858087ec76f5258'</pre> In\u00a0[21]: Copied! <pre>spark.conf.set('spark.wap.id', ia_session_id)\n</pre> spark.conf.set('spark.wap.id', ia_session_id) In\u00a0[22]: Copied! <pre>%%sql\n\nINSERT INTO nyc.permits\nVALUES (\n    'Hoboken',\n    'Television',\n    '1',\n    'United States of America',\n    '2021-11-24T23:00:00.000',\n    '2021-11-23T09:38:17.000',\n    'Mayor\\'s Office of Film, Theatre &amp; Broadcasting',\n    '613322',\n    'Shooting Permit',\n    'WASHINGTON STREET',\n    '100',\n    '2021-11-24T07:00:00.000',\n    'Episodic series',\n    '07030'\n)\n</pre> %%sql  INSERT INTO nyc.permits VALUES (     'Hoboken',     'Television',     '1',     'United States of America',     '2021-11-24T23:00:00.000',     '2021-11-23T09:38:17.000',     'Mayor\\'s Office of Film, Theatre &amp; Broadcasting',     '613322',     'Shooting Permit',     'WASHINGTON STREET',     '100',     '2021-11-24T07:00:00.000',     'Episodic series',     '07030' ) Out[22]: <p>Next, let's identify the snapshot that was tagged with the integrated audit session ID.</p> In\u00a0[23]: Copied! <pre>%%sql\n\nSELECT snapshot_id\nFROM nyc.permits.snapshots\n</pre> %%sql  SELECT snapshot_id FROM nyc.permits.snapshots Out[23]: snapshot_id 2044767572491260316 2689398741546157644 7408523516967726811 In\u00a0[24]: Copied! <pre>query = f\"\"\"\nSELECT snapshot_id\nFROM nyc.permits.snapshots\nWHERE summary['wap.id'] = '{ia_session_id}'\n\"\"\"\n\nia_session_snapshot = spark.sql(query).head().snapshot_id\n</pre> query = f\"\"\" SELECT snapshot_id FROM nyc.permits.snapshots WHERE summary['wap.id'] = '{ia_session_id}' \"\"\"  ia_session_snapshot = spark.sql(query).head().snapshot_id In\u00a0[25]: Copied! <pre>ia_session_snapshot\n</pre> ia_session_snapshot Out[25]: <pre>7408523516967726811</pre> <p>A quick check of the history table shows that this snapshot is not included as part of the current history of the table since it has not been published yet.</p> In\u00a0[26]: Copied! <pre>%%sql\n\nSELECT *\nFROM nyc.permits.history\n</pre> %%sql  SELECT * FROM nyc.permits.history Out[26]: made_current_at snapshot_id parent_id is_current_ancestor 2025-06-21 04:36:21.362000 2044767572491260316 None True 2025-06-21 05:08:54.301000 2689398741546157644 2044767572491260316 True <p>In a scenario where the audits fail and this change is not published, the <code>expire_snapshots</code> procedure will clean up the snapshot and the data files. Let's demonstrate this by calling the <code>expire_snapshots</code> procedure for all snapshots older than the current timestamp.</p> In\u00a0[27]: Copied! <pre>import time\n%sql CALL system.expire_snapshots('nyc.permits', {round(time.time() * 1000)}, 100)\n</pre> import time %sql CALL system.expire_snapshots('nyc.permits', {round(time.time() * 1000)}, 100) <pre>                                                                                </pre> Out[27]: deleted_data_files_count deleted_position_delete_files_count deleted_equality_delete_files_count deleted_manifest_files_count deleted_manifest_lists_count deleted_statistics_files_count 1 0 0 1 1 0 <p>The output from the <code>expire_snapshots</code> procedure shows that a data file, a manifest file, and a manifest list file were deleted. Furthermore, the snapshot no longer appears in the permit table's snapshots table.</p> In\u00a0[28]: Copied! <pre>%%sql\n\nSELECT *\nFROM nyc.permits.snapshots\n</pre> %%sql  SELECT * FROM nyc.permits.snapshots Out[28]: committed_at snapshot_id parent_id operation manifest_list summary 2025-06-21 04:36:21.362000 2044767572491260316 None append s3://warehouse/nyc/permits/metadata/snap-2044767572491260316-1-4cc87a9f-3242-46b1-a03a-7c45dc885f7f.avro {'engine-version': '3.5.5', 'added-data-files': '1', 'total-equality-deletes': '0', 'app-id': 'local-1750480469176', 'added-records': '1000', 'total-records': '1000', 'spark.app.id': 'local-1750480469176', 'changed-partition-count': '1', 'engine-name': 'spark', 'total-position-deletes': '0', 'added-files-size': '49719', 'total-delete-files': '0', 'iceberg-version': 'Apache Iceberg 1.8.1 (commit 9ce0fcf0af7becf25ad9fc996c3bad2afdcfd33d)', 'total-files-size': '49719', 'total-data-files': '1'} 2025-06-21 04:46:00.881000 2689398741546157644 2044767572491260316 overwrite s3://warehouse/nyc/permits/metadata/snap-2689398741546157644-1-01b64de9-8295-4443-af60-b7dcb5ff1908.avro {'engine-version': '3.5.5', 'added-data-files': '1', 'total-equality-deletes': '0', 'app-id': 'local-1750480469176', 'added-records': '482', 'deleted-data-files': '1', 'deleted-records': '1000', 'total-records': '482', 'spark.app.id': 'local-1750480469176', 'removed-files-size': '49719', 'changed-partition-count': '1', 'engine-name': 'spark', 'wap.id': '92d9d9eb680740468822d1ea99068c8a', 'total-position-deletes': '0', 'added-files-size': '26860', 'total-delete-files': '0', 'iceberg-version': 'Apache Iceberg 1.8.1 (commit 9ce0fcf0af7becf25ad9fc996c3bad2afdcfd33d)', 'total-files-size': '26860', 'total-data-files': '1'} In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/2-iceberg-integrated-audits-demo/#integrated-audits-demo","title":"Integrated Audits Demo\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/2-iceberg-integrated-audits-demo/#load-nyc-film-permits-data","title":"Load NYC Film Permits Data\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/2-iceberg-integrated-audits-demo/#generate-an-id-for-an-integrated-audit-session","title":"Generate an ID for an Integrated Audit Session\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/2-iceberg-integrated-audits-demo/#the-setup","title":"The Setup\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/2-iceberg-integrated-audits-demo/#staging-the-changes","title":"Staging The Changes\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/2-iceberg-integrated-audits-demo/#the-audit","title":"The Audit\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/2-iceberg-integrated-audits-demo/#the-publish","title":"The Publish\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/2-iceberg-integrated-audits-demo/#what-happens-when-the-audits-fail","title":"What Happens When The Audits Fail?\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/3-iceberg-table-maintenance-spark-procedures/","title":"Table Maintenance Spark Procedures","text":"<p>Reference: Table Maintenance: The Key To Keeping Your Iceberg Tables Healthy and Performant</p> In\u00a0[1]: Copied! <pre>spark\n</pre> spark <pre>Intitializing Scala interpreter ...</pre> <pre>Spark Web UI available at http://1ac96ad2acf7:4040\nSparkContext available as 'sc' (version = 3.5.5, master = local[*], app id = local-1750487975219)\nSparkSession available as 'spark'\n</pre> Out[1]: <pre>res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@74b3152b\n</pre> In\u00a0[2]: Copied! <pre>spark.sql(\"DROP TABLE IF EXISTS demo.nyc.taxis_sample\")\n</pre> spark.sql(\"DROP TABLE IF EXISTS demo.nyc.taxis_sample\") Out[2]: <pre>res1: org.apache.spark.sql.DataFrame = []\n</pre> In\u00a0[3]: Copied! <pre>spark.sql(\"\"\"\nCREATE TABLE demo.nyc.taxis_sample (\n  `VendorID` BIGINT,\n  `tpep_pickup_datetime` TIMESTAMP,\n  `tpep_dropoff_datetime` TIMESTAMP,\n  `passenger_count` DOUBLE,\n  `trip_distance` DOUBLE,\n  `RatecodeID` DOUBLE,\n  `store_and_fwd_flag` STRING,\n  `PULocationID` BIGINT,\n  `DOLocationID` BIGINT,\n  `payment_type` BIGINT,\n  `fare_amount` DOUBLE,\n  `extra` DOUBLE,\n  `mta_tax` DOUBLE,\n  `tip_amount` DOUBLE,\n  `tolls_amount` DOUBLE,\n  `improvement_surcharge` DOUBLE,\n  `total_amount` DOUBLE,\n  `congestion_surcharge` DOUBLE,\n  `airport_fee` DOUBLE)\nUSING iceberg\nTBLPROPERTIES(\n  'write.target-file-size-bytes'='5242880'\n)\n\"\"\")\n</pre> spark.sql(\"\"\" CREATE TABLE demo.nyc.taxis_sample (   `VendorID` BIGINT,   `tpep_pickup_datetime` TIMESTAMP,   `tpep_dropoff_datetime` TIMESTAMP,   `passenger_count` DOUBLE,   `trip_distance` DOUBLE,   `RatecodeID` DOUBLE,   `store_and_fwd_flag` STRING,   `PULocationID` BIGINT,   `DOLocationID` BIGINT,   `payment_type` BIGINT,   `fare_amount` DOUBLE,   `extra` DOUBLE,   `mta_tax` DOUBLE,   `tip_amount` DOUBLE,   `tolls_amount` DOUBLE,   `improvement_surcharge` DOUBLE,   `total_amount` DOUBLE,   `congestion_surcharge` DOUBLE,   `airport_fee` DOUBLE) USING iceberg TBLPROPERTIES(   'write.target-file-size-bytes'='5242880' ) \"\"\") Out[3]: <pre>res2: org.apache.spark.sql.DataFrame = []\n</pre> In\u00a0[4]: Copied! <pre>val df_202201 = spark.read.parquet(\"/home/iceberg/data/yellow_tripdata_2022-01.parquet\")\nval df_202202 = spark.read.parquet(\"/home/iceberg/data/yellow_tripdata_2022-02.parquet\")\nval df_202203 = spark.read.parquet(\"/home/iceberg/data/yellow_tripdata_2022-03.parquet\")\nval df_q1 = df_202201.union(df_202202).union(df_202203)\ndf_q1.write.insertInto(\"nyc.taxis_sample\")\n</pre> val df_202201 = spark.read.parquet(\"/home/iceberg/data/yellow_tripdata_2022-01.parquet\") val df_202202 = spark.read.parquet(\"/home/iceberg/data/yellow_tripdata_2022-02.parquet\") val df_202203 = spark.read.parquet(\"/home/iceberg/data/yellow_tripdata_2022-03.parquet\") val df_q1 = df_202201.union(df_202202).union(df_202203) df_q1.write.insertInto(\"nyc.taxis_sample\") Out[4]: <pre>df_202201: org.apache.spark.sql.DataFrame = [VendorID: bigint, tpep_pickup_datetime: timestamp_ntz ... 17 more fields]\ndf_202202: org.apache.spark.sql.DataFrame = [VendorID: bigint, tpep_pickup_datetime: timestamp_ntz ... 17 more fields]\ndf_202203: org.apache.spark.sql.DataFrame = [VendorID: bigint, tpep_pickup_datetime: timestamp_ntz ... 17 more fields]\ndf_q1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [VendorID: bigint, tpep_pickup_datetime: timestamp_ntz ... 17 more fields]\n</pre> In\u00a0[5]: Copied! <pre>spark.sql(\"SELECT file_path, file_size_in_bytes FROM nyc.taxis_sample.files\").show(100)\n</pre> spark.sql(\"SELECT file_path, file_size_in_bytes FROM nyc.taxis_sample.files\").show(100) <pre>+--------------------+------------------+\n|           file_path|file_size_in_bytes|\n+--------------------+------------------+\n|s3://warehouse/ny...|           4098378|\n|s3://warehouse/ny...|           3951238|\n|s3://warehouse/ny...|           3990037|\n|s3://warehouse/ny...|           3894699|\n|s3://warehouse/ny...|           3915456|\n|s3://warehouse/ny...|           3895987|\n|s3://warehouse/ny...|           3806277|\n|s3://warehouse/ny...|           3899172|\n|s3://warehouse/ny...|           3822840|\n|s3://warehouse/ny...|           3963021|\n|s3://warehouse/ny...|           1242601|\n|s3://warehouse/ny...|           3887960|\n|s3://warehouse/ny...|           3718812|\n|s3://warehouse/ny...|           3893136|\n|s3://warehouse/ny...|           3705416|\n|s3://warehouse/ny...|           3719417|\n|s3://warehouse/ny...|           3823555|\n|s3://warehouse/ny...|           3711923|\n|s3://warehouse/ny...|           3749498|\n|s3://warehouse/ny...|           3859935|\n|s3://warehouse/ny...|           3743970|\n|s3://warehouse/ny...|           3753909|\n|s3://warehouse/ny...|           3770138|\n|s3://warehouse/ny...|           2129775|\n|s3://warehouse/ny...|           3752993|\n|s3://warehouse/ny...|           3612792|\n|s3://warehouse/ny...|           3834524|\n|s3://warehouse/ny...|           3740475|\n|s3://warehouse/ny...|           3730257|\n|s3://warehouse/ny...|           3730578|\n|s3://warehouse/ny...|           3846061|\n|s3://warehouse/ny...|           3785702|\n|s3://warehouse/ny...|           3735734|\n|s3://warehouse/ny...|           3891194|\n|s3://warehouse/ny...|           3715606|\n|s3://warehouse/ny...|           3744550|\n|s3://warehouse/ny...|           3754543|\n|s3://warehouse/ny...|           3690781|\n|s3://warehouse/ny...|           4814844|\n+--------------------+------------------+\n\n</pre> In\u00a0[6]: Copied! <pre>spark.sql(\"ALTER TABLE nyc.taxis_sample UNSET TBLPROPERTIES ('write.target-file-size-bytes')\")\n</pre> spark.sql(\"ALTER TABLE nyc.taxis_sample UNSET TBLPROPERTIES ('write.target-file-size-bytes')\") Out[6]: <pre>res5: org.apache.spark.sql.DataFrame = []\n</pre> In\u00a0[7]: Copied! <pre>spark.sql(\"CALL demo.system.rewrite_data_files(table =&gt; 'nyc.taxis_sample', options =&gt; map('target-file-size-bytes','52428800'))\").show()\n</pre> spark.sql(\"CALL demo.system.rewrite_data_files(table =&gt; 'nyc.taxis_sample', options =&gt; map('target-file-size-bytes','52428800'))\").show() <pre>+--------------------------+----------------------+---------------------+-----------------------+\n|rewritten_data_files_count|added_data_files_count|rewritten_bytes_count|failed_data_files_count|\n+--------------------------+----------------------+---------------------+-----------------------+\n|                        39|                     3|            145327784|                      0|\n+--------------------------+----------------------+---------------------+-----------------------+\n\n</pre> In\u00a0[8]: Copied! <pre>spark.sql(\"SELECT file_path, file_size_in_bytes FROM nyc.taxis_sample.files\").show(100)\n</pre> spark.sql(\"SELECT file_path, file_size_in_bytes FROM nyc.taxis_sample.files\").show(100) <pre>+--------------------+------------------+\n|           file_path|file_size_in_bytes|\n+--------------------+------------------+\n|s3://warehouse/ny...|          49243858|\n|s3://warehouse/ny...|          48534830|\n|s3://warehouse/ny...|          40600811|\n+--------------------+------------------+\n\n</pre> In\u00a0[9]: Copied! <pre>spark.sql(\"SELECT committed_at, snapshot_id, operation FROM nyc.taxis_sample.snapshots\").show(truncate=false)\n</pre> spark.sql(\"SELECT committed_at, snapshot_id, operation FROM nyc.taxis_sample.snapshots\").show(truncate=false) <pre>+-----------------------+-------------------+---------+\n|committed_at           |snapshot_id        |operation|\n+-----------------------+-------------------+---------+\n|2025-06-21 06:49:14.548|616558229842152742 |append   |\n|2025-06-21 06:49:40.198|1836423127434635778|replace  |\n+-----------------------+-------------------+---------+\n\n</pre> In\u00a0[10]: Copied! <pre>val now = java.util.Calendar.getInstance().getTime()\nval format = new java.text.SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS\")\nval now_str = format.format(now)\n\nspark.sql(s\"CALL demo.system.expire_snapshots(table =&gt; 'nyc.taxis_sample', older_than =&gt; TIMESTAMP '$now_str', retain_last =&gt; 1)\").show()\n</pre> val now = java.util.Calendar.getInstance().getTime() val format = new java.text.SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS\") val now_str = format.format(now)  spark.sql(s\"CALL demo.system.expire_snapshots(table =&gt; 'nyc.taxis_sample', older_than =&gt; TIMESTAMP '$now_str', retain_last =&gt; 1)\").show() <pre>+------------------------+-----------------------------------+-----------------------------------+----------------------------+----------------------------+------------------------------+\n|deleted_data_files_count|deleted_position_delete_files_count|deleted_equality_delete_files_count|deleted_manifest_files_count|deleted_manifest_lists_count|deleted_statistics_files_count|\n+------------------------+-----------------------------------+-----------------------------------+----------------------------+----------------------------+------------------------------+\n|                      39|                                  0|                                  0|                           1|                           1|                             0|\n+------------------------+-----------------------------------+-----------------------------------+----------------------------+----------------------------+------------------------------+\n\n</pre> Out[10]: <pre>now: java.util.Date = Sat Jun 21 06:49:57 UTC 2025\nformat: java.text.SimpleDateFormat = java.text.SimpleDateFormat@f17b4ca5\nnow_str: String = 2025-06-21 06:49:57.220\n</pre> In\u00a0[11]: Copied! <pre>spark.sql(\"SELECT committed_at, snapshot_id, operation FROM nyc.taxis_sample.snapshots\").show(truncate=false)\n</pre> spark.sql(\"SELECT committed_at, snapshot_id, operation FROM nyc.taxis_sample.snapshots\").show(truncate=false) <pre>+-----------------------+-------------------+---------+\n|committed_at           |snapshot_id        |operation|\n+-----------------------+-------------------+---------+\n|2025-06-21 06:49:40.198|1836423127434635778|replace  |\n+-----------------------+-------------------+---------+\n\n</pre> In\u00a0[12]: Copied! <pre>spark.sql(\"CALL demo.system.rewrite_manifests('nyc.taxis_sample')\").show()\n</pre> spark.sql(\"CALL demo.system.rewrite_manifests('nyc.taxis_sample')\").show() <pre>+-------------------------+---------------------+\n|rewritten_manifests_count|added_manifests_count|\n+-------------------------+---------------------+\n|                        2|                    1|\n+-------------------------+---------------------+\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/3-iceberg-table-maintenance-spark-procedures/#table-maintenance-spark-procedures","title":"Table Maintenance Spark Procedures\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/3-iceberg-table-maintenance-spark-procedures/#rewriting-data-files","title":"Rewriting Data Files\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/3-iceberg-table-maintenance-spark-procedures/#expiring-snapshots","title":"Expiring Snapshots\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/3-iceberg-table-maintenance-spark-procedures/#rewriting-manifest-files","title":"Rewriting Manifest Files\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/4-iceberg-wap-with-branches/","title":"Write-Audit-Publish with Branches in Apache Iceberg","text":"<p>This notebook runs using the Docker Compose at https://github.com/tabular-io/docker-spark-iceberg. It's based on the Iceberg - Integrated Audits Demo.ipynb notebook.</p> In\u00a0[1]: Copied! <pre>from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"Jupyter\").getOrCreate()\n\nspark\n</pre> from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"Jupyter\").getOrCreate()  spark <pre>25/06/21 06:54:30 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n</pre> Out[1]: <p>SparkSession - in-memory</p> <p>SparkContext</p> <p>Spark UI</p> Version <code>v3.5.5</code> Master <code>local[*]</code> AppName <code>PySparkShell</code> <p>To be able to rerun the notebook several times, let's drop the <code>permits</code> table if it exists to start fresh.</p> In\u00a0[2]: Copied! <pre>%%sql\n\nCREATE DATABASE IF NOT EXISTS nyc\n</pre> %%sql  CREATE DATABASE IF NOT EXISTS nyc Out[2]: In\u00a0[3]: Copied! <pre>%%sql\n\nDROP TABLE IF EXISTS nyc.permits\n</pre> %%sql  DROP TABLE IF EXISTS nyc.permits Out[3]: <p>For this demo, we will use the New York City Film Permits dataset available as part of the NYC Open Data initiative. We're using a locally saved copy of a 1000 record sample, but feel free to download the entire dataset to use in this notebook!</p> <p>We'll save the sample dataset into an iceberg table called <code>permits</code>.</p> In\u00a0[4]: Copied! <pre>df = spark.read.option(\"inferSchema\",\"true\").option(\"multiline\",\"true\").json(\"/home/iceberg/data/nyc_film_permits.json\")\ndf.write.saveAsTable(\"nyc.permits\")\n</pre> df = spark.read.option(\"inferSchema\",\"true\").option(\"multiline\",\"true\").json(\"/home/iceberg/data/nyc_film_permits.json\") df.write.saveAsTable(\"nyc.permits\") <pre>                                                                                </pre> <p>Taking a quick peek at the data, you can see that there are a number of permits for different boroughs in New York.</p> In\u00a0[5]: Copied! <pre>%%sql\n\nSELECT borough, count(*) permit_cnt\nFROM nyc.permits\nGROUP BY borough\n</pre> %%sql  SELECT borough, count(*) permit_cnt FROM nyc.permits GROUP BY borough Out[5]: borough permit_cnt Queens 96 Brooklyn 378 Staten Island 1 Manhattan 518 Bronx 7 <p>Tables by default are not configured to allow integrated audits, therefore the first step is enabling this by setting the <code>write.wap.enabled</code> table metadata property to <code>true</code></p> In\u00a0[6]: Copied! <pre>%%sql\n\nALTER TABLE nyc.permits\nSET TBLPROPERTIES (\n    'write.wap.enabled'='true'\n)\n</pre> %%sql  ALTER TABLE nyc.permits SET TBLPROPERTIES (     'write.wap.enabled'='true' ) Out[6]: <p>We create a branch for the work we want to do. This is a copy-on-write branch, so \"free\" until we start making changes (and \"cheap\" thereafter) since only data that's changed needs to be written.</p> In\u00a0[7]: Copied! <pre>%%sql\n\nALTER TABLE nyc.permits\nCREATE BRANCH etl_job_42\n</pre> %%sql  ALTER TABLE nyc.permits CREATE BRANCH etl_job_42 Out[7]: <p>Before writing to the table we set <code>spark.wap.branch</code> so that writes (and reads) are against the specified branch of the table.</p> In\u00a0[8]: Copied! <pre>spark.conf.set('spark.wap.branch', 'etl_job_42')\n</pre> spark.conf.set('spark.wap.branch', 'etl_job_42') <p>Now make the change to the table</p> In\u00a0[9]: Copied! <pre>%%sql\n\nDELETE FROM nyc.permits\nWHERE borough='Manhattan'\n</pre> %%sql  DELETE FROM nyc.permits WHERE borough='Manhattan' Out[9]: <p>The changes are reflected in the table:</p> In\u00a0[10]: Copied! <pre>%%sql\n\nSELECT borough, count(*) permit_cnt\nFROM nyc.permits\nGROUP BY borough\n</pre> %%sql  SELECT borough, count(*) permit_cnt FROM nyc.permits GROUP BY borough Out[10]: borough permit_cnt Queens 96 Brooklyn 378 Staten Island 1 Bronx 7 <p>Note that because <code>spark.wap.branch</code> is set the above query is effectively the same as this one with <code>VERSION AS OF</code> for the branch</p> In\u00a0[11]: Copied! <pre>%%sql\n\nSELECT borough, count(*) permit_cnt\nFROM nyc.permits VERSION AS OF 'etl_job_42'\nGROUP BY borough\n</pre> %%sql  SELECT borough, count(*) permit_cnt FROM nyc.permits VERSION AS OF 'etl_job_42' GROUP BY borough Out[11]: borough permit_cnt Queens 96 Brooklyn 378 Staten Island 1 Bronx 7 <p>Another syntax (albiet less clear IMHO) for <code>VERSION AS OF</code> is a <code>branch_&lt;branch_name&gt;</code> suffix to the table:</p> In\u00a0[12]: Copied! <pre>%%sql\n\nSELECT borough, count(*) permit_cnt\nFROM nyc.permits.branch_etl_job_42\nGROUP BY borough\n</pre> %%sql  SELECT borough, count(*) permit_cnt FROM nyc.permits.branch_etl_job_42 GROUP BY borough Out[12]: borough permit_cnt Queens 96 Brooklyn 378 Staten Island 1 Bronx 7 <p>We can also inspect the unmodified <code>main</code> version of the table with <code>VERSION AS OF</code>:</p> In\u00a0[13]: Copied! <pre>%%sql\n\nSELECT borough, count(*) permit_cnt\nFROM nyc.permits VERSION AS OF 'main'\nGROUP BY borough\n</pre> %%sql  SELECT borough, count(*) permit_cnt FROM nyc.permits VERSION AS OF 'main' GROUP BY borough Out[13]: borough permit_cnt Queens 96 Brooklyn 378 Staten Island 1 Manhattan 518 Bronx 7 <p>The same <code>branch_</code> suffix words here too:</p> In\u00a0[14]: Copied! <pre>%%sql\n\nSELECT borough, count(*) permit_cnt\nFROM nyc.permits.branch_main\nGROUP BY borough\n</pre> %%sql  SELECT borough, count(*) permit_cnt FROM nyc.permits.branch_main GROUP BY borough Out[14]: borough permit_cnt Queens 96 Brooklyn 378 Staten Island 1 Manhattan 518 Bronx 7 <p>Any other user of the table will see the full set of data. We can reassure ourselves of this by unsetting <code>spark.wap.branch</code> for the session and querying the table without any <code>VERSION AS OF</code> modifier</p> In\u00a0[15]: Copied! <pre>spark.conf.unset('spark.wap.branch')\n</pre> spark.conf.unset('spark.wap.branch') In\u00a0[16]: Copied! <pre>%%sql\n\nSELECT borough, count(*) permit_cnt\nFROM nyc.permits\nGROUP BY borough\n</pre> %%sql  SELECT borough, count(*) permit_cnt FROM nyc.permits GROUP BY borough Out[16]: borough permit_cnt Queens 96 Brooklyn 378 Staten Island 1 Manhattan 518 Bronx 7 <p>How you audit the data is up to you. The nice thing about the data being staged is that you can do it within the same ETL job, or have another tool do it.</p> <p>Here's a very simple example of doing in Python. We're going to programatically check that only the four expected boroughs remain in the data.</p> <p>First, we define those that are expected:</p> In\u00a0[17]: Copied! <pre>expected_boroughs = {\"Queens\", \"Brooklyn\", \"Bronx\", \"Staten Island\"}\n</pre> expected_boroughs = {\"Queens\", \"Brooklyn\", \"Bronx\", \"Staten Island\"} <p>Then we get a set of the actual boroughs in the staged data</p> In\u00a0[18]: Copied! <pre>distinct_boroughs = spark.read \\\n    .option(\"branch\", \"etl_job_42\") \\\n    .format(\"iceberg\") \\\n    .load(\"nyc.permits\") \\\n    .select(\"borough\") \\\n    .distinct() \\\n    .toLocalIterator()\nboroughs = {row[0] for row in distinct_boroughs}\n</pre> distinct_boroughs = spark.read \\     .option(\"branch\", \"etl_job_42\") \\     .format(\"iceberg\") \\     .load(\"nyc.permits\") \\     .select(\"borough\") \\     .distinct() \\     .toLocalIterator() boroughs = {row[0] for row in distinct_boroughs} <p>Now we do two checks:</p> <ol> <li>Compare the length of the expected vs actual set</li> <li>Check that the two sets when unioned are still the same length. This is necessary, since the first test isn't sufficient alone</li> </ol> In\u00a0[19]: Copied! <pre>if (   (len(boroughs)          != len(expected_boroughs)) \\\n      or (len(boroughs)          != len(set.union(boroughs, expected_boroughs))) \\\n      or (len(expected_boroughs) != len(set.union(boroughs, expected_boroughs)))):\n    raise ValueError(f\"Audit failed, borough set does not match expected boroughs: {boroughs} != {expected_boroughs}\")\nelse:\n    print(f\"Audit has passed \ud83d\ude4c\ud83c\udffb\")\n</pre> if (   (len(boroughs)          != len(expected_boroughs)) \\       or (len(boroughs)          != len(set.union(boroughs, expected_boroughs))) \\       or (len(expected_boroughs) != len(set.union(boroughs, expected_boroughs)))):     raise ValueError(f\"Audit failed, borough set does not match expected boroughs: {boroughs} != {expected_boroughs}\") else:     print(f\"Audit has passed \ud83d\ude4c\ud83c\udffb\") <pre>Audit has passed \ud83d\ude4c\ud83c\udffb\n</pre> <p>Iceberg supports fast-forward merging of branches back to <code>main</code>, using the <code>manageSnapshots().fastForwardBranch</code> API.</p> <p>This isn't yet exposed in Spark, so the existing <code>cherrypick</code> can be used as a slightly less elegant option.</p> <p>\u2139\ufe0f Note that <code>cherrypick</code> only works for one commit.</p> <p>First, we need the snapshot ID of our branch, which we can get from the <code>.refs</code> table:</p> In\u00a0[20]: Copied! <pre>%%sql\n\nSELECT * FROM nyc.permits.refs \n</pre> %%sql  SELECT * FROM nyc.permits.refs  Out[20]: name type snapshot_id max_reference_age_in_ms min_snapshots_to_keep max_snapshot_age_in_ms etl_job_42 BRANCH 8720005413734770031 None None None main BRANCH 8397442454150540251 None None None In\u00a0[21]: Copied! <pre>query = f\"\"\"\nSELECT snapshot_id\nFROM nyc.permits.refs\nWHERE name = 'etl_job_42'\n\"\"\"\n\nwap_snapshot_id = spark.sql(query).head().snapshot_id\n</pre> query = f\"\"\" SELECT snapshot_id FROM nyc.permits.refs WHERE name = 'etl_job_42' \"\"\"  wap_snapshot_id = spark.sql(query).head().snapshot_id <p>Now we do the publish, using <code>cherrypick_snapshot</code> and the snapshot id:</p> In\u00a0[22]: Copied! <pre>publish_query = f\"CALL system.cherrypick_snapshot('nyc.permits', {wap_snapshot_id})\"\n\n%sql $publish_query\n</pre> publish_query = f\"CALL system.cherrypick_snapshot('nyc.permits', {wap_snapshot_id})\"  %sql $publish_query Out[22]: source_snapshot_id current_snapshot_id 8720005413734770031 8720005413734770031 <p>Finally, we look at the table and revel in the glory that is our published changes \ud83c\udf89</p> In\u00a0[23]: Copied! <pre>%%sql\n\nSELECT borough, count(*) permit_cnt\nFROM nyc.permits.branch_etl_job_42\nGROUP BY borough\n</pre> %%sql  SELECT borough, count(*) permit_cnt FROM nyc.permits.branch_etl_job_42 GROUP BY borough Out[23]: borough permit_cnt Queens 96 Brooklyn 378 Staten Island 1 Bronx 7 <p>We can also inspect the unmodified <code>main</code> version of the table with <code>VERSION AS OF</code>:</p> In\u00a0[24]: Copied! <pre>%%sql\n\nSELECT borough, count(*) permit_cnt\nFROM nyc.permits VERSION AS OF 'main'\nGROUP BY borough\n</pre> %%sql  SELECT borough, count(*) permit_cnt FROM nyc.permits VERSION AS OF 'main' GROUP BY borough Out[24]: borough permit_cnt Queens 96 Brooklyn 378 Staten Island 1 Bronx 7 <p>If you don't want to merge the branch you can simply <code>DROP</code> it.</p> In\u00a0[25]: Copied! <pre>%%sql\n\nALTER TABLE nyc.permits\nCREATE BRANCH new_etl_job\n</pre> %%sql  ALTER TABLE nyc.permits CREATE BRANCH new_etl_job Out[25]: In\u00a0[26]: Copied! <pre>spark.conf.set('spark.wap.branch', 'new_etl_job')\n</pre> spark.conf.set('spark.wap.branch', 'new_etl_job') In\u00a0[27]: Copied! <pre>%%sql\n\nDELETE FROM nyc.permits WHERE borough LIKE '%'\n</pre> %%sql  DELETE FROM nyc.permits WHERE borough LIKE '%' Out[27]: In\u00a0[28]: Copied! <pre>%%sql\n\nSELECT borough, count(*) permit_cnt\nFROM nyc.permits \nGROUP BY borough\n</pre> %%sql  SELECT borough, count(*) permit_cnt FROM nyc.permits  GROUP BY borough Out[28]: borough permit_cnt In\u00a0[29]: Copied! <pre>%%sql\n\nSELECT borough, count(*) permit_cnt\nFROM nyc.permits VERSION AS OF 'main'\nGROUP BY borough\n</pre> %%sql  SELECT borough, count(*) permit_cnt FROM nyc.permits VERSION AS OF 'main' GROUP BY borough Out[29]: borough permit_cnt Queens 96 Brooklyn 378 Staten Island 1 Bronx 7 In\u00a0[30]: Copied! <pre>%%sql\n\nALTER TABLE nyc.permits\nDROP BRANCH new_etl_job\n</pre> %%sql  ALTER TABLE nyc.permits DROP BRANCH new_etl_job Out[30]: <pre>25/06/21 07:09:11 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-7969d33e-6c09-4684-9f57-dbbfd3932826. Falling back to Java IO way\njava.io.IOException: Failed to delete: /tmp/blockmgr-7969d33e-6c09-4684-9f57-dbbfd3932826\n\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n\tat org.apache.spark.storage.DiskBlockManager.$anonfun$addShutdownHook$2(DiskBlockManager.scala:346)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n25/06/21 07:09:11 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-7969d33e-6c09-4684-9f57-dbbfd3932826/09. Falling back to Java IO way\njava.io.IOException: Failed to delete: /tmp/blockmgr-7969d33e-6c09-4684-9f57-dbbfd3932826/09\n\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:130)\n\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:117)\n\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n\tat org.apache.spark.storage.DiskBlockManager.$anonfun$addShutdownHook$2(DiskBlockManager.scala:346)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n25/06/21 07:09:11 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-7969d33e-6c09-4684-9f57-dbbfd3932826. Falling back to Java IO way\njava.io.IOException: Failed to delete: /tmp/blockmgr-7969d33e-6c09-4684-9f57-dbbfd3932826\n\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2122)\n\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2305)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2305)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n\tat org.apache.spark.api.java.JavaSparkContext.stop(JavaSparkContext.scala:550)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n25/06/21 07:09:11 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-7969d33e-6c09-4684-9f57-dbbfd3932826/3f. Falling back to Java IO way\njava.io.IOException: Failed to delete: /tmp/blockmgr-7969d33e-6c09-4684-9f57-dbbfd3932826/3f\n\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:130)\n\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:117)\n\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2122)\n\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2305)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2305)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n\tat org.apache.spark.api.java.JavaSparkContext.stop(JavaSparkContext.scala:550)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n25/06/21 07:09:11 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-7969d33e-6c09-4684-9f57-dbbfd3932826/1a. Falling back to Java IO way\njava.io.IOException: Failed to delete: /tmp/blockmgr-7969d33e-6c09-4684-9f57-dbbfd3932826/1a\n\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:130)\n\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:117)\n\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n\tat org.apache.spark.storage.DiskBlockManager.$anonfun$addShutdownHook$2(DiskBlockManager.scala:346)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n</pre> <p>For more information about write-audit-publish see this talk from Michelle Winters and this talk from Sam Redai.</p>"},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/4-iceberg-wap-with-branches/#write-audit-publish-with-branches-in-apache-iceberg","title":"Write-Audit-Publish with Branches in Apache Iceberg\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/4-iceberg-wap-with-branches/#load-nyc-film-permits-data","title":"Load NYC Film Permits Data\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/4-iceberg-wap-with-branches/#the-setup","title":"The Setup\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/4-iceberg-wap-with-branches/#write","title":"Write\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/4-iceberg-wap-with-branches/#inspecting-the-stagedunpublished-data","title":"Inspecting the staged/unpublished data\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/4-iceberg-wap-with-branches/#stagedunpublished-data","title":"Staged/unpublished data\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/4-iceberg-wap-with-branches/#published-data","title":"Published data\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/4-iceberg-wap-with-branches/#audit","title":"Audit\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/4-iceberg-wap-with-branches/#publish","title":"Publish\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/4-iceberg-wap-with-branches/#what-if-you-dont-want-to-publish-changes","title":"What if You Don't Want to Publish Changes?\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/4-iceberg-wap-with-branches/#create-a-new-branch","title":"Create a new branch\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/4-iceberg-wap-with-branches/#set-sparkwapbranch","title":"Set <code>spark.wap.branch</code>\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/4-iceberg-wap-with-branches/#write","title":"Write\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/4-iceberg-wap-with-branches/#audit","title":"Audit\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/4-iceberg-wap-with-branches/#whoops","title":"Whoops \ud83e\udd2d\u00b6","text":"<p>We deleted all the data</p>"},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/4-iceberg-wap-with-branches/#reassure-ourselves-that-the-data-is-still-there-in-main","title":"Reassure ourselves that the data is still there in <code>main</code>\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/4-iceberg-wap-with-branches/#abandon-changes","title":"Abandon changes\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/4-iceberg-wap-with-branches/#where-next","title":"Where Next?\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/5-iceberg-view-support/","title":"View Support","text":"In\u00a0[\u00a0]: Copied! <pre>from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"Jupyter\").getOrCreate()\n\nspark\n</pre> from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"Jupyter\").getOrCreate()  spark <pre>25/06/22 04:47:52 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n</pre> Out[\u00a0]: <p>SparkSession - in-memory</p> <p>SparkContext</p> <p>Spark UI</p> Version <code>v3.5.5</code> Master <code>local[*]</code> AppName <code>PySparkShell</code> <p>To be able to rerun the notebook several times, let's drop the table and the views if they exist to start fresh.</p> In\u00a0[2]: Copied! <pre>%%sql\n\nCREATE DATABASE IF NOT EXISTS nyc.taxis;\n</pre> %%sql  CREATE DATABASE IF NOT EXISTS nyc.taxis; Out[2]: In\u00a0[3]: Copied! <pre>%%sql\n\nDROP TABLE IF EXISTS nyc.taxis\n</pre> %%sql  DROP TABLE IF EXISTS nyc.taxis  Out[3]: In\u00a0[4]: Copied! <pre>%%sql\n\nDROP VIEW IF EXISTS nyc.long_distances\n</pre> %%sql  DROP VIEW IF EXISTS nyc.long_distances Out[4]: In\u00a0[5]: Copied! <pre>%%sql\n\nDROP VIEW IF EXISTS nyc.negative_amounts\n</pre> %%sql  DROP VIEW IF EXISTS nyc.negative_amounts Out[5]: In\u00a0[7]: Copied! <pre>%%sql\n\nCREATE TABLE nyc.taxis (\n    VendorID              bigint,\n    tpep_pickup_datetime  timestamp,\n    tpep_dropoff_datetime timestamp,\n    passenger_count       double,\n    trip_distance         double,\n    RatecodeID            double,\n    store_and_fwd_flag    string,\n    PULocationID          bigint,\n    DOLocationID          bigint,\n    payment_type          bigint,\n    fare_amount           double,\n    extra                 double,\n    mta_tax               double,\n    tip_amount            double,\n    tolls_amount          double,\n    improvement_surcharge double,\n    total_amount          double,\n    congestion_surcharge  double,\n    airport_fee           double\n)\nUSING iceberg\nPARTITIONED BY (days(tpep_pickup_datetime))\n</pre> %%sql  CREATE TABLE nyc.taxis (     VendorID              bigint,     tpep_pickup_datetime  timestamp,     tpep_dropoff_datetime timestamp,     passenger_count       double,     trip_distance         double,     RatecodeID            double,     store_and_fwd_flag    string,     PULocationID          bigint,     DOLocationID          bigint,     payment_type          bigint,     fare_amount           double,     extra                 double,     mta_tax               double,     tip_amount            double,     tolls_amount          double,     improvement_surcharge double,     total_amount          double,     congestion_surcharge  double,     airport_fee           double ) USING iceberg PARTITIONED BY (days(tpep_pickup_datetime)) Out[7]: In\u00a0[8]: Copied! <pre>df = spark.read.parquet(\"/home/iceberg/data/yellow_tripdata_2022-01.parquet\")\ndf.writeTo(\"nyc.taxis\").append()\n</pre> df = spark.read.parquet(\"/home/iceberg/data/yellow_tripdata_2022-01.parquet\") df.writeTo(\"nyc.taxis\").append() <pre>                                                                                </pre> In\u00a0[9]: Copied! <pre>%%sql\n\nSELECT * FROM nyc.taxis\n</pre> %%sql  SELECT * FROM nyc.taxis <pre>                                                                                </pre> Out[9]: VendorID tpep_pickup_datetime tpep_dropoff_datetime passenger_count trip_distance RatecodeID store_and_fwd_flag PULocationID DOLocationID payment_type fare_amount extra mta_tax tip_amount tolls_amount improvement_surcharge total_amount congestion_surcharge airport_fee 2 2022-01-09 09:24:06 2022-01-09 09:45:35 1.0 9.81 1.0 N 138 244 2 29.0 0.0 0.5 0.0 6.55 0.3 37.6 0.0 1.25 2 2022-01-09 11:27:14 2022-01-09 11:47:43 1.0 9.01 1.0 N 138 230 2 27.5 0.0 0.5 0.0 6.55 0.3 38.6 2.5 1.25 2 2022-01-09 13:14:53 2022-01-09 13:23:52 1.0 3.52 1.0 N 138 7 1 12.0 0.0 0.5 2.81 0.0 0.3 16.86 0.0 1.25 2 2022-01-09 13:51:42 2022-01-09 14:16:59 1.0 10.1 1.0 N 138 162 2 29.0 0.5 0.5 0.0 6.55 0.3 40.6 2.5 1.25 2 2022-01-09 15:09:41 2022-01-09 15:38:33 1.0 11.76 1.0 N 138 181 2 35.5 0.5 0.5 0.0 0.0 0.3 38.05 0.0 1.25 2 2022-01-09 17:05:34 2022-01-09 17:36:28 1.0 12.86 1.0 N 138 37 2 36.0 0.5 0.5 0.0 0.0 0.3 38.55 0.0 1.25 2 2022-01-09 00:02:12 2022-01-09 00:11:44 1.0 2.43 1.0 N 48 90 2 9.5 0.5 0.5 0.0 0.0 0.3 13.3 2.5 0.0 2 2022-01-09 00:00:18 2022-01-09 00:12:53 1.0 4.92 1.0 N 114 75 1 15.0 0.5 0.5 3.76 0.0 0.3 22.56 2.5 0.0 2 2022-01-09 00:00:46 2022-01-09 00:13:20 1.0 3.04 1.0 N 140 246 1 11.5 0.5 0.5 2.5 0.0 0.3 17.8 2.5 0.0 2 2022-01-09 00:00:44 2022-01-09 00:05:42 1.0 1.37 1.0 N 141 162 1 6.0 0.5 0.5 1.2 0.0 0.3 11.0 2.5 0.0 2 2022-01-09 00:03:08 2022-01-09 00:15:43 1.0 4.99 1.0 N 246 41 1 15.5 0.5 0.5 3.86 0.0 0.3 23.16 2.5 0.0 2 2022-01-09 00:00:50 2022-01-09 00:15:38 2.0 7.17 1.0 N 230 244 1 21.5 0.5 0.5 3.0 0.0 0.3 28.3 2.5 0.0 2 2022-01-09 00:01:07 2022-01-09 00:05:53 1.0 0.33 1.0 N 249 158 1 4.0 0.5 0.5 2.34 0.0 0.3 10.14 2.5 0.0 2 2022-01-09 00:00:08 2022-01-09 00:10:08 1.0 2.28 1.0 N 237 48 1 9.5 0.5 0.5 1.33 0.0 0.3 14.63 2.5 0.0 2 2022-01-09 00:00:20 2022-01-09 00:04:28 2.0 0.63 1.0 N 164 164 2 4.5 0.5 0.5 0.0 0.0 0.3 8.3 2.5 0.0 2 2022-01-09 00:02:57 2022-01-09 00:06:59 5.0 0.85 1.0 N 234 90 1 5.0 0.5 0.5 1.76 0.0 0.3 10.56 2.5 0.0 2 2022-01-09 00:00:04 2022-01-09 00:03:20 3.0 0.65 1.0 N 90 249 1 4.5 0.5 0.5 5.0 0.0 0.3 13.3 2.5 0.0 2 2022-01-09 00:00:06 2022-01-09 00:12:41 1.0 4.77 1.0 N 42 136 2 16.0 0.5 0.5 0.0 0.0 0.3 17.3 0.0 0.0 2 2022-01-09 00:01:03 2022-01-09 00:07:43 1.0 1.08 1.0 N 114 249 1 6.5 0.5 0.5 2.06 0.0 0.3 12.36 2.5 0.0 1 2022-01-09 00:03:30 2022-01-09 00:12:39 2.0 1.5 1.0 N 140 229 1 8.0 3.0 0.5 2.35 0.0 0.3 14.15 2.5 0.0 1 2022-01-09 00:52:48 2022-01-09 01:19:35 1.0 4.9 1.0 N 239 79 1 18.5 3.0 0.5 4.46 0.0 0.3 26.76 2.5 0.0 2 2022-01-09 00:15:47 2022-01-09 00:28:35 1.0 2.57 1.0 N 238 262 1 11.5 0.5 0.5 4.59 0.0 0.3 19.89 2.5 0.0 1 2022-01-09 00:02:18 2022-01-09 00:10:54 3.0 2.5 1.0 N 141 137 1 9.0 3.0 0.5 2.55 0.0 0.3 15.35 2.5 0.0 1 2022-01-09 00:21:20 2022-01-09 00:25:34 4.0 0.5 1.0 N 237 140 1 4.5 3.0 0.5 1.25 0.0 0.3 9.55 2.5 0.0 1 2022-01-09 00:52:49 2022-01-09 01:14:52 1.0 2.7 1.0 N 151 244 2 16.0 0.5 0.5 0.0 0.0 0.3 17.3 0.0 0.0 2 2022-01-09 00:00:04 2022-01-09 00:02:51 3.0 0.72 1.0 N 151 238 1 4.5 0.5 0.5 1.0 0.0 0.3 9.3 2.5 0.0 2 2022-01-09 00:18:32 2022-01-09 00:36:48 1.0 10.54 1.0 N 70 244 1 29.5 0.5 0.5 7.65 6.55 0.3 45.0 0.0 0.0 2 2022-01-09 00:19:38 2022-01-09 00:29:40 3.0 2.08 1.0 N 249 137 1 9.0 0.5 0.5 2.56 0.0 0.3 15.36 2.5 0.0 2 2022-01-09 00:31:39 2022-01-09 00:40:45 2.0 2.94 1.0 N 137 262 2 10.5 0.5 0.5 0.0 0.0 0.3 14.3 2.5 0.0 2 2022-01-09 00:49:58 2022-01-09 01:00:11 2.0 2.83 1.0 N 237 234 1 10.5 0.5 0.5 3.58 0.0 0.3 17.88 2.5 0.0 2 2022-01-09 00:04:54 2022-01-09 00:22:53 1.0 6.64 1.0 N 209 262 1 22.0 0.5 0.5 4.64 0.0 0.3 30.44 2.5 0.0 2 2022-01-09 00:56:55 2022-01-09 01:17:04 2.0 5.39 1.0 N 263 129 2 18.0 0.5 0.5 0.0 0.0 0.3 21.8 2.5 0.0 2 2022-01-09 00:07:25 2022-01-09 00:15:12 1.0 1.52 1.0 N 249 231 1 7.5 0.5 0.5 2.26 0.0 0.3 13.56 2.5 0.0 2 2022-01-09 00:30:15 2022-01-09 00:39:04 2.0 1.92 1.0 N 114 87 1 8.0 0.5 0.5 2.36 0.0 0.3 14.16 2.5 0.0 2 2022-01-09 00:55:45 2022-01-09 01:05:34 3.0 2.67 1.0 N 148 162 1 10.0 0.5 0.5 1.0 0.0 0.3 14.8 2.5 0.0 1 2022-01-09 00:11:53 2022-01-09 00:18:44 2.0 1.5 1.0 N 68 186 1 7.0 3.0 0.5 2.0 0.0 0.3 12.8 2.5 0.0 1 2022-01-09 00:09:04 2022-01-09 00:24:08 1.0 2.4 1.0 N 148 158 1 12.0 2.5 0.5 2.0 0.0 0.3 17.3 2.5 0.0 1 2022-01-09 00:28:16 2022-01-09 00:37:34 2.0 1.2 1.0 N 158 114 1 7.5 2.5 0.5 2.15 0.0 0.3 12.95 2.5 0.0 1 2022-01-09 00:39:36 2022-01-09 00:49:36 1.0 2.6 1.0 N 114 170 2 10.0 2.5 0.5 0.0 0.0 0.3 13.3 2.5 0.0 1 2022-01-09 00:53:28 2022-01-09 00:59:43 2.0 1.2 1.0 N 137 161 1 6.5 2.5 0.5 3.0 0.0 0.3 12.8 2.5 0.0 2 2022-01-09 00:19:35 2022-01-09 00:26:20 1.0 0.98 1.0 N 231 87 2 6.0 0.5 0.5 0.0 0.0 0.3 9.8 2.5 0.0 2 2022-01-09 00:50:03 2022-01-09 00:55:59 1.0 0.97 1.0 N 249 211 1 6.0 0.5 0.5 1.96 0.0 0.3 11.76 2.5 0.0 2 2022-01-09 00:18:57 2022-01-09 00:24:35 1.0 1.49 1.0 N 143 50 1 6.5 0.5 0.5 2.06 0.0 0.3 12.36 2.5 0.0 2 2022-01-09 00:29:13 2022-01-09 00:36:00 1.0 1.92 1.0 N 142 151 1 8.0 0.5 0.5 2.95 0.0 0.3 14.75 2.5 0.0 2 2022-01-09 00:05:47 2022-01-09 00:23:31 1.0 7.8 1.0 N 232 151 1 24.0 0.5 0.5 5.56 0.0 0.3 33.36 2.5 0.0 2 2022-01-09 00:39:21 2022-01-09 00:40:45 1.0 0.1 1.0 N 50 50 3 -3.0 -0.5 -0.5 0.0 0.0 -0.3 -6.8 -2.5 0.0 2 2022-01-09 00:39:21 2022-01-09 00:40:45 1.0 0.1 1.0 N 50 50 2 3.0 0.5 0.5 0.0 0.0 0.3 6.8 2.5 0.0 2 2022-01-09 00:50:55 2022-01-09 00:53:15 1.0 0.69 1.0 N 48 143 2 4.5 0.5 0.5 0.0 0.0 0.3 8.3 2.5 0.0 2 2022-01-09 00:24:45 2022-01-09 00:28:48 1.0 0.88 1.0 N 166 151 1 5.5 0.5 0.5 1.36 0.0 0.3 8.16 0.0 0.0 1 2022-01-09 00:50:30 2022-01-09 01:00:04 1.0 1.7 1.0 N 233 113 1 9.0 3.0 0.5 3.2 0.0 0.3 16.0 2.5 0.0 2 2022-01-09 00:04:37 2022-01-09 00:08:37 1.0 0.49 1.0 N 114 113 1 4.5 0.5 0.5 1.66 0.0 0.3 9.96 2.5 0.0 2 2022-01-09 00:17:15 2022-01-09 00:26:27 1.0 2.06 1.0 N 107 48 1 9.0 0.5 0.5 2.56 0.0 0.3 15.36 2.5 0.0 2 2022-01-09 00:31:03 2022-01-09 00:40:10 1.0 1.39 1.0 N 68 186 1 8.0 0.5 0.5 2.36 0.0 0.3 14.16 2.5 0.0 2 2022-01-09 00:50:16 2022-01-09 00:54:23 1.0 0.95 1.0 N 186 48 2 5.0 0.5 0.5 0.0 0.0 0.3 8.8 2.5 0.0 1 2022-01-09 00:19:51 2022-01-09 00:31:30 1.0 3.8 1.0 N 148 140 2 13.0 3.0 0.5 0.0 0.0 0.3 16.8 2.5 0.0 1 2022-01-09 00:57:05 2022-01-09 01:20:48 1.0 6.4 1.0 Y 249 41 1 22.0 3.0 0.5 5.15 0.0 0.3 30.95 2.5 0.0 2 2022-01-09 00:10:05 2022-01-09 00:15:50 1.0 1.13 1.0 N 263 236 1 6.5 0.5 0.5 2.06 0.0 0.3 12.36 2.5 0.0 2 2022-01-09 00:21:19 2022-01-09 00:27:09 1.0 1.2 1.0 N 236 141 1 6.5 0.5 0.5 2.06 0.0 0.3 12.36 2.5 0.0 2 2022-01-09 00:32:03 2022-01-09 00:40:38 1.0 2.51 1.0 N 237 164 1 9.5 0.5 0.5 2.66 0.0 0.3 15.96 2.5 0.0 2 2022-01-09 00:47:26 2022-01-09 01:05:00 1.0 5.48 1.0 N 164 87 1 19.5 0.5 0.5 4.66 0.0 0.3 27.96 2.5 0.0 2 2022-01-09 00:10:34 2022-01-09 00:38:59 2.0 9.7 1.0 N 236 231 2 31.5 0.5 0.5 0.0 0.0 0.3 35.3 2.5 0.0 2 2022-01-09 00:48:08 2022-01-09 01:02:33 2.0 3.33 1.0 N 144 48 1 13.0 0.5 0.5 3.36 0.0 0.3 20.16 2.5 0.0 1 2022-01-09 00:23:04 2022-01-09 00:53:14 1.0 18.2 2.0 N 132 90 1 52.0 3.75 0.5 10.0 6.55 0.3 73.1 2.5 1.25 2 2022-01-09 00:52:29 2022-01-09 01:02:11 1.0 4.37 1.0 N 132 139 2 14.0 0.5 0.5 0.0 0.0 0.3 16.55 0.0 1.25 2 2022-01-09 00:31:15 2022-01-09 00:38:14 1.0 0.96 1.0 N 162 48 1 6.5 0.5 0.5 2.06 0.0 0.3 12.36 2.5 0.0 2 2022-01-09 00:40:19 2022-01-09 00:43:24 3.0 0.54 1.0 N 50 48 1 4.0 0.5 0.5 1.56 0.0 0.3 9.36 2.5 0.0 2 2022-01-09 00:45:08 2022-01-09 01:00:04 1.0 3.31 1.0 N 48 79 1 13.0 0.5 0.5 4.2 0.0 0.3 21.0 2.5 0.0 2 2022-01-09 00:16:16 2022-01-09 00:20:44 1.0 0.9 1.0 N 142 48 1 5.5 0.5 0.5 1.86 0.0 0.3 11.16 2.5 0.0 2 2022-01-09 00:33:27 2022-01-09 00:37:11 1.0 0.87 1.0 N 141 229 1 5.0 0.5 0.5 1.76 0.0 0.3 10.56 2.5 0.0 2 2022-01-09 00:40:05 2022-01-09 00:44:25 1.0 0.76 1.0 N 229 162 1 5.0 0.5 0.5 1.76 0.0 0.3 10.56 2.5 0.0 2 2022-01-09 00:40:11 2022-01-10 00:31:17 5.0 7.1 1.0 N 232 238 1 26.5 0.5 0.5 0.0 0.0 0.3 30.3 2.5 0.0 2 2022-01-09 00:29:04 2022-01-09 00:35:14 2.0 2.13 1.0 N 114 170 2 8.0 0.5 0.5 0.0 0.0 0.3 11.8 2.5 0.0 2 2022-01-09 00:57:41 2022-01-09 01:25:06 1.0 19.14 2.0 N 132 79 2 52.0 0.0 0.5 0.0 0.0 0.3 56.55 2.5 1.25 1 2022-01-09 00:07:39 2022-01-09 00:12:13 2.0 1.0 1.0 N 79 144 1 5.5 3.0 0.5 2.3 0.0 0.3 11.6 2.5 0.0 1 2022-01-09 00:18:52 2022-01-09 00:23:16 3.0 0.9 1.0 N 113 79 1 5.0 3.0 0.5 1.75 0.0 0.3 10.55 2.5 0.0 1 2022-01-09 00:25:43 2022-01-09 00:38:45 2.0 3.5 1.0 N 107 66 1 13.5 3.0 0.5 1.5 0.0 0.3 18.8 2.5 0.0 1 2022-01-09 00:46:46 2022-01-09 00:55:51 1.0 2.6 1.0 N 125 246 1 10.0 3.0 0.5 2.75 0.0 0.3 16.55 2.5 0.0 1 2022-01-09 00:57:19 2022-01-09 01:28:29 1.0 12.4 1.0 N 132 61 1 36.5 1.75 0.5 9.75 0.0 0.3 48.8 0.0 1.25 2 2022-01-09 00:29:25 2022-01-09 00:30:26 1.0 0.36 1.0 N 48 48 1 3.0 0.5 0.5 1.36 0.0 0.3 8.16 2.5 0.0 2 2022-01-09 00:31:32 2022-01-09 00:36:47 1.0 1.42 1.0 N 48 246 2 6.0 0.5 0.5 0.0 0.0 0.3 9.8 2.5 0.0 2 2022-01-09 00:39:11 2022-01-09 00:45:43 1.0 1.63 1.0 N 263 74 2 7.0 0.5 0.5 0.0 0.0 0.3 10.8 2.5 0.0 1 2022-01-09 00:02:59 2022-01-09 00:15:56 2.0 3.0 1.0 N 234 209 1 11.5 3.0 0.5 2.0 0.0 0.3 17.3 2.5 0.0 1 2022-01-09 00:20:25 2022-01-09 00:25:16 1.0 1.1 1.0 N 261 45 1 6.0 3.0 0.5 1.95 0.0 0.3 11.75 2.5 0.0 1 2022-01-09 00:32:10 2022-01-09 00:33:37 2.0 0.5 1.0 N 114 113 1 3.5 3.0 0.5 1.45 0.0 0.3 8.75 2.5 0.0 1 2022-01-09 00:37:04 2022-01-09 00:38:07 1.0 0.0 1.0 N 107 107 4 3.0 3.0 0.5 0.0 0.0 0.3 6.8 2.5 0.0 1 2022-01-09 00:41:19 2022-01-09 00:44:59 1.0 1.2 1.0 N 137 233 1 5.5 3.0 0.5 1.0 0.0 0.3 10.3 2.5 0.0 2 2022-01-09 00:13:56 2022-01-09 00:27:02 1.0 3.62 1.0 N 234 263 1 13.0 0.5 0.5 3.36 0.0 0.3 20.16 2.5 0.0 2 2022-01-09 00:27:50 2022-01-09 00:42:28 1.0 6.37 1.0 N 142 244 1 20.0 0.5 0.5 4.76 0.0 0.3 28.56 2.5 0.0 1 2022-01-09 00:16:47 2022-01-09 00:26:42 1.0 2.3 1.0 N 249 230 1 9.5 3.0 0.5 2.65 0.0 0.3 15.95 2.5 0.0 1 2022-01-09 00:32:03 2022-01-09 00:39:40 2.0 1.5 1.0 N 163 237 1 8.0 3.0 0.5 2.95 0.0 0.3 14.75 2.5 0.0 2 2022-01-09 00:34:58 2022-01-09 00:43:31 1.0 1.67 1.0 N 166 41 2 8.0 0.5 0.5 0.0 0.0 0.3 9.3 0.0 0.0 2 2022-01-09 00:36:53 2022-01-09 00:37:05 2.0 0.0 5.0 N 148 148 1 40.0 0.0 0.0 8.56 0.0 0.3 51.36 2.5 0.0 2 2022-01-09 00:37:46 2022-01-09 00:48:28 1.0 2.28 1.0 N 148 186 1 10.0 0.5 0.5 2.76 0.0 0.3 16.56 2.5 0.0 2 2022-01-09 00:59:42 2022-01-09 01:05:21 1.0 1.02 1.0 N 249 90 1 6.0 0.5 0.5 4.0 0.0 0.3 13.8 2.5 0.0 2 2022-01-09 00:41:37 2022-01-09 01:02:36 1.0 6.4 1.0 N 137 166 1 20.5 0.5 0.5 4.86 0.0 0.3 29.16 2.5 0.0 2 2022-01-09 00:27:48 2022-01-09 00:48:18 1.0 8.49 1.0 N 132 86 2 25.5 0.5 0.5 0.0 0.0 0.3 28.05 0.0 1.25 2 2022-01-09 00:02:36 2022-01-09 00:07:45 1.0 2.01 1.0 N 263 229 1 7.5 0.5 0.5 2.26 0.0 0.3 13.56 2.5 0.0 2 2022-01-09 00:06:43 2022-01-09 00:41:00 1.0 18.61 2.0 N 132 90 1 52.0 0.0 0.5 18.55 6.55 0.3 81.65 2.5 1.25 2 2022-01-09 00:47:17 2022-01-09 00:57:33 1.0 1.43 1.0 N 249 144 1 8.0 0.5 0.5 1.08 0.0 0.3 12.88 2.5 0.0 2 2022-01-09 00:24:57 2022-01-09 00:26:01 1.0 0.37 1.0 N 249 90 1 3.0 0.5 0.5 1.36 0.0 0.3 8.16 2.5 0.0 In\u00a0[11]: Copied! <pre>%%sql\n\nCREATE VIEW nyc.long_distances (\n    vendor_id COMMENT 'Vendor ID',\n    pickup_date,\n    dropoff_date,\n    distance COMMENT 'Trip Distance',\n    total COMMENT 'Total amount'\n)\nAS SELECT \n  VendorID,\n  tpep_pickup_datetime,\n  tpep_dropoff_datetime,\n  trip_distance,\n  total_amount\nFROM nyc.taxis\nORDER BY trip_distance\n</pre> %%sql  CREATE VIEW nyc.long_distances (     vendor_id COMMENT 'Vendor ID',     pickup_date,     dropoff_date,     distance COMMENT 'Trip Distance',     total COMMENT 'Total amount' ) AS SELECT    VendorID,   tpep_pickup_datetime,   tpep_dropoff_datetime,   trip_distance,   total_amount FROM nyc.taxis ORDER BY trip_distance Out[11]: In\u00a0[12]: Copied! <pre>%%sql\n\nSELECT * FROM nyc.long_distances\n</pre> %%sql  SELECT * FROM nyc.long_distances <pre>                                                                                </pre> Out[12]: vendor_id pickup_date dropoff_date distance total 1 2022-01-16 01:00:55 2022-01-16 01:21:30 0.0 38.98 2 2022-01-21 02:21:34 2022-01-21 02:21:42 0.0 62.8 2 2022-01-19 06:08:05 2022-01-19 06:08:08 0.0 3.3 2 2022-01-21 06:43:11 2022-01-21 06:44:37 0.0 18.6 1 2022-01-20 03:03:52 2022-01-20 03:04:22 0.0 40.35 1 2022-01-21 00:27:47 2022-01-21 01:00:56 0.0 42.0 2 2022-01-19 08:39:19 2022-01-19 08:39:21 0.0 66.96 2 2022-01-21 02:35:03 2022-01-21 02:35:20 0.0 27.36 1 2022-01-10 06:18:18 2022-01-10 06:25:42 0.0 11.6 2 2022-01-21 00:18:35 2022-01-21 00:19:10 0.0 65.3 1 2022-01-19 06:02:03 2022-01-19 06:02:33 0.0 5.8 1 2022-01-21 02:05:08 2022-01-21 02:45:46 0.0 47.55 1 2022-01-20 07:19:26 2022-01-20 07:36:43 0.0 26.0 1 2022-01-21 00:45:57 2022-01-21 00:46:33 0.0 10.3 2 2022-01-19 06:38:36 2022-01-19 06:38:59 0.0 3.3 2 2022-01-21 02:01:24 2022-01-21 02:01:27 0.0 74.12 2 2022-01-03 04:17:04 2022-01-03 04:17:08 0.0 50.3 2 2022-01-21 00:09:33 2022-01-21 00:17:00 0.0 11.76 1 2022-01-19 06:34:39 2022-01-19 07:16:24 0.0 62.55 1 2022-01-21 02:04:33 2022-01-21 02:04:33 0.0 6.3 2 2022-01-20 03:25:39 2022-01-20 03:25:42 0.0 12.36 1 2022-01-21 00:52:08 2022-01-21 00:52:14 0.0 3.8 2 2022-01-19 00:53:38 2022-01-19 17:07:30 0.0 6.8 1 2022-01-21 06:56:41 2022-01-21 07:11:09 0.0 30.0 1 2022-01-10 06:47:47 2022-01-10 06:58:27 0.0 11.8 2 2022-01-21 00:19:21 2022-01-21 00:46:42 0.0 23.76 2 2022-01-19 06:10:42 2022-01-19 06:10:59 0.0 -60.6 2 2022-01-21 00:02:52 2022-01-21 00:03:33 0.0 99.36 1 2022-01-20 03:41:36 2022-01-20 04:04:59 0.0 29.0 2 2022-01-21 00:49:19 2022-01-21 00:49:29 0.0 8.3 1 2022-01-19 00:10:23 2022-01-19 00:45:40 0.0 48.0 1 2022-01-21 00:57:47 2022-01-21 01:11:16 0.0 18.0 2 2022-01-22 01:32:29 2022-01-22 01:44:19 0.0 12.98 1 2022-01-21 00:00:31 2022-01-21 00:06:37 0.0 18.28 2 2022-01-19 06:10:42 2022-01-19 06:10:59 0.0 60.6 1 2022-01-21 02:24:36 2022-01-21 02:24:36 0.0 6.3 2 2022-01-20 01:12:01 2022-01-20 01:12:06 0.0 52.3 2 2022-01-21 00:17:41 2022-01-21 00:17:46 0.0 6.3 1 2022-01-19 00:20:54 2022-01-19 00:39:37 0.0 55.3 2 2022-01-21 02:35:20 2022-01-21 02:35:28 0.0 147.96 2 2022-01-10 02:49:06 2022-01-10 02:49:09 0.0 45.0 2 2022-01-21 00:08:44 2022-01-21 00:08:53 0.0 78.36 2 2022-01-19 06:12:08 2022-01-19 06:13:21 0.0 -60.6 2 2022-01-21 02:53:56 2022-01-21 02:54:00 0.0 11.3 2 2022-01-20 03:55:40 2022-01-20 03:55:43 0.0 45.3 2 2022-01-21 00:55:53 2022-01-21 01:07:28 0.0 13.8 1 2022-01-19 00:35:43 2022-01-19 01:11:46 0.0 81.63 1 2022-01-21 02:21:01 2022-01-21 02:21:10 0.0 5.05 1 2022-01-03 07:00:14 2022-01-03 07:29:31 0.0 34.38 2 2022-01-21 00:41:09 2022-01-21 00:41:17 0.0 3.8 2 2022-01-19 06:12:08 2022-01-19 06:13:21 0.0 60.6 1 2022-01-21 02:50:31 2022-01-21 02:51:57 0.0 6.3 2 2022-01-20 00:24:11 2022-01-20 00:28:17 0.0 9.96 1 2022-01-21 00:22:03 2022-01-21 00:47:38 0.0 27.45 2 2022-01-19 00:05:43 2022-01-19 00:05:48 0.0 6.3 2 2022-01-21 02:24:13 2022-01-21 02:24:21 0.0 -5.05 1 2022-01-10 06:01:43 2022-01-10 06:34:35 0.0 50.55 2 2022-01-21 00:44:58 2022-01-21 00:45:09 0.0 4.45 2 2022-01-19 00:16:41 2022-01-19 00:16:47 0.0 8.16 2 2022-01-21 02:24:13 2022-01-21 02:24:21 0.0 5.05 2 2022-01-20 03:08:05 2022-01-20 03:08:14 0.0 18.12 2 2022-01-21 00:19:44 2022-01-21 00:20:08 0.0 100.0 1 2022-01-19 08:15:54 2022-01-19 08:17:19 0.0 6.8 2 2022-01-21 03:41:05 2022-01-21 03:41:16 0.0 17.16 2 2022-01-31 04:44:24 2022-01-31 04:45:42 0.0 90.3 1 2022-01-21 00:45:43 2022-01-21 00:46:02 0.0 0.31 2 2022-01-19 06:09:19 2022-01-19 06:09:29 0.0 76.55 2 2022-01-21 03:49:38 2022-01-21 03:49:48 0.0 3.8 2 2022-01-20 00:47:03 2022-01-20 00:48:27 0.0 13.3 1 2022-01-21 00:23:41 2022-01-21 00:24:39 0.0 39.35 2 2022-01-19 06:34:25 2022-01-19 06:34:42 0.0 35.38 2 2022-01-21 03:09:22 2022-01-21 03:09:34 0.0 75.3 2 2022-01-10 00:29:42 2022-01-10 00:30:25 0.0 77.05 2 2022-01-21 01:16:27 2022-01-21 01:16:39 0.0 39.39 1 2022-01-19 02:51:27 2022-01-19 03:04:43 0.0 21.0 1 2022-01-21 03:02:55 2022-01-21 03:03:23 0.0 18.35 2 2022-01-20 03:26:02 2022-01-20 03:26:51 0.0 -27.8 1 2022-01-21 01:30:40 2022-01-21 01:35:09 0.0 150.4 2 2022-01-19 00:52:46 2022-01-19 00:52:48 0.0 81.0 2 2022-01-21 03:25:39 2022-01-21 03:25:49 0.0 -19.8 1 2022-01-03 04:37:38 2022-01-03 05:06:25 0.0 34.0 1 2022-01-21 01:07:06 2022-01-21 01:07:20 0.0 3.8 2 2022-01-19 02:57:36 2022-01-19 02:57:39 0.0 13.5 2 2022-01-21 03:25:39 2022-01-21 03:25:49 0.0 19.8 2 2022-01-20 00:54:10 2022-01-20 00:59:36 0.0 30.3 1 2022-01-21 01:14:38 2022-01-21 01:38:27 0.0 33.0 2 2022-01-19 00:49:37 2022-01-19 00:49:46 0.0 -52.8 2 2022-01-21 03:27:01 2022-01-21 03:27:11 0.0 22.3 2 2022-01-10 06:22:55 2022-01-10 06:23:04 0.0 5.8 1 2022-01-21 01:54:04 2022-01-21 01:59:26 0.0 18.28 1 2022-01-19 00:12:13 2022-01-19 00:24:55 0.0 21.0 1 2022-01-21 03:04:04 2022-01-21 03:24:34 0.0 38.98 2 2022-01-20 03:26:02 2022-01-20 03:26:51 0.0 32.8 1 2022-01-21 01:44:44 2022-01-21 01:45:45 0.0 16.58 2 2022-01-19 00:50:43 2022-01-19 00:50:50 0.0 -52.8 2 2022-01-21 01:09:20 2022-01-21 01:13:54 0.0 9.96 2 2022-01-22 03:14:11 2022-01-22 03:14:17 0.0 19.13 2 2022-01-21 01:57:55 2022-01-21 02:06:05 0.0 12.36 2 2022-01-19 02:57:35 2022-01-19 02:57:37 0.0 1.69 2 2022-01-21 03:01:54 2022-01-21 03:12:25 0.0 12.8 In\u00a0[13]: Copied! <pre>%%sql\n\nCREATE OR REPLACE VIEW nyc.long_distances (\n    distance COMMENT 'Trip Distance',\n    total COMMENT 'Total amount',\n    vendor_id COMMENT 'Vendor ID',\n    pickup_date,\n    dropoff_date\n)\nAS SELECT \n    trip_distance,\n    total_amount,\n    VendorID,\n    tpep_pickup_datetime,\n    tpep_dropoff_datetime\nFROM nyc.taxis\nWHERE trip_distance &gt; 35\nORDER BY total_amount, trip_distance\n</pre> %%sql  CREATE OR REPLACE VIEW nyc.long_distances (     distance COMMENT 'Trip Distance',     total COMMENT 'Total amount',     vendor_id COMMENT 'Vendor ID',     pickup_date,     dropoff_date ) AS SELECT      trip_distance,     total_amount,     VendorID,     tpep_pickup_datetime,     tpep_dropoff_datetime FROM nyc.taxis WHERE trip_distance &gt; 35 ORDER BY total_amount, trip_distance Out[13]: In\u00a0[14]: Copied! <pre>%%sql\n\nSELECT * FROM nyc.long_distances\n</pre> %%sql  SELECT * FROM nyc.long_distances Out[14]: distance total vendor_id pickup_date dropoff_date 116.91 -408.85 2 2022-01-31 22:22:53 2022-02-01 00:34:41 51.16 -207.48 2 2022-01-14 19:25:02 2022-01-14 20:32:19 44.83 -144.6 2 2022-01-26 17:42:57 2022-01-26 19:27:50 53.05 -134.85 2 2022-01-07 15:37:09 2022-01-07 16:23:34 36.69 -131.6 2 2022-01-05 16:38:10 2022-01-05 18:08:35 46.67 -106.65 2 2022-01-12 21:27:20 2022-01-12 22:02:16 47.01 -101.6 2 2022-01-11 06:19:19 2022-01-11 07:03:04 45.06 -94.1 2 2022-01-28 18:08:24 2022-01-28 18:58:27 37.15 -53.55 2 2022-01-31 19:26:39 2022-01-31 20:02:41 35.9 0.3 1 2022-01-28 19:02:55 2022-01-28 19:50:20 55.0 3.3 1 2022-01-10 10:20:56 2022-01-10 10:21:31 96.0 5.8 1 2022-01-03 07:43:00 2022-01-03 07:43:09 38.3 6.85 1 2022-01-17 07:06:19 2022-01-17 07:52:55 118618.94 8.5 2 2022-01-18 15:51:00 2022-01-18 15:55:00 42.0 9.3 1 2022-01-02 13:42:44 2022-01-02 13:48:13 42.6 9.3 1 2022-01-04 07:24:26 2022-01-04 07:27:53 52.0 9.68 1 2022-01-09 12:54:40 2022-01-09 13:03:10 46.3 9.85 1 2022-01-03 16:00:40 2022-01-03 17:08:59 112219.77 10.21 2 2022-01-15 01:57:00 2022-01-15 02:00:00 35.2 10.3 1 2022-01-01 11:13:24 2022-01-01 11:21:31 35.8 10.3 1 2022-01-12 14:41:33 2022-01-12 14:48:55 38.2 10.3 1 2022-01-08 10:59:50 2022-01-08 11:05:00 44.2 10.3 1 2022-01-03 10:56:29 2022-01-03 11:03:48 37.4 10.55 1 2022-01-06 13:21:54 2022-01-06 13:26:15 49.7 10.7 1 2022-01-08 13:02:27 2022-01-08 13:06:16 35.9 10.8 1 2022-01-11 17:19:16 2022-01-11 17:26:54 39.4 10.8 1 2022-01-12 08:44:26 2022-01-12 08:53:56 45.1 10.8 1 2022-01-06 09:33:54 2022-01-06 09:42:06 45.4 10.8 1 2022-01-07 08:52:31 2022-01-07 08:58:30 46.0 10.8 1 2022-01-11 14:15:35 2022-01-11 14:23:15 37.4 11.15 1 2022-01-02 12:52:36 2022-01-02 12:57:12 47.8 11.3 1 2022-01-08 13:49:49 2022-01-08 13:59:43 49.8 11.3 1 2022-01-01 15:12:31 2022-01-01 15:21:20 51.2 11.3 1 2022-01-14 07:38:48 2022-01-14 07:45:00 601.5 11.3 1 2022-01-07 08:10:45 2022-01-07 08:20:30 39.6 11.31 1 2022-01-16 14:34:00 2022-01-16 14:43:03 38.0 11.33 1 2022-01-04 07:35:51 2022-01-04 07:43:25 41.4 11.6 1 2022-01-02 18:25:30 2022-01-02 18:31:42 47.0 11.6 1 2022-01-08 11:18:45 2022-01-08 11:22:10 107007.93 11.61 2 2022-01-15 18:11:00 2022-01-15 18:19:00 42.8 11.75 1 2022-01-12 15:36:54 2022-01-12 15:44:08 47.6 11.75 1 2022-01-08 09:56:22 2022-01-08 10:02:15 53.7 11.8 1 2022-01-02 11:36:54 2022-01-02 11:42:40 69.8 11.8 1 2022-01-01 16:32:57 2022-01-01 16:40:28 74.9 11.85 1 2022-01-16 10:50:22 2022-01-16 10:59:36 59.1 12.0 1 2022-01-08 10:28:15 2022-01-08 10:35:32 8665.17 12.0 2 2022-01-28 08:41:00 2022-01-28 08:48:00 30650.3 12.0 2 2022-01-15 03:13:00 2022-01-15 03:20:00 46.8 12.25 1 2022-01-04 14:17:13 2022-01-04 14:23:24 54.4 12.25 1 2022-01-11 10:57:34 2022-01-11 11:03:05 38.2 12.3 1 2022-01-04 17:12:47 2022-01-04 17:23:23 40.5 12.3 1 2022-01-10 18:06:52 2022-01-10 18:16:47 46.4 12.3 1 2022-01-05 07:53:49 2022-01-05 08:02:02 51.1 12.3 1 2022-01-03 18:03:05 2022-01-03 18:09:43 51.2 12.3 1 2022-01-12 13:56:53 2022-01-12 14:08:31 52.2 12.3 1 2022-01-16 13:24:12 2022-01-16 13:30:51 53.2 12.3 1 2022-01-03 15:14:54 2022-01-03 15:26:10 39.6 12.35 1 2022-01-08 11:36:31 2022-01-08 11:44:36 41.0 12.35 1 2022-01-14 08:03:02 2022-01-14 08:09:34 50.2 12.35 1 2022-01-05 09:32:42 2022-01-05 09:39:06 53.5 12.35 1 2022-01-02 11:44:30 2022-01-02 11:50:48 40.2 12.42 1 2022-01-14 14:22:26 2022-01-14 14:31:38 48.4 12.43 1 2022-01-03 10:31:49 2022-01-03 10:41:34 43.2 12.8 1 2022-01-08 15:47:14 2022-01-08 15:56:15 43.4 12.8 1 2022-01-05 11:13:57 2022-01-05 11:22:43 52.3 12.8 1 2022-01-15 11:02:47 2022-01-15 11:11:58 72.7 12.8 1 2022-01-10 11:14:53 2022-01-10 11:25:27 53.0 12.85 1 2022-01-16 10:32:39 2022-01-16 10:38:44 40.4 12.95 1 2022-01-07 18:47:56 2022-01-07 18:54:49 59.5 12.95 1 2022-01-11 07:15:54 2022-01-11 07:23:13 62.6 12.95 1 2022-01-01 14:57:30 2022-01-01 15:04:19 38.5 12.96 1 2022-01-09 16:32:54 2022-01-09 16:41:49 52.4 12.98 1 2022-01-06 18:17:05 2022-01-06 18:25:23 50.2 13.0 1 2022-01-16 16:23:20 2022-01-16 16:32:49 62.1 13.05 1 2022-01-08 16:13:13 2022-01-08 16:21:51 100224.11 13.28 2 2022-01-21 05:37:00 2022-01-21 05:40:00 40.5 13.3 1 2022-01-01 15:52:54 2022-01-01 16:05:13 45.9 13.3 1 2022-01-12 11:08:44 2022-01-12 11:18:25 48.5 13.3 1 2022-01-10 17:19:12 2022-01-10 17:30:49 62.3 13.3 1 2022-01-14 15:13:02 2022-01-14 15:26:34 96113.91 13.33 2 2022-01-28 07:55:00 2022-01-28 08:05:00 52.1 13.55 1 2022-01-13 07:52:47 2022-01-13 08:02:04 68.1 13.55 1 2022-01-01 13:15:57 2022-01-01 13:23:05 40.7 13.56 1 2022-01-07 08:12:17 2022-01-07 08:22:07 60.0 13.56 1 2022-01-04 14:04:12 2022-01-04 14:13:04 48.2 13.57 1 2022-01-08 16:24:19 2022-01-08 16:33:18 35595.76 13.74 2 2022-01-29 18:48:00 2022-01-29 19:00:00 67058.18 13.77 2 2022-01-16 08:47:00 2022-01-16 08:57:00 8139.12 13.79 2 2022-01-24 07:39:00 2022-01-24 07:43:00 57.1 13.8 1 2022-01-12 13:33:15 2022-01-12 13:47:33 57.2 13.8 1 2022-01-10 15:43:31 2022-01-10 15:55:43 71.4 13.8 1 2022-01-08 13:29:26 2022-01-08 13:42:09 76869.06 13.87 2 2022-01-27 06:54:00 2022-01-27 07:06:00 82340.33 13.96 2 2022-01-14 15:40:00 2022-01-14 15:46:00 49.5 14.15 1 2022-01-10 15:18:14 2022-01-10 15:28:32 51.2 14.15 1 2022-01-01 16:49:40 2022-01-01 16:59:27 53.5 14.15 1 2022-01-07 15:00:23 2022-01-07 15:10:42 55.8 14.15 1 2022-01-09 14:31:03 2022-01-09 14:40:52 57.9 14.15 1 2022-01-02 18:41:46 2022-01-02 18:51:18 59.5 14.15 1 2022-01-09 13:58:38 2022-01-09 14:08:34 In\u00a0[15]: Copied! <pre>%%sql\n\nSELECT count(*) FROM nyc.long_distances\n</pre> %%sql  SELECT count(*) FROM nyc.long_distances <pre>                                                                                </pre> Out[15]: count(1) 1340 In\u00a0[16]: Copied! <pre>df = spark.read.parquet(\"/home/iceberg/data/yellow_tripdata_2022-02.parquet\")\ndf.writeTo(\"nyc.taxis\").append()\n</pre> df = spark.read.parquet(\"/home/iceberg/data/yellow_tripdata_2022-02.parquet\") df.writeTo(\"nyc.taxis\").append() <pre>                                                                                </pre> In\u00a0[17]: Copied! <pre>%%sql\n\nSELECT * FROM nyc.long_distances\n</pre> %%sql  SELECT * FROM nyc.long_distances <pre>                                                                                </pre> Out[17]: distance total vendor_id pickup_date dropoff_date 116.91 -408.85 2 2022-01-31 22:22:53 2022-02-01 00:34:41 38.75 -286.24 2 2022-02-05 03:25:27 2022-02-05 04:26:27 75.79 -220.1 2 2022-02-17 16:05:40 2022-02-17 17:49:26 72.06 -218.85 2 2022-02-03 01:16:51 2022-02-03 02:22:09 51.16 -207.48 2 2022-01-14 19:25:02 2022-01-14 20:32:19 43.35 -202.8 2 2022-02-01 23:31:22 2022-02-02 00:24:03 47.59 -200.6 2 2022-02-25 13:07:48 2022-02-25 14:19:56 35.16 -195.6 2 2022-02-25 17:27:22 2022-02-25 18:25:38 37.14 -170.6 2 2022-02-21 16:44:09 2022-02-21 17:51:24 46.58 -158.05 2 2022-02-05 20:53:58 2022-02-05 21:42:45 44.83 -144.6 2 2022-01-26 17:42:57 2022-01-26 19:27:50 53.05 -134.85 2 2022-01-07 15:37:09 2022-01-07 16:23:34 36.69 -131.6 2 2022-01-05 16:38:10 2022-01-05 18:08:35 46.67 -106.65 2 2022-01-12 21:27:20 2022-01-12 22:02:16 46.46 -103.65 2 2022-02-21 17:24:31 2022-02-21 18:12:13 47.01 -101.6 2 2022-01-11 06:19:19 2022-01-11 07:03:04 45.06 -94.1 2 2022-01-28 18:08:24 2022-01-28 18:58:27 40.53 -77.35 2 2022-02-28 19:11:20 2022-02-28 19:53:52 43.62 -77.1 2 2022-02-26 01:26:13 2022-02-26 01:56:26 47.62 -67.6 2 2022-02-18 19:29:54 2022-02-18 20:16:56 37.15 -53.55 2 2022-01-31 19:26:39 2022-01-31 20:02:41 37.35 -2.05 2 2022-02-24 18:46:35 2022-02-24 19:47:01 50.92 -2.05 2 2022-02-21 08:53:22 2022-02-21 09:40:22 35.9 0.3 1 2022-01-28 19:02:55 2022-01-28 19:50:20 36.4 0.3 1 2022-02-06 19:56:32 2022-02-06 20:46:50 38.7 0.31 1 2022-02-18 08:55:41 2022-02-18 10:16:21 37.35 2.05 2 2022-02-24 18:46:35 2022-02-24 19:47:01 50.92 2.05 2 2022-02-21 08:53:22 2022-02-21 09:40:22 55.0 3.3 1 2022-01-10 10:20:56 2022-01-10 10:21:31 35.2 3.8 1 2022-02-27 02:38:48 2022-02-27 02:38:59 46.9 4.55 1 2022-02-06 18:46:33 2022-02-06 18:46:35 96.0 5.8 1 2022-01-03 07:43:00 2022-01-03 07:43:09 40.8 6.8 1 2022-02-15 21:01:56 2022-02-15 21:04:00 38.3 6.85 1 2022-01-17 07:06:19 2022-01-17 07:52:55 39.5 6.85 1 2022-02-16 14:14:48 2022-02-16 15:05:49 43.7 6.86 1 2022-02-02 20:14:26 2022-02-02 22:04:57 118618.94 8.5 2 2022-01-18 15:51:00 2022-01-18 15:55:00 42.0 9.3 1 2022-01-02 13:42:44 2022-01-02 13:48:13 42.6 9.3 1 2022-01-04 07:24:26 2022-01-04 07:27:53 52.0 9.68 1 2022-01-09 12:54:40 2022-01-09 13:03:10 46.3 9.85 1 2022-01-03 16:00:40 2022-01-03 17:08:59 112219.77 10.21 2 2022-01-15 01:57:00 2022-01-15 02:00:00 81856.67 10.28 2 2022-02-02 16:20:00 2022-02-02 16:25:00 35.2 10.3 1 2022-01-01 11:13:24 2022-01-01 11:21:31 35.8 10.3 1 2022-01-12 14:41:33 2022-01-12 14:48:55 38.2 10.3 1 2022-01-08 10:59:50 2022-01-08 11:05:00 42.35 10.3 2 2022-02-21 21:59:58 2022-02-21 23:35:36 44.2 10.3 1 2022-01-03 10:56:29 2022-01-03 11:03:48 37.4 10.55 1 2022-01-06 13:21:54 2022-01-06 13:26:15 49.7 10.7 1 2022-01-08 13:02:27 2022-01-08 13:06:16 35.9 10.8 1 2022-01-11 17:19:16 2022-01-11 17:26:54 39.4 10.8 1 2022-01-12 08:44:26 2022-01-12 08:53:56 45.1 10.8 1 2022-01-06 09:33:54 2022-01-06 09:42:06 45.4 10.8 1 2022-01-07 08:52:31 2022-01-07 08:58:30 46.0 10.8 1 2022-01-11 14:15:35 2022-01-11 14:23:15 66047.66 11.12 2 2022-02-14 20:01:00 2022-02-14 20:11:00 37.4 11.15 1 2022-01-02 12:52:36 2022-01-02 12:57:12 47.8 11.3 1 2022-01-08 13:49:49 2022-01-08 13:59:43 49.8 11.3 1 2022-01-01 15:12:31 2022-01-01 15:21:20 51.2 11.3 1 2022-01-14 07:38:48 2022-01-14 07:45:00 601.5 11.3 1 2022-01-07 08:10:45 2022-01-07 08:20:30 39.6 11.31 1 2022-01-16 14:34:00 2022-01-16 14:43:03 38.0 11.33 1 2022-01-04 07:35:51 2022-01-04 07:43:25 41.4 11.6 1 2022-01-02 18:25:30 2022-01-02 18:31:42 47.0 11.6 1 2022-01-08 11:18:45 2022-01-08 11:22:10 107007.93 11.61 2 2022-01-15 18:11:00 2022-01-15 18:19:00 42.8 11.75 1 2022-01-12 15:36:54 2022-01-12 15:44:08 47.6 11.75 1 2022-01-08 09:56:22 2022-01-08 10:02:15 53.7 11.8 1 2022-01-02 11:36:54 2022-01-02 11:42:40 69.8 11.8 1 2022-01-01 16:32:57 2022-01-01 16:40:28 74.9 11.85 1 2022-01-16 10:50:22 2022-01-16 10:59:36 59.1 12.0 1 2022-01-08 10:28:15 2022-01-08 10:35:32 8665.17 12.0 2 2022-01-28 08:41:00 2022-01-28 08:48:00 30650.3 12.0 2 2022-01-15 03:13:00 2022-01-15 03:20:00 40.2 12.06 1 2022-02-18 23:03:12 2022-02-19 02:53:39 46.8 12.25 1 2022-01-04 14:17:13 2022-01-04 14:23:24 54.4 12.25 1 2022-01-11 10:57:34 2022-01-11 11:03:05 38.2 12.3 1 2022-01-04 17:12:47 2022-01-04 17:23:23 40.5 12.3 1 2022-01-10 18:06:52 2022-01-10 18:16:47 46.4 12.3 1 2022-01-05 07:53:49 2022-01-05 08:02:02 51.1 12.3 1 2022-01-03 18:03:05 2022-01-03 18:09:43 51.2 12.3 1 2022-01-12 13:56:53 2022-01-12 14:08:31 52.2 12.3 1 2022-01-16 13:24:12 2022-01-16 13:30:51 53.2 12.3 1 2022-01-03 15:14:54 2022-01-03 15:26:10 39.6 12.35 1 2022-01-08 11:36:31 2022-01-08 11:44:36 41.0 12.35 1 2022-01-14 08:03:02 2022-01-14 08:09:34 50.2 12.35 1 2022-01-05 09:32:42 2022-01-05 09:39:06 53.5 12.35 1 2022-01-02 11:44:30 2022-01-02 11:50:48 40.2 12.42 1 2022-01-14 14:22:26 2022-01-14 14:31:38 48.4 12.43 1 2022-01-03 10:31:49 2022-01-03 10:41:34 43.2 12.8 1 2022-01-08 15:47:14 2022-01-08 15:56:15 43.4 12.8 1 2022-01-05 11:13:57 2022-01-05 11:22:43 52.3 12.8 1 2022-01-15 11:02:47 2022-01-15 11:11:58 72.7 12.8 1 2022-01-10 11:14:53 2022-01-10 11:25:27 7814.79 12.84 2 2022-02-11 07:17:00 2022-02-11 07:23:00 53.0 12.85 1 2022-01-16 10:32:39 2022-01-16 10:38:44 40.4 12.95 1 2022-01-07 18:47:56 2022-01-07 18:54:49 59.5 12.95 1 2022-01-11 07:15:54 2022-01-11 07:23:13 62.6 12.95 1 2022-01-01 14:57:30 2022-01-01 15:04:19 38.5 12.96 1 2022-01-09 16:32:54 2022-01-09 16:41:49 In\u00a0[18]: Copied! <pre>%%sql\n\nSELECT count(*) FROM nyc.long_distances\n</pre> %%sql  SELECT count(*) FROM nyc.long_distances <pre>                                                                                </pre> Out[18]: count(1) 2477 In\u00a0[19]: Copied! <pre>%%sql\n\nCREATE OR REPLACE VIEW nyc.negative_amounts (\n    total COMMENT 'Total amount',\n    distance COMMENT 'Trip Distance',\n    vendor_id COMMENT 'Vendor ID',\n    pickup_date,\n    dropoff_date)\n    AS SELECT total_amount, trip_distance, VendorID, tpep_pickup_datetime, tpep_dropoff_datetime\n    FROM nyc.taxis\n    WHERE total_amount &lt; 0 ORDER BY total_amount\n</pre> %%sql  CREATE OR REPLACE VIEW nyc.negative_amounts (     total COMMENT 'Total amount',     distance COMMENT 'Trip Distance',     vendor_id COMMENT 'Vendor ID',     pickup_date,     dropoff_date)     AS SELECT total_amount, trip_distance, VendorID, tpep_pickup_datetime, tpep_dropoff_datetime     FROM nyc.taxis     WHERE total_amount &lt; 0 ORDER BY total_amount Out[19]: In\u00a0[20]: Copied! <pre>%%sql\n\nSELECT * FROM nyc.negative_amounts\n</pre> %%sql  SELECT * FROM nyc.negative_amounts <pre>                                                                                </pre> Out[20]: total distance vendor_id pickup_date dropoff_date -600.3 1.29 2 2022-02-03 18:35:56 2022-02-03 18:38:57 -480.3 0.0 2 2022-01-02 17:18:54 2022-01-02 17:19:06 -408.85 116.91 2 2022-01-31 22:22:53 2022-02-01 00:34:41 -358.55 6.96 2 2022-02-01 19:32:19 2022-02-01 19:49:03 -300.3 0.0 2 2022-02-25 00:52:30 2022-02-25 00:52:35 -286.24 38.75 2 2022-02-05 03:25:27 2022-02-05 04:26:27 -273.3 0.0 2 2022-02-25 16:53:35 2022-02-25 16:53:39 -262.8 0.0 2 2022-02-26 21:14:13 2022-02-26 21:17:29 -253.3 0.1 2 2022-01-19 01:39:15 2022-01-19 01:43:05 -252.8 0.0 2 2022-02-25 00:22:02 2022-02-25 00:27:40 -252.8 0.0 2 2022-02-12 20:14:29 2022-02-12 20:14:37 -251.47 6.92 2 2022-01-09 03:43:22 2022-01-09 03:55:48 -250.3 0.0 2 2022-02-28 02:58:51 2022-02-28 02:59:00 -242.8 0.23 2 2022-01-09 03:01:45 2022-01-09 03:07:34 -232.8 0.0 2 2022-01-09 01:04:41 2022-01-09 01:04:49 -222.5 0.0 2 2022-02-21 22:04:51 2022-02-21 22:04:58 -220.1 75.79 2 2022-02-17 16:05:40 2022-02-17 17:49:26 -218.85 0.0 2 2022-02-06 06:58:13 2022-02-06 07:05:14 -218.85 72.06 2 2022-02-03 01:16:51 2022-02-03 02:22:09 -213.7 0.0 2 2022-01-17 10:41:45 2022-01-17 10:43:46 -207.48 51.16 2 2022-01-14 19:25:02 2022-01-14 20:32:19 -203.3 0.02 2 2022-02-16 22:28:37 2022-02-16 22:29:07 -202.8 43.35 2 2022-02-01 23:31:22 2022-02-02 00:24:03 -202.24 0.0 2 2022-01-30 23:16:31 2022-01-30 23:16:40 -200.6 47.59 2 2022-02-25 13:07:48 2022-02-25 14:19:56 -200.3 0.0 2 2022-02-05 04:43:18 2022-02-05 04:43:36 -200.3 0.0 2 2022-02-05 04:45:10 2022-02-05 04:45:18 -199.8 0.96 2 2022-02-21 15:53:02 2022-02-21 16:04:15 -195.6 35.16 2 2022-02-25 17:27:22 2022-02-25 18:25:38 -190.3 17.94 2 2022-01-20 01:13:17 2022-01-20 01:29:08 -177.8 0.35 2 2022-01-27 23:06:05 2022-01-27 23:09:48 -175.8 22.67 2 2022-01-22 20:43:32 2022-01-22 21:35:34 -170.6 25.87 2 2022-01-14 16:51:34 2022-01-14 17:57:44 -170.6 37.14 2 2022-02-21 16:44:09 2022-02-21 17:51:24 -164.55 0.13 2 2022-02-26 04:10:26 2022-02-26 04:10:51 -161.55 0.0 2 2022-01-19 18:57:19 2022-01-19 18:58:04 -161.55 26.88 2 2022-01-21 20:25:13 2022-01-21 21:01:03 -160.3 0.0 2 2022-02-11 19:27:39 2022-02-11 19:27:54 -158.05 46.58 2 2022-02-05 20:53:58 2022-02-05 21:42:45 -155.3 1.2 2 2022-01-31 17:49:07 2022-01-31 17:51:49 -155.0 0.06 2 2022-01-28 05:13:22 2022-01-28 05:15:48 -153.3 0.73 2 2022-02-18 12:07:54 2022-02-18 12:16:42 -152.8 0.01 2 2022-02-20 01:39:31 2022-02-20 01:48:16 -150.3 0.0 2 2022-02-12 02:50:11 2022-02-12 02:52:43 -148.1 0.0 2 2022-01-20 08:08:31 2022-01-20 08:08:41 -147.36 0.02 2 2022-01-14 23:40:09 2022-01-14 23:40:43 -145.6 0.0 2 2022-01-20 08:22:09 2022-01-20 08:22:20 -145.3 0.47 2 2022-02-23 16:44:48 2022-02-23 16:45:42 -144.6 44.83 2 2022-01-26 17:42:57 2022-01-26 19:27:50 -143.18 18.19 2 2022-01-22 00:28:31 2022-01-22 00:57:56 -142.8 0.0 2 2022-01-27 11:01:22 2022-01-27 11:01:30 -135.8 0.0 2 2022-01-05 18:26:34 2022-01-05 18:26:39 -135.05 0.54 2 2022-01-09 01:56:58 2022-01-09 01:57:16 -134.85 53.05 2 2022-01-07 15:37:09 2022-01-07 16:23:34 -133.52 0.79 2 2022-01-25 14:29:05 2022-01-25 14:33:39 -132.33 30.18 2 2022-01-30 12:27:00 2022-01-30 13:12:00 -131.6 36.69 2 2022-01-05 16:38:10 2022-01-05 18:08:35 -130.3 30.34 2 2022-01-14 09:18:04 2022-01-14 09:52:14 -130.0 0.0 2 2022-02-03 14:19:59 2022-02-03 14:20:05 -127.05 0.34 2 2022-01-25 21:43:34 2022-01-25 21:44:52 -127.05 27.85 2 2022-02-19 18:40:59 2022-02-19 19:21:36 -122.8 0.0 2 2022-02-27 05:10:28 2022-02-27 05:10:42 -121.05 32.03 2 2022-02-15 22:03:23 2022-02-15 22:39:40 -120.24 0.0 2 2022-02-28 03:04:39 2022-02-28 03:04:45 -118.76 31.68 2 2022-02-19 16:31:19 2022-02-19 17:19:45 -118.3 0.0 2 2022-01-09 03:03:35 2022-01-09 03:05:25 -117.8 19.94 2 2022-01-03 18:28:33 2022-01-03 19:12:16 -114.55 12.75 2 2022-01-19 14:34:01 2022-01-19 14:58:55 -111.61 0.0 2 2022-02-21 06:05:37 2022-02-21 06:06:04 -111.55 20.28 2 2022-02-17 01:31:55 2022-02-17 04:04:54 -110.05 19.83 2 2022-01-21 16:37:05 2022-01-21 17:38:25 -108.81 1.18 2 2022-01-20 17:16:29 2022-01-20 17:23:30 -108.8 0.0 2 2022-02-27 14:57:18 2022-02-27 14:57:23 -108.35 26.1 2 2022-01-22 10:03:44 2022-01-22 10:48:46 -107.3 0.0 2 2022-01-01 15:45:04 2022-01-01 15:45:08 -107.3 0.0 2 2022-02-13 19:49:09 2022-02-13 19:49:20 -106.65 46.67 2 2022-01-12 21:27:20 2022-01-12 22:02:16 -105.85 15.88 2 2022-01-24 14:44:14 2022-01-24 15:15:13 -104.8 14.42 2 2022-01-03 17:26:15 2022-01-03 17:48:00 -103.65 46.46 2 2022-02-21 17:24:31 2022-02-21 18:12:13 -103.3 0.0 2 2022-01-16 03:21:02 2022-01-16 03:21:58 -103.3 9.52 2 2022-02-22 12:24:25 2022-02-22 15:26:29 -103.1 27.13 2 2022-01-05 06:49:22 2022-01-05 08:37:14 -102.8 14.33 2 2022-01-01 16:18:51 2022-01-01 16:41:48 -101.6 47.01 2 2022-01-11 06:19:19 2022-01-11 07:03:04 -101.55 12.42 2 2022-01-20 14:02:48 2022-01-20 14:21:12 -100.3 0.0 2 2022-02-21 12:22:31 2022-02-21 12:22:43 -100.29 0.0 2 2022-02-11 03:31:44 2022-02-11 03:32:02 -99.55 16.17 2 2022-01-01 05:03:43 2022-01-01 05:23:59 -99.55 23.29 2 2022-02-19 03:59:55 2022-02-19 04:32:02 -99.3 0.07 2 2022-01-02 18:30:48 2022-01-02 18:30:54 -98.55 21.23 2 2022-02-20 23:35:18 2022-02-21 00:07:03 -97.6 34.07 2 2022-02-14 09:48:18 2022-02-14 10:38:11 -95.8 18.75 2 2022-02-17 17:59:17 2022-02-17 18:50:44 -95.8 31.89 2 2022-02-14 01:19:56 2022-02-14 02:38:25 -94.95 19.17 2 2022-02-27 16:38:28 2022-02-27 17:30:26 -94.9 2.23 2 2022-02-27 14:53:46 2022-02-27 15:17:28 -94.55 0.0 2 2022-02-04 00:11:35 2022-02-04 00:11:55 -94.3 5.6 2 2022-01-10 23:59:39 2022-01-11 02:46:42 -94.1 45.06 2 2022-01-28 18:08:24 2022-01-28 18:58:27 In\u00a0[21]: Copied! <pre>%%sql\n\nSHOW VIEWS in nyc\n</pre> %%sql  SHOW VIEWS in nyc Out[21]: namespace viewName isTemporary nyc long_distances False nyc negative_amounts False In\u00a0[22]: Copied! <pre>%%sql\n\nSHOW VIEWS in nyc LIKE '*neg*'\n</pre> %%sql  SHOW VIEWS in nyc LIKE '*neg*' Out[22]: namespace viewName isTemporary nyc negative_amounts False In\u00a0[23]: Copied! <pre>%%sql\n\nDESCRIBE nyc.long_distances\n</pre> %%sql  DESCRIBE nyc.long_distances Out[23]: col_name data_type comment distance double Trip Distance total double Total amount vendor_id bigint Vendor ID pickup_date timestamp dropoff_date timestamp In\u00a0[24]: Copied! <pre>%%sql\n\nDESCRIBE EXTENDED nyc.long_distances\n</pre> %%sql  DESCRIBE EXTENDED nyc.long_distances Out[24]: col_name data_type comment distance double Trip Distance total double Total amount vendor_id bigint Vendor ID pickup_date timestamp dropoff_date timestamp # Detailed View Information Comment View Catalog and Namespace demo.nyc View Query Output Columns [trip_distance, total_amount, VendorID, tpep_pickup_datetime, tpep_dropoff_datetime] View Properties ['format-version' = '1', 'location' = 's3://warehouse/nyc/long_distances', 'provider' = 'iceberg'] Created By Spark 3.5.5 In\u00a0[25]: Copied! <pre>%%sql\n\nSHOW CREATE TABLE nyc.long_distances\n</pre> %%sql  SHOW CREATE TABLE nyc.long_distances Out[25]: createtab_stmt CREATE VIEW demo.nyc.long_distances (  distance COMMENT 'Trip Distance',  total COMMENT 'Total amount',  vendor_id COMMENT 'Vendor ID',  pickup_date,  dropoff_date)TBLPROPERTIES (  'format-version' = '1',  'location' = 's3://warehouse/nyc/long_distances',  'provider' = 'iceberg')ASSELECT trip_distance, total_amount, VendorID, tpep_pickup_datetime, tpep_dropoff_datetime    FROM nyc.taxis    WHERE trip_distance &gt; 35 ORDER BY total_amount, trip_distance In\u00a0[27]: Copied! <pre>%%sql\n\nSHOW TBLPROPERTIES nyc.long_distances\n</pre> %%sql  SHOW TBLPROPERTIES nyc.long_distances Out[27]: key value location s3://warehouse/nyc/long_distances provider iceberg format-version 1 In\u00a0[28]: Copied! <pre>%%sql\n\nALTER VIEW nyc.long_distances SET TBLPROPERTIES ('key1' = 'val1', 'key2' = 'val2', 'comment' = 'This is a view comment')\n</pre> %%sql  ALTER VIEW nyc.long_distances SET TBLPROPERTIES ('key1' = 'val1', 'key2' = 'val2', 'comment' = 'This is a view comment') Out[28]: In\u00a0[29]: Copied! <pre>%%sql\n\nSHOW TBLPROPERTIES nyc.long_distances\n</pre> %%sql  SHOW TBLPROPERTIES nyc.long_distances Out[29]: key value location s3://warehouse/nyc/long_distances provider iceberg key1 val1 key2 val2 format-version 1 In\u00a0[30]: Copied! <pre>%%sql\n\nDESCRIBE EXTENDED nyc.long_distances\n</pre> %%sql  DESCRIBE EXTENDED nyc.long_distances Out[30]: col_name data_type comment distance double Trip Distance total double Total amount vendor_id bigint Vendor ID pickup_date timestamp dropoff_date timestamp # Detailed View Information Comment This is a view comment View Catalog and Namespace demo.nyc View Query Output Columns [trip_distance, total_amount, VendorID, tpep_pickup_datetime, tpep_dropoff_datetime] View Properties ['format-version' = '1', 'key1' = 'val1', 'key2' = 'val2', 'location' = 's3://warehouse/nyc/long_distances', 'provider' = 'iceberg'] Created By Spark 3.5.5 In\u00a0[31]: Copied! <pre>%%sql\n\nALTER VIEW nyc.long_distances UNSET TBLPROPERTIES ('key1')\n</pre> %%sql  ALTER VIEW nyc.long_distances UNSET TBLPROPERTIES ('key1') Out[31]: In\u00a0[32]: Copied! <pre>%%sql\n\nSHOW TBLPROPERTIES nyc.long_distances\n</pre> %%sql  SHOW TBLPROPERTIES nyc.long_distances Out[32]: key value location s3://warehouse/nyc/long_distances provider iceberg key2 val2 format-version 1 In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/5-iceberg-view-support/#view-support","title":"View Support\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/5-iceberg-view-support/#load-two-months-of-nyc-taxilimousine-trip-data","title":"Load Two Months of NYC Taxi/Limousine Trip Data\u00b6","text":"<p>This notebook uses the New York City Taxi and Limousine Commission Trip Record Data available on the AWS Open Data Registry. This contains data of trips taken by taxis and for-hire vehicles in New York City. This data is stored in an iceberg table called <code>taxis</code>.</p>"},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/5-iceberg-view-support/#create-the-table","title":"Create the table\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/5-iceberg-view-support/#write-a-month-of-data","title":"Write a month of data\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/5-iceberg-view-support/#create-a-view","title":"Create a view\u00b6","text":"<p>Let's create an Iceberg view to look at the longest distances travelled and the total amount of the trips.</p>"},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/5-iceberg-view-support/#update-view-to-order-results-differently","title":"Update View to order results differently\u00b6","text":"<p>The output isn't as helpful as imagined, so let's update the view and change the order of columns and the ordering of the results.</p>"},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/5-iceberg-view-support/#write-another-month-of-data","title":"Write another month of data\u00b6","text":"<p>Let's write another month of data and see how the results of the view change</p>"},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/5-iceberg-view-support/#create-another-view","title":"Create another view\u00b6","text":"<p>It appears that there are trips with negative total amounts. Let's display these results in a separate view</p>"},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/5-iceberg-view-support/#listing-and-describing-views","title":"Listing and describing views\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/5-iceberg-view-support/#displaying-the-create-statement-of-a-view","title":"Displaying the CREATE statement of a view\u00b6","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/5-iceberg-view-support/#altering-and-displaying-properties-of-a-view","title":"Altering and displaying properties of a view\u00b6","text":"<p>This will add a new property and also update the comment of the view. The comment will be shown when describing the view. The end of this section will also remove a property from the view.</p>"},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/deployment/","title":"Deployment","text":""},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/deployment/#docker-compose","title":"Docker-Compose","text":"docker-compose.yml<pre><code>services:\n  spark-iceberg:\n    image: tabulario/spark-iceberg\n    container_name: spark-iceberg\n    build: spark/\n    networks:\n      iceberg_net:\n    depends_on:\n      - rest\n      - minio\n    volumes:\n      - ./warehouse:/home/iceberg/warehouse\n      - ./notebooks:/home/iceberg/notebooks/notebooks\n    environment:\n      - AWS_ACCESS_KEY_ID=admin\n      - AWS_SECRET_ACCESS_KEY=password\n      - AWS_REGION=us-east-1\n    ports:\n      - 8888:8888\n      - 8080:8080\n      - 10000:10000\n      - 10001:10001\n  rest:\n    image: apache/iceberg-rest-fixture\n    container_name: iceberg-rest\n    networks:\n      iceberg_net:\n    ports:\n      - 8181:8181\n    environment:\n      - AWS_ACCESS_KEY_ID=admin\n      - AWS_SECRET_ACCESS_KEY=password\n      - AWS_REGION=us-east-1\n      - CATALOG_WAREHOUSE=s3://warehouse/\n      - CATALOG_IO__IMPL=org.apache.iceberg.aws.s3.S3FileIO\n      - CATALOG_S3_ENDPOINT=http://minio:9000\n  minio:\n    image: minio/minio\n    container_name: minio\n    environment:\n      - MINIO_ROOT_USER=admin\n      - MINIO_ROOT_PASSWORD=password\n      - MINIO_DOMAIN=minio\n    networks:\n      iceberg_net:\n        aliases:\n          - warehouse.minio\n    ports:\n      - 9001:9001\n      - 9000:9000\n    command: [\"server\", \"/data\", \"--console-address\", \":9001\"]\n  mc:\n    depends_on:\n      - minio\n    image: minio/mc\n    container_name: mc\n    networks:\n      iceberg_net:\n    environment:\n      - AWS_ACCESS_KEY_ID=admin\n      - AWS_SECRET_ACCESS_KEY=password\n      - AWS_REGION=us-east-1\n    entrypoint: |\n      /bin/sh -c \"\n      until (/usr/bin/mc alias set minio http://minio:9000 admin password) do echo '...waiting...' &amp;&amp; sleep 1; done;\n      /usr/bin/mc rm -r --force minio/warehouse;\n      /usr/bin/mc mb minio/warehouse;\n      /usr/bin/mc policy set public minio/warehouse;\n      tail -f /dev/null\n      \"\nnetworks:\n  iceberg_net:\n</code></pre> <p>This Docker Compose configuration sets up a complete Apache Iceberg development environment with the following components:</p>"},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/deployment/#spark-service-spark-iceberg","title":"Spark Service (<code>spark-iceberg</code>)","text":"<p>The main Spark service that provides the computational engine for working with Iceberg tables:</p> <ul> <li>Image: <code>tabulario/spark-iceberg</code> - A pre-configured Spark image with Iceberg support</li> <li>Ports: <ul> <li><code>8888</code>: Jupyter notebook interface</li> <li><code>8080</code>: Spark UI</li> <li><code>10000-10001</code>: Spark Thrift server ports</li> </ul> </li> <li>Volumes: Mounts local directories for warehouse data and notebooks</li> <li>Dependencies: Requires both the REST catalog and MinIO services to be running</li> </ul>"},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/deployment/#rest-catalog-service-rest","title":"REST Catalog Service (<code>rest</code>)","text":"<p>Apache Iceberg's REST catalog service for metadata management:</p> <ul> <li>Image: <code>apache/iceberg-rest-fixture</code> - Official Iceberg REST catalog</li> <li>Port: <code>8181</code> - REST API endpoint</li> <li>Configuration: <ul> <li>Connects to MinIO S3-compatible storage</li> <li>Uses <code>s3://warehouse/</code> as the warehouse location</li> <li>Configured with S3FileIO for object storage operations</li> </ul> </li> </ul>"},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/deployment/#minio-service-minio","title":"MinIO Service (<code>minio</code>)","text":"<p>S3-compatible object storage for storing Iceberg table data:</p> <ul> <li>Image: <code>minio/minio</code> - Open-source S3-compatible storage</li> <li>Ports:<ul> <li><code>9000</code>: S3 API endpoint</li> <li><code>9001</code>: MinIO web console</li> </ul> </li> <li>Credentials: admin/password (for development only)</li> <li>Storage: Serves data from <code>/data</code> directory inside container</li> </ul>"},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/deployment/#minio-client-service-mc","title":"MinIO Client Service (<code>mc</code>)","text":"<p>Initialization service that sets up the MinIO storage:</p> <ul> <li>Purpose: Creates and configures the warehouse bucket</li> <li>Actions:<ul> <li>Waits for MinIO to be ready</li> <li>Creates the <code>warehouse</code> bucket</li> <li>Sets public access policy for development</li> <li>Runs indefinitely to keep the service active</li> </ul> </li> </ul>"},{"location":"side-projects/retail-lakehouse/iceberg/spark-quickstart/deployment/#networking","title":"Networking","text":"<p>All services communicate through the <code>iceberg_net</code> custom network, enabling:</p> <ul> <li>Service discovery by container name</li> <li>Isolated network environment</li> <li>MinIO alias configuration for S3 compatibility</li> </ul> <pre><code>docker-compose up\n</code></pre> <p>Spark UI will be available at http://localhost:8080:</p> <p></p> <p>Then go to the notebook server available at http://localhost:8888:</p> <p></p>"},{"location":"side-projects/retail-lakehouse/observability/overview/","title":"Overview","text":"<p>Architecture Overview</p> <p>Metrics Collection Deployment Steps</p> <ul> <li> Deploy a Prometheus Operator using YAML files</li> <li> Deploy a Thanos using bitnami helm chart (Receiver, Store Gateway, Querier, Compactor, )</li> <li> Deploy a Thanos ruler using Prometheus Operator</li> <li> Deploy a Prometheus Alertmanager using Prometheus Operator</li> <li> Deploy a OpenTelemetry Operator</li> <li> Deploy a OpenTelemetry Collector using OpenTelemetry Operator</li> <li> Deploy a Grafana using helm chart</li> </ul> <pre><code>cd ~/Projects/retail-lakehouse/observability\nbash ./install.sh\n</code></pre> <p>The <code>install.sh</code> script in the <code>observability</code> directory is as follows:</p> install.sh <pre><code>#!/bin/bash\n\nset -euo pipefail\n\nNAMESPACE=observability\n\nkubectl create namespace $NAMESPACE || true\n\n# Install Prometheus Operator using YAML files\n# https://prometheus-operator.dev/docs/getting-started/installation/#install-using-yaml-files\nTMPDIR=$(mktemp -d)\nLATEST=v0.85.0\ncurl -s \"https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/refs/tags/$LATEST/kustomization.yaml\" &gt; \"$TMPDIR/kustomization.yaml\"\ncurl -s \"https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/refs/tags/$LATEST/bundle.yaml\" &gt; \"$TMPDIR/bundle.yaml\"\n(cd $TMPDIR &amp;&amp; kustomize edit set namespace $NAMESPACE) &amp;&amp; kubectl create -k \"$TMPDIR\"\nsleep 5\nkubectl wait --for=condition=Ready pods -l  app.kubernetes.io/name=prometheus-operator -n $NAMESPACE --timeout=180s\n\n# Deploy Jaeger using Otel Operator\n# \ncd ~/Projects/retail-lakehouse/observability/jaeger\nCERT_MANAGER_VERSION=v1.18.2\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/$CERT_MANAGER_VERSION/cert-manager.yaml\nOTEL_OPERATOR_VERSION=v0.132.0\nkubectl apply -f https://github.com/open-telemetry/opentelemetry-operator/releases/download/$OTEL_OPERATOR_VERSION/opentelemetry-operator.yaml\nkubectl create namespace jaeger || true\nkubectl apply -f otel-collector-jaeger.yaml -n  jaeger\n\n# Deploy Thanos using kube-thanos jsonnet\n# https://github.com/thanos-io/kube-thanos\ncd ~/Projects/retail-lakehouse/observability/thanos\n\nkubectl create namespace thanos || true\njb install github.com/thanos-io/kube-thanos/jsonnet/kube-thanos@main\nrm -f manifests/thanos-*\njsonnet -J vendor -m manifests/ thanos.jsonnet | xargs -I{} sh -c \"cat {} | yq -P &gt; {}.yaml; rm -f {}\" -- {}\n\nkubectl apply -f manifests/\n</code></pre>"},{"location":"side-projects/retail-lakehouse/observability/overview/#manually","title":"Manually","text":"<pre><code>kubectl apply -f manifests/prometheus-alertmanager.yaml\nkubectl get all -n thanos\n</code></pre> <p>Result</p> <pre><code>NAME                                  READY   STATUS    RESTARTS   AGE\npod/alertmanager-alertmanager-sre-0   2/2     Running   0          2m2s\npod/alertmanager-alertmanager-sre-1   2/2     Running   0          2m2s\n\nNAME                            TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                      AGE\nservice/alertmanager-operated   ClusterIP   None         &lt;none&gt;        9093/TCP,9094/TCP,9094/UDP   2m2s\n\nNAME                                             READY   AGE\nstatefulset.apps/alertmanager-alertmanager-sre   2/2     2m2s\n</code></pre> <pre><code>kubectl apply -f manifests/memcached.yaml\nk get all -n thanos\n</code></pre> <p>Result</p> <pre><code>NAME                                  READY   STATUS    RESTARTS   AGE\npod/alertmanager-alertmanager-sre-0   2/2     Running   0          6m22s\npod/alertmanager-alertmanager-sre-1   2/2     Running   0          6m22s\npod/memcached-0                       1/1     Running   0          44s\npod/memcached-1                       1/1     Running   0          33s\npod/memcached-2                       1/1     Running   0          22s\n\nNAME                            TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                      AGE\nservice/alertmanager-operated   ClusterIP   None         &lt;none&gt;        9093/TCP,9094/TCP,9094/UDP   6m22s\nservice/memcached               ClusterIP   None         &lt;none&gt;        11211/TCP                    44s\n\nNAME                                             READY   AGE\nstatefulset.apps/alertmanager-alertmanager-sre   2/2     6m22s\nstatefulset.apps/memcached                       3/3     44s\n</code></pre> <p>Traces Collection Deployment Steps</p> <ul> <li> Deploy a Jaeger</li> <li> Deploy a OpenTelemetry Collector</li> <li> Trino jmx</li> </ul>"},{"location":"side-projects/retail-lakehouse/observability/opentelemetry/how-opentelemetry-works/","title":"How OpenTelemetry Works?","text":"<p>For a system to be observable, it must be instrumented: that is, code from the system's components must emit signals, such as traces, metrics, and logs.</p> <p>Using OpenTelemetry, you can instrument your code in two primary ways:</p> <ul> <li>Zero-code solutions are great for getting started, or when you can't modify the application you need to get telemetry out of.</li> <li>Code-based solutions via official APIs and SDKs for most languages. This acts as an essential complement to the telemetry generated by zero-code solutions.</li> </ul> <p></p> <p>Zero-code Instrumentation</p> <p>See Instrumentation for more details.</p>","tags":["OpenTelemetry","SRE"]},{"location":"side-projects/retail-lakehouse/observability/opentelemetry/how-opentelemetry-works/#core-concepts","title":"Core Concepts","text":"","tags":["OpenTelemetry","SRE"]},{"location":"side-projects/retail-lakehouse/observability/opentelemetry/how-opentelemetry-works/#resource","title":"Resource","text":"<p>a Resource represents the entity that produces telemetry data. It provides metadata such as service name, version, host, or cloud environment, giving context about who emitted the signals. Without it, traces, metrics, and logs lack the identity needed for meaningful analysis.</p>","tags":["OpenTelemetry","SRE"]},{"location":"side-projects/retail-lakehouse/observability/opentelemetry/how-opentelemetry-works/#signals","title":"Signals","text":"<p>Signals are system outputs that describe the underlying activity of the operating system and applications running on a platform. It includes three primary types: logs, metrics, and traces.</p>","tags":["OpenTelemetry","SRE"]},{"location":"side-projects/retail-lakehouse/observability/opentelemetry/how-opentelemetry-works/#span","title":"Span","text":"<p>A span represents a unit of work or operation. Spans are the building blocks of Traces.</p>","tags":["OpenTelemetry","SRE"]},{"location":"side-projects/retail-lakehouse/observability/opentelemetry/how-opentelemetry-works/#traces","title":"Traces","text":"<p>Traces give us the big picture of what happens when a request is made to an application. One way to think of Traces is that they're a collection of structured logs with context, correlation, hierarchy, and more baked in.</p> <p>Here is an example Trace made up of 6 Spans:</p> <pre><code>Causal relationships between Spans in a single Trace\n\n        [Span A]  \u2190\u2190\u2190(the root span)\n            |\n     +------+------+\n     |             |\n [Span B]      [Span C] \u2190\u2190\u2190(Span C is a `child` of Span A)\n     |             |\n [Span D]      +---+-------+\n               |           |\n           [Span E]    [Span F]\n</code></pre> <p>Here is a temporal view of the same Trace:</p> <pre><code>Temporal relationships between Spans in a single Trace\n\n\u2013\u2013|\u2013\u2013\u2013\u2013\u2013\u2013\u2013|\u2013\u2013\u2013\u2013\u2013\u2013\u2013|\u2013\u2013\u2013\u2013\u2013\u2013\u2013|\u2013\u2013\u2013\u2013\u2013\u2013\u2013|\u2013\u2013\u2013\u2013\u2013\u2013\u2013|\u2013\u2013\u2013\u2013\u2013\u2013\u2013|\u2013\u2013\u2013\u2013\u2013\u2013\u2013|\u2013&gt; time\n\n [Span A\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7]\n   [Span B\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7]\n      [Span D\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7]\n    [Span C\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7]\n         [Span E\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7]        [Span F\u00b7\u00b7]\n</code></pre>","tags":["OpenTelemetry","SRE"]},{"location":"side-projects/retail-lakehouse/observability/opentelemetry/how-opentelemetry-works/#metrics","title":"Metrics","text":"<p>A metric is a measurement of a service captured at runtime. The moment of capturing a measurement is known as a metric event</p> <pre><code>+------------------+\n| MeterProvider    |                 +-----------------+             +--------------+\n|   Meter A        | Measurements... |                 | Metrics...  |              |\n|     Instrument X +-----------------&gt; In-memory state +-------------&gt; MetricReader |\n|     Instrument Y |                 |                 |             |              |\n|   Meter B        |                 +-----------------+             +--------------+\n|     Instrument Z |\n|     ...          |                 +-----------------+             +--------------+\n|     ...          | Measurements... |                 | Metrics...  |              |\n|     ...          +-----------------&gt; In-memory state +-------------&gt; MetricReader |\n|     ...          |                 |                 |             |              |\n|     ...          |                 +-----------------+             +--------------+\n+------------------+\n</code></pre>","tags":["OpenTelemetry","SRE"]},{"location":"side-projects/retail-lakehouse/observability/opentelemetry/how-opentelemetry-works/#logs","title":"Logs","text":"<p>A log is a timestamped text record, either structured (recommended) or unstructured, with optional metadata. OpenTelemetry's support for logs is designed to be fully compatible with what you already have, providing capabilities to wrap those logs with additional context and a common toolkit to parse and manipulate logs into a common format across many different sources.</p>","tags":["OpenTelemetry","SRE"]},{"location":"side-projects/retail-lakehouse/observability/opentelemetry/how-opentelemetry-works/#baggage","title":"Baggage","text":"<p>Baggage is often used in tracing to propagate additional data across services. Additionally, instrumentations automatically propagate baggage for you. Baggage is best used to include information typically available only at the start of a request further downstream (Account ID, User ID, Product ID, origin IPs, etc.)</p> <p></p> <p>Baggage Propagation</p> <p></p> <p>Baggage Use Case</p>","tags":["OpenTelemetry","SRE"]},{"location":"side-projects/retail-lakehouse/observability/opentelemetry/how-opentelemetry-works/#architecture-components","title":"Architecture Components","text":"<ul> <li>Specification: API, SDK, and Data (OLTP, Semantic Conventions)</li> <li>Collector (a vendor-agnostic way to receive, process and export telemetry data.)<ul> <li>Receivers</li> <li>Processors</li> <li>Connectors</li> <li>Exporters</li> </ul> </li> <li>Language-specific API &amp; SDK implementations</li> <li>Kubernetes operator</li> <li>Function as a Service assets</li> </ul> <p>Data Flow Overview</p> <p></p> <p>Ingress/Egress Metrics</p> <p></p> <p>Collector</p>","tags":["OpenTelemetry","SRE"]},{"location":"side-projects/retail-lakehouse/observability/opentelemetry/how-opentelemetry-works/#behind-the-scenes","title":"Behind the Scenes","text":"","tags":["OpenTelemetry","SRE"]},{"location":"side-projects/retail-lakehouse/observability/opentelemetry/how-opentelemetry-works/#references","title":"References","text":"<ul> <li>OpenTelemetry Official Documentation</li> </ul>","tags":["OpenTelemetry","SRE"]},{"location":"side-projects/retail-lakehouse/observability/opentelemetry/opentelemetry-deployment/","title":"OpenTelemetry Deployment","text":""},{"location":"side-projects/retail-lakehouse/observability/opentelemetry/opentelemetry-deployment/#deployment-patterns","title":"Deployment Patterns","text":"<p>Patterns you can apply to deploy the OpenTelemetry collector:</p> <ul> <li>No Collector</li> <li>Agent</li> <li>Gateway</li> </ul> <p></p> <p>No Collector</p> <p></p> <p>Agent</p> <p></p> <p>Gateway</p>"},{"location":"side-projects/retail-lakehouse/observability/opentelemetry/opentelemetry-deployment/#k8s","title":"K8S","text":"<p>There are 2 ways of deploying OpenTelemetry Collector on Kubernetes:</p> <ul> <li>OpenTelemetry Collector Helm Chart. This helm chart can be used to install a collector as a <code>Deployment</code>, <code>Daemonset</code>, or <code>Statefulset</code>.</li> <li>OpenTelemetry Operator for Kubernetes</li> </ul> <p>Note</p> <p>By default, <code>opentelemetry-operator</code> uses the <code>opentelemetry-collector</code> image. When the operator is installed using Helm charts, the <code>opentelemetry-collector-k8s</code> image is used. If you need a component not found in these releases, you may need to build your own collector.</p> <p>You can refer to the Install the Collector | OpenTelemetry Docs and Kubernetes Getting Started | OpenTelemetry Docs for more details.</p> <p>We'll use <code>OpenTelemetry Operator for Kubernetes</code> to deploy OpenTelemetry Collector.</p>"},{"location":"side-projects/retail-lakehouse/observability/opentelemetry/what-is-opentelemetry/","title":"What is OpenTelemetry?","text":"<p>Quote</p> <p>OpenTelemetry is an observability framework and toolkit designed to facilitate the generation, export, collection of telemetry data such as traces, metrics, and logs.</p> <p>OpenTelemetry is not an observability backend itself. --- OpenTelemetry</p> <p>What is OTel? | OTel for Beginners</p>","tags":["OpenTelemetry","SRE"]},{"location":"side-projects/retail-lakehouse/observability/opentelemetry/what-is-opentelemetry/#benefits-of-using-otel","title":"Benefits of using OTel","text":"<p>Users</p> <ul> <li>No vendor lock in</li> <li>Learn a single set of APls and conventions</li> <li>Send the data to any observability backend vendor that adopts this standard</li> </ul> <p>Vendors</p> <ul> <li>Customers may prefer to adopt open standards rather than closed and proprietary solutions.</li> <li>Reduce support and implementation costs</li> <li>Vendors can focus on their differentiating factors</li> </ul>","tags":["OpenTelemetry","SRE"]},{"location":"side-projects/retail-lakehouse/observability/prometheus/how-prometheus-works/","title":"How Prometheus Works?","text":"","tags":["Prometheus","SRE"]},{"location":"side-projects/retail-lakehouse/observability/prometheus/how-prometheus-works/#core-concepts","title":"Core Concepts","text":"","tags":["Prometheus","SRE"]},{"location":"side-projects/retail-lakehouse/observability/prometheus/how-prometheus-works/#architecture-components","title":"Architecture Components","text":"<p>Architecture Overview</p> <ul> <li>the main Prometheus server which scrapes and stores time series data</li> <li>client libraries for instrumenting application code</li> <li>a push gateway for supporting short-lived jobs</li> <li>special-purpose exporters for services like HAProxy, StatsD, Graphite, etc.</li> <li>an alertmanager to handle alerts</li> <li>various support tools</li> </ul>","tags":["Prometheus","SRE"]},{"location":"side-projects/retail-lakehouse/observability/prometheus/how-prometheus-works/#prometheus-agent","title":"Prometheus Agent","text":"<p>Prometheus Agent is a deployment model optimized for environments where all collected data is forwarded to a long-term storage solution.</p> <p>Similarly to Prometheus, Prometheus Agent will also require permission to scrape targets. Because of this, we will create a new service account for the Agent with the necessary permissions to scrape targets.</p> <p>See Introducing Prometheus Agent Mode, an Efficient and Cloud-Native Way for Metric Forwarding for more details.</p>","tags":["Prometheus","SRE"]},{"location":"side-projects/retail-lakehouse/observability/prometheus/how-prometheus-works/#behind-the-scenes","title":"Behind the Scenes","text":"","tags":["Prometheus","SRE"]},{"location":"side-projects/retail-lakehouse/observability/prometheus/how-prometheus-works/#references","title":"References","text":"","tags":["Prometheus","SRE"]},{"location":"side-projects/retail-lakehouse/observability/prometheus/prometheus-deployment/","title":"Prometheus Deployment","text":""},{"location":"side-projects/retail-lakehouse/observability/prometheus/prometheus-deployment/#k8s","title":"K8S","text":"<p>There are 3 ways of deploying Prometheus on Kubernetes:</p> <ul> <li>prometheus-operator/prometheus-operator</li> <li>prometheus-operator/kube-prometheus</li> <li>kube-prometheus-stack in prometheus-community/helm-charts</li> </ul> <p>We use the first option to deploy Prometheus Operator using YAML files.</p>"},{"location":"side-projects/retail-lakehouse/observability/prometheus/prometheus-deployment/#references","title":"References","text":"<ul> <li>Thanos and the Prometheus Operator | Prometheus Operator Docs</li> <li>Design | Thanos Docs</li> </ul>"},{"location":"side-projects/retail-lakehouse/observability/thanos/how-thanos-works/","title":"How Thanos Works?","text":"","tags":["Prometheus","Thanos","SRE"]},{"location":"side-projects/retail-lakehouse/observability/thanos/how-thanos-works/#architecture-overview","title":"Architecture Overview","text":"<ul> <li>Quick Tutorial has a simple introduction to Thanos architecture and components.</li> <li>Design has a more detailed description of the architecture and components.</li> </ul> <p>Thanos is comprised of a set of components where each fulfills a specific role</p> <ul> <li>Sidecar: connects to Prometheus, remote reads its data for query and/or uploads it to cloud storage.</li> <li>Ruler: evaluates recording and alerting rules against data in Thanos for exposition and/or upload. It can discover query nodes to evaluate recording and alerting rules.</li> <li>Store Gateway: Since store nodes and data sources expose the same gRPC Store API, clients can largely treat them as equivalent and don't have to be concerned with which specific component they are querying. It is purely a data retrieval API and does not provide complex query execution.</li> <li>Queriers: Queriers are stateless and horizontally scalable instances that implement PromQL on top of the Store APIs exposed in the cluster. Queriers participate in the cluster to be able to resiliently discover all data sources and store nodes. It implements Prometheus's v1 API to aggregate data. It is also capable of deduplicating data collected from Prometheus HA pairs.</li> <li>Compactor: a singleton process that does not participate in the Thanos cluster. Instead, it is only pointed at an object storage bucket and continuously consolidates multiple smaller blocks into larger ones. The compactor also does additional batch processing such as down-sampling and applying retention policies.</li> <li>Receiver: receives data from Prometheus's remote write WAL, exposes it, and/or uploads it to cloud storage.</li> </ul> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Prometheus \u2502 Sidecar \u2502   ...   \u2502 Prometheus \u2502 Sidecar \u2502     \u2502   Rule  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518     \u2514\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502                                \u2502           \u2502\n                Blocks                           Blocks      Blocks\n                  \u2502                                \u2502           \u2502\n                  v                                v           v\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502                   Object Storage                 \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Google Cloud Storage \u2502  \u2502 Prometheus \u2502 Sidecar \u2502   \u2502    Rule    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502                         \u2502          \u2502\n         Block File Ranges                  \u2502          \u2502\n                  \u2502                     Store API      \u2502\n                  v                         \u2502          \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502          \u2502\n                \u2502     Store    \u2502            \u2502      Store API\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518            \u2502          \u2502\n                         \u2502                  \u2502          \u2502\n                     Store API              \u2502          \u2502\n                         \u2502                  \u2502          \u2502\n                         v                  v          v\n                       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                       \u2502              Client              \u2502\n                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    Store Node    \u2502  \u2502 Prometheus \u2502 Sidecar \u2502   \u2502    Rule    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502                         \u2502          \u2502\n              \u2502                         \u2502          \u2502\n              \u2502                         \u2502          \u2502\n              v                         v          v\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                      Query layer                    \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                ^                  ^                  ^\n                \u2502                  \u2502                  \u2502\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510\n       \u2502 Alert Component \u2502  \u2502 Dashboards \u2502  ...  \u2502 Web UI \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Note</p> <ul> <li>For a given Prometheus instance, it is not recommended to run both Sidecar and Receiver simultaneously.</li> <li>If your goal is to have each Prometheus instance self-manage while still allowing Querier to connect directly to each Prometheus for real-time data, then the most natural choice is to use Sidecar. It sits alongside Prometheus, exposing data to Thanos Querier and handling the upload of historical blocks to object storage.</li> <li>Conversely, if you want to centralize the reception of metrics from multiple sources or tenants, making the front-end Prometheus instances lighter, or planning a centralized write pipeline, then using Receiver is more appropriate. It uses <code>remote_write</code> to accept data from each Prometheus and maintains a unified storage and query interface on the backend.</li> </ul>","tags":["Prometheus","Thanos","SRE"]},{"location":"side-projects/retail-lakehouse/observability/thanos/how-thanos-works/#behind-the-scenes","title":"Behind the Scenes","text":"","tags":["Prometheus","Thanos","SRE"]},{"location":"side-projects/retail-lakehouse/observability/thanos/how-thanos-works/#thanos-receiver-deep-dive","title":"Thanos Receiver Deep Dive","text":"<p>Thanos Receiver Deep Dive - Joel Verezhak, Open Systems (2024)</p> <p>Turn It Up to a Million: Ingesting Millions of Metrics with Thanos Receive - Lucas Serv\u00e9n Mar\u00edn</p> <p>Handling Billions of Metrics with Prometheus and Thanos - Ravi Hari &amp; Amit Auddy, Intuit</p>","tags":["Prometheus","Thanos","SRE"]},{"location":"side-projects/retail-lakehouse/observability/thanos/how-thanos-works/#how-thanos-stores-data","title":"How Thanos Stores Data","text":"<ul> <li>Chunk Files: hold a few hundred MB worth of chunks each. Chunks for the same series are sequentially aligned.</li> <li>Index File: holds all information needed to look up specific series</li> <li><code>meta.json</code> File: holds meta-information about a block metadata</li> </ul> <pre><code>01BX6V6TY06G5MFQ0GPH7EMXRH\n\u251c\u2500\u2500 chunks\n\u2502   \u251c\u2500\u2500 000001\n\u2502   \u251c\u2500\u2500 000002\n\u2502   \u2514\u2500\u2500 000003\n\u251c\u2500\u2500 index\n\u2514\u2500\u2500 meta.json\n</code></pre> <p>See Data in Object Storage to understand how Thanos organizes data in object storage.</p>","tags":["Prometheus","Thanos","SRE"]},{"location":"side-projects/retail-lakehouse/observability/thanos/how-thanos-works/#references","title":"References","text":"<ul> <li>Thanos | Prometheus Operator Docs</li> </ul>","tags":["Prometheus","Thanos","SRE"]},{"location":"side-projects/retail-lakehouse/observability/thanos/thanos-deployment/","title":"Thanos Deployment","text":""},{"location":"side-projects/retail-lakehouse/observability/thanos/thanos-deployment/#k8s","title":"K8S","text":"<p>There are 3 ways of deploying Thanos on Kubernetes:</p> <ul> <li>Community Helm charts</li> <li>prometheus-operator</li> <li>kube-thanos: Jsonnet based Kubernetes templates.</li> </ul> <p>We use kube-thanos to deploy Thanos components in this project.</p>"},{"location":"side-projects/retail-lakehouse/observability/thanos/thanos-deployment/#prerequisites","title":"Prerequisites","text":"<p>When deploying Thanos in Kubernetes, Ruler would hit \"too many open files\" error. The reason is minikube's default <code>fs.inotify.max_user_instances</code> is <code>128</code>, which is too small. Hence we need to increase it.</p> <p>First, ssh into minikube VM:</p> <pre><code>minikube -p retail-lakehouse ssh\n</code></pre> <p>Inside minikube VM, check the current value of fs.inotify.max_user_instances, fs.inotify.max_user_watches, and file-max</p> <pre><code>sysctl fs.inotify.max_user_watches\nsysctl fs.inotify.max_user_instances\ncat /proc/sys/fs/file-max\n</code></pre> <p>Output should be like below:</p> <pre><code>fs.inotify.max_user_watches = 1048576\nfs.inotify.max_user_instances = 128\n9223372036854775807\n</code></pre> <p>Increase <code>fs.inotify.max_user_instances</code> to <code>1024</code> by running the following command:</p> <pre><code>sudo sysctl -w fs.inotify.max_user_instances=1024\n</code></pre> <p>After that, you can exit minikube VM:</p> <pre><code>exit\n</code></pre>"},{"location":"side-projects/retail-lakehouse/observability/thanos/thanos-deployment/#references","title":"References","text":"<ul> <li>Community Thanos Kubernetes Applications | Thanos Docs </li> </ul>"},{"location":"side-projects/retail-lakehouse/trino/catalogs/","title":"Configure Catalogs","text":"<p>The following catalogs are configured:</p> trino/values-template.yaml - Catalogs Configuration<pre><code>catalogs:\n  faker: |\n    connector.name=faker\n    faker.null-probability=0.1\n    faker.default-limit=1000\n    faker.locale=en\n  bigquery: |\n    connector.name=bigquery\n    bigquery.project-id=$GCP_BQ_PROJECT_ID\n    bigquery.credentials-file=/etc/trino/bigquery/trino-sa.json\n  iceberg: |\n    connector.name=iceberg\n    iceberg.catalog.type=glue\n    iceberg.file-format=parquet\n    hive.metastore.glue.region=$AWS_REGION\n    hive.metastore.glue.default-warehouse-dir=$ICEBERG_S3_URL\n    hive.metastore.glue.aws-access-key=$AWS_ACCESS_KEY\n    hive.metastore.glue.aws-secret-key=$AWS_SECRET_KEY\n    fs.native-s3.enabled=true\n    s3.region=$AWS_REGION\n    s3.aws-access-key=$AWS_ACCESS_KEY\n    s3.aws-secret-key=$AWS_SECRET_KEY\n\n# https://trino.io/docs/current/connector/iceberg.html#authorization\n# https://trino.io/docs/current/object-storage/file-system-s3.html\n</code></pre>"},{"location":"side-projects/retail-lakehouse/trino/catalogs/#bigquery","title":"BigQuery","text":""},{"location":"side-projects/retail-lakehouse/trino/catalogs/#iceberg","title":"Iceberg","text":""},{"location":"side-projects/retail-lakehouse/trino/catalogs/#faker","title":"Faker","text":""},{"location":"side-projects/retail-lakehouse/trino/deployment/","title":"Trino Cluster Deployment","text":"<p>Prerequisites</p> <ul> <li> Complete all the requirements in the Prerequisites section.</li> <li> OAuth 2.0 Client ID and Client Secret (<code>OAUTH2_CLIENT_ID</code> and <code>OAUTH2_CLIENT_SECRET</code>) for Google authentication. See here for instructions.</li> <li> GCP service account path in your local machine (<code>GCP_SA_INPUT_PATH</code>) for accessing BigQuery Datasets under your GCP project (<code>GCP_PROJECT_ID</code>). See here for instructions.</li> <li> AWS credentials (<code>AWS_ACCESS_KEY</code> and <code>AWS_SECRET_KEY</code>), regions (<code>AWS_REGION</code>), and S3 bucket (<code>ICEBERG_S3_URL</code>) for accessing Iceberg table and Glue Data Catalog. See here for instructions.</li> <li> AWS S3 Bucket for Exchange Manager (<code>EXCHANGE_S3_URLS</code>). See here for instructions.</li> </ul> <p>Without further ado, let's get started with the deployment:</p> <pre><code>cd ~/Projects/retail-lakehouse/trino\nbash install.sh\n</code></pre> <p>The installation script <code>install.sh</code> will perform the following steps:</p> <ol> <li>Generate <code>.env</code> and <code>values.yaml</code> files</li> <li>Create the <code>trino</code> namespace.</li> <li>Generate TLS certificates and create the Kubernetes secret.</li> <li>Generate the BigQuery service account secret and create the Kubernetes secret.</li> <li>Install Trino using Helm with the generated <code>values.yaml</code>.</li> </ol> install.sh <pre><code>#!/bin/bash\n\nset -euo pipefail\n\n# \u7522\u751f .env \u548c values.yaml \u6a94\u6848\nbash generate-env.sh\n\n\n# \u8f09\u5165 .env \u7576\u4f5c\u74b0\u5883\u8b8a\u6578\nset -a\n# shellcheck disable=SC1090\nsource \".env\"\nset +a\n\nkubectl create namespace trino || true\n\nbash generate-tls-certs.sh\nkubectl apply -f ./trino-tls-secret.yaml -n trino\n\necho \"Generating Kubernetes secret for Accessing BigQuery...\"\nkubectl create secret generic trino-bigquery-secret \\\n    --from-file=trino-sa.json=\"$GCP_SA_INPUT_PATH\" \\\n    --dry-run=client -o yaml &gt; \"./trino-bigquery-secret.yaml\"\necho \"trino-bigquery-secret.yaml generated successfully.\"\nkubectl apply -f ./trino-bigquery-secret.yaml -n trino\n\nhelm repo add trino https://trinodb.github.io/charts/\nhelm repo update\nhelm install trino trino/trino \\\n  -f values.yaml \\\n  -n trino \\\n  --version 1.39.1 \\\n</code></pre> <p>For the script to work correctly, you need to set the following environment variables during the execution of the script. Another option is to set them in a <code>.env</code> file in the same directory as the script.</p> .env <ul> <li> <p><code>INTERNAL_SHARED_SECRET</code>: A secret string for internal communication security between Trino nodes. Used as <code>internal-communication.shared-secret</code>.</p> </li> <li> <p><code>OAUTH2_CLIENT_ID</code>: The client ID for OAuth 2.0 authentication, used to enable Google login for the Trino Web UI. Referenced as <code>http-server.authentication.oauth2.client-id</code>.</p> </li> <li> <p><code>OAUTH2_CLIENT_SECRET</code>: The client secret for OAuth 2.0 authentication, paired with the client ID for secure login. Used as <code>http-server.authentication.oauth2.client-secret</code>.</p> </li> <li> <p><code>GCP_BQ_PROJECT_ID</code>: The Google Cloud project ID, required for the BigQuery connector in Trino. Used as <code>bigquery.project-id</code>.</p> </li> <li> <p><code>GCP_SA_INPUT_PATH</code>: Path to the service account JSON file for Google Cloud authentication. Used to create the BigQuery service account K8S secret.</p> </li> <li> <p><code>AWS_ACCESS_KEY</code>: AWS access key for authenticating to AWS services (S3, Glue, etc.). Used for S3 and Glue access, and for exchange manager S3 configuration.</p> </li> <li> <p><code>AWS_SECRET_KEY</code>: AWS secret key, paired with the access key for AWS authentication. Used for S3, Glue, and exchange manager S3 configuration.</p> </li> <li> <p><code>AWS_REGION</code>: The AWS region where your S3 buckets and Glue Data Catalog are located. Used for S3, Glue, and exchange manager S3 configuration.</p> </li> <li> <p><code>ICEBERG_S3_URL</code>: The S3 URL (bucket path) for storing Iceberg table data. Used as <code>hive.metastore.glue.default-warehouse-dir</code>.</p> </li> <li> <p><code>EXCHANGE_S3_URLS</code>: S3 URLs for Trino's exchange manager, which handles intermediate data during distributed query execution. Used as <code>exchangeManager.baseDir</code>.</p> </li> </ul> <p>These variables are substituted into the Trino Helm values file and Kubernetes secrets using <code>envsubst</code> to configure authentication, storage, and cloud integration for your Trino deployment.</p> <p>If you don't like my script and want to do it step by step manually, please continue reading. This article will walk you through how to deploy a Trino Cluster on Kubernetes step by step, explaining each part along the way.</p>"},{"location":"side-projects/retail-lakehouse/trino/deployment/#generating-environment-and-values-files","title":"Generating Environment and Values Files","text":"install.sh:env<pre><code># \u7522\u751f .env \u548c values.yaml \u6a94\u6848\nbash generate-env.sh\n</code></pre> generate-env.sh <pre><code>#!/bin/bash\n\nset -euo pipefail\n\nENV_FILE=\".env\"\n\n# \u82e5\u6709\u820a\u7684 .env\uff0c\u5c31\u5148\u8f09\u5165\u7576\u4f5c\u9810\u8a2d\u503c\nif [[ -f \"$ENV_FILE\" ]]; then\n  set -a\n  # shellcheck disable=SC1090\n  source \"$ENV_FILE\"\n  set +a\nfi\n\n# \u5b9a\u7fa9\u4e00\u500b\u51fd\u5f0f\u4f86\u8a62\u554f\u8b8a\u6578\nask() {\n  local name=\"$1\"\n  local current=\"${!name-}\"  # \u53d6\u76ee\u524d\u74b0\u5883\u7684\u503c\uff08\u82e5\u6709\uff09\n  local prompt\n\n  if [[ -n \"${current:-}\" ]]; then\n    prompt=\"Enter $name (default: $current): \"\n  else\n    prompt=\"Enter $name: \"\n  fi\n\n  read -r -p \"$prompt\" input\n\n  # \u7a7a\u8f38\u5165\u5c31\u6cbf\u7528\u820a\u503c\uff1b\u5426\u5247\u66f4\u65b0\n  if [[ -z \"${input:-}\" &amp;&amp; -n \"${current:-}\" ]]; then\n    export \"$name=$current\"\n  else\n    export \"$name=$input\"\n  fi\n}\n\necho \"Generating .env file...\"\necho \"=== Fill in variables to generate ENV_FILE ===\"\n# \u6e05\u7a7a\u4e26\u91cd\u65b0\u5275\u5efa .env \u6a94\u6848\n&gt; \"$ENV_FILE\"\n# \u4e0d\u9700\u8981\u4f7f\u7528\u8005\u8f38\u5165\u7684\u6240\u6709\u8b8a\u6578\nINTERNAL_SHARED_SECRET=\"$(openssl rand 512 | base64)\"\nexport INTERNAL_SHARED_SECRET\nprintf 'INTERNAL_SHARED_SECRET=%q\\n' \"$INTERNAL_SHARED_SECRET\" &gt;&gt; \"$ENV_FILE\"\n# \u9700\u8981\u4f7f\u7528\u8005\u8f38\u5165\u7684\u6240\u6709\u8b8a\u6578\uff08\u9806\u5e8f\u6c7a\u5b9a\u4e92\u52d5\u9806\u5e8f\uff09\nVARS=(\n  OAUTH2_CLIENT_ID\n  OAUTH2_CLIENT_SECRET\n  GCP_BQ_PROJECT_ID\n  GCP_SA_INPUT_PATH\n  AWS_ACCESS_KEY\n  AWS_SECRET_KEY\n  AWS_REGION\n  ICEBERG_S3_URL\n  EXCHANGE_S3_URLS\n)\nfor v in \"${VARS[@]}\"; do\n  ask \"$v\"\n  val=\"${!v-}\"\n  printf '%s=%q\\n' \"$v\" \"$val\" &gt;&gt; \"$ENV_FILE\"\ndone\necho \"$ENV_FILE generated successfully.\"\n\n# \u6aa2\u67e5 envsubst command \u662f\u5426\u5b58\u5728\nif ! command -v envsubst &gt;/dev/null 2&gt;&amp;1; then\n  echo \"Error: envsubst not found. Please install gettext.\" &gt;&amp;2\n  exit 1\nfi\n\necho \"Generating values.yaml from values-template.yaml...\"\nenvsubst &lt; \"values-template.yaml\" &gt; \"values.yaml\"\necho \"values.yaml generated successfully.\"\n</code></pre>"},{"location":"side-projects/retail-lakehouse/trino/deployment/#loading-environment-variables-from-the-env-file","title":"Loading environment variables from the <code>.env</code> file","text":"<p>Load the environment variables from the <code>.env</code> file so that they can be used in subsequent commands:</p> <pre><code>source \".env\"\n</code></pre>"},{"location":"side-projects/retail-lakehouse/trino/deployment/#creating-the-namespace","title":"Creating the Namespace","text":"<p>Create the trino namespace, which is where we deploy our Trino cluster:</p> <pre><code>kubectl create namespace trino\n</code></pre> Result <pre><code>namespace/trino created\n</code></pre>"},{"location":"side-projects/retail-lakehouse/trino/deployment/#setting-up-tls-certificates","title":"Setting up TLS Certificates","text":"<p>Execute <code>generate-tls-certs.sh</code> to generate TLS certificate and Kubernetes secret and then apply the secret to the <code>trino</code> namespace:</p> install.sh:tls<pre><code>bash generate-tls-certs.sh\nkubectl apply -f ./trino-tls-secret.yaml -n trino\n</code></pre> Result <pre><code>Creating TLS certificates for Trino...\nStep 1: Creating Private Key...\nStep 1: Completed.\nStep 2: Creating Certificate...\nStep 2: Completed.\nStep 3: Combining Private Key and Certificate...\nStep 3: Completed.\nStep 4: Creating Kubernetes secret...\nStep 4: Completed.\nCertificate generation completed successfully!\n\nGenerated files:\n  - .cert/trino-dev.pem (with private key and certificate)\n  - trino-tls-secret.yaml (Kubernetes secret manifest)\n\nsecret/trino-tls-secret created\n</code></pre> <p>Once executed, the <code>trino-tls-secret.yaml</code> file will have the following structure:</p> trino-tls-secret.yaml <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  creationTimestamp: null\n  name: trino-tls-secret\ndata:\n  trino-dev.pem: xxx\n</code></pre>"},{"location":"side-projects/retail-lakehouse/trino/deployment/#configuring-bigquery-service-account","title":"Configuring BigQuery Service Account","text":"<p>Generate the BigQuery service account secret manifest file and apply it to the <code>trino</code> namespace:</p> install.sh:bq<pre><code>echo \"Generating Kubernetes secret for Accessing BigQuery...\"\nkubectl create secret generic trino-bigquery-secret \\\n    --from-file=trino-sa.json=\"$GCP_SA_INPUT_PATH\" \\\n    --dry-run=client -o yaml &gt; \"./trino-bigquery-secret.yaml\"\necho \"trino-bigquery-secret.yaml generated successfully.\"\nkubectl apply -f ./trino-bigquery-secret.yaml -n trino\n</code></pre> Result <pre><code>Generating Kubernetes secret for Accessing BigQuery...\ntrino-bigquery-secret.yaml generated successfully.\nsecret/trino-bigquery-secret created\n</code></pre> <p>Once executed, the <code>trino-bigquery-secret.yaml</code> file will have the following structure:</p> trino-bigquery-secret.yaml <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  creationTimestamp: null\n  name: trino-bigquery-secret\ndata:\n  trino-sa.json: xxx\n</code></pre>"},{"location":"side-projects/retail-lakehouse/trino/deployment/#installing-trino","title":"Installing Trino","text":"<p>First, add and update the Trino Helm repository. Then, deploy Trino in the <code>trino</code> namespace using the generated <code>values.yaml</code> file:</p> install.sh:bq<pre><code>helm repo add trino https://trinodb.github.io/charts/\nhelm repo update\nhelm install trino trino/trino \\\n  -f values.yaml \\\n  -n trino \\\n  --version 1.39.1 \\\n</code></pre> Result <pre><code>NAME: trino\nLAST DEPLOYED: Fri Jun 13 17:49:52 2025\nNAMESPACE: trino\nSTATUS: deployed\nREVISION: 1\nNOTES:\nGet the application URL by running these commands:\n  kubectl --namespace trino port-forward svc/trino 8080:8080\n  echo \"Visit http://127.0.0.1:8080 to use your application\"\n</code></pre>"},{"location":"side-projects/retail-lakehouse/trino/deployment/#verifying-the-deployment","title":"Verifying the Deployment","text":"<p>After the installation completes, verify that Trino has been deployed successfully. Show the deployed Trino release:</p> <pre><code>helm list -n trino\n</code></pre> Result <pre><code>NAME    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION\ntrino   trino           1               2025-06-13 18:08:54.435442 +0800 CST    deployed        trino-1.39.1    475    \n</code></pre> <p>Then, check the status of all resources in the <code>trino</code> namespace:</p> <pre><code>kubectl get all -n trino\n</code></pre> Result <pre><code>NAME                                     READY   STATUS    RESTARTS   AGE\npod/trino-coordinator-6fdfb7bf84-tjwfc   1/1     Running   0          5m38s\npod/trino-worker-777d595c66-dml67        1/1     Running   0          5m38s\npod/trino-worker-777d595c66-pv9h2        1/1     Running   0          5m38s\n\nNAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE\nservice/trino          ClusterIP   10.99.208.168   &lt;none&gt;        8080/TCP,8443/TCP   5m38s\nservice/trino-worker   ClusterIP   None            &lt;none&gt;        8080/TCP            5m38s\n\nNAME                                READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/trino-coordinator   1/1     1            1           5m38s\ndeployment.apps/trino-worker        2/2     2            2           5m38s\n\nNAME                                           DESIRED   CURRENT   READY   AGE\nreplicaset.apps/trino-coordinator-6fdfb7bf84   1         1         1       5m38s\nreplicaset.apps/trino-worker-777d595c66        2         2         2       5m38s\n</code></pre> <p>Perfect! The deployment is successful. As you can see, the Trino coordinator and worker pods are up and running, and all services have been created correctly. With the cluster now operational, we can proceed to access the Trino Web UI and Trino CLI to interact with the cluster.</p>"},{"location":"side-projects/retail-lakehouse/trino/deployment/#web-ui","title":"Web UI","text":"<p>To access the Trino Web UI, start by port-forwarding the Trino service to your local machine:</p> <pre><code>kubectl port-forward svc/trino 8443:8443 --namespace trino\n</code></pre> <p>Once the port forwarding is active, open your browser and navigate to <code>https://127.0.0.1:8443</code>. Since we're using a self-signed certificate for development purposes, your browser will display a security warning. This is expected behavior and safe to bypass in a development environment.</p> <p>To proceed, click \"Advanced\" and then \"Accept the Risk and Continue\" (the exact wording may vary depending on your browser).</p> <p></p> <p></p> <p>Since OAuth 2.0 authentication is enabled, you'll be automatically redirected to the Google login page for authentication.</p> <p></p> <p>After logging in with your Google account, you will be redirected back to the Trino Web UI.</p> <p></p> <p>Once authenticated successfully, you'll be redirected back to the Trino Web UI where you can monitor queries, view cluster status, and manage your Trino environment.</p>"},{"location":"side-projects/retail-lakehouse/trino/deployment/#trino-cli","title":"Trino CLI","text":"<p>For command-line access to Trino, you'll need to download and install the Trino CLI tool.</p> <p>First, download the <code>trino-cli-476-executable.jar</code> file from the Maven repository.</p> <p>Next, rename the file to <code>trino</code>, make it executable, and move it to a directory in your PATH (such as <code>/usr/local/bin</code>):</p> <pre><code>cd ~/Projects/retail-lakehouse/trino\ncurl -L -o trino-cli-476-executable.jar https://repo1.maven.org/maven2/io/trino/trino-cli/476/trino-cli-476-executable.jar\nchmod +x trino-cli-476-executable.jar\nsudo mv trino-cli-476-executable.jar /usr/local/bin/trino\n</code></pre> <p>Verify the installation by checking the version:</p> <pre><code>trino --version\n</code></pre> Result <pre><code>Trino CLI 476\n</code></pre> <p>To connect to your Trino cluster, use the CLI with external authentication enabled:</p> <pre><code>trino --server https://127.0.0.1:8443 \\\n      --external-authentication \\\n      --insecure \\\n      --user \"user@example.com\"\n</code></pre> Authentication Flow <p>The CLI authentication process works as follows:</p> <ul> <li>Start the CLI with the <code>--external-authentication</code> option and execute a query.</li> <li>The CLI starts and connects to Trino.</li> <li>A message appears in the CLI directing you to open a browser with a specified URL when the first query is submitted.</li> <li>Open the URL in a browser and follow through the authentication process.</li> <li>The CLI automatically receives a token.</li> <li>When successfully authenticated in the browser, the CLI proceeds to execute the query.</li> <li>Further queries in the CLI session do not require additional logins while the authentication token remains valid. Token expiration depends on the external authentication type configuration.</li> <li>Expired tokens force you to log in again.</li> </ul> <p>This authentication method ensures secure access to your Trino cluster while maintaining ease of use for interactive queries.</p>"},{"location":"side-projects/retail-lakehouse/trino/deployment/#cleanup","title":"Cleanup","text":"<p>To remove the Trino cluster:</p> <pre><code>helm uninstall trino --namespace trino\nkubectl delete namespace trino\n</code></pre>"},{"location":"side-projects/retail-lakehouse/trino/faker/","title":"Faker Connector","text":"<p>This guide covers how to use the faker connector in Trino to generate realistic fake data for testing and development purposes.</p>"},{"location":"side-projects/retail-lakehouse/trino/faker/#overview","title":"Overview","text":"<p>The faker connector uses the Datafaker library to generate realistic fake data. It's perfect for:</p> <ul> <li>Testing queries with realistic data</li> <li>Data pipeline development</li> <li>Performance testing</li> <li>Demo environments</li> </ul>"},{"location":"side-projects/retail-lakehouse/trino/faker/#configuration","title":"Configuration","text":"<p>The faker connector is configured with the following settings:</p> trino/values-template.yaml<pre><code>catalogs:\n  faker: |\n    connector.name=faker\n    faker.null-probability=0.1\n    faker.default-limit=1000\n    faker.locale=en\n  bigquery: |\n    connector.name=bigquery\n    bigquery.project-id=$GCP_BQ_PROJECT_ID\n    bigquery.credentials-file=/etc/trino/bigquery/trino-sa.json\n  iceberg: |\n    connector.name=iceberg\n    iceberg.catalog.type=glue\n    iceberg.file-format=parquet\n    hive.metastore.glue.region=$AWS_REGION\n    hive.metastore.glue.default-warehouse-dir=$ICEBERG_S3_URL\n    hive.metastore.glue.aws-access-key=$AWS_ACCESS_KEY\n    hive.metastore.glue.aws-secret-key=$AWS_SECRET_KEY\n    fs.native-s3.enabled=true\n    s3.region=$AWS_REGION\n    s3.aws-access-key=$AWS_ACCESS_KEY\n    s3.aws-secret-key=$AWS_SECRET_KEY\n\n# https://trino.io/docs/current/connector/iceberg.html#authorization\n# https://trino.io/docs/current/object-storage/file-system-s3.html\n</code></pre> <p>Here's a breakdown of the configuration:</p> <ul> <li><code>connector.name=faker</code>: Specifies the faker connector</li> <li><code>faker.null-probability=0.1</code>: 10% chance of null values in generated data</li> <li><code>faker.default-limit=1000</code>: Default row limit for queries</li> <li><code>faker.locale=en</code>: English locale for generated data patterns</li> </ul>"},{"location":"side-projects/retail-lakehouse/trino/faker/#creating-tables","title":"Creating Tables","text":"<p>The faker connector requires you to create tables with specific generator expressions:</p>"},{"location":"side-projects/retail-lakehouse/trino/faker/#1-prices-table","title":"1. Prices Table","text":"test-faker.sql - Prices Table<pre><code>-- Create a prices table with faker connector\nCREATE TABLE faker.default.prices (\n  currency VARCHAR NOT NULL WITH (generator = '#{Currency.code}'),\n  price DECIMAL(8,2) NOT NULL WITH (min = '0')\n);\n</code></pre>"},{"location":"side-projects/retail-lakehouse/trino/faker/#2-customer-table","title":"2. Customer Table","text":"test-faker.sql - Customer Table<pre><code>-- Create a comprehensive customer table\nCREATE TABLE faker.default.customer (\n  id UUID NOT NULL,\n  name VARCHAR NOT NULL WITH (generator = '#{Name.first_name} #{Name.last_name}'),\n  email VARCHAR NOT NULL WITH (generator = '#{Internet.emailAddress}'),\n  phone VARCHAR NOT NULL WITH (generator = '#{PhoneNumber.phoneNumber}'),\n  address VARCHAR NOT NULL WITH (generator = '#{Address.fullAddress}'),\n  born_at DATE WITH (min = '1950-01-01', max = '2005-01-01'),\n  age_years INTEGER WITH (min = '18', max = '75'),\n  group_id INTEGER WITH (allowed_values = ARRAY['10', '32', '81', '99'])\n);\n</code></pre>"},{"location":"side-projects/retail-lakehouse/trino/faker/#example-queries","title":"Example Queries","text":""},{"location":"side-projects/retail-lakehouse/trino/faker/#basic-data-generation","title":"Basic Data Generation","text":"test-faker.sql - Basic Queries<pre><code>-- Query the prices table\nSELECT * FROM faker.default.prices LIMIT 10;\n\n-- Query the customer table\nSELECT * FROM faker.default.customer LIMIT 10;\n\n-- Test the random_string function\nSELECT faker.default.random_string('#{Name.first_name}') as first_name;\n</code></pre>"},{"location":"side-projects/retail-lakehouse/trino/faker/#advanced-queries","title":"Advanced Queries","text":"test-faker.sql - Advanced Queries<pre><code>-- Join customer and prices tables\nSELECT \n    c.name as customer_name,\n    c.email,\n    p.currency,\n    p.price as order_amount\nFROM faker.default.customer c\nCROSS JOIN faker.default.prices p\nLIMIT 10;\n\n-- Create JSON objects from prices\nSELECT JSON_OBJECT(KEY currency VALUE price) AS complex_price\nFROM faker.default.prices\nLIMIT 5;\n\n-- Sample query to test various generators\nSELECT \n    faker.default.random_string('#{Name.fullName}') as full_name,\n    faker.default.random_string('#{Internet.emailAddress}') as email,\n    faker.default.random_string('#{Address.city}') as city,\n    faker.default.random_string('#{Company.name}') as company,\n    faker.default.random_string('#{Lorem.sentence}') as description;\n</code></pre>"},{"location":"side-projects/retail-lakehouse/trino/faker/#testing-commands","title":"Testing Commands","text":"<p>Run individual queries from the command line:</p> <pre><code>kubectl exec -it deployment/trino-coordinator --namespace trino -- trino --execute \"SHOW TABLES FROM faker.default;\"\nkubectl exec -it deployment/trino-coordinator --namespace trino -- trino --execute \"SELECT * FROM faker.default.customer LIMIT 5;\"\n</code></pre>"},{"location":"side-projects/retail-lakehouse/trino/faker/#available-faker-tables","title":"Available Faker Tables","text":"<p>The faker connector has been tested and verified with these tables:</p> <ul> <li> <code>prices</code> - Currency codes and decimal prices (created and tested)</li> <li> <code>customer</code> - Customer profiles with realistic data (created and tested)</li> </ul>"},{"location":"side-projects/retail-lakehouse/trino/faker/#faker-functions","title":"Faker Functions","text":"<ul> <li> <code>random_string()</code> - Generate custom fake data using Datafaker expressions</li> </ul>"},{"location":"side-projects/retail-lakehouse/trino/faker/#available-generators","title":"Available Generators","text":"<p>The faker connector supports numerous generators from the Datafaker library:</p>"},{"location":"side-projects/retail-lakehouse/trino/faker/#personal-information","title":"Personal Information","text":"<ul> <li><code>#{Name.firstName}</code>, <code>#{Name.lastName}</code>, <code>#{Name.fullName}</code></li> <li><code>#{Internet.emailAddress}</code>, <code>#{PhoneNumber.phoneNumber}</code></li> <li><code>#{Address.fullAddress}</code>, <code>#{Address.city}</code>, <code>#{Address.country}</code></li> </ul>"},{"location":"side-projects/retail-lakehouse/trino/faker/#business-data","title":"Business Data","text":"<ul> <li><code>#{Currency.code}</code>, <code>#{Company.name}</code></li> <li><code>#{Commerce.productName}</code>, <code>#{Commerce.price}</code></li> </ul>"},{"location":"side-projects/retail-lakehouse/trino/faker/#text-content","title":"Text Content","text":"<ul> <li><code>#{Lorem.sentence}</code>, <code>#{Lorem.paragraph}</code></li> <li><code>#{Lorem.words}</code>, <code>#{Lorem.characters}</code></li> </ul>"},{"location":"side-projects/retail-lakehouse/trino/faker/#dates-and-numbers","title":"Dates and Numbers","text":"<ul> <li><code>#{Date.past}</code>, <code>#{Date.future}</code></li> <li><code>#{Number.randomDouble}</code>, <code>#{Number.randomLong}</code></li> </ul>"},{"location":"side-projects/retail-lakehouse/trino/faker/#financial-data","title":"Financial Data","text":"<ul> <li><code>#{Finance.creditCard}</code>, <code>#{Finance.iban}</code></li> <li><code>#{Finance.bic}</code>, <code>#{Finance.stockTicker}</code></li> </ul> <p>For a complete list of available generators, see the Datafaker Documentation.</p>"},{"location":"side-projects/retail-lakehouse/trino/faker/#column-constraints","title":"Column Constraints","text":"<p>You can apply various constraints to faker columns:</p>"},{"location":"side-projects/retail-lakehouse/trino/faker/#value-ranges","title":"Value Ranges","text":"<pre><code>-- Numeric ranges\nage INTEGER WITH (min = '18', max = '75')\nprice DECIMAL(8,2) WITH (min = '0', max = '1000')\n\n-- Date ranges  \nbirth_date DATE WITH (min = '1950-01-01', max = '2005-01-01')\n</code></pre>"},{"location":"side-projects/retail-lakehouse/trino/faker/#allowed-values","title":"Allowed Values","text":"<pre><code>-- Specific allowed values\nstatus VARCHAR WITH (allowed_values = ARRAY['active', 'inactive', 'pending'])\npriority INTEGER WITH (allowed_values = ARRAY['1', '2', '3', '4', '5'])\n</code></pre>"},{"location":"side-projects/retail-lakehouse/trino/faker/#null-probability","title":"Null Probability","text":"<pre><code>-- Override default null probability for specific columns\noptional_field VARCHAR WITH (generator = '#{Lorem.word}', null_probability = '0.3')\n</code></pre>"},{"location":"side-projects/retail-lakehouse/trino/faker/#best-practices","title":"Best Practices","text":"<ol> <li>Use Appropriate Data Types: Match your production schema data types</li> <li>Set Realistic Constraints: Use min/max values that make sense for your domain</li> <li>Consider Cardinality: Use allowed_values for categorical data</li> <li>Test Join Performance: Cross joins can generate large result sets quickly</li> <li>Limit Result Sets: Always use LIMIT in development to avoid overwhelming queries</li> </ol>"},{"location":"side-projects/retail-lakehouse/trino/faker/#common-use-cases","title":"Common Use Cases","text":""},{"location":"side-projects/retail-lakehouse/trino/faker/#e-commerce-data","title":"E-commerce Data","text":"<pre><code>CREATE TABLE faker.default.products (\n  id UUID NOT NULL,\n  name VARCHAR NOT NULL WITH (generator = '#{Commerce.productName}'),\n  category VARCHAR WITH (allowed_values = ARRAY['electronics', 'clothing', 'books', 'home']),\n  price DECIMAL(10,2) WITH (min = '1.00', max = '999.99'),\n  description VARCHAR WITH (generator = '#{Lorem.sentence}'),\n  in_stock BOOLEAN NOT NULL\n);\n</code></pre>"},{"location":"side-projects/retail-lakehouse/trino/faker/#user-analytics","title":"User Analytics","text":"<pre><code>CREATE TABLE faker.default.user_events (\n  user_id UUID NOT NULL,\n  event_type VARCHAR WITH (allowed_values = ARRAY['login', 'logout', 'purchase', 'view']),\n  timestamp TIMESTAMP NOT NULL,\n  session_id VARCHAR WITH (generator = '#{Internet.uuid}'),\n  ip_address VARCHAR WITH (generator = '#{Internet.ipV4Address}')\n);\n</code></pre>"},{"location":"side-projects/retail-lakehouse/trino/faker/#financial-transactions","title":"Financial Transactions","text":"<pre><code>CREATE TABLE faker.default.transactions (\n  transaction_id UUID NOT NULL,\n  account_number VARCHAR WITH (generator = '#{Finance.iban}'),\n  amount DECIMAL(12,2) WITH (min = '-10000', max = '10000'),\n  currency VARCHAR WITH (generator = '#{Currency.code}'),\n  transaction_date DATE WITH (min = '2020-01-01', max = '2024-12-31'),\n  merchant VARCHAR WITH (generator = '#{Company.name}')\n);\n</code></pre>"},{"location":"side-projects/retail-lakehouse/trino/fault-tolerance/","title":"Fault-tolerant Execution in Trino","text":"<p>By default, when a Trino node experiences resource constraints or encounters failures while processing a query, the entire query fails and requires manual restart.</p> <p>Fault-tolerant execution is a feature in Trino that helps clusters recover from query failures by automatically retrying failed queries or their individual tasks. When this capability is enabled, intermediate data exchanges are cached and stored, allowing other workers to reuse this data if a worker node fails or encounters issues during query execution.</p> Fault-tolerant Trino Cluster <p>This allows Trino to handle larger queries such as batch operations without worker node interruptions causing the query to fail.</p> <p>The coordinator node uses a configured exchange manager service that buffers data during query processing in an external location, such as an S3 object storage bucket. </p> <p>Worker nodes send data to the buffer as they execute their query tasks.</p>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/fault-tolerance/#configuration","title":"Configuration","text":"<p>To enable fault-tolerant execution, you need to configure the Trino cluster with the following properties:</p> value.yaml<pre><code>server:\n  workers: 2\n  config:\n    authenticationType: \"oauth2\"\n  coordinatorExtraConfig: |\n    http-server.https.enabled=true\n    http-server.https.port=8443\n    http-server.https.keystore.path=/etc/trino/tls/trino-dev.pem\n    http-server.authentication.oauth2.issuer=https://accounts.google.com\n    http-server.authentication.oauth2.client-id=$OAUTH2_CLIENT_ID\n    http-server.authentication.oauth2.client-secret=$OAUTH2_CLIENT_SECRET\n    http-server.authentication.oauth2.principal-field=email\n    http-server.authentication.oauth2.scopes=openid,email\n    web-ui.authentication.type=oauth2\n  #   distributed-sort=true\n  #   query.max-history=50\n  exchangeManager:\n    name: filesystem\n    baseDir: $EXCHANGE_S3_URLS\n  # workerExtraConfig: |\n  #   discovery.uri=http://trino.trino.svc.cluster.local:8080\n</code></pre> value.yaml<pre><code>additionalConfigProperties:\n  - internal-communication.shared-secret=$INTERNAL_SHARED_SECRET\n  - retry-policy=TASK\n</code></pre> value.yaml<pre><code>additionalExchangeManagerProperties:\n  - exchange.s3.region=$AWS_REGION\n  - exchange.s3.aws-access-key=$AWS_ACCESS_KEY\n  - exchange.s3.aws-secret-key=$AWS_SECRET_KEY\n# additionalExchangeManagerProperties -- [Exchange manager\n# properties](https://trino.io/docs/current/admin/fault-tolerant-execution.html#exchange-manager).\n# @raw\n# Example:\n# ```yaml\n#  - exchange.s3.region=object-store-region\n#  - exchange.s3.endpoint=your-object-store-endpoint\n#  - exchange.s3.aws-access-key=your-access-key\n#  - exchange.s3.aws-secret-key=your-secret-key\n# ```\n</code></pre> <p>The three configuration blocks shown above are applied to both the coordinator and all worker nodes in the Trino cluster.</p>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/fault-tolerance/#considerations","title":"Considerations","text":"<p>When fault-tolerant execution is enabled on a cluster, write operations will fail on any catalogs that do not support fault-tolerant execution of those operations. This design ensures data consistency and prevents partial writes that could occur during query retries. Since fault-tolerant execution may retry failed tasks multiple times, write operations must be idempotent to avoid duplicate data or corrupted states. Catalogs that cannot guarantee idempotent writes are therefore excluded to maintain data integrity.</p> <p>Furthermore, it is recommended to run a dedicated fault-tolerant cluster for handling batch operations, separate from a cluster designated for higher query volume. This separation provides several key benefits: workload isolation prevents long-running batch jobs from affecting interactive queries, resource optimization allows each cluster to be tuned for its specific use case, and fault tolerance overhead (such as external storage I/O) doesn't impact performance-sensitive interactive workloads.</p> <p>Additionally, when configuring fault-tolerant execution, you can configure multiple storage locations for use by the exchange manager to help balance the I/O load between them. The exchange manager may send a large amount of data to the exchange storage, resulting in high I/O load on that storage. By distributing data across multiple storage endpoints, you can prevent storage bottlenecks and improve overall query performance during fault recovery scenarios.</p>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/fault-tolerance/#dual-cluster-architecture","title":"Dual Cluster Architecture","text":"<p>In practice, a common enterprise configuration involves deploying two distinct Trino clusters: an interactive cluster for ad-hoc queries and dashboard updates, and a batch cluster for long-running, compute-intensive, or large-scale shuffle operations. This dual-cluster architecture can be orchestrated through Trino Gateway, which intelligently routes queries to the most appropriate cluster based on query characteristics and workload requirements.</p>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/fault-tolerance/#interactive-cluster","title":"Interactive Cluster","text":"<p>The interactive cluster operates with fault-tolerant execution disabled, prioritizing low latency and high concurrency for user-facing analytics. This cluster excels at handling ad-hoc analysis, visualization tool queries, report generation, dashboard updates, cross-table joins, and data exploration tasks. These queries typically involve small to medium datasets with response times measured in seconds to tens of seconds. By avoiding external exchange storage, this cluster eliminates additional I/O overhead and latency, though it requires full query restart when node failures occur.</p>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/fault-tolerance/#batch-cluster","title":"Batch Cluster","text":"<p>Conversely, the batch cluster enables fault-tolerant execution with a configured exchange manager that stores intermediate results in external storage, supporting staged retry capabilities after node failures. This cluster handles long-running, compute-intensive, or large-scale shuffle queries such as bulk data transformations, historical data recomputation, large-scale joins, aggregation computations, data compaction and sorting, and feature generation for model training. While the external exchange mechanism enables resilience against single node failures, it incurs higher I/O and storage costs, making it unsuitable for high-volume small queries.</p>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/fault-tolerance/#trino-gateway","title":"Trino Gateway","text":"<p>Finally, Trino Gateway orchestrates query routing between the clusters, ensuring optimal resource utilization. Interactive queries and dashboard requests are directed to the interactive cluster for guaranteed low latency and high concurrency, while batch jobs and data pipeline operations are routed to the batch cluster for fault tolerance and large-scale processing capabilities. This division of labor ensures that different workload types execute in their most suitable environments while minimizing resource contention and performance conflicts.</p>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/fault-tolerance/#references","title":"References","text":"<ul> <li>Improve query processing resilience</li> <li>Fault-tolerant execution</li> </ul>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/how-it-works/","title":"How It Works?","text":"Trino: The Definitive Guide","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/how-it-works/#architecture-components","title":"Architecture Components","text":"","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/how-it-works/#coordinator","title":"Coordinator","text":"The Trino coordinator is the server that is responsible for parsing statements, planning queries, and managing Trino worker nodes. It is also the node to which a client connects to submit statements for execution. The coordinator keeps track of the activity on each worker and coordinates the execution of a query. The coordinator creates a logical model of a query involving a series of stages, which is then translated into a series of connected tasks running on a cluster of Trino workers. Coordinators communicate with workers and clients using a REST API. The coordinator is responsible for fetching results from the workers and returning the final results to the client.","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/how-it-works/#worker","title":"Worker","text":"A Trino worker is a server that is responsible for executing tasks and processing data. Worker nodes fetch data from connectors and exchange intermediate data with each other. When a Trino worker process starts up, it advertises itself to the discovery server in the coordinator, which makes it available to the Trino coordinator for task execution.","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/how-it-works/#cluster","title":"Cluster","text":"A Trino cluster consists of several Trino nodes - one coordinator and zero or more workers. The coordinator and the workers access the connected data sources. This access is configured in catalogs.","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/how-it-works/#clients","title":"Clients","text":"Clients allow you to connect to Trino, submit SQL queries, and receive the results. Clients can access all configured data sources using catalogs.","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/how-it-works/#core-concepts","title":"Core Concepts","text":"","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/how-it-works/#data-sources","title":"Data Sources","text":"<p>Trino serves as a distributed query engine capable of accessing a wide variety of data sources. These encompass data lakes, lakehouses, various relational database management systems, key-value stores, and numerous other storage systems.</p> <p>To enable data access, you must configure a catalog with the appropriate Trino connector tailored to your specific data source.</p>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/how-it-works/#connectors","title":"Connectors","text":"<p>Connectors serve as adapters that enable Trino to interface with various data sources, including data lakes built on Hadoop/Hive or Apache Iceberg, as well as relational databases like PostgreSQL.</p> <p>Think of connectors similarly to database drivers\u2014they are concrete implementations of Trino's service provider interface (SPI) that provide a standardized way for Trino to communicate with external resources through a consistent API.</p>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/how-it-works/#catalogs","title":"Catalogs","text":"<p>A catalog in Trino represents a configuration that enables access to a specific data source through a designated connector. Each catalog is defined by a collection of properties stored in configuration files within the Trino configuration directory.</p> <p>The fundamental requirement for any catalog is the <code>connector.name</code> property, which specifies which connector implementation to use for that particular data source. Beyond the connector specification, catalogs also include additional configuration details such as authentication credentials, connection URLs, and other source-specific parameters.</p> <p>Multiple catalogs can be configured simultaneously, allowing Trino to connect to various data sources. These catalogs may utilize different connector types or even multiple instances of the same connector type to access distinct data sources.</p> <p>Within each catalog's scope, you'll find a hierarchical structure consisting of schemas. These schemas serve as organizational containers for database objects including tables, views, and materialized views, providing a logical grouping mechanism for the data accessible through that catalog.</p>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/how-it-works/#behind-the-scenes","title":"Behind the Scenes","text":"","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/how-it-works/#query-execution-model","title":"Query Execution Model","text":"<p>A user starts by sending an SQL statement from a client tool, such as DBeaver, Superset, or a custom application, to the Trino cluster. The statement is a plain-text command like <code>SELECT region, COUNT(*) FROM sales GROUP BY region;</code>. It first arrives at the coordinator, which is the brain of the Trino system.</p> <p>The coordinator begins by parsing this statement, checking for syntax and semantic correctness. Once validated, the statement is transformed into a query. The query is more than just text. It's a fully-formed plan that defines how the cluster will execute the user's command.</p> <p>The query is broken down into a hierarchy of stages, forming a distributed execution plan. These stages form a tree-like structure, where each stage represents a logical step of the query, such as scanning data, performing aggregation, or returning final results.</p> Trino: The Definitive Guide Trino: The Definitive Guide <p>Although stages define what needs to be done, they don't perform the actual computation. Instead, each stage is implemented through multiple tasks. These tasks are distributed across worker nodes in the cluster. Each task is responsible for executing a portion of its parent stage, and together, the tasks accomplish the overall goal of that stage.</p> Trino: The Definitive Guide <p>Each task works on specific chunks of data called splits. A split is a subset of the overall dataset. For example, a single Parquet file or a segment of rows. The coordinator asks the storage connector (such as the Hive connector) for a list of available splits for a given table and then assigns those splits to the tasks running on various workers.</p> Trino: The Definitive Guide <p>Within each task, the execution is further broken down into one or more drivers. A driver is essentially a data pipeline that runs in a single thread. Each driver consists of a series of operators. These operators perform concrete operations like scanning a table, filtering rows, applying functions, or aggregating values. You can think of a driver as a physical pipeline built from operator components.</p> <p>For example, a driver in this case might include a table scan operator that reads rows from the <code>sales</code> table, followed by a filter operator that removes <code>NULL</code> regions, and an aggregation operator that performs the <code>GROUP BY region</code> logic.</p> Trino: The Definitive Guide <p>As data flows upward from lower stages to higher ones, exchanges come into play. These are the network mechanisms that transfer data between nodes. When one stage finishes processing, it places its output into buffers. The next stage pulls data from these buffers using an exchange client, ensuring the distributed stages stay connected even across physical nodes.</p> <p>Once the root stage finishes aggregating the results from its child stages, the final output is assembled by the coordinator and returned to the client. What the user sees is a neatly formatted result set with regions and counts, but under the hood, that simple output is the product of a highly parallel and distributed pipeline involving statements, queries, stages, tasks, splits, drivers, operators, and exchanges.</p>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/how-it-works/#client-protocol","title":"Client Protocol","text":"<pre><code>sequenceDiagram\n    participant Client\n    participant Coordinator\n\n    Client-&gt;&gt;Coordinator: Submit SQL query\n    Coordinator-&gt;&gt;Coordinator: Start processing query\n    Coordinator--&gt;&gt;Client: Return initial result + nextUri\n    loop Fetching result set\n        Client-&gt;&gt;Coordinator: Request nextUri\n        Coordinator--&gt;&gt;Client: Return data + nextUri (or FINISHED)\n    end\n    alt Client stops requesting\n        Coordinator--&gt;&gt;Client: Return USER_CANCELED\n    end</code></pre> <p>See Client Protocol | Trino for more.</p>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/how-it-works/#references","title":"References","text":"<ul> <li>29: What is Trino and the Hive connector</li> <li>Trino: The Definitive Guide</li> </ul>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/join-optimization/","title":"Join Optimization","text":"","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/join-optimization/#join-enumeration","title":"Join Enumeration","text":"<p>Understanding <code>JOIN</code> optimization is crucial because <code>JOIN</code> order significantly impacts performance, particularly affecting intermediate result sizes and network transfer volumes. Different join orders can lead to vastly different intermediate result sizes. When you start with a <code>JOIN</code> that produces a massive intermediate table, each subsequent step requires moving and processing more data, resulting in slower performance and higher resource consumption. Conversely, starting with a <code>JOIN</code> that produces a small intermediate table leads to much better performance.</p> <p>To address this challenge, Trino includes Cost-Based Join Enumeration by default. This algorithm uses table statistics (row counts, sizes, distributions) to estimate the cost of each possible <code>JOIN</code> order. It enumerates all possible <code>JOIN</code> execution sequences, compares their costs, and selects the most resource-efficient order. The system automatically chooses the lowest-cost (fastest) <code>JOIN</code> sequence for execution. This means the <code>JOIN</code> order you write in your SQL statements won't affect Trino's execution plan, Trino will automatically select the optimal <code>JOIN</code> order.</p> <p>However, accurate table statistics are essential for Trino to correctly estimate costs. You can initialize these statistics using the <code>ANALYZE</code> command. This enables Trino to make optimal <code>JOIN</code> order decisions based on the latest data distribution and sizes.</p>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/join-optimization/#join-distribution","title":"Join Distribution","text":"<p>Beyond join order optimization, Trino must also determine how to distribute data across nodes during <code>JOIN</code> execution. Since Trino is a distributed query engine with multiple nodes processing queries collaboratively, <code>JOIN</code> operations require data from both tables to be located on the same node for comparison and hash table construction.</p> <p>Trino uses a hash-based join algorithm where the system first designates one side as the build side, loading all its data into memory and creating a hash table based on the join key. The other side becomes the probe side, with the system reading its data row by row and querying the hash table using the join key to find matching rows and output results. This method is particularly effective when sufficient memory is available, and typically uses the smaller dataset as the build side to reduce memory usage and improve performance.</p> <p>There are two main types of join distributions:</p> <ul> <li>Partitioned joins: Each node participating in the query builds a hash table from only a fraction of the data. This approach requires redistributing both tables using a hash of the join key. While these joins can be slower than broadcast joins, they enable much larger joins overall.</li> <li>Broadcast joins: Each node participating in the query builds a hash table from all the data, with data replicated to each node. Broadcast joins perform better when the build side is much smaller than the probe side. However, they require that build-side tables fit in memory on each node after filtering, whereas partitioned joins only need to fit in distributed memory across all nodes.</li> </ul> <p>Info</p> <p>Trino's cost-based optimization automatically handles these decisions. With cost-based join distribution selection, Trino automatically chooses between partitioned and broadcast joins. Similarly, cost-based join enumeration automatically determines which sides serve as probe and build.</p> <p>Configuration</p> <p>Join enumeration strategy is controlled by <code>join_reordering_strategy</code> session property (or <code>optimizer.join-reordering-strategy</code> config)</p> <ul> <li><code>AUTOMATIC</code> (default)</li> <li><code>ELIMINATE_CROSS_JOINS</code></li> <li><code>NONE</code>.</li> </ul> <p>Join distribution strategy uses <code>join_distribution_type</code> session property (or <code>join-distribution-type</code> config)</p> <ul> <li><code>AUTOMATIC</code> (default)</li> <li><code>BROADCAST</code></li> <li><code>PARTITIONED</code>.</li> </ul> <p>Replicated table size can be capped using <code>join_max_broadcast_table_size</code> session property (or <code>join-max-broadcast-table-size</code> config): defaulting to 100MB.</p> <p>See Cost-based optimizations for more details.</p>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/monitoring/","title":"Monitoring with Prometheus and Grafana","text":"","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/monitoring/#what-is-jmx","title":"What is JMX?","text":"<p>JMX (Java Management Extensions) is a framework provided by Java for monitoring and managing resources while an application is running. When our application enables JMX, it starts a management interface inside the JVM, allowing you to connect locally or remotely to read the system and application runtime status, such as memory usage, thread count, and class loading statistics. It can also adjust application settings or execute specific operations without interrupting the service.</p> <p>In JMX, an MBean is a managed object. For example, <code>MemoryMXBean</code> provides JVM memory-related information. These MBeans are registered in the MBean Server for external access. If remote management is required, JMX uses RMI (Remote Method Invocation) to expose a remote access endpoint. The RMI Registry records the names and locations of accessible MBean resources, and the RMI Server receives commands from remote clients and invokes the corresponding MBean methods.</p> <p>In real-world projects, if we need to integrate JMX monitoring data into a monitoring system, a common approach is to deploy the JMX Exporter. It runs as a Java Agent, converting JMX data into a <code>/metrics</code> format readable by Prometheus, and exposes it over HTTP for Prometheus to scrape. Prometheus periodically collects these metrics, and Grafana uses the data from Prometheus to build visual dashboards, such as showing JVM heap usage, garbage collection counts, and other indicators.</p> <p>See Getting Started with Java Management Extensions (JMX): Developing Management and Monitoring Solutions for more details on JMX.</p>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/monitoring/#what-is-jmx-exporter","title":"What is JMX Exporter?","text":"<p>JMX Exporter is a specialized tool that bridges the gap between JMX monitoring data and modern observability platforms. Specifically, it serves as a process for collecting metrics using JMX MBeans and making them available for Prometheus consumption. At its core, the JMX Exporter acts as a collector that captures JMX MBean values and transforms them into a format that Prometheus can understand.</p> <p>The JMX Exporter offers three distinct deployment approaches, each suited for different operational requirements:</p> <ul> <li>Java Agent exporter: First and most importantly, the JMX Exporter Java agent runs directly as a Java agent within your application and collects JMX MBean values in real-time. Notably, the use of the JMX Exporter Java agent is strongly encouraged due to the complex application RMI configuration that would otherwise be required when running the Standalone JMX Exporter.</li> <li>Isolator Java Agent: Building upon the Java Agent approach, the JMX Exporter Isolator Java agent provides enhanced flexibility by allowing you to run multiple isolated and independent JMX Exporter Java agents simultaneously. As a result, each isolated JMX Exporter Java agent can maintain its own distinct configuration, including separate ports, rules, and other settings.</li> <li>Standalone exporter: Alternatively, the Standalone JMX Exporter operates as a completely separate application that connects to your target application using RMI to collect JMX MBean values. However, this approach requires more complex RMI configuration compared to the Java Agent options.</li> </ul> <p>Furthermore, both the Java Agent exporter and Isolator Java Agent support multiple operational modes to accommodate different monitoring architectures:</p> <ul> <li>HTTP mode: This mode collects metrics on-demand when accessed via HTTP requests, returning the metrics data as HTTP content. This represents a pull model where Prometheus actively scrapes the metrics endpoint.</li> <li>OpenTelemetry mode: In contrast, this mode periodically collects metrics and proactively pushes them to an OpenTelemetry endpoint. This follows a push model for metric delivery.</li> <li>Combined mode: Finally, this mode provides maximum flexibility by enabling both HTTP mode and OpenTelemetry mode metrics collection methods simultaneously.</li> </ul> <p>See <code>prometheus/jmx_exporter</code> for more details on JMX Exporter.</p>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/monitoring/#references","title":"References","text":"<ul> <li>Monitoring with JMX | Trino</li> <li>Trino Monitoring (JMX) with Prometheus and Grafana</li> </ul>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/oauth2/","title":"Configure OAuth 2.0 Authentication","text":"<p>To implement OAuth 2.0 authentication with HTTPS support for both Trino's Web UI and JDBC driver connections, two main configuration areas must be addressed:</p> <ul> <li>TLS/HTTPS Configuration: Enables secure connections for Trino clients accessing the Web UI and using JDBC drivers (represented by blue lines in the diagram)</li> <li>Internal Communication Security: Establishes secure communication channels between Trino cluster nodes, including coordinator and workers (represented by red lines in the diagram)</li> </ul>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/oauth2/#configure-tlshttps","title":"Configure TLS/HTTPS","text":"<p>By default, Trino operates without any security measures enabled. This configuration permits connections to the server through HTTP protocol URLs when accessing Trino via the CLI, Web UI, or other client applications.</p> <p>To enable TLS support for Trino, you have two implementation options:</p> <ul> <li>Use a load balancer or reverse proxy to handle TLS termination. This is the recommended and simplest approach for most deployments.</li> <li>Configure TLS directly on the Trino server. This method requires obtaining a valid certificate and configuring it within the Trino coordinator settings.</li> </ul> <p>Since we're deploying Trino within a local Kubernetes environment, we'll implement the second approach by configuring TLS/HTTPS directly on the Trino server using self-signed certificates.</p>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/oauth2/#automated-certificate-generation","title":"Automated Certificate Generation","text":"<p>Use the provided script to automatically generate all necessary certificates and files:</p> <pre><code>./generate-tls-certs.sh\n</code></pre> generate-tls-certs.sh <pre><code>#!/bin/bash\n\nset -euo pipefail\n\n\nCERT_DIR=\".cert\"\nmkdir -p \"$CERT_DIR\"\nrm -f \"$CERT_DIR\"/*\n\n\necho \"Creating TLS certificates for Trino...\"\n\n\necho \"Step 1: Creating Private Key...\"\nopenssl genrsa -out $CERT_DIR/private.key 2048 &gt; /dev/null\necho \"Step 1: Completed.\"\n\n\necho \"Step 2: Creating Certificate...\"\nopenssl req \\\n  -new \\\n  -x509 \\\n  -key $CERT_DIR/private.key \\\n  -config openssl.cnf \\\n  -days 365 \\\n  -extensions v3_req \\\n  -out $CERT_DIR/certificate.crt &gt; /dev/null\necho \"Step 2: Completed.\"\n\n\necho \"Step 3: Combining Private Key and Certificate...\"\ncat $CERT_DIR/private.key $CERT_DIR/certificate.crt &gt; $CERT_DIR/trino-dev.pem\nrm -f $CERT_DIR/private.key $CERT_DIR/certificate.crt\necho \"Step 3: Completed.\"\n\n\necho \"Step 4: Creating Kubernetes secret...\"\nkubectl create secret generic trino-tls-secret \\\n    --from-file=trino-dev.pem=\"$CERT_DIR/trino-dev.pem\" \\\n    --dry-run=client -o yaml &gt; \"./trino-tls-secret.yaml\"\necho \"Step 4: Completed.\"\n\n\necho \"Certificate generation completed successfully!\"\necho \"\"\necho \"Generated files:\"\necho \"  - $CERT_DIR/trino-dev.pem (with private key and certificate)\"\necho \"  - trino-tls-secret.yaml (Kubernetes secret manifest)\"\n</code></pre> Details <p>The automated script performs the following steps. Each step can also be executed manually if needed:</p> <p>Step 1: Create Private Key</p> <pre><code>echo \"Step 1: Creating Private Key...\"\nopenssl genrsa -out $CERT_DIR/private.key 2048 &gt; /dev/null\necho \"Step 1: Completed.\"\n</code></pre> <p>What this does:</p> <ul> <li>Uses RSA algorithm to generate a 2048-bit private key</li> <li>Saves the private key to <code>.cert/private.key</code></li> <li>This private key will be used to create the self-signed certificate</li> </ul> <p>Step 2: Create Self-Signed Certificate</p> <pre><code>echo \"Step 2: Creating Certificate...\"\nopenssl req \\\n  -new \\\n  -x509 \\\n  -key $CERT_DIR/private.key \\\n  -config openssl.cnf \\\n  -days 365 \\\n  -extensions v3_req \\\n  -out $CERT_DIR/certificate.crt &gt; /dev/null\necho \"Step 2: Completed.\"\n</code></pre> <p>What this does:</p> <ul> <li>Creates a new self-signed X.509 certificate using the private key</li> <li>Uses the OpenSSL configuration file (<code>openssl.cnf</code>) for certificate settings</li> <li>Certificate is valid for 365 days</li> <li>Applies <code>v3_req</code> extensions which include Subject Alternative Names (SAN)</li> <li>The certificate hostname and SAN entries are configured in <code>openssl.cnf</code></li> <li>Saves the certificate to <code>.cert/certificate.crt</code></li> </ul> openssl.cnf<pre><code>[req]\ndefault_bits       = 2048\nprompt             = no\ndefault_md         = sha256\nreq_extensions     = v3_req\ndistinguished_name = dn\n\n[dn]\nC  = TW\nST = Taipei\nL  = Taipei\nO  = Velano Collective\nOU = Data Platform\nCN = trino.trino.svc.cluster.local\n\n[v3_req]\nsubjectAltName = @alt_names\n\n[alt_names]\nDNS.1 = localhost\nIP.1  = 127.0.0.1\n</code></pre> <p>Step 3: Combine Private Key and Certificate into PEM Format</p> <pre><code>echo \"Step 3: Combining Private Key and Certificate...\"\ncat $CERT_DIR/private.key $CERT_DIR/certificate.crt &gt; $CERT_DIR/trino-dev.pem\nrm -f $CERT_DIR/private.key $CERT_DIR/certificate.crt\necho \"Step 3: Completed.\"\n</code></pre> <p>What this does:</p> <ul> <li>Combines the private key and certificate into a single PEM file</li> <li>Creates <code>.cert/trino-dev.pem</code> containing both private key and certificate</li> <li>This combined PEM file is commonly used by applications that need both components</li> <li>Note: This file contains the private key and must be protected properly</li> </ul> <p>Step 4: Create Kubernetes Secret Manifest</p> <pre><code>echo \"Step 4: Creating Kubernetes secret...\"\nkubectl create secret generic trino-tls-secret \\\n    --from-file=trino-dev.pem=\"$CERT_DIR/trino-dev.pem\" \\\n    --dry-run=client -o yaml &gt; \"./trino-tls-secret.yaml\"\necho \"Step 4: Completed.\"\n</code></pre> <p>What this does:</p> <ul> <li>Generates a Kubernetes secret YAML manifest file</li> <li>Contains the <code>trino-dev.pem</code> file for deployment</li> <li>Uses <code>--dry-run=client</code> to generate the YAML without applying it to the cluster</li> <li>The secret can be applied directly to the Kubernetes cluster with <code>kubectl apply</code></li> </ul> <p>After running the script, you will find the generated files in the <code>.cert/</code> directory, the Kubernetes secret manifest file will be named <code>trino-tls-secret.yaml</code> and it should look like this:</p> Result trino-tls-secret.yaml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  creationTimestamp: null\n  name: trino-tls-secret\ndata:\n  trino-dev.pem: xxx\n</code></pre>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/oauth2/#mount-the-tls-certificate","title":"Mount the TLS Certificate","text":"<p>To utilize the generated TLS certificate in your Trino deployment, create a Kubernetes secret and mount it to the coordinator's <code>/etc/trino/tls</code> directory.</p> <pre><code>kubectl apply -f .cert/trino-tls-secret.yaml --namespace trino\n</code></pre> trino/values-template.yaml coordinator<pre><code>coordinator:\n  jvm:\n    maxHeapSize: \"8G\"\n  livenessProbe:\n    initialDelaySeconds: 30\n    failureThreshold: 20\n  readinessProbe:\n    initialDelaySeconds: 30\n    failureThreshold: 20\n  secretMounts:\n    - name: trino-tls\n      secretName: trino-tls-secret\n      path: /etc/trino/tls\n  additionalExposedPorts:\n    https:\n      servicePort: 8443\n      name: https\n      port: 8443\n      protocol: TCP\n  additionalJVMConfig:\n    - --add-opens=java.base/java.nio=ALL-UNNAMED # https://trino.io/docs/current/connector/bigquery.html#arrow-serialization-support\n    - --sun-misc-unsafe-memory-access=allow\n  # access-control.properties: |\n  #   access-control.name=file\n  #   security.refresh-period=150s\n  #   security.config-file=/etc/trino/access-control/rules.json\n</code></pre> <p>Next, configure the coordinator to use the TLS certificate by specifying its location in the <code>http-server.https.keystore.path</code> setting:</p> trino/values-template.yaml server<pre><code>server:\n  workers: 2\n  config:\n    authenticationType: \"oauth2\"\n  coordinatorExtraConfig: |\n    http-server.https.enabled=true\n    http-server.https.port=8443\n    http-server.https.keystore.path=/etc/trino/tls/trino-dev.pem\n    http-server.authentication.oauth2.issuer=https://accounts.google.com\n    http-server.authentication.oauth2.client-id=$OAUTH2_CLIENT_ID\n    http-server.authentication.oauth2.client-secret=$OAUTH2_CLIENT_SECRET\n    http-server.authentication.oauth2.principal-field=email\n    http-server.authentication.oauth2.scopes=openid,email\n    web-ui.authentication.type=oauth2\n  #   distributed-sort=true\n  #   query.max-history=50\n  exchangeManager:\n    name: filesystem\n    baseDir: $EXCHANGE_S3_URLS\n  # workerExtraConfig: |\n  #   discovery.uri=http://trino.trino.svc.cluster.local:8080\n</code></pre>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/oauth2/#enable-and-expose-the-https-endpoint","title":"Enable and Expose the HTTPS Endpoint","text":"<p>To activate HTTPS in Trino, configure the <code>http-server.https.enabled</code> setting to <code>true</code> and specify the <code>http-server.https.port</code> (typically <code>8443</code>). Additionally, expose this port through the Kubernetes service configuration.</p> trino/values-template.yaml server<pre><code>server:\n  workers: 2\n  config:\n    authenticationType: \"oauth2\"\n  coordinatorExtraConfig: |\n    http-server.https.enabled=true\n    http-server.https.port=8443\n    http-server.https.keystore.path=/etc/trino/tls/trino-dev.pem\n    http-server.authentication.oauth2.issuer=https://accounts.google.com\n    http-server.authentication.oauth2.client-id=$OAUTH2_CLIENT_ID\n    http-server.authentication.oauth2.client-secret=$OAUTH2_CLIENT_SECRET\n    http-server.authentication.oauth2.principal-field=email\n    http-server.authentication.oauth2.scopes=openid,email\n    web-ui.authentication.type=oauth2\n  #   distributed-sort=true\n  #   query.max-history=50\n  exchangeManager:\n    name: filesystem\n    baseDir: $EXCHANGE_S3_URLS\n  # workerExtraConfig: |\n  #   discovery.uri=http://trino.trino.svc.cluster.local:8080\n</code></pre> trino/values-template.yaml coordinator<pre><code>coordinator:\n  jvm:\n    maxHeapSize: \"8G\"\n  livenessProbe:\n    initialDelaySeconds: 30\n    failureThreshold: 20\n  readinessProbe:\n    initialDelaySeconds: 30\n    failureThreshold: 20\n  secretMounts:\n    - name: trino-tls\n      secretName: trino-tls-secret\n      path: /etc/trino/tls\n  additionalExposedPorts:\n    https:\n      servicePort: 8443\n      name: https\n      port: 8443\n      protocol: TCP\n  additionalJVMConfig:\n    - --add-opens=java.base/java.nio=ALL-UNNAMED # https://trino.io/docs/current/connector/bigquery.html#arrow-serialization-support\n    - --sun-misc-unsafe-memory-access=allow\n  # access-control.properties: |\n  #   access-control.name=file\n  #   security.refresh-period=150s\n  #   security.config-file=/etc/trino/access-control/rules.json\n</code></pre>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/oauth2/#secure-internal-communication","title":"Secure Internal Communication","text":"<p>To enhance security, the Trino cluster supports secure internal communication (red lines) between nodes through authentication mechanisms, with optional TLS encryption for additional protection.</p> <p>Configure the shared secret by setting the same value in the config.properties file across all cluster nodes (both coordinator and workers):</p> <pre><code>internal-communication.shared-secret=&lt;secret&gt;\n</code></pre> <p>A large random key is recommended, and can be generated with the following Linux command:</p> <pre><code>openssl rand 512 | base64\n</code></pre> trino/values-template.yaml additionalConfigProperties<pre><code>additionalConfigProperties:\n  - internal-communication.shared-secret=$INTERNAL_SHARED_SECRET\n  - retry-policy=TASK\n</code></pre> <p>The configurations under <code>additionalConfigProperties</code> will impact all the nodes in the Trino Cluster.</p>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/oauth2/#create-google-oauth-20-client","title":"Create Google OAuth 2.0 Client","text":"<p>Since we're using Google as our OAuth2 provider, we'll need to set up a Google OAuth 2.0 client. This is a straightforward process, but there are a few specific configuration details to keep in mind for Trino.</p> <p>Here are the key configuration settings you'll need:</p> <ul> <li>Application Type: Web application</li> <li>Name: Trino (It will be only used in the Google Cloud Console for identification purposes)</li> <li>Audience: Set to internal (Typically, Trino is used within an organization, so this setting is appropriate)</li> <li>Authorized Redirect URIs: Add these two URLs:<ul> <li><code>https://127.0.0.1:8443/oauth2/callback</code></li> <li><code>https://127.0.0.1:8443/ui/logout/logout.html</code></li> </ul> </li> </ul> <p>After createing the OAuth 2.0 client, you will be provided with a client ID and a client secret and you will need to copy these values into the Trino configuration.</p>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/oauth2/#configure-oauth-20-authentication_1","title":"Configure OAuth 2.0 Authentication","text":"<p>To activate OAuth 2.0 authentication in Trino, configure the following essential properties in the coordinator settings:</p> <ul> <li>Client ID: OAuth 2.0 client identifier obtained from Google Cloud Console</li> <li>Client Secret: OAuth 2.0 client secret obtained from Google Cloud Console  </li> <li>Authorization Server URL: Google's OAuth 2.0 authorization endpoint</li> <li>Web UI Authentication Type: Must be set to <code>oauth2</code> to enable OAuth 2.0 for the Web UI</li> </ul> <p>These configurations establish the connection between Trino and the OAuth 2.0 provider, enabling secure authentication for both Web UI and programmatic access.</p> trino/values-template.yaml server<pre><code>server:\n  workers: 2\n  config:\n    authenticationType: \"oauth2\"\n  coordinatorExtraConfig: |\n    http-server.https.enabled=true\n    http-server.https.port=8443\n    http-server.https.keystore.path=/etc/trino/tls/trino-dev.pem\n    http-server.authentication.oauth2.issuer=https://accounts.google.com\n    http-server.authentication.oauth2.client-id=$OAUTH2_CLIENT_ID\n    http-server.authentication.oauth2.client-secret=$OAUTH2_CLIENT_SECRET\n    http-server.authentication.oauth2.principal-field=email\n    http-server.authentication.oauth2.scopes=openid,email\n    web-ui.authentication.type=oauth2\n  #   distributed-sort=true\n  #   query.max-history=50\n  exchangeManager:\n    name: filesystem\n    baseDir: $EXCHANGE_S3_URLS\n  # workerExtraConfig: |\n  #   discovery.uri=http://trino.trino.svc.cluster.local:8080\n</code></pre> <p>Trino can automatically retrieve OAuth 2.0 configuration settings from the OIDC provider's metadata document (such as Google's configuration endpoint). When the coordinator starts up, it fetches this metadata document and automatically configures the necessary OAuth 2.0 authentication properties, removing the need for manual configuration of these settings.</p> <p>The sequence diagram below illustrates the OAuth 2.0 authentication flow in Trino:</p> <pre><code>sequenceDiagram\n  participant Browser\n  participant Coordinator as Trino Coordinator\n  participant Server as Google OAuth Server\n\n  Browser-&gt;&gt;Coordinator: 1. Request Trino Web UI\n  Coordinator--&gt;&gt;Browser: 2. Redirect to Google OAuth (auth URL)\n  Browser-&gt;&gt;Server: 3. User login + consent authorization\n  Server--&gt;&gt;Browser: 4. Redirect back to Trino + authorization code\n  Browser-&gt;&gt;Coordinator: 5. Send authorization code to Trino /oauth2/callback\n  Coordinator-&gt;&gt;Server: 6. Exchange authorization code for access token + ID token\n  Server--&gt;&gt;Coordinator: 7. Return token(s)\n  Coordinator--&gt;&gt;Browser: 8. Display Web UI (logged in)\n\n  note right of Coordinator: Trino uses email from ID token as user identity</code></pre>","tags":["Trino"]},{"location":"side-projects/retail-lakehouse/trino/oauth2/#references","title":"References","text":"<ul> <li>TLS and HTTPS</li> <li>Secure internal communication</li> <li>OAuth 2.0 authentication</li> </ul>","tags":["Trino"]},{"location":"system-design/","title":"System Design Questions","text":""},{"location":"system-design/#schedule","title":"Schedule","text":"Date Candidate Question Jul. 26, 2025 KC Design Dropbox Jul. 23, 2025 KC Design FB Post Search Jul. 19, 2025 KC \u2705 Design a Top K Heavy Hitters Service Jul. 15, 2025 KC \u2705 Design Ticketmaster Jul. 12, 2025 KC \u2705 Design a Distributed Message Queue Jyl.  9, 2025 KC \u2705 Design Uber"},{"location":"system-design/#core-concepts","title":"Core Concepts","text":"<ul> <li> CAP Theorem</li> <li> Consistent Hashing</li> <li> Indexing</li> <li> Locking</li> <li> Consensus Algorithms</li> <li> API Performance Optimization</li> <li> Database Performance Optimization</li> <li> Scale From Zero To Millions Of Users</li> <li> Back-of-the-envelope Estimation</li> </ul>"},{"location":"system-design/#key-technologies","title":"Key Technologies","text":"<ul> <li> Kafka</li> <li> Spark</li> <li> Flink</li> <li> DynamoDB</li> <li> Elasticsearch</li> <li> Redis</li> </ul>"},{"location":"system-design/#product","title":"Product","text":"<p>In these interviews, you'll be asked to design a system behind a product.</p> <ul> <li> Design a Ride-Sharing Service Like Uber</li> <li> Design a Ticket Booking Site Like Ticketmaster</li> <li> Design a Messaging Service Like WhatsApp</li> </ul>"},{"location":"system-design/#infrastructure","title":"Infrastructure","text":"<p>In these interviews, you'll be asked to design a system that supports a particular infrastructure use case.</p> <ul> <li> Design a Distributed Message Queue</li> <li> Design a Top K Heavy Hitters Service</li> <li> Design a Rate Limiter</li> <li> Design a Notification Service</li> <li> Design a Key-Value Store</li> </ul>"},{"location":"system-design/#machine-learning","title":"Machine Learning","text":"<p>In these interviews, you'll be asked to design a system that supports a particular machine learning use case.</p> <ul> <li> Design an Instagram Ranking Model</li> <li> Deploy and Monitor an Instagram Ranking Model</li> <li> Design a Spotify Recommendation System</li> <li> Design Evaluation Framework for Ads Ranking System</li> <li> Design a System to Predict Netflix Watch Times</li> <li> Train a Model to Detect Bots</li> <li> Design a Landmark Recognition System</li> <li> Design a System to Predict Youtube Ad Conversions</li> <li> Design an ETA System for a Maps App</li> <li> Design an App Suggestion System for Phones</li> </ul>"},{"location":"system-design/#data-pipeline","title":"Data Pipeline","text":"<p>In these interviews, you'll be asked to design a system that supports a particular data pipeline use case.</p> <ul> <li> Create a Data Pipeline for Netflix Clickstream Data</li> <li> Design an ETL Process for Student Interaction</li> <li> Design an ETL Pipeline for a ML Platform for AWS</li> </ul>"},{"location":"system-design/#data-modeling","title":"Data Modeling","text":"<p>In these interviews, you'll be asked to design a data model for a particular use case.</p> <ul> <li> Design a Data Warehouse Schema for a Ride-Sharing Service</li> <li> Design a Data Warehouse Schema for Customer Support</li> <li> Design a Data Warehouse Schema for Airbnb</li> <li> Design a Data Warehouse Schema for Stripe</li> <li> Design a Data Warehouse Schema for Instagram</li> </ul>"},{"location":"system-design/#data-architecture","title":"Data Architecture","text":"<p>In these interviews, you'll be asked to design a data architecture for a particular use case.</p> <p>System Design Trade-Offs</p> Video <p></p> <ul> <li>SQL vs NoSQL</li> <li>Normalization vs Denormalization</li> <li>Consistency vs Availability</li> <li>Strong Consistency vs Eventual Consistency</li> <li>Batch Processing vs Stream Processing</li> </ul> <p></p> <ul> <li>Vertical Scaling vs Horizontal Scaling</li> <li>REST vs graphQL</li> <li>Stateful vs Stateless</li> <li>Read-through vs Write-through Caching</li> <li>Sync vs Async Processing</li> </ul> <p>What is gRPC?</p> Answer <p>gRPC</p> <p>https://www.youtube.com/shorts/t0ONFCY6NWI</p> <p>Distributed System Design Patterns</p> Video <p></p> <ul> <li>Ambassador</li> <li>Circuit Breaker</li> <li>CQRS</li> <li>Event Sourcing</li> <li>Leader Election</li> <li>Pub-Sub</li> <li>Sharding</li> </ul> <p>What is OAuth 2.0?</p> Answer <p>OAuth 2.0 is an authorization framework that allows third-party applications to access a user's resources without exposing their credentials. It involves four main roles:</p> <ul> <li>the resource owner (typically the user),</li> <li>the client (the app that wants access),</li> <li>the authorization server, and</li> <li>the resource server.</li> </ul> <p>The user gives permission to the client through the authorization server, which then issues an access token. This token is then used by the client to access the user's resources on the resource server. It keeps everything secure and user credentials safe!</p>"},{"location":"system-design/core-concepts/locking/","title":"Locking","text":"<p>What is a distributed lock and why do we need it?</p> Answer <p>A distributed lock is a mechanism used in distributed systems to ensure that only one process or thread can access a shared resource at a time. This is important for preventing conflicts and ensuring data consistency, especially when multiple processes are running on different machines. It's similar to how a traditional database lock works, but it's designed to work across multiple machines or services.</p> <p>Could you give an practical example of how it works?</p> Answer <p>Take an online ticketing platform like Ticketmaster. When a user selects a ticket and starts the checkout process, you need to temporarily lock that ticket \u2014 maybe for 10 minutes \u2014 so no one else can buy it during that time. A distributed lock helps enforce that across all servers in the system. </p> <p>A common way to implement it is using a distributed key-value store like Redis. For example, let's say we want to lock a concert ticket with ID <code>abc</code>. We can store a key like <code>abc</code> in Redis with a value like locked. Since Redis operations are atomic, if another process tries to lock the same ticket, it'll fail because the key already exists. Once the first process is done with the lock, it can set the value of <code>abc</code> to <code>unlocked</code> and another process can acquire the lock.</p> <p>Another handy feature of distributed locks is that they can be set to expire after a certain amount of time. This is great for ensuring that locks don't get stuck in a locked state if a process crashes or is killed. For example, if you set the value of <code>abc</code> to <code>locked</code> and then the process crashes, the lock will expire after a certain amount of time (like 10 minutes) and another process can acquire the lock at that point.</p> <p>What are typical use cases for distributed locks?</p> Answer <p>Sure, there are several scenarios where distributed locks are really useful, especially in large-scale systems where consistency and coordination across services or nodes are critical.</p> <p>First, a common example is in e-commerce checkout systems. Let's say you're selling a limited-edition sneaker. When a user adds it to their cart and proceeds to checkout, you want to prevent others from buying the same item at the same time. So you might use a distributed lock to \"hold\" that item for the user for a short period \u2014 maybe 10 minutes \u2014 while they complete the payment process. This helps avoid overselling.</p> <p>Another case is in ride-sharing or food delivery platforms. When a rider or customer makes a request, the system needs to assign a nearby driver. Here, a distributed lock can help ensure that one driver isn\u2019t accidentally assigned to multiple customers. The lock would be held while waiting for the driver to confirm or reject the assignment, or until a timeout.</p> <p>Lastly, in online auction platforms, distributed locks can be helpful during the final moments of bidding. For example, when a user places a bid at the last second, you want to briefly lock the auction item to ensure that only one bid is processed at a time. This avoids race conditions and ensures the integrity of the bidding process.</p> <p>What is deadlocks? How to prevent or deal with it?</p> Answer <p>Deadlocks can occur when two or more processes are waiting for each other to release a lock. Think about a situation where two processes are trying to acquire two locks at the same time. One process acquires lock A and then tries to acquire lock B, while the other process acquires lock B and then tries to acquire lock A. This can lead to a situation where both processes are waiting for each other to release a lock, causing a deadlock.</p> <p>To prevent them, you can use techniques like setting timeouts on your locks, so if a lock isn't released within a certain period, it just gets automatically released. Another approach is to implement lock ordering, where you make sure that all processes acquire locks in a specific order to avoid circular waiting.</p> <p>What is the difference between optimistic and pessimistic locking? What are the pros and cons of each approach, and when should each be used?</p> Answer <p>Optimistic and pessimistic locking are two strategies for managing resource consistency.</p> <p>With optimistic locking, the system assumes that conflicts are rare, so it doesn't lock the data when a user starts a transaction. Instead, it checks for conflicts when the transaction is committed. If there's a conflict, it will prompt the user to retry. It's great when conflicts are infrequent because it reduces the overhead of locking resources and is generally more efficient when conflicts are rare.</p> <p>On the other hand, pessimistic locking assumes that conflicts are likely, so it locks the data as soon as a transaction starts. This prevents other transactions from accessing the data until the lock is released. It's useful when you expect a lot of contention, but it can cause delays and can reduce system performance if overused especially if there are a lot of transactions happening at once.</p> <p>In terms of when to use each: optimistic locking is great for scenarios where you don't expect a lot of conflicts, and you want to keep things running smoothly and efficiently. Pessimistic locking is better when you know there will be a lot of contention and you need to ensure that no data corruption or inconsistency happens.</p> <ul> <li>Locking | Hello Interview</li> <li>Distributed Lock | Hello Interview</li> </ul>","tags":["System Design"]},{"location":"system-design/core-concepts/api-perf-opt/","title":"API Performance Optimization","text":"<p>How to Improve API Performance?</p> Answer <ul> <li>Caching</li> <li>Connection Pooling</li> <li>Pagination</li> <li>Asynchronous Processing</li> <li>Avoiding N+1 Queries</li> <li>Payload Compression</li> <li>JSON Serializers</li> </ul>","tags":["System Design"]},{"location":"system-design/core-concepts/cap/","title":"CAP","text":"<p>What is CAP Theorem?</p> Answer <p>CAP theorem is a concept in distributed computing states that a distributed data store can't simultaneously provide all three of the following guarantees: consistency, availability, and partition tolerance. According to the theorem, a system can only guarantee two out of these three at the same time. It's a useful way to think about the trade-offs when choosing distributed systems.</p> <p>With the CAP theorem,</p> <ul> <li>the first part is consistency, which means that every time you read data, you get the most up-to-date information, or you get an error if it can't provide that up-to-date information.</li> <li>The second part is availability, which means that every request you make gets a response, even if it might not be the most up-to-date information.</li> <li>And the third part is partition tolerance, which means that the system continues to work even if there are network issues that split the system into separate parts.</li> </ul> <p>Let's go through three examples, each focusing on a different pair of the CAP theorem guarantees.</p> <p>First, if we consider a system that prioritizes consistency and availability (CA), it means that every read will get the latest write, and the system will always respond to requests. However, if there's a network partition, the system might have to sacrifice partition tolerance because it will need to ensure that all nodes are perfectly in sync before responding. These systems are often used in environments where network partitions are either rare or not a big concern. A good example is a traditional relational database that runs on a single server or a cluster that doesn't usually face network issues. Banks or financial institutions often prefer CA systems because they need consistent and available data, and they can control the environment to reduce network partitions.</p> <p>Second, if we look at a system that ensures consistency and partition tolerance (CP), that means it will always give you the most up-to-date data, and it will continue to function even if there's a network partition. But it might sacrifice availability because it could refuse to respond if it can't guarantee that consistency. An example of this would be a distributed database that uses strong consistency protocols, to ensure that most of the nodes agree on the data before responding, even if that means delaying responses. This is often important in industries like healthcare or finance, where data accuracy is critical, and you want to ensure that all parts of the system reflect the same data, even if that means sacrificing a bit of availability.</p> <p>Lastly, if we focus on availability and partition tolerance (AP), that means the system will always give a response and keep working even if there's a network partition, but it might not always give you the most up-to-date data. This is what we often see in eventually consistent systems, like some NoSQL databases. An example of this would be a social media platform where you can still post updates and read posts even if some parts of the system are temporarily out of sync. These systems are designed to handle high traffic and ensure that users can always interact with the platform, even if it means that some data might not be perfectly consistent across all nodes at all times.</p>","tags":["System Design"]},{"location":"system-design/core-concepts/consensus-algorithms/","title":"Consensus Algorithms","text":"<p>How Raft Work?</p> Answer <p>Raft is a consensus algorithm that ensures all nodes in a distributed system agree on a shared log, even with failures. It works by electing a leader, which handles all client requests and replicates log entries to followers. Followers apply entries once a majority (quorum) agrees.</p> <p>Raft has three main phases: Leader Election, Log Replication, and Safety. If a follower doesn't hear from a leader, it becomes a candidate and starts an election. Once elected, the leader sends periodic heartbeats to maintain authority. This design is easier to understand and implement compared to Paxos.</p>","tags":["System Design"]},{"location":"system-design/core-concepts/consistent-hashing/","title":"Consistent Hashing","text":"<ul> <li>Consistent Hashing | Hello Interview</li> <li>Design Consistent Hashing | ByteByteGo</li> </ul> <p>What is Consistent Hashing?</p> Answer <p>Consistent hashing is a technique used to distribute data across a cluster of nodes in a way that minimizes data movement when nodes are added or removed. It allows for efficient scaling and load balancing in distributed systems.</p>","tags":["System Design"]},{"location":"system-design/core-concepts/db-perf-opt/","title":"Database Performance Optimization","text":"<p>How to Scale Your Database?</p> Answer <ul> <li>Indexing</li> <li>Materialized Views</li> <li>Denormalization</li> <li>Vertical Scaling</li> <li>Horizontal Scaling</li> <li>Database Caching</li> <li>Replication</li> <li>Sharding</li> </ul>","tags":["System Design"]},{"location":"system-design/core-concepts/indexing/","title":"Indexing","text":"<p>What is Geohashing?</p> Answer <p>Geohashing is a method of encoding geographic coordinates (latitude and longitude) into a compact string representation. It divides the world into a grid of cells, each identified by a unique alphanumeric code. This allows for efficient spatial indexing and querying of geographic data.</p> <p>What is Quadtree?</p> Answer <p>A quadtree is a tree data structure used to partition a two-dimensional space by recursively subdividing it into four quadrants or regions. It is commonly used for spatial indexing, allowing efficient storage and retrieval of spatial data, such as points, rectangles, or polygons. Each node in the quadtree represents a region of space, and its children represent subdivisions of that region.</p>","tags":["System Design"]},{"location":"system-design/infra/distributed-message-queue/","title":"Design a Distributed Message Queue","text":"<ul> <li>Kafka</li> </ul>","tags":["System Design"]},{"location":"system-design/infra/distributed-message-queue/#clarifying-questions","title":"Clarifying Questions","text":"<p>To better align the design with real-world use, I'd like to first ask some clarifying questions. This will help ensure I'm solving the right problem.</p> <ul> <li>What are the business scenarios that will rely on this system?</li> <li>What are the primary use cases for this message queue?</li> <li> <p>Who are the consumers of this system?</p> </li> <li> <p>What level of delivery guarantee is expected \u2014 at-most-once, at-least-once, or exactly-once?</p> </li> <li> <p>Do consumers need to receive messages in order (FIFO)?</p> </li> <li> <p>Should messages be persisted to disk for durability or just kept in memory?</p> </li> <li>How long should messages be retained in the system?</li> <li>Do we need the ability to replay or reprocess past messages?</li> </ul>","tags":["System Design"]},{"location":"system-design/infra/distributed-message-queue/#requirements","title":"Requirements","text":"","tags":["System Design"]},{"location":"system-design/infra/distributed-message-queue/#high-level-design-deep-dive","title":"High-Level Design &amp; Deep Dive","text":"","tags":["System Design"]},{"location":"system-design/infra/distributed-message-queue/#feedback","title":"Feedback","text":"","tags":["System Design"]},{"location":"system-design/infra/distributed-message-queue/#questions","title":"Questions","text":"<p>What is Delivery Semantics?</p> Answer <p>Delivery semantics define how a messaging system guarantees the delivery of messages between producers and consumers. There are three main types:</p> <ol> <li> <p>At most once \u2013 Messages are delivered at most one time, meaning some messages might be lost but never duplicated. It prioritizes speed over reliability.</p> </li> <li> <p>At least once \u2013 Messages are delivered one or more times. There is a risk of duplicates, but no message is lost. The system retries until acknowledged.</p> </li> <li> <p>Exactly once \u2013 Messages are delivered only once, with no duplicates or loss. It's the most complex and resource-intensive to implement, but ensures strong consistency.</p> </li> </ol> <p>What Is 2 Phase Commit (2PC)?</p> Answer <p>Two-phase commit (2PC) is a protocol used to ensure atomicity across multiple systems in a distributed transaction. It involves two steps:</p> <ol> <li>Prepare phase: The coordinator asks all participants if they're ready to commit. Each participant replies \"yes\" (ready) or \"no\" (abort).</li> <li>Commit phase: If all say \"yes,\" the coordinator tells everyone to commit. If any say \"no,\" it tells all to roll back.</li> </ol> <p>This ensures all participants either commit or abort together, maintaining consistency across systems.</p> <p>Why Not Choose SQL and NoSQL as the Storage Layer?</p> Answer <p>I wouldn't choose NoSQL or traditional SQL databases as the storage layer for a distributed message queue because neither is optimized for the core requirements of such systems. Message queues are designed around append-only, log-structured storage to support high-throughput writes, ordered reads, and offset-based consumption. NoSQL databases prioritize flexible schemas and random access, while SQL databases focus on transactional consistency\u2014both introduce unnecessary overhead for sequential streaming workloads.</p> <p>Additionally, features like time-based retention, log compaction, and message replay would need to be manually implemented in a database setup, leading to higher complexity and lower performance. That's why purpose-built storage engines like Kafka's log segments or Pulsar's BookKeeper are preferred\u2014they're specifically designed to handle the scale, efficiency, and durability that message queues demand.</p> <p>What Is Leader Election in a Distributed System?</p> Answer <p>Leader election in a distributed system is the process of designating one node as the \"leader\" among a group of distributed nodes. The leader typically coordinates actions, manages state changes, or handles critical tasks like log replication in systems like Raft or ZooKeeper.</p> <p>It's essential for ensuring consistency and avoiding conflicts when multiple nodes compete to perform operations. The election must be fault-tolerant, meaning nodes must agree on the same leader even if there are network delays or failures. Techniques include timeouts, random backoffs, or consensus algorithms like Paxos or Raft.</p>","tags":["System Design"]},{"location":"system-design/infra/key-value-store/","title":"Design a Key-Value Store","text":"<ul> <li>Redis</li> <li>Design A Key-value Store | ByteByteGo</li> </ul>","tags":["System Design"]},{"location":"system-design/infra/notification-service/","title":"Design a Notification Service","text":"","tags":["System Design"]},{"location":"system-design/infra/rate-limiter/","title":"Design a Rate Limiter","text":"<ul> <li>Design A Rate Limiter | ByteByteGo </li> </ul> <p>What is Rate Limiter?</p> Answer <p>A rate limiter is essentially a tool or mechanism that helps control the number of requests or actions that a user or system can make within a certain time frame. It's a way to prevent overloading a system with too many requests at once, which can help maintain performance and stability. It's super useful for things like APIs, where you want to make sure that no single user can overwhelm the service.</p> <p>What Are the Common Strategies for Rate Limiting? What Are the Pros and Cons of Each? When to Use Them?</p> Answer <p>There are a few common strategies for implementing rate limiting. </p> <p>One popular approach is called the token bucket algorithm. It allows a certain number of tokens to be added to a bucket at a fixed rate. Each request consumes a token, and once the tokens run out, further requests have to wait until new tokens are added. The pros are that it's flexible and can handle short bursts of traffic really well, making it great for scenarios where you have unpredictable spikes. The cons are that it can be a bit complex to implement and fine-tune. It's best used when you want to allow bursts but still maintain a steady overall rate.</p> <p>Another approach is the leaky bucket algorithm, which processes requests at a fixed rate, similar to water leaking out of a bucket. If requests come in too quickly, they get queued up and processed at that steady rate. The pros here are that it smooths out traffic and prevents sudden spikes from overwhelming your system. The cons are that it can introduce some latency if there's a sudden influx of requests. This is best used when you want to maintain a consistent flow of requests and avoid sudden surges.</p> <p>There's also the fixed window counter, where counts the number of requests in a fixed time window, and if the count exceeds a certain limit, further requests are blocked until the window resets. The pros are that it's simple and easy to implement, and it's straightforward to understand. The cons are that it can lead to something called the \"thundering herd\" problem, where a lot of requests come in right at the start of a new window. This is best used when you want a straightforward approach and you're not too concerned about handling bursts more gracefully.</p> <p>Another common strategy is the sliding window algorithm. This method is a bit more dynamic than the fixed window because it uses a rolling or sliding window of time to count requests. This means that instead of resetting the count at the start of each fixed window, it continuously adjusts, giving you a more accurate and smoother rate limiting experience. The pros are that it reduces the burstiness that can happen at boundary points of fixed windows and provides a more even distribution of requests. The cons are that it can be a bit more complex to implement than the fixed window, but it's definitely worth it when you need that smoother rate limiting. It's great for situations where traffic patterns are more constant, and you want to avoid sudden spikes.</p> <p>What's your recommendation for getting started with implementing rate limiting?</p> Answer <p>A good starting point is to begin with something simple and easy to understand, like the token bucket algorithm. It's pretty flexible and works well for a variety of use cases. You can start by setting a reasonable limit based on your system's capacity and then adjust as you see how it affects your traffic. Once you're comfortable, you can experiment with other algorithms or even combine strategies to get the best of both worlds. It's all about iterating and fine-tuning as you go!</p> <p>Where should I implement rate-limiting logic \u2014 on the server side, client side, or in the middleware? What are your suggestions for each of these options?</p> Answer <p>You can actually implement rate limiting in all of those places, and each has its own advantages.</p> <p>On the client side, implementing rate limiting can help you avoid sending too many requests in the first place, which can reduce the load on the server and potentially save resources.</p> <p>Middleware is a great option because it sits between the client and the server, so it can handle rate limiting for multiple clients in a centralized way. This is often a sweet spot because it's easy to manage and update without touching the client or server code directly.</p> <p>On the server side, you have the most control because you can enforce limits directly where the resources are being consumed, but it can also add a bit of complexity and overhead.</p> <p>So, it really depends on your specific needs, but middleware is often a nice balance between the two!</p> <p>What are the pros and cons of limiting requests by user ID versus by IP address?</p> Answer <p>When you limit requests by user ID, it allows you to have more granular control, and it ensures that each individual user is treated fairly. This can be great for applications where users have accounts and you want to prevent any one user from monopolizing resources. The downside is that you need a reliable way to identify users, which might not always be possible, especially if users don't have to log in.</p> <p>On the other hand, limiting by IP address can be easier to implement because every request comes with an IP address, so you can start rate limiting without requiring user authentication. However, the downside is that multiple users might share the same IP address, like in an office or a household, which could lead to innocent users being rate limited. Plus, IP addresses can be spoofed or changed, so it might not always be as reliable as user identification.</p> <p>Ultimately, the best approach often depends on the nature of your application and your users.</p> <p>Which situations are best for each of these strategies (limiting by user ID or IP address), in your opinion?</p> Answer <p>If you have a user-centric application where users are authenticated and you can reliably identify them, then rate limiting by user ID is a great approach. It ensures fairness and prevents any single user from abusing the system.</p> <p>On the other hand, if you have an application where users are mostly anonymous or you don't have a reliable way to identify them, then IP-based rate limiting can still be effective. It\u2019s a good starting point, even though it might not be perfect.</p>","tags":["System Design"]},{"location":"system-design/infra/social-media-post-search/","title":"Design FB Post Search","text":"<ul> <li>Design FB Post Search</li> </ul>","tags":["System Design"]},{"location":"system-design/infra/top-k-heavy-hitters/","title":"Design a Top K Heavy Hitters Service","text":"<ul> <li>Design YouTube's Top K Videos Feature</li> <li>Design Spotify Top K Songs</li> </ul>","tags":["System Design"]},{"location":"system-design/infra/top-k-heavy-hitters/#requirements","title":"Requirements","text":"","tags":["System Design"]},{"location":"system-design/infra/top-k-heavy-hitters/#core-entites-apis","title":"Core Entites &amp; APIs","text":"","tags":["System Design"]},{"location":"system-design/infra/top-k-heavy-hitters/#high-level-design","title":"High-Level Design","text":"","tags":["System Design"]},{"location":"system-design/infra/top-k-heavy-hitters/#deep-dive","title":"Deep Dive","text":"","tags":["System Design"]},{"location":"system-design/infra/top-k-heavy-hitters/#questions","title":"Questions","text":"<p>What is Count-Min Sketch?</p> Answer <p>Count-Min Sketch is a probabilistic data structure used to estimate the frequency of elements in a data stream using sublinear space. It trades off accuracy for memory efficiency.</p> <p>It works by hashing each incoming element with multiple hash functions and incrementing corresponding counters in a 2D array. To estimate the frequency of an element, it takes the minimum count across all its hash positions\u2014hence the name. It's commonly used for approximate counting in streaming systems, like estimating top-k queries or detecting heavy hitters.</p>","tags":["System Design"]},{"location":"system-design/key-technologies/dynamodb/","title":"DynamoDB","text":"<ul> <li>DynamoDB | Hello Interview</li> </ul>","tags":["DynamoDB"]},{"location":"system-design/key-technologies/elasticsearch/","title":"Elasticsearch","text":"<ul> <li>Elasicsearch | Hello Interview</li> </ul>","tags":["Elasticsearch"]},{"location":"system-design/key-technologies/flink/","title":"Flink","text":"<p>Apache Flink | Hello Interview</p>","tags":["Apache Flink"]},{"location":"system-design/key-technologies/kafka/","title":"Index","text":"<p>tags   - Apache Kafka</p>"},{"location":"system-design/key-technologies/kafka/#kafka","title":"Kafka","text":"<ul> <li>Kafka | Hello Interview</li> </ul> <p>Why is Kafka so fast? How does it work?</p> <p>What is Apache Kafka?</p> Answer <p>Apache Kafka is an open-source distributed event streaming platform. It's designed to handle real-time data feeds, allowing you to publish, subscribe to, store, and process streams of records in a fault-tolerant way. It's often used for building real-time data pipelines, streaming applications, and event-driven architectures. It was originally developed by LinkedIn and later became part of the Apache Software Foundation.</p> <p>What's the difference between Apache Kafka and RabbitMQ?</p> Answer <p>There are quite a few differences between Kafka and RabbitMQ!</p> <p>For starters, Kafka is a distributed event streaming platform, while RabbitMQ is a traditional message broker.</p> <p>In terms of storage, Kafka is built around the concept of a distributed commit log, which means all the messages are stored on disk in a durable, append-only fashion. This design allows Kafka to handle huge amounts of data and retain messages for long periods, which is great for scenarios where you might want to replay or reprocess data. RabbitMQ, on the other hand, focuses more on transient message delivery, and while it does have persistent storage options, it's generally optimized for ensuring that messages are delivered quickly to consumers rather than storing them for long-term replay.</p> <p>In terms of fault tolerance, Kafka is designed with replication in mind, so it can handle node failures gracefully. RabbitMQ also supports clustering and mirrored queues for fault tolerance, but it's not as inherently scalable as Kafka.</p> <p>And when it comes to consuming messages, Kafka consumers pull messages from a partitioned log, which allows them to reprocess messages if needed. RabbitMQ consumers, however, typically get messages pushed to them, which can be more straightforward for certain use cases.</p> <p>After a message is consumed, the behavior differs quite a bit between Kafka and RabbitMQ. In Kafka, consuming a message doesn't actually remove it from the queue. Instead, the message remains in the log for a configured retention period, and the consumer just keeps track of its own offset, which is basically a pointer to where it last read. This means that multiple consumers can read the same message at different times, and you can even rewind or reprocess messages if needed. In RabbitMQ, once a message is consumed and acknowledged, it's typically removed from the queue, so it's more of a one-time delivery model.</p> <p>What are the differences between Apache Kafka and Apache Pulsar?</p> Answer <p>Apache Kafka and Apache Pulsar differ mainly in architecture, message storage, and feature sets:</p> <p>Kafka uses a monolithic architecture where brokers handle both storage and serving. It stores messages in local disk files and relies on tools like ZooKeeper (or KRaft) for metadata. Pulsar, in contrast, has a decoupled architecture: brokers handle ingestion and dispatch, while BookKeeper handles durable storage, enabling better scalability and tiered storage support out of the box.</p> <p>Pulsar natively supports multi-tenancy, geo-replication, and topic-level message-level acknowledgment, while Kafka often requires external tools for those. However, Kafka has broader ecosystem support and is more mature in terms of community, integrations, and operational familiarity.</p> <p>What Is 2 Phase Commit (2PC) in Kafka?</p> Answer <p>In Kafka, the Two-Phase Commit (2PC) is used to achieve exactly-once semantics (EOS) across Kafka and an external system (like Flink or a database). Kafka supports this via its transactional API, which allows a producer to write to multiple partitions atomically.</p> <p>Phase 1 (Prepare): The producer sends data and marks it as part of a transaction. Kafka buffers these messages but doesn't expose them to consumers yet.</p> <p>Phase 2 (Commit or Abort): Once all messages are sent, the producer issues a commit or abort. Kafka then either makes all messages visible atomically or discards them.</p> <p>This mechanism ensures consistency but adds complexity and overhead, so it's used only when strict EOS is required.</p> <p>What is in-sync replication (ISR) in Kafka?</p> Answer <p>In Kafka, the In-Sync Replicas (ISR) are the set of replicas that are fully caught up with the leader for a given partition. They have received and acknowledged all messages the leader has committed. Kafka only considers a write successful when it's replicated to all brokers in the ISR set.</p> <p>ISR is critical for fault tolerance and exactly-once or at-least-once delivery guarantees. If the leader fails, Kafka will elect a new leader from the ISR to ensure no committed messages are lost. Replicas that fall behind too much are removed from the ISR until they catch up.</p>"},{"location":"system-design/key-technologies/redis/","title":"Redis","text":"<p>Redis | Hellow Interview</p>","tags":["Redis"]},{"location":"system-design/key-technologies/spark/","title":"Apache Spark","text":"What are The Similarities and Differences Between Apache Spark and Apache Flink? <p>Apache Spark and Apache Flink are both powerful distributed data processing frameworks, but they have some key similarities and differences.</p> <p>In terms of similarities, both frameworks support batch and stream processing, and they both provide high-level APIs in multiple languages, like Java, Scala, and Python. They also focus on fault tolerance, scalability, and exactly-once processing semantics.</p> <p>When it comes to differences, One of the main differences is that Flink is often considered to have a more native stream processing capability, meaning it was designed from the ground up to handle real-time data streams with very low latency. Spark, on the other hand, started primarily as a batch processing framework and later introduced Structured Streaming, which provides micro-batch processing.</p> <p>This means that while both can handle streaming data, Flink often achieves lower latency.</p> when you execution a query in spark, how does it work behind the scene? <p>When you run a query in Spark, it first goes through a few logical and physical planning phases. Spark builds a logical plan from your code, optimizes it into a physical plan, and then breaks it into stages and tasks. Those tasks are distributed across worker nodes by the driver.</p> <p>Each stage corresponds to a set of transformations that can be pipelined together, and between stages, data is shuffled if needed. Spark uses DAG scheduling to manage dependencies, and executors on worker nodes run the tasks and return results back to the driver.</p>","tags":["Apache Spark"]},{"location":"system-design/key-technologies/spark/#structure-streaming","title":"Structure Streaming","text":"What is Trigger Types in Apache Spark Structure Streaming <p>In Apache Spark Structured Streaming, trigger types determine how often the streaming query processes data and produces results. There are a few different trigger types you can use:</p> <ul> <li>Default Trigger: This is the default mode where Spark continuously processes data in micro-batches as soon as it arrives.</li> <li>Fixed Interval Trigger: You can specify a fixed processing interval (for example, every 10 seconds). Spark will wait for that interval to pass before processing the next micro-batch.</li> <li>One-Time Trigger: This trigger processes all available data once and then stops. It's useful for scenarios where you want to run a streaming job as a batch job.</li> <li>Continuous Trigger: This is an experimental trigger where Spark processes data continuously with very low latency, though it's still in development</li> </ul> What is Output Mode in Apache Spark Structured Streaming? <p>In Apache Spark Structured Streaming, the output mode basically defines how the processed data is written to the output sink. There are three main output modes:</p> <ol> <li> <p>Append Mode: Only the new rows that were added to the result table since the last trigger are written to the sink. This is the default mode for most sinks.</p> </li> <li> <p>Complete Mode: The entire updated result table is written to the sink every time there's a trigger. This mode is useful for aggregations where you want to keep updating the whole result.</p> </li> <li> <p>Update Mode: Only the rows that were updated since the last trigger are written to the sink. This is kind of a middle ground between Append and Complete modes.</p> </li> </ol> What is Watermarking in Apache Spark Structured Streaming? <p>Watermarking in Apache Spark Structured Streaming is a way to handle late data in event-time based processing. Essentially, it allows you to define how late data can arrive and still be considered for processing. You set a watermark on an event-time column, and Spark will use that to manage state and clean up old data, ensuring that the system doesn't hold onto too much state for too long. It's really useful for managing streaming data efficiently.</p> What is Time Window in Apache Spark Structured Streaming?","tags":["Apache Spark"]},{"location":"system-design/product/dropbox/","title":"Design Dropbox","text":"","tags":["System Design"]},{"location":"system-design/product/messaging-service/","title":"Design a Messaging Service Like WhatsApp","text":"<p>Design WhatsApp | Hello Interview</p> <p> - Designing a Chat Application | ByteByteGo</p> <p> - Build a Simple Chat Application with Redis | ByteByteGo</p> <p> - How Discord Stores Trillions of Messages | ByteByteGo - How Discord Stores Trillions of Messages | Discord</p> <ul> <li>Designing Facebook Messenger | System Design Interview</li> </ul>","tags":["System Design"]},{"location":"system-design/product/ride-sharing-service/","title":"Design a Ride-Sharing Service Like Uber","text":"<ul> <li>Design a Ride-Sharing Service Like Uber | Hello Interview</li> </ul>","tags":["System Design"]},{"location":"system-design/product/ride-sharing-service/#requirements","title":"Requirements","text":"","tags":["System Design"]},{"location":"system-design/product/ride-sharing-service/#core-entites-apis","title":"Core Entites &amp; APIs","text":"","tags":["System Design"]},{"location":"system-design/product/ride-sharing-service/#system-design","title":"System Design","text":"","tags":["System Design"]},{"location":"system-design/product/ticket-booking-site/","title":"Design a Ticket Booking Site Like Ticketmaster","text":"<ul> <li>Design a Ticket Booking Site Like Ticketmaster | Hello Interview</li> <li>System Design for Ticketmaster | System Design School</li> </ul>","tags":["System Design"]},{"location":"system-design/product/ticket-booking-site/#requirements","title":"Requirements","text":"","tags":["System Design"]},{"location":"system-design/product/ticket-booking-site/#core-entities-apis","title":"Core Entities &amp; APIs","text":"","tags":["System Design"]},{"location":"system-design/product/ticket-booking-site/#system-design","title":"System Design","text":"","tags":["System Design"]},{"location":"system-design/product/ticket-booking-site/#questions","title":"Questions","text":"<p>What is Dual-write Problem?</p> Answer <p>The dual-write problem occurs when a system writes to two separate systems (e.g., a database and a message queue) in a non-atomic way. If one write succeeds and the other fails, the two systems become inconsistent, leading to data loss or duplication.</p> <p>For example, if a service writes to a database and then publishes a Kafka message, and the Kafka write fails, consumers might never know about the database change. Solving this requires atomicity across systems, often via distributed transactions or outbox patterns.</p> <p>How to Handle Dual-write Problem?</p> Answer <p>A common way to handle the dual-write problem is using the outbox pattern. Instead of writing to the database and message queue separately, the service writes both the main data and an event to the same database in a single transaction. A separate process (or CDC tool) then reads the outbox table and publishes the event to the message queue.</p> <p>Another approach is using transactional message queues that support atomic writes with the database, like using Kafka with transactional producers or a distributed transaction coordinator (e.g., XA). However, these are complex and often less preferred due to performance and operational overhead.</p>","tags":["System Design"]},{"location":"tech-notes/","title":"Tech Notes","text":"<p>This section contains my technical notes and learnings across various data engineering, streaming, and infrastructure technologies. Each topic represents hands-on experience, research, or study materials that I've compiled while working with these technologies.</p>"},{"location":"tech-notes/#what-youll-find-here","title":"What You'll Find Here","text":"<ul> <li>Data Processing Frameworks: Spark, Flink, and their practical applications</li> <li>Data Storage &amp; Formats: Iceberg, Hive, and modern table formats</li> <li>Streaming Technologies: Kafka, RisingWave, and stream processing concepts</li> <li>Orchestration Tools: Airflow, dbt, SQLMesh for workflow management</li> <li>Cloud &amp; Infrastructure: AWS services, Polaris catalog management</li> <li>System Design: DDIA concepts, latency optimization, and architecture patterns</li> </ul>"},{"location":"tech-notes/#how-to-use-these-notes","title":"How to Use These Notes","text":"<p>These notes are organized by technology stack and range from high-level concepts to detailed implementation guides. Many include practical examples, diagrams, and references to production use cases.</p> <p>Feel free to explore based on your current learning goals or project requirements.</p>"},{"location":"tech-notes/streaming-processing-window-types/","title":"Streaming Processing Window Types","text":"","tags":["Apache Spark","Apache Flink","Streaming Processing"]},{"location":"tech-notes/streaming-processing-window-types/#time","title":"time","text":"<ul> <li>Event time</li> <li>Arrival time</li> <li>Processing time</li> </ul>","tags":["Apache Spark","Apache Flink","Streaming Processing"]},{"location":"tech-notes/streaming-processing-window-types/#window-types","title":"Window Types","text":"<ul> <li>Tumbling Windows<ul> <li>Flink: A tumbling windows assigner assigns each element to a window of a specified window size. Tumbling windows have a fixed size and do not overlap.</li> <li>Spark: Tumbling windows are a series of fixed-sized, non-overlapping and contiguous time intervals. An input can only be bound to a single window.</li> </ul> </li> <li>Sliding Windows<ul> <li>Flink: The sliding windows assigner assigns elements to windows of fixed length. Similar to a tumbling windows assigner, the size of the windows is configured by the window size parameter. An additional window slide parameter controls how frequently a sliding window is started. Hence, sliding windows can be overlapping if the slide is smaller than the window size. In this case elements are assigned to multiple windows.</li> <li>Spark: Sliding windows are similar to the tumbling windows from the point of being \"fixed-sized\", but windows can overlap if the duration of slide is smaller than the duration of window, and in this case an input can be bound to the multiple windows.</li> </ul> </li> <li>Session Windows<ul> <li>Flink: The session windows assigner groups elements by sessions of activity. Session windows do not overlap and do not have a fixed start and end time</li> <li>Spark: Session window has a dynamic size of the window length, depending on the inputs. A session window starts with an input, and expands itself if following input has been received within static or dynamic gap duration.</li> </ul> </li> <li>Global Windows</li> </ul>","tags":["Apache Spark","Apache Flink","Streaming Processing"]},{"location":"tech-notes/airflow/how-airflow-works/","title":"How Airflow Works?","text":"Scheduler <p>handles both triggering scheduled workflows, and submitting Tasks to the executor to run. The executor, is a configuration property of the scheduler, not a separate component and runs within the scheduler process. There are several executors available out of the box, and you can also write your own.</p> Dag Processor <p>parses DAG files and serializes them into the metadata database</p> Web Server <p>presents a handy user interface to inspect, trigger and debug the behaviour of dags and tasks.</p> Metadata Database <p>Airflow components use to store state of workflows and tasks.</p> Worker (Optional) <p>executes the tasks given to it by the scheduler.</p> Triggerer (Optional) <p>executes deferred tasks in an asyncio event loop</p> <p>Architecture Overview</p>","tags":["Apache Airflow"]},{"location":"tech-notes/claude-code/","title":"Claude Code","text":"<p>Claude Code: A Highly Agentic Coding Assistant</p> <ul> <li>Claude Code internal \u4e0d index \u4f60\u7684 codebases\uff0c\u5b83\u4f7f\u7528agentic search\u4f86\u5e6b\u52a9\u4f60\u627e\u5230\u76f8\u95dc\u7a0b\u5f0f\u78bc\u3002</li> <li>\u6709memory\u6a5f\u5236\uff0c\u53ef\u4ee5\u8a18\u4f4f\u4f60\u7684\u504f\u597d\u9084\u6709\u904e\u53bb\u5c0d\u8a71\u7684\u4e0a\u4e0b\u6587</li> <li>\u5df2\u7d93\u5167\u5efa\u4e86\u5f88\u591a\u597d\u7528\u7684\u5de5\u5177\uff0c\u50cf\u662f\u7db2\u8def\u641c\u5c0b\u3001\u641c\u5c0b\u6a94\u6848\u3001\u8b80\u5beb\u6a94\u6848\u3001\u57f7\u884c\u6307\u4ee4\u3001run sub agents\u7b49</li> </ul>","tags":["Claude Code"]},{"location":"tech-notes/dbt/","title":"dbt","text":""},{"location":"tech-notes/dbt/#domain-ownership","title":"Domain Ownership","text":""},{"location":"tech-notes/dbt/#pipelines","title":"Pipelines","text":""},{"location":"tech-notes/dbt/#concrete-example","title":"Concrete Example","text":""},{"location":"tech-notes/dbt/#environments","title":"Environments","text":""},{"location":"tech-notes/dbt/#enabling-business-friendly-access-and-optimizing-access-control-through-customized-dbt-project-configuration","title":"Enabling Business-Friendly Access and Optimizing Access Control Through Customized dbt Project Configuration","text":""},{"location":"tech-notes/dbt/#problem-statement","title":"Problem Statement","text":"<p>By default, dbt creates all datasets in a single GCP project. This behavior does not meet our requirements due to the following reasons:</p> <ol> <li>Insufficient IAM isolation: Managing all environments in one project increases the risk of permission misconfigurations.</li> <li>Resource conflicts: Other GCP services used in BigQuery may conflict across environments if they share the same project.</li> <li>Unfriendly table naming for downstream users: Final table names include technical prefixes like <code>fct</code> or <code>dim</code>, which may confuse non-technical users such as marketers or editors.</li> </ol> <p>To address this, we customize dbt's behavior to:</p> <ul> <li> Isolate each environment in its own GCP project.</li> <li> Assign datasets by domain in staging and production.</li> <li> Simplify final table names for easier consumption.</li> </ul>"},{"location":"tech-notes/dbt/#data-sink","title":"Data Sink","text":"<p>All raw data ingested through tools such as Airbyte, Fivetran, or custom data pipelines is initially landed in the <code>sink</code> project.</p> <p>Datasets in this project are source-oriented, organized by their origin. For example, <code>food_ga4</code>, <code>app_store</code>, <code>google_play</code> etc.</p> <p>This separation provides a clear boundary between raw source data and downstream transformations.</p>"},{"location":"tech-notes/dbt/#staging-environment","title":"Staging Environment","text":"<ul> <li>The GCP project used is the <code>stg</code> project.</li> <li>All models are deployed in datasets by domain (e.g., <code>news</code>, <code>food</code>, <code>fashion</code>, etc.)</li> <li>Model names retain their original names, such as <code>stg_*</code>, <code>int_*</code>, <code>fct_*</code>, <code>dim_*</code>.</li> <li>All models are configured to expire after a certain duration to reduce storage costs.</li> </ul>"},{"location":"tech-notes/dbt/#production-environment","title":"Production Environment","text":"<ul> <li>The GCP project used is the <code>prod</code> project.</li> <li>Final models (data products) are deployed in datasets by domain (e.g., <code>news</code>, <code>food</code>, <code>fashion</code>, etc.). All its upstream models are deployed in the <code>hidden</code> dataset in the same project in order to encapsulate the implementation details.</li> <li>Model names are simplified to aliases only, without technical prefixes, to make them more user-friendly for non-technical stakeholders (e.g., articles, app_installs, fb_summary_daily).</li> </ul>"},{"location":"tech-notes/dbt/#implementation-summary","title":"Implementation Summary","text":"<ul> <li>Customize macros<ul> <li>generate_database_name</li> <li>generate_schema_name</li> <li>generate_alias_name</li> </ul> </li> <li>Use <code>if else</code> condition with <code>target.name</code> in <code>dbt_project.yml</code> to dynamically configure settings based on the environment.     dbt_project.yml<pre><code>models:\ncustom_project_name:\n    +materialized: view\n    +hours_to_expiration: |\n    {%- if target.name == \"prod\" -%} \"{{ none }}\"\n    {%- else -%} 168\n    {%- endif -%}\n</code></pre></li> </ul>"},{"location":"tech-notes/dbt/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Partitioned tables</li> <li>Incremental model with <code>insert_overwrite</code> strategy: generates a <code>merge</code> statement that replaces entire partitions in the destination table.<ul> <li>Static partition <pre><code>{% set partitions_to_replace = [\n    'timestamp(current_date)',\n    'timestamp(date_sub(current_date, interval 1 day))'\n] %}\n\n{{\nconfig(\n    materialized = 'incremental',\n    incremental_strategy = 'insert_overwrite',\n    partition_by = {\n        'field': 'event_time',\n        'data_type': 'timestamp',\n        \"granularity\": \"day\"\n    },\n    partitions = partitions_to_replace\n)\n}}\n\nwith events as (\n\n    select * from {{ref('events')}}\n\n    {% if is_incremental() %}\n        -- recalculate yesterday + today\n        where timestamp_trunc(event_timestamp, day) in ({{ partitions_to_replace | join(',') }})\n    {% endif %}\n\n),\n\n... rest of model ...\n</code></pre></li> <li><code>on_schema_change</code>: <code>append_new_columns</code></li> </ul> </li> <li>Materialized model as <code>table</code> if the model is accessed multiple times in the downstream models, to avoid recomputing the same data repeatedly.</li> </ul>"},{"location":"tech-notes/dbt/#data-mesh","title":"Data Mesh","text":"<ul> <li>How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh</li> <li>Data Mesh Principles and Logical Architecture</li> </ul> <p>4 Key Principles:</p> <p></p> <p>Domain-oriented Decentralized Data Ownership</p> <p></p> <ul> <li>From Medallion to DDD architecture </li> <li>CODEOWNERS</li> </ul> <p></p> <p></p> <p>Data Products</p> <p></p>"},{"location":"tech-notes/ddia/ch8-transactions/","title":"Ch8 transactions","text":"","tags":["DDIA"]},{"location":"tech-notes/ddia/ch8-transactions/#ch8-transactions","title":"Ch8 - Transactions","text":"","tags":["DDIA"]},{"location":"tech-notes/ddia/ch8-transactions/#distributed-transactions","title":"Distributed Transactions","text":"","tags":["DDIA"]},{"location":"tech-notes/flink/how-flink-works/","title":"How Flink Works","text":"Source: Flink Doc","tags":["Apache Flink"]},{"location":"tech-notes/flink/how-flink-works/#architecture-components","title":"Architecture Components","text":"Source: Flink Doc Anatomy of a Flink Cluster","tags":["Apache Flink"]},{"location":"tech-notes/flink/how-flink-works/#jobmanager","title":"JobManager","text":"<p>coordinating the distributed execution of Flink Applications</p> <ul> <li>ResourceManager</li> <li>Dispatcher</li> <li>JobMaster</li> </ul>","tags":["Apache Flink"]},{"location":"tech-notes/flink/how-flink-works/#taskmanager","title":"TaskManager","text":"<ul> <li>Task Slot</li> <li> <p>For distributed execution, Flink chains operator subtasks together into tasks. Chaining operators together into</p> </li> <li> <p>Each task is executed by one thread</p> </li> </ul>","tags":["Apache Flink"]},{"location":"tech-notes/flink/how-flink-works/#tasks-and-operators-chains","title":"Tasks and Operators chains","text":"<ul> <li>Flink chains operator subtasks together into tasks.</li> <li>Chaining operators together into tasks is a useful optimization: it reduces the overhead of thread-to-thread handover and buffering, and increases overall throughput while decreasing latency.</li> </ul>    Source: Flink Doc","tags":["Apache Flink"]},{"location":"tech-notes/flink/how-flink-works/#task-slots-and-resources","title":"Task Slots and Resources","text":"<ul> <li>Note that no CPU isolation happens here; currently slots only separate the managed memory of tasks.</li> <li>By adjusting the number of task slots, users can define how subtasks are isolated from each other. </li> <li>Having one slot per TaskManager means that each task group runs in a separate JVM (which can be started in a separate container, for example).</li> <li>Having multiple slots means more subtasks share the same JVM. Tasks in the same JVM share TCP connections (via multiplexing) and heartbeat messages. They may also share data sets and data structures, thus reducing the per-task overhead.</li> </ul>    Source: Flink Doc <ul> <li>By default, Flink allows subtasks to share slots even if they are subtasks of different tasks, so long as they are from the same job. </li> <li>2 benefits<ul> <li>A Flink cluster needs exactly as many task slots as the highest parallelism used in the job.</li> <li>It is easier to get better resource utilization.</li> </ul> </li> </ul>    Source: Flink Doc","tags":["Apache Flink"]},{"location":"tech-notes/flink/how-flink-works/#streaming-processing","title":"Streaming Processing","text":"Source: Flink Doc    Source: Flink Doc    Source: Flink Docs    Source: Flink Docs    Source: Flink Docs","tags":["Apache Flink"]},{"location":"tech-notes/flink/how-flink-works/#flink-application-execution","title":"Flink Application Execution","text":"<ul> <li>Flink Application Cluster</li> <li>Flink Session Cluster</li> </ul>","tags":["Apache Flink"]},{"location":"tech-notes/flink/how-flink-works/#apis","title":"APIs","text":"Flink APIs <p>From high to low level:</p> <ul> <li>SQL: High-level Language</li> <li>Table API: Declarative DSL</li> <li>DataStream API: Core API</li> <li>Stateful and timely stream processing: building blocks, \u900f\u904eDataStream API\u7684Process Function\u4f86\u4f7f\u7528</li> </ul>","tags":["Apache Flink"]},{"location":"tech-notes/hive/architecture/","title":"Hive Architecture","text":""},{"location":"tech-notes/iceberg/aws-s3-tables/","title":"Compaction - AWS EMR vs. AWS S3 Tables","text":"<p>Useful Information</p> <ul> <li>It takes EMR ~25 seconds of compute time to compact 1GB of data with an m5.xlarge instance. \u2192 \\(0.0017/GB** or **\\)0.17 for 100GB.</li> <li>Running Iceberg compaction with AWS EMR comes out to be ~3x cheaper than if you let AWS S3 Tables do it for you.</li> </ul> <p></p> <p>Cost of Compaction: AWS EMR vs. AWS S3 Tables</p> <p>Experiment Settings</p> <p>The writer produced a nominal 1\u202fGB / minute across 100 partitions, resulting in approximately 100 files / minute, each ranging between 7\u202fMB and 15\u202fMB.</p> <p></p> <p>Experiment Execution Details</p> <p></p> <p>the average file size over ~20h time period</p> <p>Cons</p> <ul> <li>Poor observability<ul> <li>AWS only provides a CLI command (S3 Tables maintenance job status) to retrieve the last compaction status</li> <li>Arbitrary until 3h delay cannot customize</li> <li>No built in monitoring available</li> </ul> </li> <li>Flawed Approach to Compaction<ul> <li>Only can choose target file size</li> <li>It does not recognize that ideal compaction configurations are specific to different types of readers and writers.</li> <li>Compaction not effective for low latency workloads</li> </ul> </li> </ul>","tags":["Apache Iceberg"]},{"location":"tech-notes/iceberg/disaster-recovery/","title":"Disaster Recovery with Apache Iceberg","text":"<ul> <li>The Catalog Is Missing or Points to the Wrong <code>metadata.json</code> \u2192 <code>register_table</code> to re-establish the link to your latest <code>metadata.json</code>.</li> <li>File Paths Have Changed During Restore \u2192 use <code>rewrite_table_path</code> to rewrite metadata and create a clean recovery copy</li> </ul>"},{"location":"tech-notes/iceberg/disaster-recovery/#best-practices-for-iceberg-disaster-recovery","title":"Best Practices for Iceberg Disaster Recovery","text":"<p>Implement proactive backup practices</p> <ul> <li>Always Backup Metadata and Data Together</li> <li>Track the Latest <code>metadata.json</code> in Every Backup</li> <li>Check for File Path Changes Before Recovery</li> <li>Automate Validation Post-Restore</li> <li>Dry-Run Your Recovery Plan</li> </ul>"},{"location":"tech-notes/iceberg/disaster-recovery/#references","title":"References","text":"<ul> <li>Disaster Recovery for Apache Iceberg Tables \u2013 Restoring from Backup and Getting Back Online</li> </ul>"},{"location":"tech-notes/iceberg/iceberg-best-practices-in-aws/","title":"Best Practices for Optimizing Apache Iceberg Workloads in AWS","text":"","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"tech-notes/iceberg/iceberg-best-practices-in-aws/#general-best-practices","title":"General Best Practices","text":"<p>Tip</p> <ul> <li> Use Iceberg format version 2.</li> <li> Use the AWS Glue Data Catalog as your data catalog.</li> <li> Use the AWS Glue Data Catalog as lock manager.</li> <li> Use Zstandard (ZSTD) compression</li> </ul>","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"tech-notes/iceberg/iceberg-best-practices-in-aws/#optimizing-storage","title":"Optimizing Storage","text":"<p>Tip</p> <ul> <li> Enable S3 Intelligent-Tiering</li> <li> Archive or delete historic snapshots<ul> <li>Delete old snapshots: <code>expire_snapshots</code></li> <li>Set retention policies for specific snapshots: use Historical Tags</li> <li>Archive old snapshots: S3 Tags + S3 Life cycle rules</li> </ul> </li> <li> Delete orphan files<ul> <li><code>remove_orphan_files</code></li> <li><code>VACUUM</code> statement: equals to <code>expire_snapshots</code> + <code>remove_orphan_files</code> in Spark.</li> </ul> </li> </ul>","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"tech-notes/iceberg/iceberg-best-practices-in-aws/#historical-tags","title":"Historical Tags","text":"<p>to mark specific snapshots and define a retention policy for them.</p> <pre><code>ALTER TABLE glue_catalog.db.table\nCREATE TAG 'EOM-01' AS OF VERSION 30 RETAIN 365 DAYS\n</code></pre> <p></p> <p>Historical Snapshot Tag</p>","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"tech-notes/iceberg/iceberg-best-practices-in-aws/#s3-tags","title":"S3 Tags","text":"<pre><code>spark.sql.catalog.my_catalog.s3.write.tags.my_key1=my_val1\nspark.sql.catalog.my_catalog.s3.delete-enabled=false\nspark.sql.catalog.my_catalog.s3.delete.tags.my_key=to_archive\nspark.sql.catalog.my_catalog.s3.write.table-tag-enabled=true\nspark.sql.catalog.my_catalog.s3.write.namespace-tag-enabled=true\n</code></pre>","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"tech-notes/iceberg/iceberg-best-practices-in-aws/#vacuum-statement","title":"<code>VACUUM</code> statement","text":"<pre><code>CREATE TABLE my_table (\n    ...\n)\nTBLPROPERTIES (\n    'vacuum_max_snapshot_age_seconds' = '432000', -- 5 days\n    'vacuum_min_snapshots_to_keep' = '1',\n    'vacuum_max_metadata_files_to_keep' = '100'\n);\n</code></pre> <pre><code>VACUUM glue_catalog.db.my_table\n</code></pre>","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"tech-notes/iceberg/iceberg-best-practices-in-aws/#optimizing-read-performance","title":"Optimizing Read Performance","text":"<p>Tip</p> <ul> <li> Partitioning<ul> <li>Partition your data<ul> <li>Identify columns that are frequently used in queries</li> <li>Choose a low cardinality partition column to avoid creating an excessive number of partitions</li> </ul> </li> <li>Use Hidden Partitioning</li> <li>Use Partition Evolution</li> </ul> </li> <li> Tuning File Size<ul> <li>Set target file size and row group size: <code>write.target-file-size-bytes</code>, <code>write.parquet.row-group-size-bytes</code>, <code>write.distribution-mode</code></li> <li>Run regular compaction</li> </ul> </li> <li> Optimize Column Statistics<ul> <li><code>write.metadata.metrics.max-inferred-column-defaults</code>: <code>100</code></li> </ul> </li> <li> Choose the Right Update Strategy (CoR)<ul> <li><code>write.update.mode</code>, <code>write.delete.mode</code>, and <code>write.merge.mode</code> can be set at the table level or independently on the application side.</li> </ul> </li> <li> Use ZSTD Compression<ul> <li><code>write.&lt;file_type&gt;.compression-codec</code></li> </ul> </li> <li> Set the Sort Order<ul> <li><code>ALTER TABLE ... WRITE ORDERED BY</code></li> <li><code>ALTER TABLE ... WRITE DISTRIBUTED BY PARTITION</code></li> <li><code>ALTER TABLE prod.db.sample WRITE DISTRIBUTED BY PARTITION LOCALLY ORDERED BY category, id</code></li> </ul> </li> </ul> <ul> <li>As a rule of thumb, \"too many partitions\" can be defined as a scenario where the data size in the majority of partitions is less than 2-5 times the value set by <code>target-file-size-bytes</code>.</li> </ul>","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"tech-notes/iceberg/iceberg-best-practices-in-aws/#optimizing-write-performance","title":"Optimizing Write Performance","text":"<p>Tip</p> <ul> <li> Set the Table Distribution Mode<ul> <li><code>write.distribution-mode</code>: <code>none</code>, <code>hash</code>, <code>range</code></li> <li>Spark Structured Streaming applications \u2192 set <code>write.distribution-mode</code> to <code>none</code>.</li> </ul> </li> <li> Choose the Right Update Strategy (MoR)</li> <li> Choose the Right File Format<ul> <li>Set <code>write-format</code> to Avro (row-based format) if write speed is important for your workload.</li> </ul> </li> </ul> <ul> <li>By default, Iceberg's compaction doesn't merge delete files unless you change the default of the <code>delete-file-threshold property</code> to a smaller value.</li> </ul>","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"tech-notes/iceberg/iceberg-best-practices-in-aws/#maintaining-tables-by-using-compaction","title":"Maintaining Tables by Using Compaction","text":"","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"tech-notes/iceberg/iceberg-best-practices-in-aws/#common-compaction-properties","title":"Common Compaction Properties","text":"Configuration Default Description <code>max-concurrent-file-group-rewrites</code> 5 Maximum number of file groups to be simultaneously rewritten <code>partial-progress.enabled</code> false Enable committing groups of files prior to the entire rewrite completing <code>partial-progress.max-commits</code> 10 Maximum amount of commits that this rewrite is allowed to produce if partial progress is enabled <code>rewrite-job-order</code> None Force the rewrite job order based on the value. Options are <code>bytes-asc</code>, <code>bytes-desc</code>, <code>files-asc</code>, and <code>files-desc</code> <code>max-file-group-size-bytes</code> 100GB Specifies the maximum amount of data that can be rewritten in a single file group <code>min-file-size-bytes</code> 75% of target file size Files under this threshold will be considered for rewriting regardless of any other criteria <code>max-file-size-bytes</code> 180% of target file size Files with sizes above this threshold will be considered for rewriting regardless of any other criteria <code>min-input-files</code> 5 Any file group exceeding this number of files will be rewritten regardless of other criteria <code>delete-file-threshold</code> 2147483647 Minimum number of deletes that needs to be associated with a data file for it to be considered for rewriting","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"tech-notes/iceberg/iceberg-best-practices-in-aws/#example","title":"Example","text":"<p>Partitions &amp; File Groups</p> <ul> <li>The Iceberg table consists of four partitions</li> <li>The Spark application creates a total of four file groups to process.</li> <li>A file group is an Iceberg abstraction that represents a collection of files that will be processed by a single Spark job. That is, the Spark application that runs compaction will create four Spark jobs to process the data.</li> <li>The partition labeled <code>month=01</code> includes two file groups because it exceeds the maximum size constraint of 100 GB.</li> <li>In contrast, the <code>month=02</code> partition contains a single file group because it's under 100 GB.</li> <li>The <code>month=03</code> partition doesn't satisfy the default minimum input file requirement of five files. As a result, it won't be compacted.</li> <li>Lastly, although the <code>month=04</code> partition doesn't contain enough data to form a single file of the desired size, the files will be compacted because the partition includes more than five small files.</li> </ul>","tags":["The Lakehouse Series","Apache Iceberg"]},{"location":"tech-notes/iceberg/iceberg-in-prod-netflix-2023/","title":"Apache Iceberg in Production: Insights from Netflix in 2023","text":"<p>AWS re:Invent 2023 - Netflix\u2019s journey to an Apache Iceberg\u2013only data lake</p>","tags":["The Lakehouse Series","Apache Iceberg","Netflix"]},{"location":"tech-notes/iceberg/migration/","title":"Migrate from Hive to Iceberg","text":"","tags":["The Lakehouse Series","Apache Iceberg","Apache Hive"]},{"location":"tech-notes/iceberg/migration/#migrating-existing-tables-to-apache-iceberg-aws","title":"Migrating existing tables to Apache Iceberg | AWS","text":"<ul> <li>In-place migration</li> <li> <p>Full data migration</p> </li> <li> <p>If your table file format is Parquet, ORC, or Avro, consider in-place migration</p> </li> <li> <p>For other formats such as CSV, JSON, and so on, use full data migration.</p> </li> <li> <p>If you want to evolve the table schema by using Iceberg native capabilities, consider in-place migration</p> </li> <li> <p>If you want to delete entire columns from data files, we recommend that you use full data migration.</p> </li> <li> <p>For large tables where it's prohibitively expensive to rewrite all the table partitions, consider using in-place migration and run compaction (with sorting enabled) for the most frequently accessed partitions.</p> </li> <li>Merging small files into larger files requires rewriting the dataset. In this case, consider using full data migration.</li> </ul>","tags":["The Lakehouse Series","Apache Iceberg","Apache Hive"]},{"location":"tech-notes/iceberg/migration/#insider","title":"Insider","text":"Summary (How We Migrated Our Data Lake to Apache Iceberg) <ul> <li>The company migrated their production data lake from Apache Hive to Apache Iceberg, achieving a 90% cost saving for Amazon S3.</li> <li>Two main strategies for migrating data tables are in-place migration and full migration.</li> <li>In-place migration keeps the data files as-is and adds Iceberg metadata, requiring stopping all write operations until completed to avoid syncing new data.</li> <li>Full migration allows data migration without downtime, transform the data while migrating, but is more costly due to running both systems in parallel and storing double the data during transition.</li> <li>The company chose full migration to switch to Parquet file format with z-std compression and alter the partitioning strategy.</li> <li>They migrated tables partially and parallelized the migration job using multiple Spark jobs to accelerate the process.</li> <li>A temporary pipeline was built using a Spark job to read data from the Hive table, apply transformations, and write into the Iceberg table.</li> <li>An existing Redis instance stored migrated and non-migrated partition metadata in a Set data structure to avoid redundant operations and costs.</li> <li>The Spark job accepts the table name as a parameter and migrates data until there are no partitions left to be migrated, popping partitions from the Redis set.</li> <li>The Spark job runs as a step in an Amazon EMR cluster which shuts down after completion; EMR Serverless can be used instead.</li> <li>Grafana was used to create a dashboard to monitor migration progress, fetching migrated partitions data from Redis.</li> <li>After confirming the migration's success, the Hive tables' data, RDS instance serving the Hive Metastore, and migration job and services were deleted.</li> </ul> Summary (Apache Iceberg Reduced Our Amazon S3 Cost by 90%) <ul> <li>The new generation data lake table formats (Apache Hudi, Apache Iceberg, and Delta Lake) enable cost-effective cloud solutions for big data analysis with ACID transactions, schema evolution, time travel, and more.</li> <li>Apache Hive was designed with HDFS in mind, which does not translate well to object storage systems like Amazon S3.</li> <li>Apache Hudi, Apache Iceberg, and Delta Lake are designed with the modern cloud infrastructure in mind to perform better and cost less on the cloud with petabyte-scale datasets.</li> <li>The main drawback of Apache Hive is the lack of grained metadata and relying on the directory listing the partitions with O(N) complexity for query planning.</li> <li>Apache Iceberg uses a snapshot approach and performs an O(1) RPC to read the snapshot file.</li> <li>Apache Iceberg tables can scale easily without worrying about the performance with increased partition count.</li> <li>At Insider, the migration of their Hive-backed data lake to Iceberg was motivated by cost-effectiveness.</li> <li>Insider ended up using Apache Parquet file format with Zstandard compression as the result of the benchmarks on their data.</li> <li>The reasons that cause Hive to have performance issues on the cloud also lead to extra costs.</li> <li>With Apache Iceberg, it is possible to configure the file size for a table with write.target-file-size-bytes parameter, which is 512MB by default.</li> <li>Querying a table reads much fewer objects, which reduces HeadObject and GetObject costs of Amazon S3 dramatically, which was around 90% in Insider's case after migrating to Iceberg.</li> <li>Insider saved around 20% of EC2 and EMR costs.</li> <li>Apache Iceberg also provides a wide set of table management features such as schema evolution, hidden partitioning, time travel, and more.</li> </ul>","tags":["The Lakehouse Series","Apache Iceberg","Apache Hive"]},{"location":"tech-notes/iceberg/migration/#natural-intelligence","title":"Natural Intelligence","text":"<p>Melting the ice \u2014 How Natural Intelligence simplified a data lake migration to Apache Iceberg</p> <ul> <li>Hive-to-Iceberg CDC: Automatically synchronize Hive tables with Iceberg using a custom change data capture (CDC) process to support existing consumers. (NI chose partition-level synchronization)</li> <li>Continuous schema synchronization: Automated schema sync processes compared Hive and Iceberg schemas, reconciling differences while maintaining type compatibility.</li> <li>Iceberg-to-Hive reverse CDC: To enable the data team to transition ETL jobs to write directly to Iceberg while maintaining compatibility with existing Hive-based processes not yet migrated, a reverse CDC from Iceberg to Hive was implemented</li> <li>Alias management in Snowflake</li> <li>Table replacement</li> </ul>","tags":["The Lakehouse Series","Apache Iceberg","Apache Hive"]},{"location":"tech-notes/iceberg/migration/#i-spent-2-hours-migrating-hive-tables-to-iceberg-on-aws-using-a-metadata-only-approach","title":"I Spent 2 Hours Migrating Hive Tables to Iceberg on AWS Using a Metadata-Only Approach","text":"Summary <ul> <li><code>add_files</code> approach</li> <li>Apache Iceberg brings ACID transactions, schema evolution, and efficient querying to data lakes.</li> <li>A metadata-only migration from Hive to Iceberg offers substantial benefits without data movement.</li> <li>In metadata-only migration, the underlying data files remain in their original S3 location.</li> <li>In metadata-only migration, only the table metadata changes from Hive format to Iceberg format.</li> <li>In metadata-only migration, the process is typically much faster and more efficient.</li> <li>Prerequisites: An AWS account with appropriate permissions, Existing Hive tables in AWS Glue Data Catalog, Apache Spark with Iceberg dependencies, Basic familiarity with SQL and PySpark.</li> <li>Create an empty Iceberg table that will serve as the migration target, matching the Hive table schema.</li> <li>Use Iceberg\u2019s add_files procedure to import the existing data files without moving them.</li> <li>The add_files procedure links existing Parquet files, creates Iceberg metadata, and allows Iceberg to take ownership of file management.</li> <li>Verify data accessibility after migration and test write operations.</li> <li>The add_files procedure can import files from specific partitions, doesn\u2019t create a new table, creates metadata without moving files, and doesn\u2019t verify schema compatibility.</li> <li>Once migration is complete, Iceberg takes ownership of the files, managing them through standard processes.</li> <li>Best practices: Ensure schema compatibility, match partitioning schemes, use Parquet files, test in non-production, and consider backups of Glue Data Catalog and table metadata.</li> <li>For large tables, consider a phased approach: Migrate one partition at a time, validate each partition before proceeding, and use Spark\u2019s parallelism.</li> <li>The metadata-only approach minimizes migration time, reduces costs, and maintains data availability.</li> </ul>","tags":["The Lakehouse Series","Apache Iceberg","Apache Hive"]},{"location":"tech-notes/iceberg/migration/#references","title":"References","text":"<ul> <li>Migrating a Hive Table to an Iceberg Table Hands-on Tutorial | Dremio Blog</li> <li>How to Migrate a Hive Table to an Iceberg Table | Dremio Blog</li> <li>Hive Table migration | Iceberg Doc</li> </ul>","tags":["The Lakehouse Series","Apache Iceberg","Apache Hive"]},{"location":"tech-notes/iceberg/resources/","title":"Iceberg Resources","text":""},{"location":"tech-notes/iceberg/resources/#turbocharging-efficiency-slashing-costs-mastering-spark-iceberg-joins-with-storage-partitioned","title":"Turbocharging Efficiency &amp; Slashing Costs: Mastering Spark &amp; Iceberg Joins with Storage-Partitioned","text":"<ul> <li>Optimizing data pipelines has become synonymous with cost savings in cloud computing.</li> <li>Apache Spark and new table formats like Iceberg, Delta, and Hudi are improving how large datasets are managed.</li> <li>Expedia Group\u2122 uses Spark and Iceberg to improve data processing workflows; storage-partitioned join (SPJ) is a feature that promises to greatly improve performance.</li> <li>Apache Iceberg is a high-performance table format for large analytics datasets that overcomes the limitations of Hive tables by providing features such as ACID transactions, schema evolution, partition evolution, and hidden partitioning.</li> <li>In distributed systems, non-broadcast joins are expensive operations due to data shuffling between nodes.</li> <li>Data shuffling involves significant network I/O and can drastically impact performance.</li> <li>Understanding Spark execution plans is essential for grasping performance.</li> <li>Sort-merge join is the default strategy for handling non-broadcasted joins in Spark.</li> <li>Iceberg sets write.spark.fanout.enabled to false by default, which forces a local sort before writing the data.</li> <li>Shuffle-hash join improves performance over sort-merge join by eliminating the CPU-intensive sorting steps.</li> <li>Storage-partitioned join (SPJ) removes 2 exchanges and 2 sorts.</li> <li>The storage-partitioned join is a new approach built on the concept of bucketed joins.</li> <li>Triggering SPJ requires the partition schema of the two joined Iceberg tables to be exactly the same and specific configurations to be applied.</li> <li>Scenarios tested are based on real-world data, with data size referring to deserialized data into memory.</li> <li>In Scenario 1, using a storage-partitioned join reduces the duration to just six minutes, with a cost of only $6.7, compared to 11 minutes and $12.2 for a sort-merge join.</li> <li>By combining three use cases, Expedia anticipates saving $5,000 for their next data pipeline.</li> <li>One of the most significant benefactors of the storage-partitioned join (SPJ) is the merge statement.</li> <li>SPJ is not applicable for non-equi-joins.</li> <li>Storage-partitioned joins, similar to the bucketed join, require both tables to follow the same partitioning schema, which can be challenging.</li> </ul> <ul> <li>How We Migrated Our Data Lake to Apache Iceberg | Insider Engineering | Oct. 2022</li> <li>Apache Iceberg Reduced Our Amazon S3 Cost by 90% | Insider Engineering | Sep. 2022</li> <li>Building a Feature Store with Apache Iceberg on AWS | Insider Engineering | Sep. 2023</li> </ul>"},{"location":"tech-notes/iceberg/definitive-guide/ch4-tuning/","title":"Ch4 Optimizing the Performance of Apache Iceberg","text":"<p>Disclaimer: The content on this page is created purely from personal study and self-learning. It is intended as personal notes and reflections, not as professional advice. There is no commercial purpose, financial gain, or endorsement intended. While I strive for accuracy, the information may contain errors or outdated details. Please use the content at your own discretion and verify with authoritative sources when needed.</p> <p>Tip</p> <ul> <li> Compaction<ul> <li>Binpack</li> <li>Sorting</li> <li>Z-order</li> </ul> </li> <li> Partitioning</li> <li> CoR vs. MoR</li> <li> Metric Collection</li> <li> Write Distribution Mode</li> <li> Datafile Bloom Filters</li> </ul>","tags":["Apache Iceberg"]},{"location":"tech-notes/iceberg/definitive-guide/ch4-tuning/#compaction","title":"Compaction","text":"<p>In this scenario, we may have been streaming some data into our <code>musicians</code> table and noticed that a lot of small files were generated for rock bands, so instead of running compaction on the whole table, which can be time-consuming, we targeted just the data that was problematic. We also tell Spark to prioritize file groups that are larger in bytes and to keep files that are around 1 GB each with each file group of around 10 GB.</p> <p>A file group is an Iceberg abstraction that represents a collection of files that will be processed by a single Spark job.</p> <pre><code>-- Rewrite Data Files CALL Procedure in SparkSQL\nCALL catalog.system.rewrite_data_files(\n  table =&gt; 'musicians',\n  strategy =&gt; 'binpack',\n  where =&gt; 'genre = \"rock\"',\n  options =&gt; map(\n    'rewrite-job-order','bytes-asc',\n    'target-file-size-bytes','1073741824', -- 1GB\n    'max-file-group-size-bytes','10737418240' -- 10GB\n  )\n)\n</code></pre> <p></p> <p>The result of having the max file group and max file size set to 10 GB and 1 GB, respectively</p>","tags":["Apache Iceberg"]},{"location":"tech-notes/iceberg/definitive-guide/ch4-tuning/#sorting","title":"Sorting","text":"<p>Sorted </p> <p>After creating the table, you set the sort order of the table, which any engine that supports the property will use to sort the data before writing and will also be the default sort field when using the sort compaction strategy</p> <pre><code>ALTER TABLE catalog.nfl_teams WRITE ORDERED BY team;\n</code></pre> <pre><code>CREATE TABLE catalog.nfl_teams \n    AS (SELECT * FROM non_iceberg_teams_table ORDER BY team);\n\nALTER TABLE catalog.nfl_teams WRITE ORDERED BY team;s\n</code></pre> <p>if you wanted to rewrite the entire dataset with all players sorted by team globally, you could run the following statement:</p> <pre><code>CALL catalog.system.rewrite_data_files(\n  table =&gt; 'nfl_teams',\n  strategy =&gt; 'sort',\n  sort_order =&gt; 'team ASC NULLS LAST'\n)\n</code></pre>","tags":["Apache Iceberg"]},{"location":"tech-notes/iceberg/definitive-guide/ch4-tuning/#z-order","title":"Z Order","text":"<p>Z-order</p> <pre><code>CALL catalog.system.rewrite_data_files(\n  table =&gt; 'people',\n  strategy =&gt; 'sort',\n  sort_order =&gt; 'zorder(age,height)'\n)\n</code></pre> <p></p> <p>Z-ordering based on age and height</p>","tags":["Apache Iceberg"]},{"location":"tech-notes/iceberg/definitive-guide/ch4-tuning/#partitioning","title":"Partitioning","text":"","tags":["Apache Iceberg"]},{"location":"tech-notes/iceberg/definitive-guide/ch4-tuning/#cor-vs-mor","title":"CoR vs. MoR","text":"","tags":["Apache Iceberg"]},{"location":"tech-notes/iceberg/definitive-guide/ch4-tuning/#metrics-collection","title":"Metrics Collection","text":"","tags":["Apache Iceberg"]},{"location":"tech-notes/iceberg/definitive-guide/ch4-tuning/#write-distribution-mode","title":"Write Distribution Mode","text":"<p>Write distribution mode requires an understanding of how massively parallel processing (MPP) systems handle writing files.</p> <p>The write distribution is how the records to be written are distributed across these tasks. If no specific write distribution mode is set, data will be distributed arbitrarily. The first X number of records will go to the first task, the next X number to the next task, and so on.</p> <pre><code>ALTER TABLE catalog.MyTable SET TBLPROPERTIES (\n    'write.distribution-mode'='hash',\n    'write.delete.distribution-mode'='none',\n    'write.update.distribution-mode'='range',\n    'write.merge.distribution-mode'='hash',\n);\n</code></pre>","tags":["Apache Iceberg"]},{"location":"tech-notes/iceberg/definitive-guide/ch4-tuning/#datafile-bloom-filters","title":"Datafile Bloom Filters","text":"<p>A bloom filter is a way of knowing whether a value possibly exists in a dataset.</p> <p>Bloom filters are handy because they can help us avoid unnecessary data scans.</p> <p>You can enable the writing of bloom filters for a particular column in your Parquet files (this can also be done for ORC files) via your table properties:</p> <pre><code>ALTER TABLE catalog.MyTable SET TBLPROPERTIES (\n    'write.parquet.bloom-filter-enabled.column.col1'= true,\n    'write.parquet.bloom-filter-max-bytes'= 1048576\n);\n</code></pre> <p>Then engines querying your data may take advantage of these bloom filters to help make reading the datafiles even faster by skipping datafiles where bloom filters clearly indicate that the data you need doesn't exist.</p>","tags":["Apache Iceberg"]},{"location":"tech-notes/kafka/exactly-once/","title":"Exactly Once Semantics in Kafka","text":"","tags":["Apache Kafka"]},{"location":"tech-notes/kafka/exactly-once/#idempotent-producers","title":"Idempotent Producers","text":"","tags":["Apache Kafka"]},{"location":"tech-notes/kafka/exactly-once/#configuration","title":"Configuration","text":"<ul> <li>KRaft cluster with 3 brokers and 3 controllers, eliminating ZooKeeper dependency</li> <li>Replication factor of 3, ensuring each partition maintains three replicas for fault tolerance</li> <li>Minimum In-Sync Replicas (ISR) set to 2, requiring at least two replicas to acknowledge writes</li> <li>Acknowledgment level of <code>acks=all</code>, guaranteeing that producers wait for all in-sync replicas</li> <li>Destination topic configured with 2 partitions (A, B) for parallel processing<ul> <li>Partition A leadership: Broker 1</li> <li>Partition B leadership: Broker 2</li> </ul> </li> <li>Idempotent producers enabled (<code>enable.idempotence=true</code>)</li> </ul>","tags":["Apache Kafka"]},{"location":"tech-notes/kafka/exactly-once/#key-flow-steps","title":"Key Flow Steps","text":"<ol> <li><code>Producer 1</code> initializes and gets PID</li> <li><code>Producer 1</code> sends first batch to <code>Partition A</code></li> <li>Network jitter, it causes <code>Producer 1</code> to resend the same batch to <code>Partition A</code></li> <li><code>Broker 1</code> fails, triggering leader election for <code>Partition A</code></li> <li><code>Producer 1</code> sends second batch to new <code>Partition A</code> leader</li> <li><code>Producer 2</code> initializes with different PID</li> <li><code>Producer 1</code> sends first batch to <code>Partition B</code> (no fencing conflict)</li> </ol> <pre><code>sequenceDiagram\n    autonumber\n\n    participant P2 as Producer 2\n    participant P1 as Producer 1\n    participant B1 as Broker 1 &lt;br/&gt; (Partition A \u2192 L) &lt;br/&gt; (Partition B \u2192 F)\n    participant B2 as Broker 2 &lt;br/&gt; (Partition A \u2192 F) &lt;br/&gt; (Partition B \u2192 L)\n    participant B3 as Broker 3 &lt;br/&gt; (Partition A \u2192 F) &lt;br/&gt; (Partition B \u2192 F)\n    participant C as KRaft Controllers\n\n    Note over P1: Initialize\n    rect rgb(245,245,245)\n        P1-&gt;&gt;B1: Send InitProducerId() request\n        B1-&gt;&gt;C: Ask controllers to allocate PIDs\n        C--&gt;&gt;B1: Return a range of PIDs\n        B1--&gt;&gt;P1: PID (1025) + Epoch (0)\n    end\n\n    Note over P1: Send first batch (seq=0) to Partition A\n    rect rgb(245,245,245)\n        P1-&gt;&gt;B1: Produce(PID=1025, epoch=0, seq=0, batch)\n        B1-&gt;&gt;B1: Check if sequence number matches expected? YES &lt;br/&gt; Write batch to log\n        par Replicate\n            B1-&gt;&gt;B2: Replicate batch\n            B1-&gt;&gt;B3: Replicate batch\n        end\n        B2--&gt;&gt;B1: Replication completed\n        B1--&gt;&gt;P1: Acknowledged\n        B3--&gt;&gt;B1: Replication completed\n    end\n\n    Note over P1: Network jitter, resend same batch to Partition A\n    rect rgb(245,245,245)\n        P1-&gt;&gt;B1: Produce(PID=1025, epoch=0, seq=0, batch) (Retry)\n        B1-&gt;&gt;B1: Detect duplicate sequence and skip write\n        B1--&gt;&gt;P1: Acknowledged (duplicate skipped)\n    end\n\n    Note over B1: Leader Election (Partition A)\n    rect rgb(245,245,245)\n        B1-xB1: Down\n        C-&gt;&gt;B2: Promote to Leader\n        B2-&gt;&gt;B2: Recover from log (PID=1025, seq=0 already present)\n        B1-&gt;&gt;B1: Up\n    end\n\n    Note over P1: Send second batch (seq=1) to Partition A\n    rect rgb(245,245,245)\n        P1-&gt;&gt;B2: Produce(PID=1025, epoch=0, seq=1, batch)\n        B2-&gt;&gt;B2: Check if sequence number matches expected? YES &lt;br/&gt; Write batch to log\n        par Replicate\n            B2-&gt;&gt;B1: Replicate batch\n            B2-&gt;&gt;B3: Replicate batch\n        end\n        B1--&gt;&gt;B2: Replication completed\n        B2--&gt;&gt;P1: Acknowledged\n        B3--&gt;&gt;B2: Replication completed\n    end\n\n    Note over P2: Initialize\n    rect rgb(245,245,245)\n        P2-&gt;&gt;B1: Send InitProducerId() request\n        B1-&gt;&gt;C: Ask controllers to allocate PIDs\n        C--&gt;&gt;B1: Return a range of PIDs\n        B1--&gt;&gt;P2: PID (8591) + Epoch (0)\n    end\n\n    Note over P1: Send first batch (seq=0) to Partition B (not fenced by Producer 2)\n    rect rgb(245,245,245)\n        P1-&gt;&gt;B2: Produce(PID=1025, epoch=0, seq=0, batch)\n        B2-&gt;&gt;B2: Check if sequence number matches expected? YES &lt;br/&gt; Write batch to log\n        par Replicate\n            B2-&gt;&gt;B1: Replicate batch\n            B2-&gt;&gt;B3: Replicate batch\n        end\n        B1--&gt;&gt;B2: Replication completed\n      B2--&gt;&gt;P1: Acknowledged\n      B3--&gt;&gt;B2: Replication completed\n    end\n</code></pre>","tags":["Apache Kafka"]},{"location":"tech-notes/kafka/exactly-once/#transactional-producers","title":"Transactional Producers","text":"<p>Building upon the foundation of idempotent producers, Kafka's transactional producers provide the ultimate guarantee for exactly-once semantics by enabling atomic writes across multiple partitions. This advanced capability transforms Kafka from a reliable message broker into a distributed transaction system, ensuring that either all messages in a transaction are committed together, or none at all.</p> <p>To understand the inner workings of Transactional producers, I recommend the following video, which provides a clear and accessible explanation of how Transactional producers operate and their underlying processes.</p> <p>Apache Kafka\u00ae Transactions: Message Delivery and Exactly-Once Semantics (2022)</p> <p>After watching the video, you can also refer to the examples and explanations in this article. Having two different sources explaining the same concept will help deepen your understanding.</p> <p>Next, we will explain this concept using the following simplified example. This example includes the following key components:</p> <ul> <li>Source Topic: the origin of messages to be processed</li> <li>Data Processing Application: the core component that consumes, processes, and produces messages atomically<ul> <li>Source Consumer: reads messages from the source topic</li> <li>Transactional Producer: produces messages to the destination topic within a transaction</li> </ul> </li> <li>Destination Topic: the target topic for processed messages</li> <li>Destination Consumer: reads messages from the destination topic with <code>read-committed</code> isolation level</li> </ul> <p></p> <p>Transactional Producer Example</p> <p>Before diving into how this example works in practice, let's first examine the two important roles behind Transactional Producers: the Group Coordinator and Transaction Coordinator.</p> <p>Both Transaction Coordinators and Group Coordinators operate as specialized roles within existing Kafka brokers rather than as separate service processes, ensuring efficient resource utilization and simplified deployment. These coordinators rely on internal topics to maintain their state and coordinate operations across the cluster.</p>","tags":["Apache Kafka"]},{"location":"tech-notes/kafka/exactly-once/#group-coordinator","title":"Group Coordinator","text":"<p>The Group Coordinator manages all aspects of consumer group operations through the <code>__consumer_offsets</code> internal topic. This compacted topic serves as the persistent storage for consumer group offset information, enabling position tracking across the entire cluster. Consumer groups periodically commit their reading position (offset) for each partition to this topic, creating a durable record that enables consumers to resume from the correct position after restarts or rebalancing scenarios.</p> <p>The <code>__consumer_offsets</code> topic is partitioned using the consumer group ID as the partition key, calculated through <code>hash(group.id)</code>. This partitioning strategy ensures that all offset information for a specific consumer group is consistently stored within the same partition, providing locality and consistency for group-related operations. For example, a consumer group with ID \"analytics-group\" would always map to the same partition regardless of cluster topology changes.</p> <p>The Group Coordinator responsibility is automatically assigned to the broker that serves as the leader for the corresponding <code>__consumer_offsets</code> partition. This design eliminates the need for separate coordination infrastructure while ensuring that each group has a single authoritative coordinator. The coordinator is responsible for:</p> <ul> <li>managing consumer group membership through join and leave operations</li> <li>orchestrating partition assignment across group members</li> <li>handling offset commit and fetch operations</li> <li>coordinating the complex rebalancing process when group membership changes or partition assignments need redistribution</li> </ul>","tags":["Apache Kafka"]},{"location":"tech-notes/kafka/exactly-once/#transaction-coordinator","title":"Transaction Coordinator","text":"<p>The Transaction Coordinator orchestrates transactional operations through the <code>__transaction_state</code> internal topic, which serves as the cornerstone for Kafka's exactly-once semantics. This topic maintains comprehensive transaction metadata including transaction state, associated producers, and all involved partitions. The coordinator records critical commit and abort decisions in this topic, enabling atomic writes across multiple partitions and ensuring that transactions are either fully completed or completely rolled back.</p> <p>The <code>__transaction_state</code> topic uses the transactional ID as its partition key, implementing a <code>hash(transactional.id)</code> partitioning scheme. This approach ensures that all transaction state for a specific transactional producer is consistently managed by the same partition and coordinator. For instance, a transactional producer with ID \"order-processor\" would always be managed by the same Transaction Coordinator, providing consistency and enabling proper producer fencing mechanisms.</p> <p>Similar to Group Coordinators, the Transaction Coordinator role is automatically assigned to the broker that leads the corresponding <code>__transaction_state</code> partition. This coordinator is responsible for:</p> <ul> <li>managing transactional producer state including Producer ID (PID) and epoch tracking</li> <li>maintaining detailed records of transaction operations throughout their lifecycle</li> <li>generating control batches that mark transaction boundaries in partition logs</li> <li>orchestrating the sophisticated two-phase commit protocol across multiple partitions to ensure atomicity</li> </ul>","tags":["Apache Kafka"]},{"location":"tech-notes/kafka/exactly-once/#core-transaction-apis","title":"Core Transaction APIs","text":"<p>Now that we understand the two critical roles operating behind the scenes on brokers in the transaction mechanism, let's shift our focus to producers. Transactional Producers introduce a sophisticated API that orchestrates the entire transaction lifecycle through essential operations:</p> <ul> <li><code>initTransactions()</code>: Registers the producer with the transaction coordinator</li> <li><code>beginTransaction()</code>: Marks the start of a new transaction</li> <li><code>send()</code>: Publishes messages within the transaction context</li> <li><code>sendOffsetsToTransaction()</code>: Includes consumer offsets in the transaction</li> <li><code>commitTransaction()</code>: Atomically commits all transaction operations</li> <li><code>abortTransaction()</code>: Cancels the transaction and discards all changes</li> </ul>","tags":["Apache Kafka"]},{"location":"tech-notes/kafka/exactly-once/#configuration_1","title":"Configuration","text":"<p>Transactional Producer Example</p> <p>Our demonstration environment showcases a robust, production-ready setup that highlights the distributed nature of transactional processing:</p> <ul> <li>KRaft cluster with 3 brokers and 3 controllers, eliminating ZooKeeper dependency</li> <li>Replication factor of 3, ensuring each partition maintains three replicas for fault tolerance</li> <li>Minimum In-Sync Replicas (ISR) set to 2, requiring at least two replicas to acknowledge writes</li> <li>Acknowledgment level of <code>acks=all</code>, guaranteeing that producers wait for all in-sync replicas</li> <li>Destination topic configured with 2 partitions (A, B) for parallel processing<ul> <li>Partition A leadership: <code>Broker 1</code></li> <li>Partition B leadership: <code>Broker 2</code></li> </ul> </li> <li>Group Coordinator: <code>Broker 1</code></li> <li>Transaction Coordinator: <code>Broker 3</code></li> <li>Transactional identity: <code>transactional.id = my-txn-app</code></li> </ul> <p></p> <p>Kafka Cluster Breakdown</p>","tags":["Apache Kafka"]},{"location":"tech-notes/kafka/exactly-once/#key-flow-steps_1","title":"Key Flow Steps","text":"<p>The following sequence diagrams illustrate the complete transaction lifecycle, including normal operations, error scenarios, and producer fencing mechanisms.</p> <ol> <li>Initialization Phase: Producer configures <code>transactional.id</code> and calls <code>initTransactions()</code> to register with Transaction Coordinator, receiving PID and epoch</li> <li>Transaction Boundary: Producer calls <code>beginTransaction()</code> to mark the start of atomic operation scope</li> <li>Partition Registration: First <code>send()</code> to each partition triggers <code>AddPartitionsToTxn()</code> request to register partition in transaction state</li> <li>Message Production: Non-blocking <code>send()</code> operations write batches to multiple partitions with sequence numbers for idempotence</li> <li>Offset Integration: <code>sendOffsetsToTransaction()</code> includes consumer group offsets in the transaction for exactly-once consumption</li> <li>Two-Phase Commit: <ul> <li>Prepare Phase: Producer calls <code>commitTransaction()</code> or <code>abortTransaction()</code> to initiate coordinator-driven 2PC</li> <li>Commit Phase: Coordinator sends <code>WriteTxnMarker()</code> to all registered partitions in parallel to write control batches</li> </ul> </li> <li>Consumer Isolation: Read-committed consumers only see data from successfully committed transactions</li> <li>Producer Fencing: New producer with same <code>transactional.id</code> gets incremented epoch, invalidating previous producer instances</li> </ol> <pre><code>sequenceDiagram\n    autonumber\n    participant P2 as Producer 2&lt;br/&gt;(same transactional.id)\n    participant P1 as Producer 1&lt;br/&gt;(transactional.id=\"my-txn-app\")\n    participant B1 as Broker 1&lt;br/&gt;(Partition A \u2192 Leader)&lt;br/&gt;(Group Coordinator)\n    participant B2 as Broker 2&lt;br/&gt;(Partition B \u2192 Leader)\n    participant B3 as Broker 3&lt;br/&gt;(Transaction Coordinator)\n    participant C as Destination Consumer&lt;br/&gt;(isolation.level=read_committed)\n\n    rect rgb(245,245,245)\n        Note over P1: Initialize\n        P1-&gt;&gt;P1: Configure TRANSACTIONAL_ID_CONFIG=\"my-txn-app\"\n        P1-&gt;&gt;P1: initTransactions()\n        P1-&gt;&gt;B3: InitProducerId(transactional.id=\"my-txn-app\")\n        B3-&gt;&gt;B3: Transaction Coordinator registers or updates transactional ID\n        B3--&gt;&gt;P1: PID (1025) + Epoch (0)\n    end\n\n    rect rgb(245,245,245)\n        Note over P1: Begin Transaction\n        P1-&gt;&gt;P1: beginTransaction()\n    end\n\n    par NON-BLOCKING SENDS\n        rect rgb(245,245,245)\n            Note over P1: Send first batch to Partition A of destination topic (NON-BLOCKING)\n            P1-&gt;&gt;P1: send()\n            P1-&gt;&gt;B3: AddPartitionsToTxn()\n            B3-&gt;&gt;B3: Transaction Coordinator registers Partition A in transaction state topic\n            B3--&gt;&gt;P1: Partition A registered successfully\n\n            P1-&gt;&gt;B1: Produce(PID=1025, epoch=0, seq=0, batch)\n            B1-&gt;&gt;B1: Check if sequence number matches expected? YES &lt;br/&gt; Write batch to log\n            par Replicate\n                B1-&gt;&gt;B2: Replicate batch\n                B1-&gt;&gt;B3: Replicate batch\n            end\n            B2--&gt;&gt;B1: Replication completed\n            B1--&gt;&gt;P1: Acknowledge\n            B3--&gt;&gt;B1: Replication completed\n        end\n    and NON-BLOCKING SENDS\n        rect rgb(245,245,245)\n            Note over P1: Send first batch to Partition B of destination topic (NON-BLOCKING)\n            P1-&gt;&gt;P1: send()\n            P1-&gt;&gt;B3: AddPartitionsToTxn()\n            B3-&gt;&gt;B3: Transaction Coordinator registers Partition A in transaction state topic\n            B3--&gt;&gt;P1: Partition B registered successfully\n\n            P1-&gt;&gt;B2: Produce(PID=1025, epoch=0, seq=0, batch)\n            B2-&gt;&gt;B2: Check if sequence number matches expected? YES &lt;br/&gt; Write batch to log\n            par Replicate\n                B2-&gt;&gt;B1: Replicate batch\n                B2-&gt;&gt;B3: Replicate batch\n            end\n            B1--&gt;&gt;B2: Replication completed\n            B2--&gt;&gt;P1: Acknowledged\n            B3--&gt;&gt;B2: Replication completed\n        end\n    end\n\n    rect rgb(245,245,245)\n        Note over P1: Consumer Offset Integration\n        P1-&gt;&gt;P1: sendOffsetsToTransaction()\n        P1-&gt;&gt;B3: AddOffsetsToTxn()\n        B3-&gt;&gt;B3: Transaction Coordinator registers consumer group offsets in transaction state topic\n        B3--&gt;&gt;P1: Consumer offsets registered successfully\n        P1-&gt;&gt;B1: TxnOffsetCommit()\n        B1-&gt;&gt;B1: Group Coordinator commits consumer group offsets in consumer offsets topic\n        B1--&gt;&gt;P1: Offsets registered successfully\n    end\n\n    alt COMMIT\n        rect rgb(245,245,245)\n            Note over P1,B3: 2PC (COMMIT)\n            P1-&gt;&gt;P1: commitTransaction()\n            P1-&gt;&gt;B3: EndTxn(COMMIT)\n\n            par\n                B3-&gt;&gt;B1: WriteTxnMarker(COMMIT) for Partition A\n                B1-&gt;&gt;B1: Write COMMIT control batch to log\n                B1-&gt;&gt;B2: Replicate COMMIT control batch\n                B1-&gt;&gt;B3: Replicate COMMIT control batch\n                B2--&gt;&gt;B1: Replication completed\n                B1--&gt;&gt;B3: Acknowledged\n                B3--&gt;&gt;B1: Replication completed\n            and\n                B3-&gt;&gt;B2: WriteTxnMarker(COMMIT) for Partition B\n                B2-&gt;&gt;B2: Write COMMIT control batch to log\n                B2-&gt;&gt;B1: Replicate COMMIT control batch\n                B2-&gt;&gt;B3: Replicate COMMIT control batch\n                B1--&gt;&gt;B2: Replication completed\n                B2--&gt;&gt;B3: Acknowledged\n                B3--&gt;&gt;B2: Replication completed\n            and\n                B3-&gt;&gt;B1: WriteTxnMarker(COMMIT) for consumer offsets topic\n                B1-&gt;&gt;B1: Write COMMIT control batch to log\n                B1-&gt;&gt;B2: Replicate COMMIT control batch\n                B1-&gt;&gt;B3: Replicate COMMIT control batch\n                B2--&gt;&gt;B1: Replication completed\n                B1--&gt;&gt;B3: Acknowledged\n                B3--&gt;&gt;B1: Replication completed\n            end\n\n            B3-&gt;&gt;B3: Transaction Coordinator updates transaction state to COMMITTED\n            B3--&gt;&gt;P1: Transaction COMMITTED successfully\n        end\n\n\n    else ABORT\n        rect rgb(245,245,245)\n            Note over P1,B3: 2PC (ABORT)\n            P1-&gt;&gt;P1: abortTransaction()\n            P1-&gt;&gt;B3: EndTxn(ABORT)\n\n            par\n                B3-&gt;&gt;B1: WriteTxnMarker(ABORT) for Partition A\n                B1-&gt;&gt;B1: Write ABORT control batch to log\n                B1-&gt;&gt;B2: Replicate ABORT control batch\n                B1-&gt;&gt;B3: Replicate ABORT control batch\n                B2--&gt;&gt;B1: Replication completed\n                B1--&gt;&gt;B3: Acknowledged\n                B3--&gt;&gt;B1: Replication completed\n            and\n                B3-&gt;&gt;B2: WriteTxnMarker(ABORT) for Partition B\n                B2-&gt;&gt;B2: Write ABORT control batch to log\n                B2-&gt;&gt;B1: Replicate ABORT control batch\n                B2-&gt;&gt;B3: Replicate ABORT control batch\n                B1--&gt;&gt;B2: Replication completed\n                B2--&gt;&gt;B3: Acknowledged\n                B3--&gt;&gt;B2: Replication completed\n            and\n                B3-&gt;&gt;B1: WriteTxnMarker(COMMIT) for consumer offsets topic\n                B1-&gt;&gt;B1: Write COMMIT control batch to log\n                B1-&gt;&gt;B2: Replicate COMMIT control batch\n                B1-&gt;&gt;B3: Replicate COMMIT control batch\n                B2--&gt;&gt;B1: Replication completed\n                B1--&gt;&gt;B3: Acknowledged\n                B3--&gt;&gt;B1: Replication completed\n            end\n\n            B3-&gt;&gt;B3: Transaction Coordinator updates transaction state to ABORTED\n            B3--&gt;&gt;P1: Transaction ABORTED successfully\n        end\n    end\n\n\n\n    rect rgb(245,245,245)\n        Note over C,B2: Destination Consumer Skips Aborted Data and Reads Committed Data\n        C-&gt;&gt;B1: Fetch from Partition A\n        C-&gt;&gt;B2: Fetch from Partition B\n        B1--&gt;&gt;C: Return only COMMITTED data (skip uncommitted/aborted)\n        B2--&gt;&gt;C: Return only COMMITTED data (skip uncommitted/aborted)\n    end\n\n    rect rgb(245,245,245)\n        Note over P2: Initialize\n        P2-&gt;&gt;P2: Configure TRANSACTIONAL_ID_CONFIG=\"my-txn-app\"\n        P2-&gt;&gt;P2: initTransactions()\n        P2-&gt;&gt;B3: InitProducerId(transactional.id=\"my-txn-app\")\n        B3-&gt;&gt;B3: Transaction Coordinator registers or updates transactional ID\n        B3--&gt;&gt;P2: New identity: PID (1025) + Epoch (1)\n\n        Note over P1,B3: Old Producer (P1) fenced out&lt;br/&gt;Subsequent writes from Producer 1 are rejected with INVALID_PRODUCER_EPOCH\n    end</code></pre>","tags":["Apache Kafka"]},{"location":"tech-notes/kafka/exactly-once/#advanced-topics","title":"Advanced Topics","text":"<p>Kafka's transaction mechanism internally implements the Two-Phase Commit (2PC) protocol, a technique commonly used in distributed systems to ensure data consistency across multiple nodes. The following video provides an in-depth explanation of the two-phase commit concept and how it operates:</p> <p>Distributed Systems 7.1: Two-phase commit</p> <p>To understand the historical evolution of Kafka's transaction mechanism, you can refer to the following videos and KIPs list. These resources provide a comprehensive explanation of the journey from the initial design to subsequent improvements and optimizations.</p> <p>Transactions in Action: the Story of Exactly Once in Apache Kafka (2023)</p> EOS KIPs <ul> <li>KIP-98: Exactly Once Delivery and Transactional Messaging</li> <li>KIP-185: Make exactly once in order delivery per partition the default Producer setting</li> <li>KIP-360: Improve reliability of idempotent/transactional producer</li> <li>KIP-447: Producer scalability for exactly once semantics</li> <li>KIP-484: Expose metrics for group and transaction metadata loading duration</li> <li>KIP-618: Exactly-Once Support for Source Connectors</li> <li>KIP-691: Enhance Transactional Producer Exception Handling</li> <li>KIP-679: Producer will enable the strongest delivery guarantee by default</li> <li>KIP-854: Separate configuration for producer ID expiry</li> <li>KIP-890: Transactions Server-Side Defense<ul> <li>eliminates hanging transactions on ALL clients</li> <li>includes new client changes to strengthen EOS</li> </ul> </li> <li>KIP-936: Throttle number of active PIDs</li> <li>KIP-939: Support Participation in 2PC</li> </ul>","tags":["Apache Kafka"]},{"location":"tech-notes/kafka/exactly-once/#faqs","title":"FAQs","text":"Why PID Ranges Instead of Individual IDs? <p>The evolution from individual Producer ID allocation to range-based distribution represents a crucial scalability optimization introduced in KIP-730: Producer ID generation in KRaft mode.</p> <p>The Bottleneck Problem:</p> <p>If every new producer required a separate request to the controller for a unique ID, the controller would become a severe bottleneck, especially in high-throughput environments with frequent producer initialization.</p> <p>The Range-Based Solution:</p> <p>Kafka's modern approach distributes the allocation burden:</p> <ul> <li>Controllers allocate continuous ranges rather than individual IDs</li> <li>Example: Broker-1 receives range [1000, 1999] containing 1000 available IDs</li> <li>Each broker's local <code>ProducerIdManager</code> can allocate IDs within its assigned range</li> <li>New ranges are requested only when the current range is exhausted</li> </ul> <p>Benefits of Range-Based Allocation:</p> <ul> <li>Dramatically reduces controller load (no per-producer coordination required)</li> <li>Distributes allocation pressure across all brokers</li> <li>Maintains PID uniqueness through non-overlapping ranges</li> <li>Enables horizontal scaling of producer initialization</li> </ul>","tags":["Apache Kafka"]},{"location":"tech-notes/kafka/exactly-once/#references","title":"References","text":"<ul> <li>PROTOCOL API KEYS<ul> <li>FindCoordinator</li> <li>InitProducerId</li> <li>AddPartitionsToTxn</li> <li>Produce</li> <li>AddOffsetsToTxn</li> <li>TxnOffsetCommit</li> <li>EndTxn</li> <li>WriteTxnMarker</li> </ul> </li> <li>Towards Debezium exactly-once delivery | Debezium Blog</li> <li>Exactly once delivery | Debezium Docs</li> <li>Transactions in Apache Kafka | Confluent Blog</li> <li>Exactly-once semantics with Kafka transactions | Strimzi</li> </ul>","tags":["Apache Kafka"]},{"location":"tech-notes/kafka/iceberg-kafka-sink-connector/","title":"Deep Dive into Kafka Connect Icerberg Sink Connector","text":"<p>Apache Iceberg provides a Kafka Connect Sink Connector that writes data from Kafka topics into Iceberg tables. This connector supports Exactly-Once Semantics (EOS), ensuring that each message is written to the target Iceberg table exactly once, preventing data duplication.</p> <p>The sink relies on KIP-447 for exactly-once semantics. This requires Kafka 2.5 or later.</p> <p>The Iceberg Kafka Connect Sink Connector documentation explicitly states that this connector relies on KIP-447. In order to understand how it achieves exactly-once semantics, we need to first examine KIP-447.</p>","tags":["Apache Kafka","Apache Iceberg"]},{"location":"tech-notes/kafka/iceberg-kafka-sink-connector/#kip-447-producer-scalability-for-exactly-once-semantics","title":"KIP-447: Producer scalability for exactly once semantics","text":"<p>Prior to KIP-447, the Transaction Coordinator was unaware that consumer group partition assignments could change dynamically through rebalancing. This resulted in an expectation that each transactional producer would be bound to a single source topic partition. Consequently, each source topic partition required its own dedicated producer instance with separate threads, caches, and network connections. This approach not only consumed excessive resources but also negatively impacted output batching efficiency, leading to poor overall scalability.</p> <p>KIP-447 aims to enable a single Transactional Producer to handle multiple source topic partitions simultaneously. The solution involves allowing the Transaction Coordinator to track consumer metadata, enabling it to determine whether zombie producers should be fenced, whether a producer can take over certain source partitions, and make reasonable decisions when partition assignments change. This eliminates the need for a static assignment architecture. A single producer instance can now handle offset commits for multiple source partitions and writes to multiple destination partitions, significantly reducing resource consumption while improving output throughput, scalability, efficiency, and resource utilization.</p> <p>After knowing the above, we can now look at how Iceberg Kafka Connect Sink Connector achieve exactly-once semantics.</p>","tags":["Apache Kafka","Apache Iceberg"]},{"location":"tech-notes/kafka/iceberg-kafka-sink-connector/#architecture-overview","title":"Architecture Overview","text":"<p>Kafka Connect Iceberg Sink | Iceberg Summit 2024</p> <p></p> <p>Intro to the Iceberg Kafka Connect sink (2023)</p> <p>Let's see how it works</p>","tags":["Apache Kafka","Apache Iceberg"]},{"location":"tech-notes/kafka/iceberg-kafka-sink-connector/#coordinator","title":"Coordinator","text":"<p>The Coordinator is the central command center that orchestrates commit operations across all distributed worker tasks in the Iceberg Kafka Connect Sink system. It acts like a conductor directing when and how data should be committed to Iceberg tables.</p> <p>The Coordinator's primary responsibilities include:</p> <ul> <li>initiating commit cycles by broadcasting <code>StartCommit</code> events</li> <li>collecting <code>DataWritten</code> and <code>DataComplete</code> responses from all Workers</li> <li>executing the actual Iceberg table commits</li> <li>managing offset tracking to ensure exactly-once semantics</li> </ul> <p>It maintains global commit state and coordinates the timing of all distributed operations.</p> <p>The Coordinator runs co-located with one of the Worker nodes and contains no data processing logic. It performs only three coordination tasks: checking commit intervals, listening for coordination events, and monitoring timeouts.</p>","tags":["Apache Kafka","Apache Iceberg"]},{"location":"tech-notes/kafka/iceberg-kafka-sink-connector/#workers","title":"Workers","text":"<p>Workers are the distributed data processing engines that handle the actual ingestion of Kafka records and transform them into Iceberg-compatible data files. Each Worker operates independently on its assigned topic partitions while coordinating with the central Coordinator.</p> <p>Workers' main responsibilities include</p> <ul> <li>consuming and buffering Kafka records through their <code>SinkWriter</code> component</li> <li>listening for <code>StartCommit</code> signals from the Coordinator</li> <li>completing write operations when instructed</li> <li>reporting back with <code>DataWritten</code> and <code>DataComplete</code> events.</li> </ul> <p>They bridge Kafka's streaming model with Iceberg's batch-oriented storage format.</p> <p>Workers use a composition pattern with <code>SinkWriter</code> components, where the <code>SinkWriter</code> handles pure data processing (record routing, file writing, offset tracking) while the Worker manages coordination aspects (control topic communication, event handling). Each Worker can handle multiple topic partitions simultaneously, enabling horizontal scalability.</p>","tags":["Apache Kafka","Apache Iceberg"]},{"location":"tech-notes/kafka/iceberg-kafka-sink-connector/#control-topic","title":"Control Topic","text":"<p>The Control Topic is the communication backbone that enables coordination between the Coordinator and all Workers in the distributed system. It functions as a dedicated Kafka topic that carries all coordination events and ensures reliable message delivery between components.</p> <p>The Control Topic's responsibilities include</p> <ul> <li>carrying five types of coordination events (<code>StartCommit</code>, <code>DataWritten</code>, <code>DataComplete</code>, <code>CommitToTable</code>, <code>CommitComplete</code>)</li> <li>maintaining event ordering through Producer ID-based partitioning</li> <li>providing persistent audit trails for system recovery.</li> </ul> <p>It implements a broadcast pattern where all participants listen to the same topic but process only relevant events. It also employs a Producer ID-based partitioning strategy. This partitioning strategy ensures that events from the same component (Coordinator or Worker) are assigned to the same partition, maintaining event ordering. For the coordination process, this ordering guarantee is crucial as it ensures that events sent by each component won't become out of order due to network delays or partition reassignments.</p> <p>The topic uses different consumer group strategies - persistent groups for the Coordinator and transient UUID-based groups for Workers. All interactions use Kafka's transactional operations to maintain exactly-once semantics, with Avro serialization ensuring cross-version compatibility and efficient transmission.</p>","tags":["Apache Kafka","Apache Iceberg"]},{"location":"tech-notes/kafka/iceberg-kafka-sink-connector/#how-commit-coordination-works","title":"How Commit Coordination Works","text":"","tags":["Apache Kafka","Apache Iceberg"]},{"location":"tech-notes/kafka/iceberg-kafka-sink-connector/#launching","title":"Launching","text":"<p>System initialization begins by performing leadership election, checking if the current worker has the first topic partition (lexicographically ordered). If elected leader, it creates a Coordinator instance that runs in a dedicated thread with no direct interaction with the co-located Worker on the same node. Worker initialization happens lazily during the first <code>save()</code> call, creating a <code>SinkWriter</code> for data writing and instantiating a Worker instance.</p> <p>The Coordinator and Worker establish their communication infrastructure by connecting to the control topic for coordination messages, setting up transactional producers with specific transaction IDs for exactly-once semantics, subscribing to the control topic with appropriate consumer groups, and performing an initial poll to initialize consumer group coordination.</p>","tags":["Apache Kafka","Apache Iceberg"]},{"location":"tech-notes/kafka/iceberg-kafka-sink-connector/#before-commit","title":"Before Commit","text":"<p>The Coordinator maintains commit state by tracking elapsed time since the last commit and monitoring whether the configured commit interval has been reached. It buffers incoming responses, storing data file information from workers and completion signals indicating workers have finished processing their assigned partitions.</p> <p>The Coordinator runs continuously in its dedicated thread, checking if the commit interval is reached and generating a new UUID to send <code>StartCommit</code> events when ready. It polls the control topic every second to receive worker responses and monitors for commit timeouts that would trigger partial commits.</p> <p>Workers accumulate records in their <code>SinkWriter</code> during this phase without immediate processing.</p>","tags":["Apache Kafka","Apache Iceberg"]},{"location":"tech-notes/kafka/iceberg-kafka-sink-connector/#committing","title":"Committing","text":"<p>The commit phase begins when the Coordinator broadcasts a <code>StartCommit</code> event to the control topic with a new commit UUID. Each Worker handles this by calling their <code>SinkWriter</code> to finalize accumulated data, creating partition offset assignments for all assigned partitions (including empty ones), and sending <code>DataWritten</code> events (one per table) containing data/delete files plus a single <code>DataComplete</code> event with all partition assignments.</p> <p>The Coordinator processes incoming worker responses by collecting data file information and completion signals. When all expected partitions have responded, it triggers the actual commit process which groups data files by table and executes parallel table commits across multiple threads.</p> <p>For each table, the Coordinator loads the table from the catalog, filters files based on previously committed offsets to avoid duplicates, deduplicates files by location, and chooses between append-only operations (for data-only scenarios) or row delta operations (when both data and delete files are present). It sets crucial snapshot properties including commit ID, control topic offsets in JSON format, and valid-through timestamp. All operations use transactional guarantees, sending records and committing offsets atomically.</p> <p>After each table commit succeeds, the Coordinator sends a <code>CommitToTable</code> event containing the commit ID, table reference, snapshot ID of the newly created snapshot, and valid-through timestamp. This event serves as a notification that the specific table has been successfully committed with its new snapshot.</p>","tags":["Apache Kafka","Apache Iceberg"]},{"location":"tech-notes/kafka/iceberg-kafka-sink-connector/#after-commit","title":"After Commit","text":"<p>Upon successful table commits, the Coordinator commits its control topic offsets, clears the accumulated commit state, and sends a <code>CommitComplete</code> event with the commit ID and valid-through timestamp. This notification informs all participants that the commit cycle completed successfully.</p> <p>The Coordinator performs state reset by clearing completion signals, resetting the current commit ID, and preparing for the next commit cycle. The system includes robust error handling where the Coordinator catches all exceptions during commits, logs warnings, and continues to the next cycle while always ensuring state cleanup. For timeout scenarios, partial commits are executed with a null valid-through timestamp to indicate degraded consistency guarantees while maintaining system operation.</p> <pre><code>sequenceDiagram\n    participant ST as Source Topic\n    participant W1 as Worker 1&lt;br/&gt;(Node 1)\n    participant W2 as Worker 2&lt;br/&gt;(Node 2)\n    participant CT as Control Topic&lt;br/&gt;(Kafka Cluster)\n    participant C as Coordinator&lt;br/&gt;(Node 2)\n    participant T1 as Iceberg Table 1\n    participant T2 as Iceberg Table 2\n\n    Note over ST,T2: Data Ingestion Phase\n    rect rgb(245, 245, 245)\n        ST-&gt;&gt;W1: SinkRecords (batch)\n        ST-&gt;&gt;W2: SinkRecords (batch)\n        W1-&gt;&gt;W1: Buffer records in SinkWriter\n        W2-&gt;&gt;W2: Buffer records in SinkWriter\n    end\n\n    Note over ST,T2: Commit Coordination Phase\n    rect rgb(245, 245, 245)\n        C-&gt;&gt;C: Check commit interval reached\n        C-&gt;&gt;CT: StartCommit event (UUID)\n        CT-&gt;&gt;W1: StartCommit event\n        CT-&gt;&gt;W2: StartCommit event\n    end\n\n    Note over ST,T2: Worker Processing Phase\n    rect rgb(245, 245, 245)\n        W1-&gt;&gt;W1: sinkWriter.completeWrite()\n        W2-&gt;&gt;W2: sinkWriter.completeWrite()\n\n        W1-&gt;&gt;CT: DataWritten event (Table 1)\n        W1-&gt;&gt;CT: DataWritten event (Table 2)\n        W1-&gt;&gt;CT: DataComplete event (all partitions)\n\n        W2-&gt;&gt;CT: DataWritten event (Table 1)\n        W2-&gt;&gt;CT: DataComplete event (all partitions)\n\n        CT-&gt;&gt;C: DataWritten events (all tables)\n        CT-&gt;&gt;C: DataComplete events (all workers)\n    end\n\n    Note over ST,T2: Table Commit Phase\n    rect rgb(245, 245, 245)\n        C-&gt;&gt;C: Group files by table, parallel commit\n        C-&gt;&gt;T1: Commit data files (AppendFiles/RowDelta)\n        C-&gt;&gt;T2: Commit data files (AppendFiles/RowDelta)\n\n        C-&gt;&gt;CT: CommitToTable event (Table 1, snapshot ID)\n        C-&gt;&gt;CT: CommitToTable event (Table 2, snapshot ID)\n    end\n\n    Note over ST,T2: Commit Completion Phase\n    rect rgb(245, 245, 245)\n        C-&gt;&gt;C: Commit control topic offsets\n        C-&gt;&gt;CT: CommitComplete event (commit ID, valid-through timestamp)\n        CT-&gt;&gt;W1: CommitComplete event\n        CT-&gt;&gt;W2: CommitComplete event\n\n        C-&gt;&gt;C: Reset commit state for next cycle\n    end</code></pre>","tags":["Apache Kafka","Apache Iceberg"]},{"location":"tech-notes/kafka/iceberg-kafka-sink-connector/#writing-data-behind-the-scenes","title":"Writing Data Behind the Scenes","text":"<p>The data writing journey begins when Workers receive batches of Kafka Connect <code>SinkRecords</code> from their assigned topic partitions. Each Worker immediately forwards these records to its <code>SinkWriter</code> component, which serves as the main orchestrator for transforming streaming Kafka data into Iceberg table files. The <code>SinkWriter</code> maintains a map of table names to <code>RecordWriter</code> instances and tracks source offsets for position management, setting up the foundation for the sophisticated data processing pipeline that follows.</p>","tags":["Apache Kafka","Apache Iceberg"]},{"location":"tech-notes/kafka/iceberg-kafka-sink-connector/#multi-table-routing","title":"Multi-Table Routing","text":"<p>Upon receiving records, the <code>SinkWriter</code> first determines which Iceberg tables should receive each record through its routing mechanism. The system supports both static and dynamic routing strategies to handle diverse data flow requirements.</p> <p>Static routing operates through the <code>routeRecordStatically()</code> method, which has two modes: when no route field is configured, it distributes each record to all configured tables in the connector settings. When a route field is specified, the system extracts the field value from each record and applies regex pattern matching against each configured table's regex pattern. Only tables whose patterns successfully match the route field value receive the record, enabling content-based selective routing.</p> <p>Dynamic routing uses the <code>routeRecordDynamically()</code> method, which extracts the route field value from each record and converts it to lowercase to use directly as the table name. This approach enables the system to create and write to tables dynamically based on actual data content, supporting use cases like multi-tenant applications where table names are determined by tenant identifiers embedded in the data.</p>","tags":["Apache Kafka","Apache Iceberg"]},{"location":"tech-notes/kafka/iceberg-kafka-sink-connector/#auto-table-creation","title":"Auto-Table Creation","text":"<p>When dynamic routing identifies a table that doesn't exist, or when static routing encounters missing tables with auto-creation enabled, the <code>IcebergWriterFactory</code> automatically creates the necessary Iceberg infrastructure through its <code>createWriter()</code> method. The factory first attempts to load the table from the catalog, and if it fails with a <code>NoSuchTableException</code>, it proceeds with auto-creation.</p> <p>The auto-creation process involves inferring the appropriate schema from sample records using type inference algorithms, creating partition specifications based on connector configuration, and establishing namespaces if they don't already exist.</p> <p>The factory handles race conditions gracefully through retry logic with exponential backoff, ensuring that multiple workers can operate concurrently without conflicts when attempting to create the same tables simultaneously. When a table cannot be found and auto-creation is disabled, the factory returns a <code>NoOpWriter</code> that silently discards records, allowing the system to continue operating without interruption.</p>","tags":["Apache Kafka","Apache Iceberg"]},{"location":"tech-notes/kafka/iceberg-kafka-sink-connector/#type-conversion-and-schema-evolution","title":"Type Conversion and Schema Evolution","text":"<p>As records flow to their designated tables, the <code>RecordConverter</code> performs comprehensive type mapping from Kafka Connect types to Iceberg types, handling primitives, complex nested structures like structs and maps, temporal data types, and specialized types such as UUIDs and decimals.</p> <p>Schema evolution is implemented in the <code>IcebergWriter.convertToRow()</code> method through a sophisticated monitoring system. When schema evolution is enabled, the system uses a <code>SchemaUpdate.Consumer</code> to track changes during record conversion. If the consumer detects changes (when <code>updates.empty()</code> returns false), the writer immediately flushes the current file via the <code>flush()</code> method, applies schema updates using <code>SchemaUtils.applySchemaUpdates()</code>, reinitializes the writer with the new schema through <code>initNewWriter()</code>, and reconverts the record using the updated schema. This ensures that schema changes are applied atomically without corrupting existing files, while maintaining backward compatibility and data integrity throughout the evolution process.</p>","tags":["Apache Kafka","Apache Iceberg"]},{"location":"tech-notes/kafka/iceberg-kafka-sink-connector/#multi-table-writing","title":"Multi-Table Writing","text":"<p>The system achieves multi-table writing through the <code>SinkWriter</code>'s coordination of multiple <code>RecordWriter</code> instances. Each table receives its own dedicated writer instance, either an <code>IcebergWriter</code> for valid tables or a <code>NoOpWriter</code> for missing tables when auto-creation is disabled. The <code>SinkWriter</code> maintains a map of table names to <code>RecordWriter</code> instances, allowing it to route each record to the appropriate writers based on the routing logic.</p> <p>When processing records, the system creates separate <code>IcebergWriterResult</code> objects for each table, containing metadata about the data files, delete files, and partition information specific to that table. This design enables true multi-table parallelism, where different tables can have different schemas, partition strategies, and file formats while sharing the same data stream.</p>","tags":["Apache Kafka","Apache Iceberg"]},{"location":"tech-notes/kafka/iceberg-kafka-sink-connector/#partitioned-writing","title":"Partitioned Writing","text":"<p>The writing process adapts to both partitioned and unpartitioned table configurations. For partitioned tables, the <code>PartitionedAppendWriter</code> determines the appropriate partition key for each record and routes it to the correct partition-specific writer, supporting various partition transforms including temporal partitioning by year, month, day, or hour, as well as bucket and truncate transforms for hash-based and range-based partitioning. Each partition maintains its own writer instance, allowing for efficient parallel writing while ensuring that records land in their correct partition locations within the table structure.</p>","tags":["Apache Kafka","Apache Iceberg"]},{"location":"tech-notes/kafka/iceberg-kafka-sink-connector/#file-completion-and-result-collection","title":"File Completion and Result Collection","text":"<p>When the commit cycle begins and <code>completeWrite()</code> is called, the <code>SinkWriter</code> coordinates the finalization of all accumulated data. It calls <code>complete()</code> on all active writers to flush remaining data to storage, collects metadata from all generated data files and delete files, captures the final Kafka offset positions for each processed partition, and returns a comprehensive <code>SinkWriterResult</code> containing both the file metadata and offset tracking information. This completes the transformation from streaming Kafka records to batch-oriented Iceberg files, with all the necessary metadata prepared for the distributed commit coordination process that follows.</p> <p>The <code>SinkWriterResult</code> serves as the bridge between data writing and coordination, containing a list of <code>IcebergWriterResult</code> objects (one per table) and a map of source offsets for each processed topic partition. Each <code>IcebergWriterResult</code> includes the table identifier, lists of data files and delete files generated during writing, and the partition type specification from the target table.</p> <p><code>DataWritten</code> and <code>DataComplete</code> Event Generation: When the Worker receives a <code>StartCommit</code> event, it triggers <code>sinkWriter.completeWrite()</code> and then generates coordination events from the results. For each <code>IcebergWriterResult</code> in the <code>SinkWriterResult</code>, the Worker sends a separate <code>DataWritten</code> event containing the commit ID, table reference, and file metadata (data files and delete files). After sending all table-specific <code>DataWritten</code> events, the Worker sends a single <code>DataComplete</code> event that includes the commit ID and <code>TopicPartitionOffset</code> assignments for all assigned partitions, signaling to the Coordinator that this Worker has finished processing all its data for the current commit cycle.</p>","tags":["Apache Kafka","Apache Iceberg"]},{"location":"tech-notes/latency/","title":"Latency","text":"<p>Latency in a distributed system</p>"},{"location":"tech-notes/polaris/policy/","title":"Apache Polaris Policy","text":"<p>Apache\u202fPolaris \u4e2d\u7684 policy\uff08\u653f\u7b56\uff09 \u662f\u7528\u4f86\u600e\u9ebc\u505a\u7684\uff1f\u7c21\u55ae\u800c\u8a00\uff0c\u5b83\u662f\u7528\u4f86\u7d71\u4e00\u5b9a\u7fa9\u3001\u7ba1\u7406\u548c\u57f7\u884c\u8cc7\u6599\u6cbb\u7406\u8207\u64cd\u4f5c\u898f\u5247\u7684\u7d50\u69cb\u5316\u8a2d\u5b9a\u3002\u4ee5\u4e0b\u4ee5\u53f0\u7063\u5e38\u7528\u7684\u7e41\u9ad4\u4e2d\u6587\u4f86\u6dfa\u767d\u8aaa\u660e\uff1a</p>"},{"location":"tech-notes/polaris/policy/#polaris-policy","title":"\ud83c\udf10 \u4ec0\u9ebc\u662f Polaris \u7684 Policy\uff1f","text":"<ul> <li>\u5728 Polaris \u4e2d\uff0cPolicy\uff08\u653f\u7b56\uff09 \u662f\u4e00\u7a2e\u300c\u7d50\u69cb\u5316\u5be6\u9ad4\u300d\uff08structured entity\uff09\uff0c\u7528\u4f86\u5b9a\u7fa9\u300c\u7576\u67d0\u4e9b\u689d\u4ef6\u7b26\u5408\u6642\u300d\u53ef\u4ee5\u5c0d\u54ea\u4e9b\u8cc7\u6e90\u505a\u54ea\u4e9b\u64cd\u4f5c\u7684\u898f\u5247\u3002\u5b83\u5305\u542b\u540d\u7a31\u3001\u985e\u578b\u3001\u8aaa\u660e\u3001\u5167\u5bb9\u8207\u7248\u672c\u7b49\u8cc7\u8a0a\uff0c\u53ef\u4ee5\u9644\u52a0\u5230 Catalog\u3001Namespace\u3001Table \u6216 View \u7b49\u8cc7\u6e90\u4e0a(polaris.apache.org)\u3002</li> </ul>"},{"location":"tech-notes/polaris/policy/#policy","title":"\ud83e\uddf0 Policy \u7684\u529f\u80fd\u8207\u7528\u9014","text":"<ol> <li> <p>\u96c6\u4e2d\u6cbb\u7406\u898f\u5247    Polaris \u63d0\u4f9b\u4e00\u65bd\u884c\u653f\u7b56\u7684\u4e2d\u5fc3\uff08policy store\uff09\uff0c\u53ef\u4ee5\u7d71\u4e00\u8a2d\u5b9a\u5982\u8cc7\u6599\u58d3\u7e2e\u3001\u5feb\u7167\u904e\u671f\u3001\u5b64\u5152\u6a94\u79fb\u9664\u7b49\u64cd\u4f5c\u898f\u5247\uff0c\u4e26\u5c07\u653f\u7b56\u4ee5 REST API \u7684\u65b9\u5f0f CRUD\uff08\u5efa\u7acb\uff0f\u8b80\u5beb\uff0f\u4fee\u6539\uff0f\u522a\u9664\uff09\u7ba1\u7406\u3002</p> </li> <li> <p>\u8cc7\u6e90\u7d1a\u5225\u653f\u7b56\u61c9\u7528\u8207\u7e7c\u627f    \u53ef\u4ee5\u5c07\u653f\u7b56\u5206\u5225\u639b\u4e0a\u81f3\u4e0d\u540c\u5c64\u7d1a\uff08Catalog \u2192 Namespace \u2192 Table \u6216 View\uff09\uff0c\u4e26\u652f\u63f4\u300c\u53ef\u7e7c\u627f\u300d\u8207\u5426\u7684\u8a2d\u5b9a\u3002\u5b50\u5c64\u6703\u7e7c\u627f\u81ea\u4e0a\u5c64\u653f\u7b56\uff0c\u82e5\u5728\u4e0b\u5c64\u5b9a\u7fa9\u4e86\u540c\u985e\u578b\u653f\u7b56\uff0c\u6703\u8986\u84cb\u4e0a\u5c64\u8a2d\u5b9a(polaris.apache.org)\u3002</p> </li> <li> <p>\u63d0\u5347\u8cc7\u6599\u5e73\u53f0\u64cd\u4f5c\u4e00\u81f4\u6027\u8207\u6548\u80fd    \u4f8b\u5982\uff0c\u900f\u904e <code>system.data-compaction</code> \u653f\u7b56\u81ea\u52d5\u8abf\u6574\u58d3\u7e2e\u7b56\u7565\u3001\u6216 <code>system.snapshot-expiry</code> \u8655\u7406\u904e\u6642\u5feb\u7167\u8cc7\u6599\uff0c\u8b93\u5e73\u53f0\u904b\u4f5c\u66f4\u6709\u898f\u5283\u3001\u66f4\u53ef\u63a7\u3002</p> </li> <li> <p>\u653f\u7b56\u7248\u672c\u7ba1\u7406\u8207\u6f14\u9032    Polaris \u7684\u653f\u7b56\u5167\u5bb9\u6703\u4f9d schema version \u81ea\u52d5\u8ffd\u8e64\u7248\u672c\uff0c\u652f\u6301\u8a2d\u5b9a\u4e0d\u540c\u7248\u672c\u7684\u898f\u5247\uff0c\u4e5f\u652f\u63f4\u653f\u7b56 schema \u6f14\u9032([snowflake.com][2])\u3002</p> </li> </ol>"},{"location":"tech-notes/polaris/policy/#table","title":"\ud83d\udca1 \u8209\u4f8b\uff1a\u4ee5\u653f\u7b56\u63a7\u5236 Table \u7684\u6e05\u7406\u8207\u58d3\u7e2e","text":"<p>\u5047\u8a2d\u8981\u5c0d\u67d0\u5f35 Iceberg table \u8a2d\u5b9a\u6bcf\u9031\u5feb\u7167\u904e\u671f\u70ba 7 \u5929\uff1a</p> <pre><code>{\n  \"name\": \"snapshot-expiry-policy\",\n  \"type\": \"system.snapshot-expiry\",\n  \"description\": \"\u904e\u671f\u5feb\u7167\u5b9a\u671f\u6e05\u7406\",\n  \"content\": \"{\\\"version\\\":\\\"2025\u201102\u201103\\\",\\\"enable\\\":true,\\\"config\\\":{\\\"snapshot_retention_days\\\":7}}\"\n}\n</code></pre> <p>\u4f60\u53ef\u4ee5\u628a\u5b83 attach \u5230\u67d0\u500b Namespace\uff0c\u82e5\u5141\u8a31\u7e7c\u627f\uff08inheritable\uff09\uff0c\u5176\u5e95\u4e0b\u6240\u6709 table \u6703\u81ea\u52d5\u5957\u7528\u9019\u500b\u5feb\u7167\u6e05\u7406\u653f\u7b56\u3002</p>"},{"location":"tech-notes/polaris/policy/#_1","title":"\u2705 \u7e3d\u7d50","text":"\u4e3b\u8981\u529f\u80fd \u8aaa\u660e \u7d71\u4e00\u6cbb\u7406\u653f\u7b56 \u4e00\u500b\u4e2d\u5fc3\u5b9a\u7fa9\u8207\u7ba1\u7406\u6240\u6709\u7b56\u7565 \u5404\u5c64\u7d1a\u5957\u7528\u8207\u7e7c\u627f Catalog \u2192 Namespace \u2192 Table \u7684\u7b56\u7565\u5b89\u6392 \u81ea\u52d5\u57f7\u884c\u65e5\u5e38\u64cd\u4f5c \u81ea\u52d5\u58d3\u7e2e\u3001\u5feb\u7167\u904e\u671f\u7b49\u7dad\u904b\u5de5\u4f5c \u652f\u63f4\u7248\u672c\u7ba1\u7406 \u653f\u7b56\u5167\u5bb9\u53ef\u96a8\u7248\u672c\u5347\u7d1a\u8207\u6f14\u9032 <p>\u900f\u904e Polaris \u7684 Policy \u6846\u67b6\uff0c\u4f01\u696d\u53ef\u4ee5\u4ee5\u7a0b\u5f0f\u5316\u65b9\u5f0f\u7ba1\u7406\u8cc7\u6599\u64cd\u4f5c\u6d41\u7a0b\u3001\u7c21\u5316\u7dad\u904b\u8907\u96dc\u5ea6\uff0c\u4e26\u78ba\u4fdd\u6cbb\u7406\u898f\u5247\u5728\u4e0d\u540c\u8cc7\u6e90\u5c64\u7d1a\u4e2d\u4e00\u81f4\u6027\u7684\u57f7\u884c\u3002</p> <p>\u5982\u4f60\u9700\u8981\u7279\u5b9a\u653f\u7b56\u985e\u578b\u3001\u8a9e\u6cd5\u7bc4\u4f8b\u6216 API \u64cd\u4f5c\u6d41\u7a0b\uff0c\u6211\u53ef\u4ee5\u518d\u63d0\u4f9b\u8a73\u7d30\u6307\u5f15\uff01</p>"},{"location":"tech-notes/polaris/resources/","title":"Apache Polaris Resources","text":"<ul> <li>External Identity Providers</li> <li>Policy Store</li> <li>HTTP Caching via eTAG</li> <li>JDBC-based persistence layer</li> <li>Smater Compaction Conflict Handling</li> <li>LoadTable Snapshot Filtering</li> <li>Quarkus Runtime</li> <li>Event Listener Framework</li> <li>Catalog Federation</li> </ul> <ul> <li>Create a table from Spark, then query the table from Snowflake</li> <li>Create an Iceberg table in Snowflake and push it into Polaris, then query the table from Spark</li> </ul> <p>Roadmap</p>"},{"location":"tech-notes/risingwave/","title":"RisingWave","text":"","tags":["RisingWave"]},{"location":"tech-notes/risingwave/#execution-modes","title":"Execution Modes","text":"<ul> <li>Streaming: RisingWave allows users to predefine SQL queries with <code>CREATE MATERIALIZED VIEW</code> statement. RisingWave continuously listens changes in upstream tables (in the <code>FROM</code> clause) and incrementally update the results automatically.</li> <li>Ad-hoc: Also like traditional databases, RisingWave allows users to send <code>SELECT</code> statement to query the result. At this point, RisingWave reads the data from the current snapshot, processes it, and returns the results.</li> </ul> <ul> <li>Overview of data processing | RisingWave Docs</li> </ul>","tags":["RisingWave"]},{"location":"tech-notes/risingwave/#risingwave-vs-apache-flink","title":"RisingWave vs. Apache Flink","text":"<p>For an easy on-ramp to real-time processing, RisingWave is an excellent choice. It offers a simple, cost-efficient, SQL-based solution that can be quickly deployed. This makes it ideal for data-driven businesses of any size that require real-time processing capabilities. Alternatively, if you require low-level API access that integrates seamlessly into your JVM-based technical stack, Apache Flink is the preferred option. Flink is well-suited for businesses with large teams that prefer building custom solutions tailored to their specific needs.</p> <p></p> <p></p> <p></p>","tags":["RisingWave"]},{"location":"tech-notes/risingwave/#architecture","title":"Architecture","text":"<ul> <li>Apache Flink adopts a big-data style, coupled-compute-storage architecture that is optimized for scalability; </li> <li>RisingWave in contrast implements a cloud-native, decoupled compute-storage architecture that is optimized for cost efficiency.</li> </ul> <p>RisingWave comprises several key components:</p> <ul> <li>Serving node: handles user requests and is designed to be compatible with the PostgreSQL wire protocol, allowing tools like psql to connect seamlessly</li> <li>Streaming node: executes streaming queries. This involves managing their state and performing computations such as aggregations and joins.</li> <li>Meta node: manages cluster metadata by interacting with a meta store, which serves as the persistence layer for metadata. RisingWave supports Postgres, MySQL, and SQLite as meta store options. </li> <li> <p>Compactor: RisingWave employs a Log Structured Merge (LSM) Tree storage model, meaning that all data operations are handled in an append-only manner, even deletions are represented as tombstone records.</p> </li> <li> <p>Architecture | RisingWave Docs</p> </li> </ul>","tags":["RisingWave"]},{"location":"tech-notes/spark/how-spark-works/","title":"How Spark Works","text":"<p>Cluster Overview</p>","tags":["Apache Spark"]},{"location":"tech-notes/spark/how-spark-works/#architecture-components","title":"Architecture Components","text":"","tags":["Apache Spark"]},{"location":"tech-notes/spark/how-spark-works/#driver-program","title":"Driver Program","text":"The process running the <code>main()</code> function of the application and creating the <code>SparkContext</code>","tags":["Apache Spark"]},{"location":"tech-notes/spark/how-spark-works/#cluster-manager","title":"Cluster Manager","text":"An external service for acquiring resources on the cluster (e.g. standalone manager, YARN, Kubernetes)","tags":["Apache Spark"]},{"location":"tech-notes/spark/how-spark-works/#worker-node","title":"Worker Node","text":"Any node that can run application code in the cluster","tags":["Apache Spark"]},{"location":"tech-notes/spark/how-spark-works/#executor","title":"Executor","text":"A process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them. Each application has its own executors. <p>See Cluster Mode Overview for more details on the architecture components.</p>","tags":["Apache Spark"]},{"location":"tech-notes/spark/how-spark-works/#kubeflow-spark-operator","title":"Kubeflow Spark Operator","text":"<p>When Spark 2.3 introduced Kubernetes as an official scheduler backend, it created challenges in managing Spark application lifecycles on Kubernetes. Unlike traditional workloads such as Deployments and StatefulSets, Spark applications required different approaches for submission, execution, and monitoring, making them difficult to manage idiomatically within Kubernetes environments.</p> <p>The Kubeflow Spark Operator addresses these challenges by implementing the operator pattern to manage Spark applications declaratively. It allows users to specify Spark applications in YAML files without dealing with complex <code>spark-submit</code> processes, while providing native Kubernetes-style status tracking and monitoring capabilities that align with other Kubernetes workloads.</p> <p></p> <p>Architecture diagram of the Kubeflow Spark Operator</p>","tags":["Apache Spark"]},{"location":"tech-notes/spark/how-spark-works/#spark-connect","title":"Spark Connect","text":"<p>Introduced in Spark 3.4, Spark Connect,  decoupled client-server architecture that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol</p> <p></p> <p>The Spark Connect API is a language-agnostic protocol</p> <p></p> <p>How Spark Connect Works</p>","tags":["Apache Spark"]},{"location":"tech-notes/spark/performance-tuning/","title":"Spark Performance Tuning","text":"<p>Spark Performance Tuning</p> <p>Tip</p> <ul> <li> Caching Data</li> <li> Tuning Partitions</li> <li> Leveraging Statistics</li> <li> Optimizing the Join Strategy</li> <li> Adaptive Query Execution<ul> <li> Coalescing Post Shuffle Partitions</li> <li> Splitting skewed shuffle partitions</li> <li> Converting Shuffle Sort Merge Join (SMJ) to Broadcast Hash Join (BHJ)</li> <li> Converting Shuffle Sort Merge Join to Shuffle Hash Join</li> <li> Optimizing Skew Join</li> <li> Advanced Customization</li> </ul> </li> <li> Storage Partition Join</li> </ul>","tags":["Apache Spark"]},{"location":"tech-notes/spark/learning-spark/ch1-2-intro-to-spark/","title":"Ch1 &amp; 2 Introduction to Spark","text":"<p>Disclaimer: The content on this page is created purely from personal study and self-learning. It is intended as personal notes and reflections, not as professional advice. There is no commercial purpose, financial gain, or endorsement intended. While I strive for accuracy, the information may contain errors or outdated details. Please use the content at your own discretion and verify with authoritative sources when needed.</p> <p></p> <p>Each executor's core gets a partition of data to work on</p> <p></p> <p>Each executor has 4 cores, each core gets a partition of data to work on</p> <p>A Spark application is converted by the driver into one or more jobs during execution, with each job then transformed into a DAG as an execution plan. Nodes in the DAG are divided into different stages based on whether operations can be executed serially or in parallel, with stage boundaries typically determined by computational boundaries that require data transfer between executors. Each stage contains multiple tasks that are distributed across executors for execution, where each task corresponds to a core and processes a data partition.</p> <p></p> <p>Spark Application DAG</p> <p>Spark transformations fall into two categories based on their dependency patterns. Narrow dependencies occur when each output partition depends on data from only one input partition, making operations like <code>filter()</code> and <code>contains()</code> efficient to execute.</p> <p>In contrast, wide dependencies require data shuffling across the cluster, as seen in operations like <code>groupBy()</code> or <code>orderBy()</code>, where multiple input partitions must be accessed to produce a single output partition, resulting in data being redistributed and persisted to storage.</p> <p></p> <p>Narrow vs. Wide Dependencies</p>","tags":["Apache Spark"]},{"location":"tech-notes/spark/learning-spark/ch3-structured-apis/","title":"Ch3 Structured APIs","text":"<p>Disclaimer: The content on this page is created purely from personal study and self-learning. It is intended as personal notes and reflections, not as professional advice. There is no commercial purpose, financial gain, or endorsement intended. While I strive for accuracy, the information may contain errors or outdated details. Please use the content at your own discretion and verify with authoritative sources when needed.</p> <p></p> <p>When errors are detected using the Structured APIs</p> <p>At the core of the Spark SQL engine are the Catalyst Optimizer and Project Tungsten. Together, these support the high-level DataFrame and Dataset APIs and SQL queries.</p> <p></p> <p>Spark SQL and its stack</p> <p>The Catalyst Optimizer takes a computational query and converts it into an execution plan. It goes through four transformational phases:</p> <ol> <li>Analysis<ul> <li>any columns or table names will be resolved by consulting an internal Catalog</li> </ul> </li> <li>Logical Optimization<ul> <li>Applying a standard-rule based optimization approach</li> <li>the Catalyst optimizer will first construct a set of multiple plans and then</li> <li>using its cost-based optimizer (CBO), assign costs to each plan.</li> <li>These plans are laid out as operator trees </li> </ul> </li> <li>Physical Planning</li> <li>Code Generation<ul> <li>The final phase of query optimization involves generating efficient Java bytecode to run on each machine. </li> </ul> </li> </ol> <p></p> <p>A Spark computation's four-phase journey</p> <p>Real Example:</p> <pre><code>// In Scala\n// Users DataFrame read from a Parquet table\nval usersDF  = ...\n// Events DataFrame read from a Parquet table\nval eventsDF = ...\n// Join two DataFrames\nval joinedDF = users\n  .join(events, users(\"id\") === events(\"uid\"))\n  .filter(events(\"date\") &gt; \"2015-01-01\")\n</code></pre> <p></p> <p>An example of a specific query transformation</p>","tags":["Apache Spark"]},{"location":"tech-notes/spark/learning-spark/ch7-tuning/","title":"Ch7 Optimizing and Tuning Spark Applications","text":"<p>Disclaimer: The content on this page is created purely from personal study and self-learning. It is intended as personal notes and reflections, not as professional advice. There is no commercial purpose, financial gain, or endorsement intended. While I strive for accuracy, the information may contain errors or outdated details. Please use the content at your own discretion and verify with authoritative sources when needed.</p> <p></p> <p>Executor Memory Layout</p> <p></p> <p>Broadcast Hash Join</p> <ul> <li>Optimizing and Tuning for Efficiency<ul> <li>Viewing and Setting Apache Spark Configurations<ul> <li>3 ways to specify Spark configurations<ul> <li>configuration files</li> <li>spark-submit conf</li> <li>programmatically interface via the spark shell</li> </ul> </li> </ul> </li> <li>Scaling Spark for Large Workloads<ul> <li> dynamic resouce allocation</li> <li> executors' memory</li> <li> executors' shuffle service</li> <li> maximizing spark parallelism<ul> <li><code>spark.sql.shuffle.partitions</code> 200 by default, reduce </li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Caching and Persistence of Data</p> <ul> <li>DataFrame.cache()</li> <li>DataFrame.persist()<ul> <li><code>MEMORY_ONLY</code></li> <li><code>MEMORY_ONLY_SER</code></li> <li><code>MEMORY_AND_DISK</code></li> <li><code>MEMORY_AND_DISK_SER</code></li> <li><code>DISK_ONLY</code></li> <li><code>OFF_HEAP</code></li> </ul> </li> <li>When to Cache and Persist<ul> <li>Iterative ML training</li> <li>DataFrames accessed commonly</li> </ul> </li> <li>When not to Cache and Persist<ul> <li>Too big to fit in memory</li> </ul> </li> </ul> </li> <li> <p>A Family of Spark Joins</p> <ul> <li>Broadcast Joins</li> <li>Shuffle Sort-Merge Joins<ul> <li>When each key within two large data sets can be sorted and hashed to the same partition by Spark</li> <li>When you want to perform only equi-joins to combine two data sets based on matching sorted keys</li> <li>When you want to prevent Exchange and Sort operations to save large shuffles across the network</li> </ul> </li> </ul> </li> <li> <p>Inspecting the Spark UI</p> </li> </ul> <p>Join Strategies</p>","tags":["Apache Spark"]},{"location":"tech-notes/sqlmesh/risingwave-integration/","title":"Risingwave integration","text":"<ul> <li>Streamlining Real-Time Pipelines: Managing RisingWave with SQLMesh | SQLMesh Blog</li> <li>Use SQLMesh to streamline pipelines</li> </ul>","tags":["SQLMesh","RisingWave"]},{"location":"tech-notes/sqlmesh/sqlmesh-vs-dbt-core/","title":"Comparisons: SQLMesh vs dbt Core","text":"<p>As dbt Labs announced that they are no longer maintain dbt core and move forward to dbt Fusion engine. Everyone started talking about sequel mash. Which is another. Analytics engineer engineering tools. Open source. </p> <p>And at the very beginning of. Second match launching. The CEO of Sidomesh, the founder of. Cyclone Mash. As always, roasted DBT for its functionalities. And how single match is better than DBT? And SQL, mash even support. Migrating from DBT projects to sqlmass project in a few clicks. Trying to attend DBT users to sqlmesh. </p> <p>So in this article I'm going to talk about. What are the features that Sigma? mesh? Has the DBT open source version doesn't have. And how you can benefit from them. </p> <ul> <li>Virtual Data Environments</li> <li>Multi-engine Support</li> <li>Multi-project / Multi-repo Support</li> <li>Column-level Lineage and Smart Recalculation</li> <li>Plan Mechanism and Automatic Backfill Calculation</li> <li>Semantic / Metrics Layer</li> <li>Local Debuggable Python Models</li> <li>Built-in Scheduler</li> </ul>","tags":["SQLMesh","dbt"]},{"location":"tech-notes/sqlmesh/sqlmesh-vs-dbt-core/#virtual-data-environments","title":"Virtual Data Environments","text":"<p>Virtual Data Environments \u662f SQLMesh \u7684\u6838\u5fc3\u5275\u65b0\uff0c\u900f\u904e\u5206\u96e2\u865b\u64ec\u5c64\u8207\u5be6\u9ad4\u5c64\u4f86\u89e3\u6c7a\u50b3\u7d71\u8cc7\u6599\u74b0\u5883\u7ba1\u7406\u7684\u554f\u984c\u3002\u6bcf\u7576\u6a21\u578b\u767c\u751f\u8b8a\u5316\u6642\uff0cSQLMesh \u6703\u81ea\u52d5\u5efa\u7acb\u65b0\u7684\u6a21\u578b\u5feb\u7167\uff08model snapshot\uff09\uff0c\u4e26\u70ba\u5176\u5206\u914d\u57fa\u65bc\u6307\u7d0b\u8b58\u5225\uff08fingerprint\uff09\u7684\u552f\u4e00\u8868\u683c\uff0c\u8b93\u4e0d\u540c\u7248\u672c\u7684\u540c\u4e00\u6a21\u578b\u53ef\u4ee5\u540c\u6642\u5b58\u5728\u800c\u4e0d\u4e92\u76f8\u885d\u7a81\u3002</p> <p>\u7cfb\u7d71\u63a1\u7528\u96d9\u5c64\u67b6\u69cb\uff1a\u5be6\u9ad4\u5c64\uff08physical layer\uff09\u5132\u5b58\u771f\u5be6\u7684table\u8207view\uff0c\u800c\u865b\u64ec\u5c64\uff08virtual layer\uff09\u5247\u900f\u904eview\u63d0\u4f9b\u9593\u63a5\u5b58\u53d6\u3002\u4e0b\u6e38\u6d88\u8cbb\u8005\u6c38\u9060\u4e0d\u76f4\u63a5\u5b58\u53d6\u5be6\u9ad4\u8cc7\u6599\u96c6\uff0c\u800c\u662f\u900f\u904e\u865b\u64ec\u5c64\u7684view\uff0c\u9019\u8b93 SQLMesh \u80fd\u5920\u5373\u6642\u66f4\u65b0view\u6307\u5411\u4e0d\u540c\u7684\u8cc7\u6599\u96c6\u7248\u672c\uff0c\u800c\u4e0d\u5f71\u97ff\u4f7f\u7528\u8005\u3002</p> <p>SQLMesh \u80fd\u5920\u81ea\u52d5\u5075\u6e2c\u8b8a\u66f4\u4e26\u9032\u884c\u5206\u985e\uff0c\u5c07\u76f4\u63a5\u4fee\u6539\u5206\u70ba\u300c\u7834\u58de\u6027\u300d\uff08\u5f71\u97ff\u4e0b\u6e38\u6a21\u578b\uff09\u6216\u300c\u975e\u7834\u58de\u6027\u300d\uff08\u50c5\u5f71\u97ff\u4fee\u6539\u7684\u6a21\u578b\u672c\u8eab\uff09\u8b8a\u66f4\u3002\u900f\u904e\u8a9e\u610f\u5c64\u7d1a\u7684 SQL \u5206\u6790\uff0c\u7cfb\u7d71\u53ef\u8a08\u7b97\u8a9e\u610f\u5dee\u7570\u4e26\u81ea\u52d5\u6e1b\u5c11\u4e0d\u5fc5\u8981\u7684\u91cd\u7b97\uff0c\u78ba\u4fdd\u6b63\u78ba\u6027\u7684\u540c\u6642\u9054\u5230\u6700\u5927\u6548\u7387\u3002</p> <p>\u5efa\u7acb\u65b0\u74b0\u5883\u50c5\u9700\u8981\u5efa\u7acb\u4e00\u7d44\u65b0\u7684\u6aa2\u8996\uff0c\u56e0\u6b64\u6210\u672c\u6975\u4f4e\u4e14\u5e7e\u4e4e\u5373\u6642\u3002\u65b0\u74b0\u5883\u7acb\u5373\u64c1\u6709\u4ee3\u8868\u6027\u8cc7\u6599\uff08\u91cd\u7528\u751f\u7522\u74b0\u5883\u8cc7\u6599\uff09\uff0c\u7121\u9700\u8907\u88fd\u8cc7\u6599\u6216\u624b\u52d5\u8a2d\u5b9a\u3002\u7576\u8b8a\u66f4\u90e8\u7f72\u5230\u751f\u7522\u74b0\u5883\u6642\uff0c\u4e5f\u662f\u900f\u904e\u865b\u64ec\u5c64\u64cd\u4f5c\uff0c\u78ba\u4fdd\u958b\u767c\u74b0\u5883\u89c0\u5bdf\u5230\u7684\u7d50\u679c\u8207\u751f\u7522\u74b0\u5883\u5b8c\u5168\u4e00\u81f4\u3002</p> <p>\u9019\u7a2e\u8a2d\u8a08\u5e36\u4f86\u986f\u8457\u512a\u52e2\uff1a\u74b0\u5883\u5efa\u7acb\u6210\u672c\u4f4e\u3001\u8cc7\u6599\u7acb\u5373\u53ef\u7528\u3001\u81ea\u52d5\u8b8a\u66f4\u5206\u985e\u3001\u5373\u6642\u56de\u6efe\u80fd\u529b\u3001\u4ee5\u53ca\u751f\u7522\u90e8\u7f72\u6642\u7684\u8cc7\u6599\u8207\u7a0b\u5f0f\u78bc\u540c\u6b65\u3002\u76f8\u6bd4 dbt \u9700\u8981\u91cd\u5efa\u6574\u5957\u958b\u767c schema \u7684\u65b9\u5f0f\uff0cVirtual Data Environments \u5927\u5e45\u964d\u4f4e\u904b\u7b97\u8207\u5132\u5b58\u6210\u672c\uff0c\u540c\u6642\u63d0\u4f9b\u66f4\u5b89\u5168\u53ef\u9760\u7684\u958b\u767c\u9ad4\u9a57\u3002</p> <p></p> Snapshots <p>A snapshot is a record of a model at a given time. Along with a copy of the model, a snapshot contains everything needed (a copy of all macro definitions and global variables at the time the snapshot is taken) to evaluate the model and render its query. This allows SQLMesh to have a consistent view of your project's history and its data as the project and its models evolve and change. Snapshots are generated automatically when a new <code>plan</code> is created and applied.</p> Fingerprinting <p>Snapshots have unique fingerprints that are derived from their models. SQLMesh uses these fingerprints to determine when existing tables can be reused, or whether a backfill is needed as a model's query has changed. No new fingerprint is generated when a model is modified only superficially, such as through query formatting.</p> <ul> <li>Virtual Data Environments | SQLMesh Blog</li> <li>SQLMesh and virtual data environments | Medium</li> <li>Environments | SQLMesh Docs </li> </ul>","tags":["SQLMesh","dbt"]},{"location":"tech-notes/sqlmesh/sqlmesh-vs-dbt-core/#multi-engine-support","title":"Multi-engine Support","text":"<p>\u540c\u4e00\u500b\u5c08\u6848\u4e2d\u4e0d\u540c\u6a21\u578b\u53ef\u4ee5\u4f7f\u7528\u4e0d\u540c\u67e5\u8a62\u5f15\u64ce\uff08\u5982\u540c\u6642\u7528 Spark\u3001DuckDB\u3001Snowflake \u7b49\uff09\uff0c\u964d\u4f4e vendor lock\u2011in\uff1bdbt Core \u4e00\u6b21\u53ea\u80fd\u5c0d\u63a5\u55ae\u4e00 adapter\u3002</p> <p>In a multi-engine project with a shared data catalog, the model-specific gateway is responsible for the physical layer, while the default gateway is used for managing the virtual layer.</p> <p>The data landscape continues to evolve, and one of the most significant paradigm shifts now taking place is the emergence of data lakes as the new architecture of choice for modern organizations. Recent developments, most notably AWS's integration with Apache Iceberg, underscore the growing momentum toward vendor neutrality and adoption of open standards.</p> <p>interoperatibility</p> <p>proliferration of data sources, types and applications</p> <p>one project, multiple engines</p> <p>So you can run individual models specific engines. </p> <p>how?</p> <ul> <li>the models you'll be running should reside in a shared data catalog, and</li> <li>for each engine you'll use, you must secure admin permissions to perform read/write operations on said shared catalog</li> </ul> <p></p> <pre><code>gateways:\n  duckdb:\n    connection:\n      type: duckdb\n      catalogs:\n        main_db:\n          type: postgres\n          path: 'dbname=main_db user=postgres host=127.0.0.1'\n      extensions:\n        - name: iceberg\n  postgres:\n    connection:\n      type: postgres\n      database: main_db\n      user: user\n      password: password\n      host: 127.0.0.1\n      port: 5432\ndefault_gateway: postgres\n</code></pre> <ul> <li>the PostgreSQL engine is set as the default gateway, so it will be used to manage views in your virtual layer.</li> <li>Meanwhile, DuckDB\u2019s <code>ATTACH</code> feature enables read-write access to the PostgreSQL catalog\u2019s physical tables.</li> <li>when a model\u2019s gateway is explicitly set to DuckDb, it will be materialized within the PostgreSQL main_db catalog, but it will be evaluated using DuckDB\u2019s engine.</li> </ul> <p></p> <pre><code>MODEL (\n  name orders.order_ship_date,\n  kind FULL,\n  gateway duckdb,\n);\n\nSELECT\n  l_orderkey,\n  l_shipdate\nFROM\n  iceberg_scan('data/bucket/lineitem_iceberg', allow_moved_paths = true);\n</code></pre> <p>Given this configuration, when a model\u2019s gateway is set to DuckDB, the DuckDB engine will perform the calculations before materializing the physical table in the PostgreSQL main_db catalog.</p> <p>In the <code>order_ship_date</code> model, the DuckDB engine is set, which will be used to create the physical table in the PostgreSQL database. This allows you to efficiently scan data from an Iceberg table, or even query tables directly from S3 when used with the HTTPFS extension.</p> <p>In this example, using both PostgreSQL and DuckDB in a single project offers the best of both worlds: PostgreSQL handles transactional workloads with its robust ACID compliance and relational capabilities, while DuckDB excels at fast, in-memory analytics on large datasets like CSV or Parquet files. Unlike PostgreSQL, which requires indexing to achieve optimal query performance, DuckDB will run complex analytical queries much faster without the need for such setup. This approach lets you leverage PostgreSQL\u2019s versatility for transactional operations and DuckDB\u2019s speed for analytics, applying the optimal engine to each model to streamline your workflow and maximize overall performance.</p> <p>a view in the default gateway can access a table in another gateway</p> <ul> <li>shared virtual layer</li> </ul> <p></p> <p>The gateways denote the execution engine, while both the virtual layer\u2019s views and the physical layer's tables reside in Postgres</p> <ul> <li>gateway-managed virtual layer</li> </ul> <p>If your project's engines don\u2019t have a mutually accessible catalog or your raw data is located in different engines, you may prefer for each model's virtual layer view to exist in the gateway that ran the model.</p> <pre><code>gateways:\n  redshift:\n    connection:\n      type: redshift\n      user: &lt;redshift_user&gt;\n      password: &lt;redshift_password&gt;\n      host: &lt;redshift_host&gt;\n      database: &lt;redshift_database&gt;\n    variables:\n      gw_var: 'redshift'\n  athena:\n    connection:\n      type: athena\n      aws_access_key_id: &lt;athena_aws_access_key_id&gt;\n      aws_secret_access_key: &lt;athena_aws_secret_access_key&gt;\n      s3_warehouse_location: &lt;athena_s3_warehouse_location&gt;\n    variables:\n      gw_var: 'athena'\n  snowflake:\n    connection:\n      type: snowflake\n      account: &lt;snowflake_account&gt;\n      user: &lt;snowflake_user&gt;\n      database: &lt;snowflake_database&gt;\n      warehouse: &lt;snowflake_warehouse&gt;\n    variables:\n      gw_var: 'snowflake'\n\ndefault_gateway: redshift\ngateway_managed_virtual_layer: true\n\nvariables:\n  gw_var: 'global'\n  global_var: 5\n</code></pre> <p></p> <ul> <li>SQLMesh sets a new precedent with support for multi-engine projects | SQLMesh Docs</li> <li>Multi-Engine guide | SQLMesh Docs</li> </ul>","tags":["SQLMesh","dbt"]},{"location":"tech-notes/sqlmesh/sqlmesh-vs-dbt-core/#multiproject-multirepo","title":"**Multi\u2011project / Multi\u2011repo","text":"<p>\u5b98\u65b9\u6587\u4ef6\u8207\u793e\u7fa4\u5747\u63d0\u5230\u9019\u662f\u64f4\u5927\u91cf\u4f7f\u7528\u8005\u8207\u5718\u968a\u5354\u4f5c\u7684\u95dc\u9375\u512a\u52e2\u3002 (sqlmesh.readthedocs.io, Listed)</p> <p>SQLMesh supports:</p> <ul> <li>SQLMesh repo + SQLMesh repo</li> <li>SQLMesh repo + dbt repo</li> <li>dbt repo + dbt repo</li> </ul> <pre><code>project: repo_1\n\ngateways:\n...\n</code></pre> <ul> <li>Multi-Repo guide</li> </ul>","tags":["SQLMesh","dbt"]},{"location":"tech-notes/sqlmesh/sqlmesh-vs-dbt-core/#built-in-scheduler","title":"Built-in Scheduler","text":"<p>SQLMesh \u672c\u8eab\u5c31\u63d0\u4f9b\u5167\u5efa\u7684 cron \u5f0f\u6392\u7a0b\u8207\u57f7\u884c\u8ffd\u8e64\uff0c\u4e0d\u5fc5\u518d\u984d\u5916\u90e8\u7f72 Airflow / Dagster \u7b49\u5916\u90e8\u7de8\u6392\u5668\uff1bdbt Core \u6c92\u6709\u5167\u5efa\u6392\u7a0b\u3002 (sqlmesh.readthedocs.io, Medium)</p>","tags":["SQLMesh","dbt"]},{"location":"tech-notes/sqlmesh/sqlmesh-vs-dbt-core/#python-models","title":"\u672c\u5730\u53ef\u9664\u932f\u7684 Python Models**","text":"<p>SQLMesh \u7684 Python model \u5728\u672c\u5730\u57f7\u884c\uff0c\u5c0d\u4efb\u4f55\u652f\u63f4\u7684\u8cc7\u6599\u5eab\u90fd\u9069\u7528\u4e14\u53ef\u8a2d breakpoint\uff1bdbt \u7684 Python models \u53d7\u9650\u65bc\u5177\u5b8c\u6574 Python runtime \u7684\u5c11\u6578\u5e73\u53f0\u3002 (sqlmesh.readthedocs.io)</p>","tags":["SQLMesh","dbt"]},{"location":"tech-notes/sqlmesh/sqlmesh-vs-dbt-core/#semantic-metrics-layer","title":"Semantic / Metrics Layer","text":"<p>\u53ef\u7528 <code>METRIC()</code> \u5b9a\u7fa9\u53ef\u91cd\u7528\u7684\u5546\u696d\u6307\u6a19\uff1b\u800c dbt Core \u672c\u8eab\u6c92\u6709\u5167\u5efa semantic layer\uff08\u76f8\u95dc\u80fd\u529b\u4e3b\u8981\u5728 dbt Cloud\uff09\u3002 (sqlmesh.readthedocs.io, Datacoves)</p>","tags":["SQLMesh","dbt"]},{"location":"tech-notes/sqlmesh/sqlmesh-vs-dbt-core/#plan-backfill","title":"Plan \u6a5f\u5236\u8207\u81ea\u52d5 Backfill \u8a08\u7b97","text":"<p><code>sqlmesh plan</code> \u6703\u6bd4\u5c0d\u672c\u5730\u8207\u76ee\u6a19\u74b0\u5883\u5dee\u7570\uff0c\u81ea\u52d5\u5224\u65b7\u54ea\u4e9b\u6a21\u578b\u3001\u54ea\u4e9b\u65e5\u671f\u5340\u9593\u9700\u8981\u56de\u586b\uff0c\u8b93\u4f60\u5728\u57f7\u884c\u524d\u5be9\u95b1\u6240\u6709\u8b8a\u66f4\u3002 (sqlmesh.readthedocs.io, sqlmesh.readthedocs.io)</p>","tags":["SQLMesh","dbt"]},{"location":"tech-notes/sqlmesh/sqlmesh-vs-dbt-core/#columnlevel-lineage","title":"Column\u2011level Lineage \u8207\u667a\u6167\u91cd\u7b97","text":"<p>SQLMesh \u5177\u5099\u6b04\u4f4d\u7b49\u7d1a\u8840\u7de3\u8207\u72c0\u614b\u5316\u57f7\u884c\uff0c\u53ea\u91cd\u5efa\u53d7\u5f71\u97ff\u7684\u4e0b\u6e38\u6a21\u578b\uff0c\u6e1b\u5c11\u4e0d\u5fc5\u8981\u7684\u8a08\u7b97\u8207\u7b49\u5f85\u6642\u9593\u3002 ([Reddit][1], tobikodata.com)</p>","tags":["SQLMesh","dbt"]},{"location":"tech-notes/sqlmesh/sqlmesh-vs-dbt-core/#support-wap-pattern-with-apache-iceberg","title":"Support WAP Pattern with Apache Iceberg","text":"<ul> <li>Introducing WAP Pattern Support with Apache Iceberg</li> </ul>","tags":["SQLMesh","dbt"]},{"location":"tech-notes/sqlmesh/sqlmesh-vs-dbt-core/#support-streaming-framworks","title":"Support Streaming Framworks","text":"<p>Streamlining Real-Time Pipelines: Managing RisingWave with SQLMesh</p> <p>Comparisons: SQLMesh vs. dbt Core</p> <p>[1]: SQLMesh versus dbt Core - Seems like a no-brainer</p>","tags":["SQLMesh","dbt"]},{"location":"tech-notes/treasure-data/","title":"Treasure Data","text":"<p>Treasure Data | NotebookLM</p>"},{"location":"tech-notes/treasure-data/#metrics","title":"Metrics","text":"<p>In 2023, Treasure Data currently serves over 700 accounts and more than 6,000 users. Since 2022, platform usage has doubled. Each day, the system handles 2.5 million+ Trino queries and over 100,000 Hive queries. It processes more than 200 trillion rows daily and performs over 10 billion S3 GET requests per day to read partition data from AWS S3.</p>"},{"location":"tech-notes/treasure-data/#overview","title":"Overview","text":"How Treasure Data reduced ad spend and increased marketing ROI by 20% Treasure Data Enterprise Customer Data Platform | Demo"},{"location":"tech-notes/treasure-data/#tech-stack","title":"Tech Stack","text":"<ul> <li>Wvlet, similar to BigQuery's pipe query syntax</li> <li>Trino</li> <li>Hive 4 (Hive 4.0.x comes with Iceberg 1.4.3 included.)</li> <li>Plazma (Closed Source)<ul> <li>PostgreSQL (metadata)</li> <li>S3<ul> <li>Real-time Storage</li> <li>Archive Storage</li> </ul> </li> </ul> </li> <li>fluentd</li> <li>embulk: Data Connector and Result Export</li> </ul> LinkedIn Post"},{"location":"tech-notes/treasure-data/#data-architecture","title":"Data Architecture","text":"Data Architecture 2021 (Source: Secure exchange SQL - Treasure Data at Trino Summit 2023)     Data Architecture 2023 (Source: Secure exchange SQL - Treasure Data at Trino Summit 2023)     Pros and Cons (Source: What is Treasure Data CDP?)  Zero-Copy Integrations Between Your CDP and Data Warehouse"},{"location":"tech-notes/treasure-data/#data-ingestion","title":"Data Ingestion","text":"<ul> <li>Google Analytics Data API Import Integration</li> </ul>"},{"location":"tech-notes/treasure-data/#questions","title":"Questions","text":"<ul> <li>Data Lakehouse Architecture? proprietary (MPC1)</li> <li>How to deal with dbt Core? (dbt Fusion)</li> <li>SQLMesh</li> </ul> <p>Amazon S3 Parquet Export Integration Amazon Elastic MapReduce Integration</p> <p></p> <p>https://www.linkedin.com/in/pramodmanjappa/</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/category/data/","title":"Data","text":""},{"location":"blog/category/mkdocs/","title":"MkDocs","text":""},{"location":"blog/category/cicd/","title":"CI/CD","text":""}]}